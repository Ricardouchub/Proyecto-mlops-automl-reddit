brand,comment_id,text,subreddit,created_utc,score
Intel,nmflglf,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",hardware,2025-10-31 20:39:13,30
Intel,nmdl0rr,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,hardware,2025-10-31 14:32:50,53
Intel,nmm72sv,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,hardware,2025-11-01 23:31:51,2
Intel,nmf7n07,They said their high idle power is an architecture issue so they can't fix that,hardware,2025-10-31 19:25:22,17
Intel,nmn2kpy,Fine wine,hardware,2025-11-02 02:36:42,1
Intel,nme3l5e,God forbid Intel supports Day 1 GPU drivers longer than 5 years,hardware,2025-10-31 16:03:57,-23
Intel,nmfdha7,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",hardware,2025-10-31 19:56:31,11
Intel,nmvlitt,So just like AMD then.,hardware,2025-11-03 13:41:12,1
Intel,nmfexoy,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",hardware,2025-10-31 20:04:15,26
Intel,nmhjwz4,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",hardware,2025-11-01 04:34:17,6
Intel,niqvrj6,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",hardware,2025-10-10 09:51:49,29
Intel,nis4lzk,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",hardware,2025-10-10 14:45:18,13
Intel,nisn7g8,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,hardware,2025-10-10 16:15:59,5
Intel,niz80tx,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,hardware,2025-10-11 18:17:30,1
Intel,njgcnqq,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",hardware,2025-10-14 14:31:48,1
Intel,njxoogl,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",hardware,2025-10-17 07:35:47,1
Intel,nir35ti,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",hardware,2025-10-10 10:58:36,19
Intel,nir6blp,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",hardware,2025-10-10 11:23:30,15
Intel,nisagox,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",hardware,2025-10-10 15:13:44,3
Intel,nis0as0,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,hardware,2025-10-10 14:23:44,2
Intel,njr6343,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",hardware,2025-10-16 06:29:51,1
Intel,nit88y1,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",hardware,2025-10-10 17:58:19,8
Intel,njxozxf,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,hardware,2025-10-17 07:39:00,1
Intel,niruidx,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,hardware,2025-10-10 13:53:56,5
Intel,nircpja,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,hardware,2025-10-10 12:08:58,30
Intel,nisb3yn,That's not what Peterson was talking about context wise when he addressed this in the video.,hardware,2025-10-10 15:16:53,2
Intel,nito8rq,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",hardware,2025-10-10 19:19:56,-3
Intel,nis0ezd,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),hardware,2025-10-10 14:24:18,16
Intel,nirfv6f,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",hardware,2025-10-10 12:29:32,17
Intel,niw1ezc,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",hardware,2025-10-11 04:09:31,5
Intel,njr69nx,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",hardware,2025-10-16 06:31:38,1
Intel,nisec0d,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",hardware,2025-10-10 15:32:43,1
Intel,nizcijr,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,hardware,2025-10-11 18:41:45,1
Intel,nitxcqr,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",hardware,2025-10-10 20:07:36,1
Intel,njgdemn,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,hardware,2025-10-14 14:35:38,1
Intel,nitzoza,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",hardware,2025-10-10 20:19:35,1
Intel,nj6xna9,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,hardware,2025-10-13 00:16:37,2
Intel,nj7rqtc,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",hardware,2025-10-13 03:27:42,1
Intel,nifgt95,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",hardware,2025-10-08 14:41:36,36
Intel,nighktm,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",hardware,2025-10-08 17:39:48,29
Intel,nihvtct,MLID must have an aneurysm seeing the guy still employed at Intel,hardware,2025-10-08 21:50:09,13
Intel,nifmhnd,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",hardware,2025-10-08 15:09:23,31
Intel,nihklk5,Igpus not discrete gpus,hardware,2025-10-08 20:51:31,11
Intel,nigirli,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",hardware,2025-10-08 17:45:26,11
Intel,nii5519,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,hardware,2025-10-08 22:44:00,9
Intel,nifuaux,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,hardware,2025-10-08 15:47:29,9
Intel,niis0eh,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,hardware,2025-10-09 01:01:50,4
Intel,nigp77c,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,hardware,2025-10-08 18:16:46,13
Intel,nigl085,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",hardware,2025-10-08 17:56:04,0
Intel,nilqzn1,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,hardware,2025-10-09 14:41:05,3
Intel,nig3u1g,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,hardware,2025-10-08 16:33:52,18
Intel,nijp5fy,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,hardware,2025-10-09 04:46:48,4
Intel,nigpqtx,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",hardware,2025-10-08 18:19:29,7
Intel,nihx213,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",hardware,2025-10-08 21:56:59,5
Intel,nigh9jb,You stole what I was going to say... take my upvote.,hardware,2025-10-08 17:38:17,5
Intel,nijrll5,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",hardware,2025-10-09 05:07:37,9
Intel,niixhey,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",hardware,2025-10-09 01:35:25,74
Intel,nij1t7l,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,hardware,2025-10-09 02:02:14,34
Intel,nijk1dw,that's insane vram density,hardware,2025-10-09 04:05:01,12
Intel,nijb3xq,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",hardware,2025-10-09 02:59:53,17
Intel,niiwt1u,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 01:31:12,2
Intel,nimo4dq,Is there a fork of chrome that runs on gpus,hardware,2025-10-09 17:24:22,2
Intel,nindfe4,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",hardware,2025-10-09 19:31:05,2
Intel,nikln6u,but is that faster than a single 5090?,hardware,2025-10-09 10:10:09,2
Intel,nik40r8,Is this enough VRAM for modern gaming?,hardware,2025-10-09 07:06:00,-3
Intel,nil0agd,Nvidia: ill commit s------e,hardware,2025-10-09 12:07:51,-8
Intel,nil8gj0,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",hardware,2025-10-09 12:59:34,18
Intel,nikh8oe,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",hardware,2025-10-09 09:25:55,-13
Intel,nij5zhc,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",hardware,2025-10-09 02:27:37,41
Intel,niml0xn,I don't think servers are supposed to stay idle for long.,hardware,2025-10-09 17:09:18,2
Intel,niqkqmc,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",hardware,2025-10-10 07:57:46,2
Intel,nijeto3,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,hardware,2025-10-09 03:25:25,43
Intel,nilc2tl,Could that make it very cost effective for any particular use cases?,hardware,2025-10-09 13:21:04,6
Intel,nim2kjx,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",hardware,2025-10-09 15:38:17,3
Intel,nij8lcp,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,hardware,2025-10-09 02:43:33,30
Intel,nijhbrl,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",hardware,2025-10-09 03:44:18,5
Intel,nilfpel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",hardware,2025-10-09 13:41:48,6
Intel,nin9mev,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",hardware,2025-10-09 19:11:33,3
Intel,nijt662,At least its a human hallucination and not AI hallucination.,hardware,2025-10-09 05:21:24,15
Intel,nik407y,Can also be bad translation.,hardware,2025-10-09 07:05:52,2
Intel,nijj2tk,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",hardware,2025-10-09 03:57:40,18
Intel,nilgnxj,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",hardware,2025-10-09 13:47:09,2
Intel,nili8e7,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",hardware,2025-10-09 13:55:46,5
Intel,niokcbz,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,hardware,2025-10-09 23:22:33,11
Intel,nilzeoj,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",hardware,2025-10-09 15:22:43,25
Intel,nime2oj,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",hardware,2025-10-09 16:35:12,19
Intel,nilf97b,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 13:39:16,1
Intel,nim7yeq,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,hardware,2025-10-09 16:04:46,-4
Intel,nilnsw1,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",hardware,2025-10-09 14:24:56,-22
Intel,nionxww,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,hardware,2025-10-09 23:43:45,4
Intel,nimlaz8,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",hardware,2025-10-09 17:10:41,4
Intel,nimhv2z,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,hardware,2025-10-09 16:53:51,5
Intel,nimkx2o,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",hardware,2025-10-09 17:08:47,21
Intel,nin877l,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,hardware,2025-10-09 19:04:21,8
Intel,nir42up,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,hardware,2025-10-10 11:05:57,4
Intel,nirvt3d,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,hardware,2025-10-10 14:00:42,2
Intel,nixqbsp,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",hardware,2025-10-11 13:25:20,1
Intel,nimkw5p,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,hardware,2025-10-09 17:08:39,7
Intel,nilrz1f,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",hardware,2025-10-09 14:45:58,21
Intel,nilwkhh,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",hardware,2025-10-09 15:08:45,9
Intel,nioq01t,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,hardware,2025-10-09 23:56:01,5
Intel,nirvexf,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",hardware,2025-10-10 13:58:40,3
Intel,nimlpyk,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",hardware,2025-10-09 17:12:42,3
Intel,nimm20x,Celestial was based on Xe3p.,hardware,2025-10-09 17:14:19,7
Intel,niokiba,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",hardware,2025-10-09 23:23:33,1
Intel,niy2dcz,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,hardware,2025-10-11 14:38:03,3
Intel,nimmzhl,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,hardware,2025-10-09 17:18:48,15
Intel,nirvxip,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",hardware,2025-10-10 14:01:21,3
Intel,nin3vrr,That's not what my colleague's say,hardware,2025-10-09 18:42:31,1
Intel,nilz2fy,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,hardware,2025-10-09 15:21:03,25
Intel,nim3npl,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",hardware,2025-10-09 15:43:35,-15
Intel,ninrarq,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",hardware,2025-10-09 20:40:49,-4
Intel,niy1tvt,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",hardware,2025-10-11 14:35:03,1
Intel,nj09dos,Xe3p is a significant architectural advancement says Tom Petersen.,hardware,2025-10-11 21:46:07,2
Intel,nim48wq,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",hardware,2025-10-09 15:46:27,-14
Intel,nim6va1,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",hardware,2025-10-09 15:59:21,7
Intel,nimks6h,"So much buzzwords, yet it sounds like a stroke.  You need help.",hardware,2025-10-09 17:08:07,5
Intel,nip4uap,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",hardware,2025-10-10 01:23:47,4
Intel,nim4t3o,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",hardware,2025-10-09 15:49:13,13
Intel,nino52j,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",hardware,2025-10-09 20:24:55,-2
Intel,nirwlsr,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,hardware,2025-10-10 14:04:53,1
Intel,ninor5p,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",hardware,2025-10-09 20:27:56,-7
Intel,ninre0y,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",hardware,2025-10-09 20:41:15,7
Intel,nixpud1,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,hardware,2025-10-11 13:22:13,1
Intel,ninu5gl,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",hardware,2025-10-09 20:55:03,-1
Intel,nio14ia,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",hardware,2025-10-09 21:31:45,7
Intel,nfxvgt2,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,hardware,2025-09-24 13:00:51,30
Intel,nfxs3mr,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",hardware,2025-09-24 12:41:08,34
Intel,nfxrqaw,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,hardware,2025-09-24 12:38:53,15
Intel,nfy52qu,I do have to wonder how much we are missing out by these features being proprietary rather than having graphics vendors work with other stake holders and each other to make open cross compatible upscaling and frame generation techniques. It's been great for nvidia but bad for the ecosystem as a whole for everything to be so fractured.,hardware,2025-09-24 13:53:35,12
Intel,nfz9psy,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",hardware,2025-09-24 17:11:03,7
Intel,nfyjkgh,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",hardware,2025-09-24 15:06:03,5
Intel,nfxz0x9,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,hardware,2025-09-24 13:20:54,2
Intel,nfz9315,we are witnessing the downfall of pc gaming in real time,hardware,2025-09-24 17:08:01,2
Intel,ng021ru,Why should they? They are going to buy NVidia GPUs for everything now.,hardware,2025-09-24 19:27:35,2
Intel,nfxotxk,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-24 12:20:56,1
Intel,nfxyif8,This could be awesome more completion The better,hardware,2025-09-24 13:18:01,1
Intel,ng09fp7,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,hardware,2025-09-24 20:03:42,0
Intel,nfxyqy3,"Multi-Post News Generation, with three articles interpolated per source.",hardware,2025-09-24 13:19:21,46
Intel,nfxswgf,I'm excited for Intels new GPU,hardware,2025-09-24 12:45:57,4
Intel,ng28c6g,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",hardware,2025-09-25 02:43:17,10
Intel,ng09kyr,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",hardware,2025-09-24 20:04:24,3
Intel,nfy71ww,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,hardware,2025-09-24 14:03:45,-7
Intel,nfxzd6m,Competition *,hardware,2025-09-24 13:22:47,1
Intel,nfydv2b,Obligatory article quoting reddit post quoting another article quoting original reddit post.,hardware,2025-09-24 14:38:06,11
Intel,nfy8e3s,"Fake frames, fake articles! /s",hardware,2025-09-24 14:10:35,8
Intel,ng9nabn,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,hardware,2025-09-26 07:24:34,6
Intel,ng3cb6o,"Yes but FSR support is all over the place. Look at what optiscaler is doing, we could have had an open standard for upscalers that FSR XESS and DLSS could have been built on top of meaning much wider support across games instead of every game needing specific implementation and leaving us with outdated upscalers that we need driver overrides and DLL swaps to get around.  Microsoft is only now working on a directX based upscaler API that solves this problem. We should have had something like that years ago like we did for RT.",hardware,2025-09-25 08:18:28,3
Intel,nfy9ky6,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",hardware,2025-09-24 14:16:39,6
Intel,nfy9qc1,"It makes adoption slower though. Especially for smaller devs and the smaller graphics vendors. Even now with pretty large games we still have software lumen only. If RT was pushed as an open standard earlier we might actually have more games and better implementation across the whole market, not just nvidias pet projects.",hardware,2025-09-24 14:17:24,5
Intel,ngt9vbz,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",hardware,2025-09-29 11:53:54,1
Intel,ngt9z2u,"patents expire after 15 years, they will have to share it then.",hardware,2025-09-29 11:54:37,1
Intel,nfykmyd,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,hardware,2025-09-24 15:11:13,12
Intel,nfyiwdp,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",hardware,2025-09-24 15:02:46,9
Intel,ng9ndqw,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,hardware,2025-09-26 07:25:31,3
Intel,ngta2hh,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,hardware,2025-09-29 11:55:17,1
Intel,ngtlhim,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",hardware,2025-09-29 13:07:31,1
Intel,nfyowfp,"> RT in games is now done through DirectX APIs, which are vendor agnostic.  Now being the key word, Nvidia launched their RTX before DXR was even available.  AMD's slow adoption is absolutely one of the problems which might not have not been so delayed if AMD, Nvidia, console makers and Microsoft worked on RT together from the start.   This is all just musing really. Maybe nvidia did us all a favour by breaking the mold forcing everyone else to play catch up.",hardware,2025-09-24 15:31:47,5
Intel,ngtvb8k,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",hardware,2025-09-29 14:02:39,1
Intel,ng9nh90,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,hardware,2025-09-26 07:26:29,3
Intel,nf42stl,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,hardware,2025-09-19 18:12:40,36
Intel,nf2t8pz,"Okay then, what's the Xe GPU roadmap looking like then?",hardware,2025-09-19 14:34:57,75
Intel,nf43nvh,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",hardware,2025-09-19 18:16:54,12
Intel,nf4q7tq,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,hardware,2025-09-19 20:09:01,12
Intel,nf31f8h,They barely lived on before the deal.,hardware,2025-09-19 15:13:57,35
Intel,nf53r9t,"It'll live on our hearts, yes.",hardware,2025-09-19 21:17:43,11
Intel,nf3f8y9,Lol if you believe that I have a bridge to sell you in Brooklyn,hardware,2025-09-19 16:20:00,16
Intel,nf7xz74,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",hardware,2025-09-20 09:42:21,3
Intel,nf8zcs6,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",hardware,2025-09-20 14:09:11,1
Intel,nfacpei,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,hardware,2025-09-20 18:18:08,1
Intel,nf8psx7,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,hardware,2025-09-20 13:16:21,0
Intel,nf605ki,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",hardware,2025-09-20 00:24:16,5
Intel,nfcc5gm,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",hardware,2025-09-21 00:57:40,3
Intel,nf2xqty,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",hardware,2025-09-19 14:56:28,28
Intel,nf53uu0,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,hardware,2025-09-19 21:18:14,4
Intel,nf69muk,"Always selling out actually, they just can't produce that much",hardware,2025-09-20 01:23:31,12
Intel,nf3gd7v,And I have another if you think nVidia is capable of keeping this deal running for that long...,hardware,2025-09-19 16:25:26,5
Intel,nfho1ld,Intel will hopefully split their fab business from the rest of the company either way.,hardware,2025-09-21 21:10:12,1
Intel,nf3itwc,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",hardware,2025-09-19 16:37:23,-21
Intel,nfe3v2w,> they just can't produce that much  because they are losing money on them,hardware,2025-09-21 09:31:45,6
Intel,nfkj39o,Trade bridges.,hardware,2025-09-22 09:21:12,1
Intel,nf6x83x,Intel's arc is dead with or without nvidia deal.,hardware,2025-09-20 04:05:06,1
Intel,nf3naf4,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",hardware,2025-09-19 16:58:38,22
Intel,nf41fhg,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,hardware,2025-09-19 18:05:57,16
Intel,nf5ky00,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",hardware,2025-09-19 22:54:47,9
Intel,nfjjjlm,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",hardware,2025-09-22 03:50:01,1
Intel,nf72gvm,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",hardware,2025-09-20 04:47:19,1
Intel,nf5bseq,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",hardware,2025-09-19 22:01:34,5
Intel,nf5z77h,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",hardware,2025-09-20 00:18:20,3
Intel,nfjkc77,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,hardware,2025-09-22 03:55:54,2
Intel,nf73hs7,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",hardware,2025-09-20 04:55:37,0
Intel,nf6x2qk,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",hardware,2025-09-20 04:03:55,1
Intel,nflcaft,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",hardware,2025-09-22 13:04:14,2
Intel,nf729ff,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",hardware,2025-09-20 04:45:38,2
Intel,nf74cdn,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",hardware,2025-09-20 05:02:36,2
Intel,nfbo7bs,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,hardware,2025-09-20 22:33:22,139
Intel,nfb2xr8,Far more than I expected them to come out at. Damn.,hardware,2025-09-20 20:35:14,19
Intel,nfarr9a,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",hardware,2025-09-20 19:36:31,58
Intel,nfbz1kb,I’m getting one when it releases in Australia,hardware,2025-09-20 23:37:15,4
Intel,nfj2y23,Thats great and all but when will there be stock? (Canada),hardware,2025-09-22 02:00:50,2
Intel,ng8k9a2,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,hardware,2025-09-26 02:15:59,1
Intel,ngid0f0,I'm disappointed.  My order was canceled,hardware,2025-09-27 17:12:34,1
Intel,nfaolmx,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-20 19:19:41,1
Intel,nfbxe5d,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",hardware,2025-09-20 23:27:31,135
Intel,nfc739o,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,hardware,2025-09-21 00:26:10,18
Intel,nfateyp,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",hardware,2025-09-20 19:45:24,205
Intel,nfb479l,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",hardware,2025-09-20 20:41:48,76
Intel,nfb166b,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",hardware,2025-09-20 20:26:03,39
Intel,nfb3pdb,The 3090 does not have ECC on it's VRAM nor certified drivers,hardware,2025-09-20 20:39:13,18
Intel,nfc4ak9,Totally not worth it.,hardware,2025-09-21 00:08:54,-3
Intel,nfb6hdv,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,hardware,2025-09-20 20:53:44,-10
Intel,nfd2k0s,"I'm more looking forward to the B50, but obviously local pricing is everything.",hardware,2025-09-21 03:53:22,5
Intel,njp8t1o,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",hardware,2025-10-15 22:34:30,1
Intel,nfbgzuw,"It has SR-IOV, certified drivers and other professional features...",hardware,2025-09-20 21:51:30,22
Intel,nfc15d7,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",hardware,2025-09-20 23:49:42,9
Intel,nfdv26x,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",hardware,2025-09-21 08:04:03,13
Intel,nfbzlhb,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,hardware,2025-09-20 23:40:29,-13
Intel,nfeqn5s,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",hardware,2025-09-21 12:41:54,7
Intel,nfauwnn,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",hardware,2025-09-20 19:53:22,23
Intel,nfau9ib,Do not underestimate the lack of CUDA.,hardware,2025-09-20 19:49:57,31
Intel,nfdmqe2,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",hardware,2025-09-21 06:45:03,2
Intel,nfbbfys,A used 3090 is only $100 more.,hardware,2025-09-20 21:20:29,1
Intel,nfauhs7,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",hardware,2025-09-20 19:51:10,-17
Intel,nfc1012,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,hardware,2025-09-20 23:48:50,-6
Intel,nfb5jdp,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,hardware,2025-09-20 20:48:47,30
Intel,nfc0fwm,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",hardware,2025-09-20 23:45:29,18
Intel,nfbovre,Atlas 300i duo,hardware,2025-09-20 22:37:31,-5
Intel,nfb510h,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",hardware,2025-09-20 20:46:06,28
Intel,nfb3ro9,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",hardware,2025-09-20 20:39:33,8
Intel,nfc234i,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,hardware,2025-09-20 23:55:21,3
Intel,njpmmsp,Yeah it was very scummy imo,hardware,2025-10-15 23:55:13,1
Intel,nfcbz1s,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",hardware,2025-09-21 00:56:35,1
Intel,nfc72xn,It was meant to be a joke. Not so funny I guess.,hardware,2025-09-21 00:26:06,5
Intel,nfek88y,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",hardware,2025-09-21 11:57:29,11
Intel,nfsgdep,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",hardware,2025-09-23 16:15:20,2
Intel,nfd2naj,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,hardware,2025-09-21 03:54:03,12
Intel,nfc0nds,Why does this matter?,hardware,2025-09-20 23:46:45,20
Intel,nfv9260,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",hardware,2025-09-24 00:45:10,2
Intel,nfb63fj,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,hardware,2025-09-20 20:51:42,39
Intel,nfbbr35,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",hardware,2025-09-20 21:22:13,10
Intel,nfbedl2,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",hardware,2025-09-20 21:36:51,8
Intel,nfbifxk,Used market is freaking insane. It is better to grab new or open box.,hardware,2025-09-20 21:59:41,5
Intel,nfcx3qi,Over here used 3090s are sold for 500-600€.,hardware,2025-09-21 03:13:43,1
Intel,nfbbgrb,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",hardware,2025-09-20 21:20:36,30
Intel,nfhd7ci,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,hardware,2025-09-21 20:19:33,1
Intel,nfb5o6f,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,hardware,2025-09-20 20:49:29,21
Intel,nfayuhx,the super gpus are not expected to release soon?,hardware,2025-09-20 20:14:00,9
Intel,nfcl0od,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",hardware,2025-09-21 01:53:39,14
Intel,nfcm0px,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",hardware,2025-09-21 02:00:02,9
Intel,nfc9w3x,Used 4070 Supers dont sell for nearly $700+ on the low dude.,hardware,2025-09-21 00:43:40,5
Intel,nfco0nb,Yes running business of used cards is how its done.....,hardware,2025-09-21 02:12:43,3
Intel,nfdimrg,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,hardware,2025-09-21 06:07:29,8
Intel,nfbr125,Typically run at 250W though to be fair.,hardware,2025-09-20 22:50:29,6
Intel,nfc1wom,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",hardware,2025-09-20 23:54:16,-7
Intel,nfbgadn,not on the Vram like professional cards,hardware,2025-09-20 21:47:32,6
Intel,nfdjdro,Q4.,hardware,2025-09-21 06:14:16,2
Intel,nfdm9xu,I thought it was funny ¯\_(ツ)_/¯,hardware,2025-09-21 06:40:53,2
Intel,nfjxn7s,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,hardware,2025-09-22 05:47:02,3
Intel,nfddcxx,"Yeah, apparently a year’s worth of it",hardware,2025-09-21 05:20:13,-5
Intel,nfcew77,Because they're super late to the party.,hardware,2025-09-21 01:14:55,-8
Intel,nfb6fud,"Wow, 800-900USD after 5 years is more than I would have expected.",hardware,2025-09-20 20:53:31,18
Intel,nfdc18j,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",hardware,2025-09-21 05:08:46,5
Intel,nfc1e0n,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,hardware,2025-09-20 23:51:10,-7
Intel,nfc1j2x,For what exactly do they need vram without cuda ?,hardware,2025-09-20 23:52:00,-6
Intel,nfbq925,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",hardware,2025-09-20 22:45:52,-11
Intel,nfksagq,Gamers are not niche. They are 20 billion a year business.,hardware,2025-09-22 10:46:43,-1
Intel,nfbohl3,"False, they’re releasing in december or jan. So about 3 months from now",hardware,2025-09-20 22:35:06,6
Intel,nfbxm75,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",hardware,2025-09-20 23:28:51,12
Intel,nfbksrd,"The 3090 Ti did, but the standard 3090 did not.",hardware,2025-09-20 22:13:21,9
Intel,nfjz0lo,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",hardware,2025-09-22 05:59:38,3
Intel,nfdem0c,"If intel felt they could have released this safely earlier, they would have",hardware,2025-09-21 05:31:12,5
Intel,nfb6vit,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,hardware,2025-09-20 20:55:46,24
Intel,nfc975k,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",hardware,2025-09-21 00:39:26,1
Intel,nfg0b8g,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,hardware,2025-09-21 16:40:24,3
Intel,nfcmei6,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,hardware,2025-09-21 02:02:24,7
Intel,nfc3jog,"rendering, working on big BIM / CAD models, medical imaging,...",hardware,2025-09-21 00:04:18,17
Intel,nfc3y1b,CUDA isnt the only backend used by AI frameworks,hardware,2025-09-21 00:06:45,24
Intel,nfde0a9,"I use opencl for doing gpgpu simulations, this card would be great for it",hardware,2025-09-21 05:25:54,2
Intel,nfbqi8f,You mean your 0 profile history because you have it private?,hardware,2025-09-20 22:47:22,8
Intel,nfd62l7,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",hardware,2025-09-21 04:20:02,0
Intel,nfbqms5,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,hardware,2025-09-20 22:48:07,-1
Intel,nfksqka,Spoken like a true gamer.,hardware,2025-09-22 10:50:25,1
Intel,nfbt6h6,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,hardware,2025-09-20 23:03:03,2
Intel,nfcz630,You still get about 85% performance compared to stock settings.,hardware,2025-09-21 03:28:33,2
Intel,nfdr09z,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",hardware,2025-09-21 07:24:57,-3
Intel,nfbvai4,What are they doing that's making them money? Or are theu selling the compute somehow?,hardware,2025-09-20 23:15:17,19
Intel,nfc1jx5,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",hardware,2025-09-20 23:52:08,-7
Intel,nfc9f6k,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",hardware,2025-09-21 00:40:47,1
Intel,nfkntvr,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",hardware,2025-09-22 10:07:27,3
Intel,nfcsv0e,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",hardware,2025-09-21 02:44:26,3
Intel,nfdll3m,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,hardware,2025-09-21 06:34:32,1
Intel,nfks66x,It is the only functional one.,hardware,2025-09-22 10:45:44,1
Intel,nfbqlfx,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",hardware,2025-09-20 22:47:54,-5
Intel,nfd6ez2,0 people are using autodesk with an intel arc gpu lmao,hardware,2025-09-21 04:22:47,-1
Intel,nfbriv2,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",hardware,2025-09-20 22:53:25,0
Intel,nfkvz03,"Oh no, im part of 60% of world populatioin. how terrible.",hardware,2025-09-22 11:15:48,0
Intel,nfd2pwi,Which is 36% higher performance per watt.       ... to be fair.,hardware,2025-09-21 03:54:35,5
Intel,nfdsvbs,The reason they didn't is because it would have been worse to release it earlier,hardware,2025-09-21 07:43:03,0
Intel,nfbxbg7,"Gamers Nexus has a documentary on the GPU smuggling business, where even the 3060 12GBs are being used: https://www.youtube.com/watch?v=1H3xQaf7BFI  The 4090s, 3090s and other cards? There's such a demand for those in China that people will buy them up and smuggle them.",hardware,2025-09-20 23:27:04,14
Intel,nfbw0b9,Software Development,hardware,2025-09-20 23:19:26,-7
Intel,nfc44n9,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,hardware,2025-09-21 00:07:53,8
Intel,nfcbku1,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:54:09,-3
Intel,nfcbqrf,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:55:09,-4
Intel,nfkwldk,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",hardware,2025-09-22 11:20:33,1
Intel,nfcv5fj,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",hardware,2025-09-21 03:00:04,8
Intel,nfeidh5,Which ones?,hardware,2025-09-21 11:43:23,1
Intel,nfcjfv3,Private profile = Complete troll.  You're not an exception to this rule.,hardware,2025-09-21 01:43:42,11
Intel,nfbs5c3,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,hardware,2025-09-20 22:57:03,0
Intel,nfdu2bk,"Right man, my point is that it shouldn’t have been",hardware,2025-09-21 07:54:31,-1
Intel,nfbxo4k,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,hardware,2025-09-20 23:29:11,12
Intel,nfcdfer,"Hell, one of them was just the cooler without the actual GPU.",hardware,2025-09-21 01:05:34,10
Intel,nfcde1y,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",hardware,2025-09-21 01:05:20,-1
Intel,nfgcxg6,for example every local image and video generation system I've seen.,hardware,2025-09-21 17:38:32,3
Intel,nfbsbxa,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",hardware,2025-09-20 22:58:06,2
Intel,nfc0g5t,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",hardware,2025-09-20 23:45:32,-1
Intel,nfducqd,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,hardware,2025-09-21 07:57:16,3
Intel,nfd4nt4,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",hardware,2025-09-21 04:09:11,5
Intel,nfh4t9j,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,hardware,2025-09-21 19:41:40,1
Intel,nfd0war,And the one directly under that was the actual PCB...,hardware,2025-09-21 03:41:08,5
Intel,nfgecr2,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,hardware,2025-09-21 17:44:40,0
Intel,nfksgbg,Propaganda is 90% of chinas economic output though.,hardware,2025-09-22 10:48:04,0
Intel,ne6ahg8,"> Neweggâs  I know this is mojibake, but this kinda sounds like a Lithuanian versions of Newegg lol",hardware,2025-09-14 14:46:52,182
Intel,ne5xw93,The competing product is sometimes slower while also being twice the price.  If this wasn't a success then Nvidia would be  unbeatable,hardware,2025-09-14 13:39:18,224
Intel,ne6669f,"What's the word on the B60? Even more VRAM (24GB), and double the memory bandwidth. I see it listed as ""released"" in various places, but can't figure out where to actually buy one.",hardware,2025-09-14 14:24:15,61
Intel,ne680ly,"My RTX a4000 doesn't support SR-IOV. I don't know about newer series, but at the time you had to buy the A5000($2500) or A6000 and then there are some crazy licence fees to use it.  For 350 i will buy it when it gets available just for this.",hardware,2025-09-14 14:34:03,60
Intel,ne6hcg6,L1 techs had a great feature on these.,hardware,2025-09-14 15:21:17,22
Intel,ne62jka,"Profitable product for Intel, wouldn't suprise me if Xe3P and onwards for dGPUs happens because stuff like this can do easy returns.",hardware,2025-09-14 14:04:39,31
Intel,ne8mbnv,1:4 ratio of FP64 performance is a pleasant surprise,hardware,2025-09-14 21:19:44,11
Intel,ne6anee,"Honest question here: what makes it a ""workstation gpu"" that does it differently than say like a low end 5060/AMD equivalent?   Iis it just outputting 1080p ""faster""?",hardware,2025-09-14 14:47:44,13
Intel,ne65b1l,"Its also just a whole 95 cards sold. (Past month, I’m unsure if its been up longer)",hardware,2025-09-14 14:19:36,18
Intel,ne6rl68,"It will never be in stock again. It’s good for AI, hosting pass through srvio to VMs without licensing and a number of other things outside of gaming.",hardware,2025-09-14 16:11:13,5
Intel,ne5wmh6,"Hello 79215185-1feb-44c6! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-14 13:32:02,-2
Intel,ne6nm7f,Let me know when it shows up on the steam hardware survey. That's the only barometer for success that true hardware enthusiasts care about.,hardware,2025-09-14 15:52:11,-23
Intel,ne7zdif,what's up my Neweggâs,hardware,2025-09-14 19:32:23,89
Intel,ne95tlq,My favorite game  Fallout Neweggas,hardware,2025-09-14 23:05:48,25
Intel,ne9ba6q,It also sounds like an ikea lamp name or something,hardware,2025-09-14 23:35:36,7
Intel,ne7p3l1,Geriau nei Bilas Gatesas,hardware,2025-09-14 18:44:05,3
Intel,ne6o1us,I gotta spell Neweggâs?!?,hardware,2025-09-14 15:54:14,0
Intel,ne6355p,"The B50 appears to be an decent low-end workstation GPU, at least as long as the intended workloads don't effectively require CUDA in any way, shape, or fashion.    My one lingering question is what use cases actually *require* the certifications of a workstation-class GPU (which would rule out something like a relatively similar consumer-tier RTX 5060 Ti / 16GB) but wouldn't benefit from CUDA?  Then again I'm not exactly an expert in the field, so I could be completely off-base here.",hardware,2025-09-14 14:07:57,69
Intel,ne7314f,"I'm disappointed in Nvidia's inability to put out a stable driver since last December, I'm waiting to see if a competitor card will come out that meets my wants for an upgrade.",hardware,2025-09-14 17:04:11,6
Intel,ne8v3zo,Intel might be using the B50 as a pipe cleaner for the B60's drivers to prepare it for a retail launch in Q1 2026    IF they're doing this then it's a sound strategy,hardware,2025-09-14 22:05:58,15
Intel,ne6kilx,"Double the memory bandwidth of trash is still trash.  Edit: Y'all can downvote me all you want, but 250GB/s is just slightly more than the 200GB/s of my low-profile 70W GTX1650 GDDR6 that I bought for €140 in 2019. Its absolutely pathetic and should be unacceptable for a new product in 2025, let alone a product of $350 !!!. Even double of this (\~500GB/s) of the B60 is less than a RTX3060. Pathetic products.",hardware,2025-09-14 15:37:09,-72
Intel,ne7d8mq,SR-IOV is the selling feature for me and why I have one ordered. Getting a Tesla P4 with nvidias vgpu licensing working is a pain in the ass and expensive.  I'll get it and sit on it until SR-IOV is released in case of scalpers/stock issues. If it doesn't pan out I'll either just sell it on or drop it into my home media server for the AV1 encoding/basic AI stuff.,hardware,2025-09-14 17:49:31,20
Intel,ne6eqfv,"Last time I checked GRID licensing can be faked out, but yes, only Quadro/Tesla and Turing/Pascal(IIRC) through driver mods can use Nvidia's vGPU.",hardware,2025-09-14 15:08:14,-9
Intel,ne70sx3,"The professional market is smaller than gaming and even more slanted towards Nvidia. This might be a nice side business, but can't remotely justify developing these cards.   Not even clear it's profitable either. The numbers here are negligible so far.",hardware,2025-09-14 16:54:03,2
Intel,nealgik,"Do people actually need and use FP64 at all anymore? I've got one or two original Titan cards that I haven't thrown out although I've never used them for this purpose either, because they apparently have very high FP64 numbers and if I recall correctly can operate in ECC mode as well.",hardware,2025-09-15 04:31:20,10
Intel,ne6c0of,"iirc, SR-IOV and VDI support in the coming months, toggleable ECC support, and it is ISV certified",hardware,2025-09-14 14:54:44,46
Intel,ne6dpqm,"So I spec our PCs at work. We do anything from traditional office work, to intense engineering tasks. On our engineering computers we run MatLAB, Ansys, Solidworks, MathCAD, LTSpice, Xilinx, Altium and other such apps. Lots of programming, VMs, design work, simulation testing, number crunching, and on occasion AI work.   This means we spec systems like with RTX Pro 500, RTX Pro 2000, RTX A4000, A4500, A6000s. The reason we have these rather than cheaper GeForce cards is mostly 3 things. Power/form factor, Driver certification, pro GPU features.   So typically Nvidia keeps the top binned chips for their professional cards meaning the power efficiency to performance is top tier. So we can get high performance single slot or low profile cards, or get some serious GPU performance in relatively small laptops. Drivers usually are validated better than the GeForce drivers, so they include better bug testing, and the apps we use validate performance with the cards which helps us evaluate performance. They also have way more vram like the RTX A4000 has 20GB of vram while being just a supped up 4070. Then from a feature perspective they have better VM passthrough support, or you can enable the vram to run in ECC mode for error correction. Very important when running 24-48 hour simulations.",hardware,2025-09-14 15:03:08,20
Intel,ne6yi39,Software support is a thing. CAD applications like solidworks and inventor don’t officially support the GeForce rtx or radeon rx line of gpus and they’re considered untested unsupported options. You can’t get any tech support if you’re using them. For a business that needs those apps you need a workstation gpu. They also come with ECC vram,hardware,2025-09-14 16:43:33,10
Intel,nebc25u,ECC memory.,hardware,2025-09-15 08:46:16,1
Intel,neb16s5,"That kind of puts it into perspective.  Also, let my take a guess:  Newegg sells them well because of how dirt cheap they are, people buying actually expensive Pro cards will more likely do it directly via their system integrator.",hardware,2025-09-15 06:53:42,4
Intel,neaqhtz,Oh when they get enough enterprise customers they will definitely charge licensing fees,hardware,2025-09-15 05:14:05,2
Intel,ne70g18,How many A2000s show on the hw survey? Because that's the Nvidia variant and it has been around for a long time..,hardware,2025-09-14 16:52:26,18
Intel,ne878lu,Lmao,hardware,2025-09-14 20:08:06,12
Intel,nebbyvw,Komentarą skaitai per tapšnoklį ar vaizduoklį?,hardware,2025-09-15 08:45:18,1
Intel,ne66mwj,I am no expert but don't these gpus have ECC vram. That's a enough to get labs/professionals to buy them.   You don't want the headache of lacking error correctiin in a professional environment.,hardware,2025-09-14 14:26:44,78
Intel,ne6xv28,"I seriously considered getting one for my homelab. I would really like some SR-IOV, and giving multiple VMs access to transcoding would be very useful. Ultimately decided against it because at the moment my CPU alone is powerful enough, I have other uses for the PCIe slot, and I would have to import one. But it's something I'm going to check in on whenever I'm browsing for new hardware from now on.",hardware,2025-09-14 16:40:31,5
Intel,ne8a75b,"I know in my field of work, solidworks certified hardware is one such application where certain features are gated behind workstation class cards.",hardware,2025-09-14 20:21:25,3
Intel,ne95ql6,Vulcan does fine with inferencing.,hardware,2025-09-14 23:05:20,3
Intel,nfekr88,"all the professional graphics stuff is where this can matter, i.e. CAD, large display walls, traffic control, video studios etc",hardware,2025-09-21 12:01:22,1
Intel,neavc5w,people are buying B60's  https://www.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/,hardware,2025-09-15 05:57:49,3
Intel,ne8ur5d,"Most Zen-1 parts had much worse single core performance than Kaby Lake,    People still cheered on the competition anyway despite it's shortcomongs",hardware,2025-09-14 22:03:59,19
Intel,neprgo9,"GTX 1650 has only 4GB of RAM at 128 GB/s; RTX 3060 is only 360 GB/s, and only 12 GB--or maybe just 8 GB for some cards--of RAM. But thanks for playing.   Edit: relevant username. Up voting you for jebaiting the crap out of all of us.",hardware,2025-09-17 14:56:51,0
Intel,ne6jp74,you really dont want to fuck around with software licensing as a business. vendors do inventory audits to ensure nobody's exceeding their license allocations. piracy would automatically invite a lawsuit.,hardware,2025-09-14 15:33:02,40
Intel,ne8fips,Grid licensing can be faked if you depend on a sketchy github driver that only works on Turing GPUs. You certainly don't want to be doing that in a professional setting where licensing costs are not a massive expense anyways.,hardware,2025-09-14 20:46:12,12
Intel,ne8eldw,"I believe mobile is the main reason they continue developing ARC IP, highly integrated SoC are crucial for lower power consumption and performance per watt, as more and more mobile designs are becoming more integrated (see strix halo for example) Intel knows it has to continue developing graphics IP that is competitive with competition. As for discrete cards, this is a battle in the long run to win, but it will take serious investment, we can hope that they won't axe as part of cost cutting measure.",hardware,2025-09-14 20:41:52,10
Intel,ne93icy,"The B50 (16Xe cores) is pretty cut down compared to the full G21 (20Xe)die, it has 2600mhz boost clocks instead of the 2850mhz on the gaming cards, it uses 14GB/s memory (19Gbps on gaming cards) and it has a 128bit bus with 8 memory chips (B580 has 192bit bus with 6 memory chips)  The only costly thing about is the 2 additional memory chips.   I'm not saying it's extremely profitable but it can't be too expensive to make since a portion of the volume is likely faulty G21 dies that can't make a B580 or B580.   If Intel can sell the B580 for $250 without too much pain, then the B50 is probably making a profit",hardware,2025-09-14 22:52:49,1
Intel,nealnf3,"Yes, to the point where I’m considering picking up a Titan V on eBay. It’s a must for scientific computing, single precision floats accumulate errors fast in iterative processes.",hardware,2025-09-15 04:32:57,12
Intel,ne6dmlw,I recognize those as words...,hardware,2025-09-14 15:02:42,5
Intel,ne7cbc2,I think it was obvious I was being facetious.,hardware,2025-09-14 17:45:29,-4
Intel,ne8jx5n,> You don't want the headache of lacking error correctiin in a professional environment.  I think Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU. That was the explanation that I got back in the university years when the IT department would use the cheapest possible professional GPUs instead of high end consumer GPUs.,hardware,2025-09-14 21:07:34,31
Intel,ne6qvax,"Yeah, if ECC is a hard requirement for whatever reason then that would certainly rule out all the GeForce-branded RTX cards.    Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a [RTX PRO 2000 Blackwell](https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-2000/) instead, which fits the same niche as the B50 (i.e. low-profile 2-slot @ <75w with 16GB of VRAM) while being faster and having a far superior feature set.",hardware,2025-09-14 16:07:47,21
Intel,ne6p0f3,I’ve been working in a professional aechtiectural environment for 5 years and haven’t seen the need for ECC once.   Can you explain situations where it’s needed? I’ve always wondered.,hardware,2025-09-14 15:58:50,4
Intel,ne8kqkn,"SR-IOV for (license-free!) vGPU is IMHO the killer feature here, perhaps along with being able to get 16GB of VRAM per card relatively cheaply and without needing auxiliary power.  Both open up interesting server and workstation use cases that can't be had cheaply from the competition.",hardware,2025-09-14 21:11:39,12
Intel,ne95wnj,Have you tried GPU paravirtualization?,hardware,2025-09-14 23:06:16,1
Intel,nebgla3,"You can buy it off AIB Partners but you can't buy it at retail (i.e. microcenter, newegg] and it doesn't have an official MSRP yet.   The prices you see now are what AIB's want to charge in bulk orders.    If you want to know how much let's say 5 B60's cost you have to get a quote from a distributor",hardware,2025-09-15 09:33:31,3
Intel,ne8r5hr,They need GPU IP for two things: client and AI. Anything else is expendable.,hardware,2025-09-14 21:44:37,6
Intel,ne9x6oh,"Yes, my point was *if* they have the gaming cards, they can justify the professional line, but it's not nearly big enough to justify making a dGPU to begin with.",hardware,2025-09-15 01:47:23,5
Intel,ne6f8e1,"SR-IOV is Virtual GPU (SR-IOV is IO Virtualization used to split PCIe lanes into virtual functions so their physical function can be shared between VMs). No consumer cards support Virtual GPU right now besides Pascal/Turing with driver hacks. AMD's SR-IOV offerings are [very limited](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#virtualization-support), [And Nvidia has a bigger selection](https://docs.nvidia.com/vgpu/gpus-supported-by-vgpu.html) but their budget VGPU options are being phased out (P40).  I believe VDI is Microsoft's implementation. I believe I've done VDI on my RTX 2070 before (I have done seamless sharing between host and VM), but I don't know if it's possible with AMD. Someone please correct me if I'm wrong here, I'm more familiar with the Linux side / vGPU than VDI.  ECC is Error Correcting RAM. I generally don't understand the use case for ECC either, but it is ubiquitous in HPC. All server boards support ECC RAM.  In modern environments most of these features need 16GB of VRAM minimum, but if you ever wanted to try it on a consumer card, you could get an old RTX 20 series and try it out with some driver mods. Optionally, the P40 is still pretty cheap ($250 used) and doesn't need those hacks at the cost of drawing a lot of power, which Intel has solved with their Battlemage Pro platform (by far the cheapest VRAM/$/W you can get).",hardware,2025-09-14 15:10:44,31
Intel,neanu9y,>Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU.   A380 and A770 is also on the certified gpu list. But otherwise that statement is correct.,hardware,2025-09-15 04:51:08,12
Intel,ne6s65g,Can you buy an rtx pro 2000?  If i had to guess what percentage of wafers are for b200 chips i would say 90%.  I don't think there are enough pro 2000s around. I don't think there are enough gpus around in most cases.,hardware,2025-09-14 16:13:57,7
Intel,neaqr14,"> Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a RTX PRO 2000 Blackwell instead  It is interesting. IDK how common it is, but one of my university labs had computers donated from nvidia with nice quadro GPUs for their time.",hardware,2025-09-15 05:16:19,2
Intel,neb03p4,Pathetic 1:64 ratio of FP64 flops,hardware,2025-09-15 06:43:15,2
Intel,neb28v9,RTX 3090 Ti and RTX 4090 have ecc. Not that they're cheap.,hardware,2025-09-15 07:03:44,2
Intel,ne705od,"It's needed whenever the work you're doing matters and a single-bit error could cause significant harm.  Something like audio or video, a single-bit error probably isn't very noticeable.  Calculations, it absolutely depends on what you're calculating and which bit gets flipped; flipping the low-order bit in a number might not matter much and flipping the high-order bit could cause a big error.  Networking, it depends on whether the protocols you use have more error checks at a higher level (TCP does; UDP does not).  If in doubt, you want ECC, but market segmentation to mostly restrict ECC support to ""server"" chips and boards and charge more for ECC memory means you'll overpay for it.",hardware,2025-09-14 16:51:06,17
Intel,nea091d,"They're so good, I wish there was a single slot variant.   I want to put them in my MS-01s. The Sparkle A310, being the main candidate for deployment in those machines, only has 4GB and its maximum h264 encoding throughput actually drops below the iGPU (although its h265 and AV1 throughput slaps the 12900H/13900H). It's just a little too low to comfortably handle the Plex server usage I have, so the iGPU remains in service until a suitable competitor arrives.",hardware,2025-09-15 02:05:50,3
Intel,nebghm1,"IIRC, that requires a Windows host right? That's a non starter for many people unfortunately",hardware,2025-09-15 09:32:29,3
Intel,necmv9o,my guy sr-iov is a type of gpu paravirtualization.,hardware,2025-09-15 14:23:28,1
Intel,nenukwd,The Asrock b60s are $599,hardware,2025-09-17 06:30:55,1
Intel,ne90ue7,AI doesn't even need a GPU; it can have its own accelerators - see Gaudi.,hardware,2025-09-14 22:37:50,1
Intel,ne8u0oh,"One use case for ECC, is when the data is critical and can’t be lost.",hardware,2025-09-14 21:59:59,10
Intel,nea1lfo,>  I generally don't understand the use case for ECC either  Its for when you don't want errors to just be ignored?   How is that hard to understand?,hardware,2025-09-15 02:14:04,12
Intel,nebc61d,ECC should be in literally all memory.,hardware,2025-09-15 08:47:24,1
Intel,ne70ox2,"\>but their budget VGPU options are being phased out (P40).     I mean, the T4, L4 , and A16 exists...     I'm also not sure why low end workstation GPU needs SRIOV support.",hardware,2025-09-14 16:53:32,-9
Intel,ne6xml5,I see them at least available in my stores. although mostly as backordres via remote warehouses but they seem readily available with some shipment time.,hardware,2025-09-14 16:39:25,7
Intel,nee4dd7,"but you can still do gpu paravirtualization without sr-iov using Mediated Passthrough, API Forwarding (RemoteFX) or Dedicated Device Assignment",hardware,2025-09-15 18:42:34,2
Intel,neozcix,"Where can you get them? And are they for sale yet, or pre-orders, or...?",hardware,2025-09-17 12:27:00,1
Intel,ne9vlfz,"The problem with Gaudi (I know, I've written code and run training runs on it) is simply that the programming model is not oneAPI, or whatever oneAPI becomes. Yes, pytorch works, but people care a lot about software longevity and long term vision when buying $5mm+ of GPUs (and these are the purchases Intel cares about that can actually start to offset the cost of development).   The whole purpose behind Falcon Shores (and now Jaguar Shores, if it will even happen) is to put Gaudi performance (i.e. tensor cores) in an Xe-HPC package. Unifying graphics and compute packages is what NVIDIA was able to achieve but not yet AMD, and it's really great for encouraging ML development in oneAPI.  See this post to see where Intel would like to be: https://pytorch.org/blog/pytorch-2-8-brings-native-xccl-support-to-intel-gpus-case-studies-from-argonne-national-laboratory/ (they don't mention the ""XPU"" because it's Ponte Vecchio, which are iiuc worse than A100s).",hardware,2025-09-15 01:38:00,8
Intel,ne9xfff,"Intel can't get people even in an AI shortage. No one wants to deal with an ASIC. That's why their AI solution is GPUs, starting with (hopefully) Jaguar Shores. So it's that or bust.",hardware,2025-09-15 01:48:50,5
Intel,neav264,I spit my coffee reading that. Gaudi? The platform that nobody uses that Intel has to revise their sales estimates down each half quarter?,hardware,2025-09-15 05:55:13,2
Intel,neaiu6t,"Yup. For example you are doing a structural integrity physics simulation, and a single flipped bit can ruin your 1 week long run (and your liability insurer will reject your claim, a lot of them have standards requiring calculations to be done only on ECC for sensible reasons).",hardware,2025-09-15 04:10:17,11
Intel,ne78lg0,"Great example of why certain people shouldn't reply if they don't have knowledge in the area.  - Tesla T4 is $650 Used and has 16GB of VRAM. - Tesla L4 is $2000 Used and has 24GB of VRAM. - Tesla A16 is $3000 Used and has 64GB of VRAM.  Compared to:  - Arc Pro B50 is $350 new and comes with 16GB of VRAM. - Tesla P40 is $275 used and comes with 24GB of VRAM.  If all you care is vGPU / VDI for a small amount of hosts, then no, you're not getting a Tesla A16. What kind of joke suggestion is that?",hardware,2025-09-14 17:29:09,15
Intel,neea3nk,"Mediated pass though requires big ass license fees vGPU/MxGPU, and isn't FOSS other than Intel's currently broken support that they abandoned for SR-IOV support.  API forwarding only support limited host/guest setups, and even more limited API support. The only FOSS support is VirGL, which only support Linux host/guest and only OpenGL.  Obviously fixed pass though is an option, but even that isn't without issue. NVIDIA only recently removed the driver restriction, they could add it back at any time. Plus you are limited on virtual machine by the physical GPU count. It works with Intel GPUs and is FOSS with them.  SR-IOV on Intel fixes all of that. It works amazingly well with their iGPUs, has no license issues, and is fully FOSS.",hardware,2025-09-15 19:10:25,5
Intel,ne79g56,Hey no need to be aggressive towards the other user. Your comments are very helpful and I appreciated them a lot but keep it constructive please!,hardware,2025-09-14 17:32:53,9
Intel,ne79riz,"LMAO, I actually have quite a bit of knowledge in this area.  If all you care for is VDI for a small number of VMs, then you'd go GPU passthrough. vGPU / MxGPU often requires higher levels of hypervisor software tier (i.e. VMware vSphere Enterprise Plus), requiring more money. For KVM hosts, setting up vGPU is a lot more difficult and time consuming than just straight up GPU passthrough.  Only two groups of people would be interested in GPU virtualization / splitting:  * Enterprise, in which they wouldn't care about the used card prices.  * Enthusiasts, in which they wouldn't want to pay for vGPU prices anyway. So why bother catering to this crowd?",hardware,2025-09-14 17:34:16,-13
Intel,ne8go27,"Full GPU passthrough is not a solution that many people would consider because it is clumsier than using sr-iov (or potentially VirtIO GPU Venus). Plus for each extra passthrough instance I would have to add in another GPU and this greatly increases power consumption, heat output and cooling requirements. The process is not all that much more complicated at least on Turing GPUs with a hacked driver on KVM guests at least. Plus for passthrough, you probably still need an NVIDIA card because last I checked AMD cards still had a random kernel panic issue after being passed through.  My assumption is that sr-iov on the b50 will allow users an affordable way to have multiple guests on one host GPU without increasing power draw and paying for expensive alternatives and expensive vGPU subscriptions.",hardware,2025-09-14 20:51:40,10
Intel,ne92tmq,"...first time I heard people prefer SRIOV over GPU passthrough because it's ""clumsier"" lol. I'm sure setting up mdev devices in KVM, finding the correct corresponding GPU instances, making them persistent through reboot, then edit virsh xml for each individual VM is a lot easier than just doing IOMMU passthrough. /s     Again, enthusiasts don't care about power consumption / heat output / cooling requirements for their lab environment. Enterprise that do care about them are very willing to pay extra cost to get a production ready driver. You're creating a hypothetical situation that simply does not exist in the real world.",hardware,2025-09-14 22:49:00,-7
Intel,nd4rusz,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",hardware,2025-09-08 18:55:24,85
Intel,nd5dm3n,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,hardware,2025-09-08 20:41:18,25
Intel,nd8r4il,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",hardware,2025-09-09 10:54:51,10
Intel,nd57awu,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,hardware,2025-09-08 20:11:04,29
Intel,nd5z0zb,Too late for me I already went with a 9060xt but hell I had dreamt of it!,hardware,2025-09-08 22:32:44,4
Intel,nd4ofdy,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",hardware,2025-09-08 18:38:32,3
Intel,nd5mkse,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,hardware,2025-09-08 21:25:37,4
Intel,nd8g10o,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,hardware,2025-09-09 09:13:47,2
Intel,nd4ety2,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-08 17:51:42,1
Intel,ndaptcd,Depending on the price I might give it a shot.,hardware,2025-09-09 17:16:31,1
Intel,nd7euko,Love it going to get one if I can scratch some money together,hardware,2025-09-09 03:38:15,1
Intel,nd5tii5,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,hardware,2025-09-08 22:02:05,-3
Intel,nd6wdjt,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",hardware,2025-09-09 01:46:51,0
Intel,nd4lt4f,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,hardware,2025-09-08 18:25:36,-14
Intel,nd5ec1f,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",hardware,2025-09-08 20:44:45,-7
Intel,nd50mkd,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,hardware,2025-09-08 19:38:23,16
Intel,nd7icfg,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",hardware,2025-09-09 04:02:28,-3
Intel,nd91aqv,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",hardware,2025-09-09 12:07:11,7
Intel,nd5nmhj,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",hardware,2025-09-08 21:31:01,22
Intel,nd50bqm,I have a B580 and the driver seems pretty stable to me at this point.,hardware,2025-09-08 19:36:56,39
Intel,nd4xpjv,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",hardware,2025-09-08 19:24:08,8
Intel,nd5kt0f,my b580 has been stable,hardware,2025-09-08 21:16:33,9
Intel,nd7ucd7,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",hardware,2025-09-09 05:39:29,2
Intel,nd86r6j,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",hardware,2025-09-09 07:37:52,2
Intel,nd7ihdv,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,hardware,2025-09-09 04:03:27,4
Intel,nd4pva0,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,hardware,2025-09-08 18:45:36,14
Intel,nd4qkrx,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",hardware,2025-09-08 18:49:07,17
Intel,nd57ffs,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,hardware,2025-09-08 20:11:40,39
Intel,nd5dyvw,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.  Edit: my bad, I didn't know it was for only a specific game, though still not a good result for b580 overall",hardware,2025-09-08 20:43:00,1
Intel,nd63tmr,Where do you find such information?,hardware,2025-09-08 23:00:34,17
Intel,nd648nd,Source: I made it the fk up,hardware,2025-09-08 23:03:03,5
Intel,nd55qwm,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",hardware,2025-09-08 20:03:30,17
Intel,nd4z315,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",hardware,2025-09-08 19:30:52,12
Intel,nd7nnb6,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,hardware,2025-09-09 04:42:33,44
Intel,nd6d7u0,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",hardware,2025-09-08 23:55:26,13
Intel,nd6ck88,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",hardware,2025-09-08 23:51:40,8
Intel,nd5oln7,Try Mechwarrior 5: Clans on high and say there are no problems again.,hardware,2025-09-08 21:36:04,-12
Intel,nd6bnfg,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",hardware,2025-09-08 23:46:18,15
Intel,nd89f2e,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,hardware,2025-09-09 08:05:10,11
Intel,nd6cup4,"It doesn't even run, it crashes left and right on an Arc.",hardware,2025-09-08 23:53:20,-3
Intel,nd6drxa,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",hardware,2025-09-08 23:58:41,4
Intel,nd6en4h,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",hardware,2025-09-09 00:03:40,-1
Intel,nc8cvxo,SR-IOV at that price. Who cares about anything else.,hardware,2025-09-03 17:55:15,169
Intel,nc9qxn7,Intel would be stupid to axe there Graphic card division if this proves to be successful.,hardware,2025-09-03 22:00:54,87
Intel,nc9seh0,"Single-slot variant or custom cooler please, my MS-A2 running proxmox is demanding this card.",hardware,2025-09-03 22:08:49,19
Intel,ncc2zjm,About 66% overall performance of a B580 it looks like. That's really nice for a 70W card.,hardware,2025-09-04 07:35:40,13
Intel,nc98dp9,"This is exciting, definitely looking forward to the b60 as well.",hardware,2025-09-03 20:28:14,25
Intel,nc8kczs,"Obligatory ""Intel is exiting the GPU business any moment now"".",hardware,2025-09-03 18:32:12,84
Intel,nc8gp00,how hard are tehse to actually buy?,hardware,2025-09-03 18:14:07,17
Intel,ncaibo5,"Buying one, this is impressive",hardware,2025-09-04 00:36:23,14
Intel,nicvml9,"if compared by price to performance ratio,  ARC B50 is slower than RTX 5060 in terms of price and performance",hardware,2025-10-08 02:29:55,1
Intel,nca78up,Its better than a 1.5 year old bottom of the range card....well done i guess.,hardware,2025-09-03 23:32:04,-24
Intel,nca1tmp,Better than NVIDIA? lol .... oooookay,hardware,2025-09-03 23:01:43,-33
Intel,nc8e96z,"Haven't seen the video, but I'm already buying one if that's the case",hardware,2025-09-03 18:01:47,47
Intel,ncafc02,Literally it could be a damn arc a310 or rx6400 and people would buy that card at $350 without licencing bs. For anything VDI related the B50 is huge.,hardware,2025-09-04 00:18:35,34
Intel,ncbdpjm,Intel’s “MOAAAAAR CORES” in the GPU space???,hardware,2025-09-04 03:54:14,6
Intel,ncde8b8,what is that? SR-IOV?,hardware,2025-09-04 13:38:29,5
Intel,nccuj5h,Super interesting!  Wonder how well it would handle AI tasks like Frigate while another VM uses it for inference or a third doing video transcoding with Plex.,hardware,2025-09-04 11:42:43,3
Intel,ncbojo9,16 GB VRAM too,hardware,2025-09-04 05:20:40,5
Intel,nc9ulp7,They will eventually axe it.,hardware,2025-09-03 22:20:53,21
Intel,nccun4t,Instead of axing it maybe spin it off like AMD did with Global Foundries?,hardware,2025-09-04 11:43:28,2
Intel,ncbkppb,And if it isn’t successful?,hardware,2025-09-04 04:48:20,1
Intel,ncbs3pt,They do have 3rd party vendors for ARC PRO Cards this time around so it most likely will happen.,hardware,2025-09-04 05:52:01,10
Intel,nc9di83,"The B60 is more exciting to me just for that 24GB VRAM. Still, at this price point the B50 is a pretty compelling buy tbh.",hardware,2025-09-03 20:52:30,17
Intel,nc95it5,I think it would be really stupid for them to do so.,hardware,2025-09-03 20:14:44,45
Intel,nc8hjt4,You can preorder from newegg now. They ship later this month.,hardware,2025-09-03 18:18:26,32
Intel,nc8xqpv,"One Swedish retailer I checked has them coming into stock next week (10 September) and open to orders, how much stock there will be however, I have no clue.",hardware,2025-09-03 19:37:13,10
Intel,nce6pwg,Same. I put my preorder in. Plan to put it into one of my sff builds.,hardware,2025-09-04 15:55:20,5
Intel,ndhamhf,Why is this impressive for $350 USD? How will this be useful for you? I’m not being sarcastic. I am genuinely curious.,hardware,2025-09-10 17:12:23,0
Intel,ncdy79x,What did bottom of the range cards cost 1.5 years ago?  How much VRAM did they have?  Did they support SR-IOV?   Just think for a bit sometimes.,hardware,2025-09-04 15:15:31,13
Intel,ncdy1t2,It quite literally is. Watch the fucking video.,hardware,2025-09-04 15:14:51,12
Intel,nc8xenf,"Wendell confirmed as much in the comments, looking forward to his future testing of the card.",hardware,2025-09-03 19:35:34,29
Intel,ncc2mdm,What does AMD have in this product segment?,hardware,2025-09-04 07:31:59,3
Intel,nd8jouo,"Ingen av de kortene greier LLM, hvis du tror det.",hardware,2025-09-09 09:50:11,0
Intel,nd8rcrq,"Ingen av de kortene greier LLM, hvis du tror det.",hardware,2025-09-09 10:56:38,0
Intel,ncdf0ln,"I didn't know either so I looked it up.   ""SR-IOV (Single Root Input/Output Virtualization) is a PCI Express technology that allows a single physical device to appear as multiple separate virtual devices, significantly improving I/O performance in virtualized environments by giving virtual machines direct access to hardware. This bypasses the overhead of a software-based virtual switch, resulting in lower latency and higher throughput for demanding applications by dedicating virtual functions (VFs) to guest VMs.""",hardware,2025-09-04 13:42:39,13
Intel,nccz58k,Everyone (including Nvidia) is moving toward APUs with large GPUs onboard. Why would Intel kill their chance at competing in that market?  They've already withstood the most painful part of the transition. There's no point in stopping now.,hardware,2025-09-04 12:12:40,37
Intel,ncegs0k,They are keeping their Fabs which is even more expensive to maintain why would they sell GPU not to mention their iGPUs are pretty Damm good nowdays not like meme in Intel HD4400 even though they could play any game /jk.,hardware,2025-09-04 16:42:52,3
Intel,ncenqdx,Doubt they have the revenue to spin out successfully without significant investment from outside sources.,hardware,2025-09-04 17:15:36,5
Intel,nc9xu5x,Sadly Intel has a recent history of making poor life choices.,hardware,2025-09-03 22:39:04,57
Intel,ncaqsmf,"Maybe it's just me, but this reads as AI generated.",hardware,2025-09-04 01:26:28,13
Intel,ncar7n9,"I dunno man, I was building a PC for work and the 3050 was the cheapest Nvidia card I can get and the 7600 is the cheapest from AMD. Huge price gap between the two, by about 100 USD. AMD really needs to buck up their APUs to render cheap GPUs surplus or have something cheaper than a 7600 to price match the 3050.",hardware,2025-09-04 01:28:53,3
Intel,nce2wy8,"They're comparing it to an entry-level NVIDIA GPU, the A1000. Saying that Intel GPUs are ""better than NVIDIA"" as a universal statement is flat-out wrong. Let's see some competition to the RTX 5070 Ti, 5080, or 5090. NVIDIA has zero competition on mid-range and high-end GPUs.",hardware,2025-09-04 15:37:44,-17
Intel,nceqap4,Radeon Pro V710 and you can't even buy it retail.,hardware,2025-09-04 17:27:25,12
Intel,ncdfanm,thanks 🙏,hardware,2025-09-04 13:44:06,2
Intel,ncd1hnr,Because intel shareholders are super short sighted.,hardware,2025-09-04 12:26:50,29
Intel,nc9zg8o,this comment is the weirdest version of 'corporations are people' that i've encountered,hardware,2025-09-03 22:48:13,15
Intel,ncb4jkb,Lmao seriously the formatting and the amount of bolded words just screams AI,hardware,2025-09-04 02:50:42,6
Intel,ncar9b0,It's AI generated in your mind,hardware,2025-09-04 01:29:09,-8
Intel,ncaz7h1,"because AMD has a bad habit of leaving a bunch of their older cards in the channel and having them become the low end...  CPU and GPU, AM4 lives for so long because there are still piles of the stuff in the channel and just gets slow tiny discounts till its gone in full  its like their demand forecast is too optimistic or something but at this point I think its deliberate",hardware,2025-09-04 02:16:51,2
Intel,ncee75r,Because this is not a gaming GPU and thus the A1000 is the correct card to compare with.,hardware,2025-09-04 16:30:17,13
Intel,ncel88g,Good luck using those super gpus to host multiple gpu accelerated vm with one card. Nvidia won't let you.,hardware,2025-09-04 17:03:58,9
Intel,ncemu0g,"Yes, compare an Arc Pro to a GeForce, totally the same market.",hardware,2025-09-04 17:11:26,7
Intel,nchqdw5,"That seems more mid or high tier rather than these relatively low tier gpus, the b50 is a cut down b580...  Also the v710 seems like the kind of ""passive"" gpu that's ""passive"" as long as it's next to several 7,000 rpm fans.  So it would probably not work very well as a retail car because it's rack server focused.",hardware,2025-09-05 03:02:19,1
Intel,ncdxuek,"I think the really loud short sighted shareholders have quieted down a bit after it became clear they helped drive the company to where they are. Hell, they're probably not even shareholders anymore.",hardware,2025-09-04 15:13:52,10
Intel,nca5gwf,The weirdest version was Citizens United,hardware,2025-09-03 23:22:10,21
Intel,ncbz15z,Did you not figure out why they’re bolded?,hardware,2025-09-04 06:56:47,2
Intel,nci2bk5,I'm aware but it's the only current gen GPU for graphics workloads that has virtualization support from AMD.,hardware,2025-09-05 04:23:02,5
Intel,ncifqdy,"The current Chairman of the board, Frank Yeary, is one of these stupid short sighted people. He REALLY wants to sell the fabs, and is probably the reason Intel went through their latest round of layoffs (Lip-Bu Tan wanted to raise money from Wall Street, Yeary apparently sabotaged it).",hardware,2025-09-05 06:14:10,8
Intel,ncbjmt2,"if corps are people, they should be allowed to vote, right ?",hardware,2025-09-04 04:39:39,2
Intel,ncbjx8i,"Not only that, but because they are people, they should also be able to fund gigantic super PACS to get a candidate into office. I love America!",hardware,2025-09-04 04:41:58,7
Intel,ncd1vg1,"u/michaellarabel It would be super cool to have Molecular Dynamics benchmarks for these kind of cards, since you already use them for CPU testing and a few of them (e.g. GROMACS) support APIs from all three vendors (CUDA, ROCm, SyCL, + OpenCL)",hardware,2025-09-04 12:29:05,4
Intel,ner1h1d,"I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other, the leader can change in a generation or two, which hasn't been the case often in recent history.  Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.",hardware,2025-09-17 18:36:40,56
Intel,ner49pn,"Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.",hardware,2025-09-17 18:50:20,70
Intel,ner8xmt,Youtube link:  [https://www.youtube.com/watch?v=Y9SwluJ9qPI](https://www.youtube.com/watch?v=Y9SwluJ9qPI),hardware,2025-09-17 19:12:31,17
Intel,nerwnif,I cannot wait to see what's in store for 2026 Mac Studios and the M5 CPU. Especially if M5 Ultra makes its debut. AI workloads should see a significant performance boost by 3-4x? I wonder if M5 Ultra will offer 1000GB/s memory bandwidth.,hardware,2025-09-17 21:04:08,36
Intel,ner54hk,6 wide e core holy shit,hardware,2025-09-17 18:54:23,30
Intel,neqzzpm,"A major exciting aspect for me is the massive boost to Raytracing performance. The M4 Max is the closest anyone has ever come to matching Nvidia in 3D Raytraced Rendering, beating out even AMD. In Blender M4 Max performs somewhere in between an RTX 4070M and 4080M.  A 56% leap in RT performance would essentially put an M5 Max closer to a RTX 5090M than anyone before at a fraction of the power.",hardware,2025-09-17 18:29:21,38
Intel,neugekl,"Is there a link for English? If not, can you summarize how they tested sustained performance and how much is the improvement over previous generations?",hardware,2025-09-18 06:51:14,6
Intel,netnyfa,"The E core having more improvements than just 50% larger L2 is a nice surprise, but damn the efficiency and performance of it is insane. 29% and 22% more performance, at the same power draw is insane, clocking like 6.7% higher too. They used to be behind the others in performance with the E cores but had better efficiency but now they both have better performance and efficiency.  As for GPU, I always wanted them to focus on GPU performance next and they finally are doing it. Very nice, the expected 2x FP16 performance, which now matches the M4 which is insane(M5 will be even more insane). Gpu being 50-60% faster is a nice sight to see. For RT performance(I still find it not suited for mobile but M5 will be a separate matter) I’m surprised that the massive increase is just from 2nd gen dynamic caching, the architecture of the RT core is the same, just basically a more efficient scheduler which improves utilization and less waste.  For the phone, vapor chamber is nice, them being conservative on having a low temperature limit can both be a good and bad thing which is shown, the good thing is that it means the surface temperature is lower so the user won’t get burned holding the device, and the bad thing is that it can leave performance off the table which is shown. As that can probably handle like another extra watt of heat and performance. Battery life is very nice, the fact that it can match other phones with like over 1000mAh bigger battery is funny. As people always flexing over how they have like a 4000, 5000mAh+ battery, of course having a bigger capacity is better, but the fact that Apple is more efficient with it and can have the same battery life at a much smaller battery speaks volumes about it.",hardware,2025-09-18 03:04:06,8
Intel,newpjmq,"The occupancy characteristics of A19 Pro are quite incredible. 67% occupancy for a RT workload.  Look at Chips and cheese's SER testing. 36-44% ray occupancy with SER in Cyberpunk 2077 RT Overdrive.     Assuming NVIDIA can get this working on 60 series an effective a 52-86% uplift. After OMM and SER this seems like the third ""low hanging"" RT fruit optimization. Anyone serious about a PT GPU architecture NEEDs dynamic caching like Apple. And no this is not RDNA 4's Dynamic VGPR, it's a much bigger deal. Register file directly in L1$ has unique benefits.",hardware,2025-09-18 15:59:37,4
Intel,nes030p,"Nice been waiting for this. P-core frontend improvements and branch, and a wider E-core with it's newer memory subsystem shows great YoY gains as usual. Though I am not surprised since it's been years leading up to this that Apple has steadily have been increasing power/freq to get the rest of it's performance gains, although IPC YoY is still class leading. The wider e-core takes the stage which is now commonly being focused in the industry (ex. Intel: Skymont etc). Excited for any outlet doing die analysis (I don't know if kurnal has done it yet).  Real generational GPU gains, instead of last year's YoY tick. Supposedly GPU size has not increased and that is impressive. Massive FP16 compute matching the M4, really shows their commitment to ML (as if naming 'tensor cores' wasn't obvious) and this will greatly help with prompt processing if you're into local models. Finally with a vapour chamber in the PRO models, performance overall really stretches it's legs and sustained is really respectable.  Also, since I'm skimming, I'm assuming A19 base like it's predecessor is a different SoC to the Pro. It is also really really refreshing to see the base A19 be better than the 18 Pro, little to no stagnation and a year at that. The base iPhone 17 looks like a reaaly reallly good option, more than ever, wished they didn't drop the Plus model. But man, I feel like waiting another year, hearing rumours about N2 and new packaging tech excites me.  That said, looking forward to QC, MT, and Samsung. SD8EG5 seems to be closing the gap, and that'll be very interesting tho those GB numbers don't tell things like power.",hardware,2025-09-17 21:21:03,9
Intel,nergppp,"""Generations ahead of other ARMs M cores"".   Uhm we are getting the Dimensity 9500 and 8 elite gen 5 next week    The C1 Pro has 20% IPC improvement IRRC, plus this is N3P   Let's not jump to conclusions before seeing the competition    I also wonder if QC made changes to the E cores",hardware,2025-09-17 19:49:30,18
Intel,nf0sz1w,>SLC (Last Level Cache in Apple's chips) has increased from 24MB to 32MB  Really tells you how pathetically stringent AMD has been with cache sizes on their apus (no die size excuse allowed here because they never use leasing nodes specially N4 was already old when Strix point debuted),hardware,2025-09-19 05:27:21,2
Intel,nerudqn,">A19 Pro E core is **generations ahead** of the M cores in competing ARM chips.  >A19 Pro E is 11.5% faster than the Oryon M(8 Elite) and A720M(D9400) while USING 40% less power (0.64 vs 1.07) in SPECint and 8% faster while USING 35% lower power in SPECfp.  >A720L in Xiaomi's X Ring is somewhat more competitive.  No, Apple's 202**6** E-cores are just **+3% (int perf)** and **+1% (fp perf)** vs Arm's 202**5** E-cores, though at **-22% (int)** and **-16% (fp)** less power.  Note: Geekwan's chart is wrong. The Xiaomi O1 does not use the A72**0.** It uses the upgraded A72**5** from the X925 generation. Not sure how Geekerwan got the name wrong, as they recently reviewed it.  Integer  |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%|  Floating point  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%|  I would not call this ""generations"" ahead.",hardware,2025-09-17 20:53:36,1
Intel,nezl76m,I always wonder how they plot the architecture map and figure out such as the depth of LDQ kind of things...Is it public somewhere? That kind of detail won't be able to get via regular benchmark right?,hardware,2025-09-19 00:45:37,1
Intel,nf76uxj,A19 Pro GPU is now only 1.62GHz vs 1.68GHz in A18 Pro while having the same number of ALUs (768). Does that mean the increased performance is basically due to memory bandwidth increase?,hardware,2025-09-20 05:23:42,1
Intel,nftq0t1,"At this point all the flagship phones are great. I have an iPhone 14 pro max and a Samsung Galaxy S23 Ultra. But I’ve had iPhones for years and i couldn’t switch if i wanted too i have way too much invested in ios (games, apps, music etc). But I ordered a 17 Pro Max 1tb thought about 2tb’s but have a 1tb now and still have 450gb’s free and i download everything and never delete anything. Im trading in the Samsung and giving iPhone to my mom. Still think of buying a cheap phone that gives me the $1100 trade in because it’s any condition and I hate trading in a phone thats practically new.",hardware,2025-09-23 19:51:55,1
Intel,ng9npz2,-21 freezer lol https://browser.geekbench.com/v6/cpu/14055289,hardware,2025-09-26 07:28:53,1
Intel,njvb52i,"The snapdragon 8 elite gen 5 still beats it though , and handily at that. Unless I am wrong",hardware,2025-10-16 21:51:22,1
Intel,nerrr9i,">Power however has gone up by 16% and 20% in respective tests leading to an overall P/W regression at peak.  That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade in floating point. The 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm:  |SoC / SPEC|Fp Pts|Fp power|Fp Perf / W|Perf / W %| |:-|:-|:-|:-|:-| |A19 Pro P-core|17.37|10.07 W|1.70 Pts / W|84.2%| |A19 P-core|17.13|8.89 W|1.93 Pts / W|95.5%| |A18 Pro P-core|15.93|8.18 W|1.95 Pts / W|96.5%| |A18 P-core|15.61|8.11 W|1.92 Pts / W|95.0%| |A17 Pro P-core|12.92|6.40 W|2.02 Pts / W|100%| |8 Elite L|14.18|7.99 W|1.77 Pts / W|87.6%| |O1 X925|14.46|7.94 W|1.82 Pts / W|90.1%| |D9400 X925|14.18|8.46 W|1.68 Pts / W|83.2%|  These are *phones*. Apple, Arm, Qualcomm, etc. ought to keep max. power in check. This is *o*n par with MediaTek's X925, a bit worse than the 8 Elite, and much worse than Xiaomi's X925.  I would've loved to see efficiency (joules) measured, like AnandTech did. That would show us at least if ""race to idle"" can undo this high 1T power draw or not in terms of battery drain.",hardware,2025-09-17 20:41:23,-4
Intel,nerpzuu,"All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P. Apple buys first dibs on the wafers so they always have that advantage, it isn't always about the architecture itself.  It will be more interesting when there are Qualcomm chips out with their architecture on this N3P node, and the Mediatek chips with the usual off the shelf ARM cores on this node too to compare.",hardware,2025-09-17 20:33:10,-7
Intel,nfc55oq,"Those GPU stats are false. According to Tom's Guide, in 3D Mark Solar Bay Unlimited, the 17 Pro Max is only 10% faster than the s25 ultra https://www.tomsguide.com/phones/iphones/iphone-17-pro-max-review#section-iphone-17-pro-max-performance-cooling",hardware,2025-09-21 00:14:10,-1
Intel,neqz076,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-17 18:24:39,-11
Intel,neqyojo,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-17 18:23:10,-14
Intel,ner4der,">I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Me too. I'd imagine a marginal increase in size over A18 Pro. The P cores have mostly stayed the same. And the E cores despite major changes are unlikely to contribute to a major increase in area (if I'm right, individually they occupy around 0.6-0.7mm2 per core). The extra cache (around 2MB) should increase area slightly. SLC area as well should contribute to that increase.  I'd imagine the GPU with the new RT units, doubled FP16 units, new tensor cores, and general uarch improvements are the major contributor to any notable area increase.  Plus I still don't feel like the approaches these companies are taking are aligned very much in terms of GPU architectures. For eg, Apple's been very focussed on improving compute performance on their GPU. Qualcomm less so.  >Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other,   True that. Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp. But Apple's done extremely well with this upgrade. The E core jump has made them close the nT gap with Qualcomm while using much lower power.  GPU is a case where technically Qualcomm could take raw perf crown. But Apple's RT dominance, Tensor cores and general compute lead might help them in the desktop space.  >Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.  Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs. They consume similar or even more area for their core architecture while lagging in performance while using significantly more power. The x86 ecosystem and compatibility is the only reason they'd survive Oryon.",hardware,2025-09-17 18:50:49,29
Intel,netwfkn,Having so many SoC makers on arm competing against each other by one upping each other yearly is bearing fruit vs x86. i don't know how Intel and AMD long term can fare at this rate. Arm CPUs are still showing double digit gains yearly.,hardware,2025-09-18 04:02:47,14
Intel,neu40ew,Most of apples RT gains are from optmsiing how the GPU deals with divergence.  This is not dedicated RT silicon so much as making the GPU be able to maintain much higher throughput when there is lots of divergence.  RT operations have a shit tone of divergence.,hardware,2025-09-18 05:00:56,6
Intel,nfntq8k,I can't wait to see die shots and measurements for these chips. The A18 Pro and A18 die shots were really interesting to see what was compacted or lost for the base model chip. I have a feeling that there will be bigger differences for the A19 Pro and A19 with that giant SLC on the former. Die areas will also be interesting. Cache isn't cheap for area and I'd also love to see inside the new E-cores and GPU.,hardware,2025-09-22 20:34:30,2
Intel,ner599v,">Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  I wouldn't say its a medium core yet tbh. Apple's E cores are still sub 1mm2 in area usage. Compared to other ""M cores"" they are still relatively small. I imagine the A19 Pro E core is fairly larger but the A18 E core was around 0.6/0.7mm2 in size. I'd imagine its not grown a whole lot.  >They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.  I'd imagine they're saving up these to mention Blender perf etc in M5 keynote.",hardware,2025-09-17 18:55:01,27
Intel,ner97f2,4 minutes ago. Damn. I should have waited. I'll post it!,hardware,2025-09-17 19:13:49,14
Intel,nerym62,"An M5 Ultra would offer 1.23 Tb/sec of bandwidth scaling from the A19 Pro.  M5 (128-bit, LPDDR5X-9600) -> 153.6 GB/s M5 Pro (256-bit, LPDDR5X-9600) -> 307.2 GB/s M5 Max (512-bit, LPDDR5X-9600) -> 614.4 GB/s M5 Ultra (1024-bit, LPDDR5X-9600) ->1228.8 GB/s",hardware,2025-09-17 21:13:39,24
Intel,ner5ngo,"Haha. I mean they are actually pretty late to this tbh. Most ""E/M cores from other competitors"" are similar in size if not bigger. I'd imagine Apple's E core is stuck between a true M core like the A7xx and a true E core like A5xx in terms of area, although it probably leans toward the A7xx in that regard.",hardware,2025-09-17 18:56:53,19
Intel,nesz0ua,What does 6 wide mean? What units?,hardware,2025-09-18 00:36:10,1
Intel,nesjazw,"[https://www.reddit.com/r/hardware/comments/1jcoklb/enable\_rt\_performance\_drop\_amd\_vs\_nvidia\_20202025/](https://www.reddit.com/r/hardware/comments/1jcoklb/enable_rt_performance_drop_amd_vs_nvidia_20202025/)  In gaming RDNA4 RT isn't that far behind Blackwell. Other than that raytraced rendering like in Blender AMD has been for a while far behind. It won't be until Blender 5.0 till we see any improvements to HIPRT. Though for the longest time since following HIP it's been rather mediocre and my expectations are low for next release, though their [PRs](https://projects.blender.org/blender/blender/pulls/145281) make it seem they've been doing some work. It's a low priority for AMD which is unfortunate.",hardware,2025-09-17 23:06:05,8
Intel,ner9dq0,>	beating out even AMD  Was that one really surprising?,hardware,2025-09-17 19:14:39,2
Intel,nes24jj,Is that Metal vs Optix or Metal vs Cuda?,hardware,2025-09-17 21:31:31,1
Intel,neumrwz,"The video has a mostly accurate English captions option. CPU P core is up 10%, E core is up by 25%, GPU perf is up by 40% and sustained performance is up by 50%.",hardware,2025-09-18 07:52:53,10
Intel,neu4z8x,> just basically a more efficient scheduler which improves utilization and less waste.  When you take a look at GPUs doing RT task you see that they tend to be very poorly utilized.  GPUs are not designed for short running diverging workloads. But RT is exactly that. So you end up with a huge amount of divergence and or lots of wave like submissions of very small batches of work (so have a large scheduling overhead).     There is a HUGE amount of perfomance left on the table for this type of task for HW vendors that are able to reduce the impact on GPU utilization that divergence has.,hardware,2025-09-18 05:08:55,5
Intel,newyqf1,Maybe Apple measures occupancy differently in their tools. I wouldn't be too sure comparing these two. But I'd definitely think a combination of SER and Dynamic Caching present in A19 should result in very good utilization compared to other uarchs.,hardware,2025-09-18 16:43:46,4
Intel,nfd8klj,"That is very impressive, RT tends to have very poor occupancy as it is a heavily branching workload!",hardware,2025-09-21 04:40:04,2
Intel,netw9xh,>Supposedly GPU size has not increased and that is impressive.   Important to note that the supporting SLC which is a major reason for improvements om the GPU side has increased from 24Mb to 32Mb. Which would increase area a bit.,hardware,2025-09-18 04:01:38,6
Intel,nerkv5x,">The C1 Pro has 20% IPC improvement IRRC, plus this is N3P  N3P is 5% faster than N3E. By TSMC's own claim..  Also I can't find a source for a 20% IPC improvement. ARM's claim is 16% IPC improvement. And that is not without a power cost since ARM claims that at similar performance, power reduction is only 12%.  https://newsroom.arm.com/blog/arm-c1-cpu-cluster-on-device-ai-performance  >Let's not jump to conclusions before seeing the competition   I mean I agree. But I don't see how the C1 Pro is supposed to cross a 95% P/W disparity. (4.17 points using 0.64W vs 3.57 points using 1.07W) using D9400",hardware,2025-09-17 20:09:09,16
Intel,nf0tzxz,"Tbf, Apple had a 32Mb SLC back in the A15 Bionic. They reduced the size of that afterward to 24Mb. Its not like the size significantly mattered in GPU performance until now.",hardware,2025-09-19 05:35:43,1
Intel,nery3xl,A 30% lead in P/W is a generations ahead in this day and age. Considering the successor (C1 Pro) is stated by ARM to reduce power by just 12% at iso performance leaving Apple with a comfortable lead for a year. Also I specifically was a bit confused by their choice to compare the A725L instead of the M variant.,hardware,2025-09-17 21:11:11,14
Intel,nezsmw1,There's a guy on twitter who does the microbenchmarking for them.,hardware,2025-09-19 01:28:21,2
Intel,nf79l31,"No. I'm positive memory bandwidth offers very little in terms of performance upgrades. If you recall, the A16 was essentially the same GPU architecture as the A15 but used LPDDR5 instead of LPDDR4X, yet there were practically zero performance improvements.  I don't think anyone has investigated the A19's GPU microarchitecture thoroughly. But the main improvements seem to come from the increase in SLC size (System Level Cache which serves the GPU) from 24Mb to 32Mb and the newly improved Dynamic Caching. Its very likely there are a lot more changes responsible for that 40%+ improvement that we don't know about.",hardware,2025-09-20 05:47:18,1
Intel,nfd8co2,With most GPU tasks you are never ALU limited.,hardware,2025-09-21 04:38:19,1
Intel,nl1tnnh,"8 elite gen 5 loses in ST perf, only matches in MT perf at same power, wins slightly in GPU perf, loses in GPU RT perf. It doesn't beat it handily by any metric.",hardware,2025-10-24 00:05:07,1
Intel,nerth3n,">That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade and the 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm  Technically we need to compare P/W at similar power or similar performance. Peak power P/W is not a very accurate measure to compare gen on gen. And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.   An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.",hardware,2025-09-17 20:49:22,17
Intel,netxg4n,Im having my doubts here. Howis the A19 p core much better than the A19 pro p-core? Aren’t they exactly the same p cores?,hardware,2025-09-18 04:10:12,3
Intel,nerrcls,">All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P.   N3P is a mere 5% faster than N3E when comparing similar microarchitectures... This is straight from TSMC's marketing slides.  Comparitively, barring the P core (which did see an ok improvement), the E core and the GPU have seen 30%+ improvements. The node has nothing to do with it.  If Qualcomm loses to, matches or exceeds A19 Pro this year it would be because of their updated microarchitectures and have barely anything to do with minor single digit improvements offer by a sub node.",hardware,2025-09-17 20:39:28,15
Intel,neu4jji,in the end what matters is the sicion you can buy so you can compare them.,hardware,2025-09-18 05:05:19,2
Intel,nf0v0x1,Please tell us more about how we can never compare AMD vs Intel chips by your logic,hardware,2025-09-19 05:44:15,2
Intel,nfcfd2t,"Tom's Guide tested basic Solar Bay. This is the older version of the benchmark with less raytraced surfaces.  Geekerwan tested the modern, updated version of Solar Bay referred to as Solar Bay Extreme. This new benchmark has a much higher raytraced load, with far more reflective and transparent surfaces and much more detailed scene with more geometry.  Please kindly read the benchmark title mentioned in the posts. Or atleast watch the videos. Before commenting.",hardware,2025-09-21 01:17:53,1
Intel,nf0v37m,Bad bot,hardware,2025-09-19 05:44:47,1
Intel,ner7ckc,"I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  I just wouldn't call the race so early, but it does seem very likely, that AMD will be behind. I just dont think it is as bad as it seems. AMD was plagued by Zen5% and still on 4nm for client, they might hit heavy with client dedicated improvements and N3, but in the end we have to see, x86 client performance really seems to struggle rn (and whatever intel is doing...).",hardware,2025-09-17 19:04:59,7
Intel,nernd6v,">Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs.  Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.  Also... Let's not forget about ARM. A SoC with ARM CPU cores and an Nvidia/AMD GPU could absolutely ruin Qualcomm's day regardless of how better/worse their custom CPU cores are.",hardware,2025-09-17 20:20:52,5
Intel,nhafpsx,>Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp  No,hardware,2025-10-02 01:16:15,1
Intel,nexnqt0,"IMO, Microsoft needs to push hard to switch to Windows-on-ARM, or else they risk an Android-like OS for Laptops swooping in and filling the gap left by those who do not want to go the Apple route. It feels like a crucial moment for both Windows and Intel/AMD, at least in the x86 product space. It retains PC Gaming at this point, but if that nutt is cracked via even half-decent, *compatible* emulation, then ... sayonara!",hardware,2025-09-18 18:41:54,5
Intel,ner8bhp,"Yeah, the E core isn't quite there yet, but it's good to know they're moving in that direction with now stubborn they are with P cores, refusing to include more than what can be run at the equivalent of all core turbo.",hardware,2025-09-17 19:09:33,11
Intel,nes3cj4,"Wow that will be insane and adding the new ""Tensor"" elements added to the GPU cores will make it a formidable AI workstation.  Especially when NVIDIA monopoly is only offering small VRAM GPU cards at absurd prices.",hardware,2025-09-17 21:37:52,16
Intel,neu4a08,That would be amazing. Id love to see them put some hbm there too,hardware,2025-09-18 05:03:07,1
Intel,neuqao2,It’s highly unlikely they will go past a 256 bit bus. You run out of pins and layers to route a bus that wide. Gets extremely expensive. Still neat bandwidth!,hardware,2025-09-18 08:28:29,1
Intel,nfjr9p6,The m5 ultra should be on par with a 4090?,hardware,2025-09-22 04:50:52,1
Intel,nerh62x,Skymont (Intel’s E core) is 8 wide (9 wide at first stage then bottlenecks to 8 I think iiuc),hardware,2025-09-17 19:51:36,12
Intel,nevbky1,"According to AnandTech, Apple's E cores were based on Apple's Swift core, back when they had a single core type  Previous die shots show Apple's E cores were close to Arm's A5xx in core area (larger, but far smaller than A7xx core only). But in terms of core+L2 Apple's E cores are similar to Arm's A7xx in core+L2 area  I'd argue it's the other way around, Apple's E cores are the true E cores  Whereas Arm's A7xx were stuck between being an M core and an E core  Now Arm has split their A7xx core to Cx Premium (M core) & Cx Pro (true E core)  Arm's A5xx/Cx Nano are a very odd core, almost no else makes a similar in-order core. Arm's A5xx/Cx Nano are more like Intel's LP E cores, instead of Apple's E cores",hardware,2025-09-18 11:35:22,7
Intel,nesz8nh,The decoder,hardware,2025-09-18 00:37:26,12
Intel,neuonvq,"Amd is very far behind in rt.  You re linking gaming benchmarks, thats not rt thats mixed use.    Just look at path tracing results for a more representative comparison",hardware,2025-09-18 08:11:51,13
Intel,ner9wud,"Hey, they made an effort with RDNA 4. I think that should surpass the M4 Max. I just can't find any proper scores for it.",hardware,2025-09-17 19:17:13,19
Intel,nes2dxt,"Metal vs Optix.  https://youtu.be/0bZO1gbAc6Y?feature=shared  https://youtu.be/B528kGH_xww?feature=shared This is a more detailed video with individual comparisions and a lot more GPUs.  Its a lot more varied. In Lone Monk, it hangs with a desktop class 5070. In Classroom, it hangs neck to neck with a 4060Ti. In Barbershop, it falls behind a desktop 4060Ti. In scanlands, it falls behind a 4060.   If we consider Classroom as a baseline average, a theoretical 60% faster M5 Max, like the jump we saw in Solar Bay, would land hot on the heels of a desktop class 5070Ti, a 300W card. Competing with a 65W laptop GPU.  Edit; The Youtuber is using the binned 32C variant. A 40C variant would surpass the 5070ti.",hardware,2025-09-17 21:32:51,6
Intel,neuej06,"Yeah I forgot what was the term before but I remember, it’s just like Nvidia’s Shader Execution Reordering introduced in Ada Lovelace.",hardware,2025-09-18 06:33:54,1
Intel,newy5n1,I have a query regarding RT workloads. Would offsetting RT performance to the CPU with the help of accelerators help? Or is that not the case and it would be even worser on CPUs.,hardware,2025-09-18 16:41:01,1
Intel,nexio2d,"Sure it might be different, but I doubt it. Occupancy is just threads used/total threads.  It's interesting how first gen dynamic caching + SER (apple equivalent) is hardly better than NVIDIA in terms of occupancy. Yet only 44%. So only slightly better than NVIDIA (upper end of range). Seems like first gen was more about laying the groundwork while second gen is really about pushing dynamic caching allocation granularity and efficiency. At least so it seems.  Oh for sure. That occupancy is incredibly high. \~1.5x uplift vs A18 Pro. Getting 70% occupancy in RT workload is really unheard of. Apple engineers did a fine job.  AMD might opt for this nextgen if they're serious, but it's a massive undertaking in terms of R&D, but could massively benefit PT and branch code, ideal for GPU work graphs.",hardware,2025-09-18 18:17:21,4
Intel,ngmjqfa,Agreed and you can see that by comparing with occupancy numbers for competitors.       Anyone who's serious about RT needs to copy whatever Apple is doing xD,hardware,2025-09-28 10:08:16,1
Intel,nez7dv6,"C1 Pro is two generations ahead of the A720 in the 9400. Also, Xiaomi demostrated a much more efficient implementation of the A720 cores in their O1 chip (4.06 points at 0.82 W).  Edit: actually, it seems like the O1 uses A725 cores. Perhaps that is what they are referring to in the video as ""A720 L""",hardware,2025-09-18 23:23:49,2
Intel,nf0vdfo,It’s been a BIG bottleneck in AMD’s apus since Vega 11 back in 2018. Doubling from 8CU to 16CU in 860m vs 890m gets you only +30%.   AMD is just so damn stringent with area despite jacking up the price on Strix point massively on an old ass node.,hardware,2025-09-19 05:47:13,2
Intel,nes028x,"You ought to do the math first. Power is the denominator. 12% reduction in power is *substantial*.  Integer: A19 Pro E-core is 3% faster at 12% less power vs claimed C1-Pro.   |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%| |\-12% power|4.06|100%|0.72W|5.64|114%|  Floating point: A19 Pro E-core is 1% faster at 4% less power vs claimed C1-Pro.  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%| |\-12% power|6.07|100%|0.96 W|6.32|113%|  Hardly ""generations ahead"".",hardware,2025-09-17 21:20:56,-2
Intel,nf7bmfd,Thanks for your reply.  CPU monkey reported 2GHz instead of 1.62GHz. So maybe that's where most of the gain comes from.  I suppose the tensor core like matmul units also boost performance for graphics and AI.,hardware,2025-09-20 06:05:07,1
Intel,nfdgjzr,Maybe the improvement is from the new matmul units?,hardware,2025-09-21 05:48:36,1
Intel,nerzs8z,"I am deeply suspicious of all of these power measurements. Separating p core power usage from other aspects of the soc is difficult on iOS. Which is not a criticism of your summary, but I’d be wary of drawing anything definitive about the efficiency of the p cores.",hardware,2025-09-17 21:19:32,4
Intel,nesrzwb,"It's not a knock on the design, it's how apple is configuring the CPU. It doesn't matter that performance at the same power is improved if the default clocks on the real product put it way past the sweet spot into diminishing returns so bad it regresses efficiency.   On one hand, the power inflation isn't causing problems if the 23% increased battery life per Wh is anything to go by, but on the other, what's the point of chasing peak performance like this if your boost/scheduling algorithms never allow that speed to make an impact on responsiveness?",hardware,2025-09-17 23:55:18,1
Intel,nery8a6,"Geekerwan's results are ***average*** power, not peak power, IIRC. These are real, actual frequency bins that Apple has allowed.  These frequency bins *will* be hit by some workloads, but just not nearly as long as SPEC & active cooling will allow. It would be good to revisit Apple's boosting algorithms, but IIRC, they hit 100% of Apple's design frequency in normal usage.  It's not like users have a choice here; we can't say, ""Please throttle my A19 Pro to the same power draw as the A18 Pro."" Low power mode neuters much of the phone, so it's rarely used all the time.  //  I find avgerage power useful two reasons:  1. *How* performance vs power were balanced; here, performance took precedence while not keeping power stable.  2. It also shows, when nodes are not the same, where the node's improvements went. Here, an N3P core delivers notably worse perf / W versus an N3E core. TSMC claims [up to 5% to 10% less power](https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track) on N3P vs N3E.  I agree 10W is not common and SPEC is a severe test, but it's more the *pattern* that has emerged on Apple's fp power and whether it's worth it:  2023 - A17 Pro P-Core: 6.40W  2025 - A19 Pro P-Core: 10.07W  Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   >And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.  I agree it's rare, but why would Apple allow 10W? Were many workloads ***lacking*** fp performance that users prefer a bit less battery life for +9% fp perf vs the A18 Pro?  Of course, to most, battery life is more important, IMO, which is why core energy is most crucial, but missing here.  //  >An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.  So the question becomes: do users want slightly better perf and 5% more power? On a phone, I'm of the opinion that power is paramount and should be forced to lower levels.",hardware,2025-09-17 21:11:46,0
Intel,neu0m6n,">Aren’t they exactly the same p cores?  Definitely not. See the SPEC perf / GHz: A19 is nearly just the A18 Pro. Thus, this seems to be the final picture:  A19 Pro = new uarch, 32MB SLC, 12GB LPDDR5X-9600  A19 = binned OC of last year's A1**8** Pro, w/ faster LPDDR5X-8533, but smaller SLC (12MB)  New uArch clearly didn't pan out as expected in fp efficiency. A19 Pro may sit at a flatter part of the freq / power curve, A19 Pro may have more leakage, A19 Pro's [faster RAM](https://en.wikipedia.org/wiki/Apple_A19) may eat *a lot* power (Geekerwant tests mainboard power, not purely CPU power), etc.",hardware,2025-09-18 04:34:04,1
Intel,nfcgk7w,I'm sorry you had to take time away from ripping off others' content to correct my mistake,hardware,2025-09-21 01:25:24,-1
Intel,ner89fq,">I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  M2 and Zen 4 launched around the same period. The desktop chips score around 5-10% faster in Geekbench while using 20W per core power and 30-40W more for the I/O die. Taking the ST crown by a hair's width while using 5-10x more power isn't a win at all imo.",hardware,2025-09-17 19:09:17,8
Intel,nerolok,">Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.   I'd agree if they performed as well as their clocks suggest. Whats the point of clocking to 5.7Ghz if a mobile CPU clocked at 4.4Ghz leads you in absolute performance by 15% (Geekbench) while using a tenth of the total power.",hardware,2025-09-17 20:26:36,9
Intel,nezdr68,This snapdragon era is Microsoft's third time trying arm (windows rt and surface pro x). Hopefully third times the charm,hardware,2025-09-19 00:01:23,6
Intel,nes52e4,Their E-core is faster than something like Tiger lake per clock. It’s about 60-70% as fast as the best desktop CPU from 2020 and probably as fast as a U-series chip from then under sustained loads.  The real takeaway for me is that the rumored A19 MacBook is going to dominate the $600 laptop market segment if it releases before Christmas.,hardware,2025-09-17 21:46:45,16
Intel,ner8qa3,"Weirdly enough, I feel like them not letting the E core balloon in size has helped them in the long run. Seems they're focussed on maximizing performance within a limited area. Their E cores have had exceptional jumps in performance since the A12 almost every generation being 25%+ in improvements.  I'd wager that E core dominance is the primary reason why the iPhones are able to match Android manufacturers using super large and dense batteries in terms of endurance.",hardware,2025-09-17 19:11:31,17
Intel,neu2ajk,Doesn't mac studio already 500gb vs nvidia workstations of 96gbs?,hardware,2025-09-18 04:47:07,3
Intel,neskg6l,NVIDIA currently fist pumping the air over their failed ARM acquisition rn,hardware,2025-09-17 23:12:34,4
Intel,neun1xa,Its a bit unlikely. Maybe for a version of the M series dedicated for a Mac Pro. But one of the main reasons they can get away with this design is because its very scalable. All the way from A series to Mx Max series. Adding HBM would probably require a dedicated SoC redesign for a very niche product segment in Macs.,hardware,2025-09-18 07:55:36,4
Intel,neuqz81,The M4 Max already uses a 512 bit bus. Does it not?,hardware,2025-09-18 08:35:23,3
Intel,neri14i,Its technically not a true 9 wide core. I think its 3+3+3.,hardware,2025-09-17 19:55:40,15
Intel,nes6z1y,E-Core is relative. Skymont is more of a C-core (area optimized) than what we typically think of as an E-core (energy optimized).,hardware,2025-09-17 21:56:51,13
Intel,neu4ctl,with a viable width ISA it is better to look at the typcile throughput not the peak throuput as you very rarely are able to decode 8 instructions per clock cycle.,hardware,2025-09-18 05:03:46,2
Intel,nerdt8m,"With the 9070? I don’t think I’ve seen any results showing that either, however all I’ve looked at is the blender benchmark charts",hardware,2025-09-17 19:35:44,1
Intel,neuqhpy,"the shader re-ordering is different. (apple also do this).  Even with shader re-ordering you have the issue that you're still issuing 1000s of very small jobs.    GPUs cant do lots of small jobs, they are designed to do the same task to 1000s of pixels all at once.     If  you instead give them 1000 tasks were each pixel does something differnt the GPU cant run that all at the same time... in addition the overhead for setup and teardown of each of these adds even more void space between them.   So apple are doing a hybrid approach, for large groups of function calls they do re-ordering (like NV) but for the functions were there is not enough work to justify a seperate dispatch they do function calling.    This is were dynamic coaching jumpstart in.      Typical when you compile your shader for the GPU the driver figures out the widest point within that shader (the point in time were it need the most FP64 units at once, and register count).  Using this it figures out how to spread the shader out over the GPU.  Eg a given shader might need at its peak 30 floating pointer registers.   But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time.   If you have a shader with lots of branching logic (like function calls to other embedded shaders) the driver typically needs to figure out the absolute max for registers and fp units etc.  (the worst permutation of function branches that could have been taken).  Often this results in a very large occupancy footprint that means only a very small number of instances of this shader can run at once on your GPU.  But in realty since most of these branches are optional when running it will never use all these resources.  The dynamic cache system apple has been building is all about enabling these reassures to be provided to shaders at runtime in a smarter way so that you can run these supper large shader blocks with high occupancy as the registers and local memory needed can be dynamically allocated to each thread depending on the branch it takes",hardware,2025-09-18 08:30:29,8
Intel,ney7oxo,"While RT does have lots of branching logic (and CPUs are much better at dealign with this) you also want to shader the result when a ray intercepts, and this is stuff GPUs are rather good at (if enough rays hit that martial)   We have had CPU RT for a long time and so long as you constrain the martial space a little GPUs these days, even with all the efficiency  loss are still better at it.   (there are still high end films that opt for final render on cpu as it gives them more flexibility in the shaders they use) but for a game were it is all about fudging it GPUs are orders of magnitude faster, you just have so much more fp compute throughput on the GPU even if it is running at 20% utilization that is still way faster than any cpu.",hardware,2025-09-18 20:16:47,5
Intel,ngmxbr9,"Its not just useful for RT but also for a lot of other situations, being able to just call out to functions on the GPU and not pay a HUGE divergence penalty (you still pay some) opens up GPU compute to a load more cases were we currently might not bother.",hardware,2025-09-28 12:07:27,2
Intel,nes14p7,I'm a bit confused. You think lagging 15% behind in P/W the competition for an entire year is not being a generation behind?  ARM themselves have managed only a 15% jump this year. So it will essentially be 2 years before we get an E core that matches the A19 pro. And this is just considering the Xiomi's SoC. Mediatek's and Qualcomm's which dominate the majority of the market lag even further behind.,hardware,2025-09-17 21:26:24,9
Intel,nf7c0uo,I'd advise against using CPUmonkey as a reliable source. They're known to make up numbers. (Reported M1 Pro/Max Cinebench scores 6 months before they launched based on predictions),hardware,2025-09-20 06:08:45,1
Intel,nes4cv8,"I agree to some degree; Geekerwan notes they are testing mainboard power, not core power (if you Google Translate their legend).  For me, I assume all the major power draws on the motherboard *are* contributing to the overall SPEC performance, too.   If the faster LPDDR5X-9600 in the A19 Pro eats more power, it's fair to include that power because ***all*** A19 Pros will ship with LPDDR5X-9600. That was Apple's choice to upgrade to this faster memory.    Now, you're very right: we can't purely blame the uArch at Apple. It may well be the DRAM or the boost algorithms (like we saw at the A18 Pro launch last year) and—at Apple specifically—even the marketing overlords.  It's also why I'm a big proponent of JEDEC speeds & timings & volts in desktop CPU tests, much to the chagrin of a few commenters.",hardware,2025-09-17 21:43:06,7
Intel,nes0tlj,Why is separating p-core power usage from SOC power uniquely difficult on iOS?,hardware,2025-09-17 21:24:50,1
Intel,nes018z,I understand your reasonings. But its the only semblance of comparison we have to date between different SoC. I've learned not to look a gift horse in the mouth.,hardware,2025-09-17 21:20:48,1
Intel,nerz3p1,">Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   >That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   I mean this has been a trend long before the A17. Apple has been increasing peak power little by little since the A12 Bionic.  I remember reading Anandtech articles about it.",hardware,2025-09-17 21:16:05,5
Intel,nfcj3et,Don't be a sour puss now because you didn't check your sources before commenting. Mistakes happen.  >ripping off others' content   Eh? Are you stupid? You're pissed that someone posted a hardware review ON A hardware sub? The entire sub exists to discuss hardware bozo.,hardware,2025-09-21 01:41:28,2
Intel,nerbuxh,"While the efficiency for this is BAD, I dont think its 20W per core.  When we look at results by [Phoronix](https://www.phoronix.com/review/amd-zen4-zen4c-scaling/2) we can see \~7-8W per core for this (not great numbers, because its a weird chip and different node), which is still very bad. AMD certainly has some power issues, but many of which, i.e. inefficient I/O dies, are not really dependent on the CPU uArch and could switch at any moment. They certainly have much more inefficient chips at the moment than both Apple and Qualcomm. For Zen 6 we expect a major update to the desktop chiplet architectur which could bring some much needed improvements in terms of I/O though.  They have reasonably fast cores, and I think they are not in a terrible position, even though it is far from good. I think what is interesting for AMD to look out for is that they keep moving fast, instead of intel who didnt move fast since like 14nm, and AMD has strong cores. Additionally AMD has (including from the datacenter) an enterprise need to make the CPUs more efficient.  So yeah, very bad CPUs efficiency wise. A bit behind, but not terribly on perf per node wise, efficiency on the desktop is an afterthought for AMD, clearly, but they are moving constantly and are improving. It might be AMD Laptop Zen 6 has again like 35W TDP, for 3000 Geekbench and be dead, but with some client oriented tweaks I see chances (maybe just from the patterns in the tea leaves in my mug)",hardware,2025-09-17 19:26:30,6
Intel,nerrlad,"I originally saw this on Phoronix' forums, but I can't find the link to the comment so I'll send this one instead: [https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png](https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png)  Zen 5 is now behind, yes, but it isn't really that bad.",hardware,2025-09-17 20:40:36,5
Intel,nf8dztb,"Per clock, the A19 Pro E core is competitive with Golden Cove. Atleast in SPECint.  M4 E core-> 3.53 points at 2.88Ghz (1.23 p/Ghz) or IPC i9 14900k-> 9.93 points at 6.00Ghz (1.65 p/Ghz) or IPC  A19 Pro has a 22% jump in IPC in SPECint (might be some variations due to time difference and lack of knowledge about the subtests ran, but still gives a good picture)  22% IPC jump over M4/A18 E core = 122% of 1.23 = 1.50 A19 P core = 1.50 p/Ghz  Golden Cove in i9 14900K has a mere 10% lead over A19 Pro in perf/clock in SPECint.  https://youtu.be/EbDPvcbilCs?feature=shared  Source: I9 14900K compared with M4 in this review.",hardware,2025-09-20 11:59:43,3
Intel,neu482t,It does but the Macs are limited in other ways (memory speed among other things),hardware,2025-09-18 05:02:41,3
Intel,netuj31,It wasn't for a lack of trying.,hardware,2025-09-18 03:49:13,8
Intel,nf2fm06,Yeah there was some rumors of a server version and thy have a patent for a multi level cache setup but they also patent and prototype plenty of things that never get released.   https://www.techpowerup.com/277760/apple-patents-multi-level-hybrid-memory-subsystem,hardware,2025-09-19 13:24:45,2
Intel,neuu5xp,"Oh huh, it does but it’s 128 bit per channel. So memory on 4 sides of the die. Wild, don’t see that normally except in FPGA for data logging (or GPUs)",hardware,2025-09-18 09:07:21,2
Intel,netvlsr,"The difference seems a bit drastic in open data benchmarks.   https://youtu.be/B528kGH_xww?feature=shared  Testing individual scenes, the 9070xt and M4 Max seem neck and neck.  The M4 Max at best (in Lone Monk) is 5070 desktop class and at worst (in Scanlands) is 4060 desktop class. On average, I'd say in Blender, it is neck and neck with an RTX 4060Ti desktop card. I think a theoretical M5 Max should be on par with a 5070Ti if we see the same 60% bump in RT performance.",hardware,2025-09-18 03:56:47,3
Intel,nerhwya,Apparently Cinebench 2024 GPU is not compatible with RDNA4 cards lol. So I can't find any scores to compare.,hardware,2025-09-17 19:55:07,1
Intel,newfwdv,"So does dynamic caching ensure that the total size will ""always"" be the same as whats being called? As in certain cases it is still possible that there can be wastage like for the example you said ""Eg a given shader might need at its peak 30 floating pointer registers. But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time."" on that, there would be 10 registers wasted doing nothing, if it cant find any else thats <10 registers to fit in that.",hardware,2025-09-18 15:14:05,2
Intel,ngnf1x5,Yeah you're right. Should've said branchy or low occupancy code.  Combining such a HW side change with change on SW side with GPU work graphs API could indeed open up many usecases and new possibilities that are well beyond the current way of doing things. I can't wait to see what Apple does when Work Graphs arrives in a future Metal release.,hardware,2025-09-28 14:00:56,2
Intel,nesav03,"Well when you put it like that (13-14% behind in efficiency) saying ""generations behind"" certainly sounds misleading.  All it would take for another arm vendor to beat that is jumping one node(let) earlier than Apple, which is certainly doable given the lower volumes, although traditionally it rarely happens. Or jumping earlier to anything that might give them an advantage really, e.g. lpddr6.",hardware,2025-09-17 22:18:15,5
Intel,nesu2qs,"It's not ""generation**s**"" behind as you originally wrote. It's being compared to cores from **a year ago** already, mate.  I fully expect MediaTek to adopt C1-Pro and Qualcomm for sure will also update.  Apple's E-cores are simply nowhere near as dominant as they used to be in the A55 era.  EDIT: before we speculate more using marketing for Arm and hard results for Apple, let's check back in a few months to see how C1-Pro  *actually* performs and how Qualcomm's smaller cores actually perform.",hardware,2025-09-18 00:07:30,4
Intel,netwip5,Its a bit harder since we don't have any A19 Pro die shots yet. But Apple's E cores have always been sub 1mm2 compared to A7xx cores.,hardware,2025-09-18 04:03:25,2
Intel,nes5hmu,Yeah. All fair points. I don’t disagree. It’s still fun to speculate!,hardware,2025-09-17 21:48:58,3
Intel,nes2dlk,"Because we don’t have the tools. On macOS Apple provides powermetrics but Apple states that the figures can’t necessarily be relied on. On some very specific tests you can narrow down power to a cpu core, kinda. Spec tests often stress other aspects like memory, so I would use the provided figures as a guide. Either as a “p core bad” or “p core good” conclusion.",hardware,2025-09-17 21:32:48,1
Intel,nes1r2v,"Oh for sure. It’s not a criticism of either Geekerwan or yourself. They are doing a great job with the available options and I appreciate your summary. I just find it a little amusing when people dissect milliwatt differences as absolutely accurate. We just don’t have the tools, and people are keen to jump on the “p core doomed” bandwagon.",hardware,2025-09-17 21:29:35,1
Intel,nes2xxk,"The power increaseas are definitively (and definitely) accelerating, though, worse than it used to be.  [**Geekerwan P-core power**](https://youtu.be/iSCTlB1dhO0?t=422) **| SPECint2017 floating point**  *% is from the previous generation*  A14: 5.54W  A15: 5.54W (+0% power YoY)  A16: 6.06W (+9% power YoY)  A17 Pro: 6.74W (+11% power YoY)  A18 Pro: 8.18 W (+21% power YoY)  A19 Pro: 10.07 W (+23% power YoY)  //  AnandTech did the *great* work of measuring energy consumption / joules. That really proved that race to idle was working; more instanteous power under load, less overall energy   [**AnandTech P-core Power | SPECint2017 floating point**](https://web.archive.org/web/20240907062349/https://www.anandtech.com/Show/Index/16983?cPage=15&all=False&sort=0&page=2&slug=the-apple-a15-soc-performance-review-faster-more-efficient)  A14: 4.72W, 6,753 joules  A15: 4.77W, 6,043 joules (+1% power YoY, -11% energy YoY)  Average power went up, but energy consumption went down.",hardware,2025-09-17 21:35:43,2
Intel,nfcjm81,You copied and pasted someone else's data and now you're acting like a hero,hardware,2025-09-21 01:44:49,-1
Intel,net2bpw,"Still very bad used to work in their IO team, and the idle eatt performance is still very bad. Their idle core loading is also not as good as I would expect for their next generation, and its sad to see the future generation lose to even on M3 on benchmarks. The LP core was supposed to help in this situation, but I just can't see past the fact on why the gap is actually widening over time. I hope Intel can come in and close this gap.",hardware,2025-09-18 00:55:13,5
Intel,neru5qx,"The 9950x lags by 8% behind the M4. M5 is another 10% on top of this lead. M series chips use around 8W of power in total to achieve this perf including memory and what not. 9950x is like 20W per core, another 40W for the I/O and another unknown amount for mempry.",hardware,2025-09-17 20:52:33,4
Intel,nf8k821,I was comparing overall score.  Geekerwan got 4.17 at 2.58GHz which is ~1.62pt/GHz which is a little higher than your speculation and a mere 1.8% IPC difference from that Golden Cove number.  The fact that it did this at 0.64w in such a tiny core is absolutely incredible. It once again raises the question about how much ISA matters. I don't see any x86 E/C-cores getting anywhere close to those numbers.  https://youtu.be/Y9SwluJ9qPI?t=260,hardware,2025-09-20 12:42:15,1
Intel,neu5yj1,one will think the massive vram capacity just override the disadvantages.,hardware,2025-09-18 05:17:08,1
Intel,ney6vfr,"dynamic caching would let more copies of the shader run given that is knows the chances that every copy hits that point were it needs 30 registers is very low.   If that happens then one of those threads is then stalled but the other thing it can do is dynamicly at runtime convert cache, and thread local memroy to registers and vice versa.   So what will happen first is some data will be evicted from cache and those bits will be used as registers.   maybe that shader has a typical width of just 5 registers and only in some strange edge case goes all the way up to 30.   With a width of 5 it can run 20 copies on a GPU core that has a peak 100 registers.",hardware,2025-09-18 20:12:58,3
Intel,ngpzxjn,"Metal has supported GPU side dispatch for a long time, (long before work graphs were a thing in DX) Barries and fences in metal are used by the GPU to create a dependency graph between passes and this is resolved GPU side (not CPU side).   I don't see the explicit need for some extra feature here as we already have \*and have had for a long time since metal 2.1\*",hardware,2025-09-28 21:29:23,2
Intel,nevepm7,"Historically, the core only area for Apple's E cores have always been between Arm's A5xx & A7xx cores, but closer to Arm's A5xx  But for core+L2 (using sL2/4), Apple's E cores have always been similar to A7xx cores in mid config",hardware,2025-09-18 11:56:17,1
Intel,nes29nv,"I agree that the presence of inaccuracies is very likely. And I certainly don't think the P core is doomed for a 10% jump in what is essentially a a very minor node upgrade.  But considering the video does go into the P core's architecture where the only substantial changes were the size of the Reorder Buffer and a marginally better branch predictor, the performance numbers make sense.",hardware,2025-09-17 21:32:15,4
Intel,netbw6q,This is a combination of the meaningless smartphone benchmark game (95% of users would be perfectly fine with 6 of Apple's latest E-cores) and the need to have a powerful chip in laptops/desktops all sharing the same core design.,hardware,2025-09-18 01:50:19,0
Intel,nfckcjc,"I'm confused. And legitimately concerned about your mental faculties.  Literally in your previous comment, you posted Tom's Guide data, data thats not ""yours"" to try and discredit my post. And checking your post history, you also posted a Mrwhosetheboss video to discuss battery life comparison in an another subReddit.   So are you a hypocrite since you're ""stealing"" data as well? I'm not stealing anyone's data. I'm correcting your stupidly incorrect conclusion with a source to back it up. Just like you attempted to lol.",hardware,2025-09-21 01:49:24,2
Intel,neuiclx,"You see 7-8W increase going from 1 Core to 2 Core, which indicates that AMD has a huge base overhead but the core doesn't use 20W.   So its a problem with the SoC design not with the CPU design that it uses 20W for ST.  Its more like running a ST workload draws 20W SoC + 8W CPU stuff. So if they had better SoC design they could substantially boost efficiency even with the same cores.",hardware,2025-09-18 07:09:23,2
Intel,nescl7s,"Comparing the 9950X to the M4/M5 is a bit of a stretch... I'm not saying AMD is as good, but if they did a ""Lunar Lake"" with Zen 5C + on package memory they wouldn't really be that far off.  I want x86 to belong in the museum too, but sadly the ISA doesn't really matter (that much) and AMD isn't exactly incompetent... EPYC CPUs are still dominant and this is what they're truly targeting...",hardware,2025-09-17 22:28:00,3
Intel,nf8puxr,"Oh I was referring to this statement in your previous comment.  >Their E-core is faster than something like Tiger lake per clock.  By ""per clock"" I assumed you mentioned IPC.",hardware,2025-09-20 13:16:40,2
Intel,nfb2xd7,"I wouldn't recommend comparing IPC from two different runs of SPEC2017. There could be numerous different changes or subtests we don't know about. For eg, compare IPC numbers here vs the test used in A18 review. There are some notable differences in scores and IPC.",hardware,2025-09-20 20:35:11,1
Intel,neu6scx,Oh it's a huge benefit (I have a M2 Ultra at work) but we still use Nvidia. The cuda ecosystem is far more mature and widely supported with better support for embedded and datacenter scale compute.,hardware,2025-09-18 05:24:15,6
Intel,nf3x91p,"I see, so dynamic caching can make it so a shader doesnt have to be 30 registers wide if it doesnt have to do 30 often so it doesnt have to reserve that much space and waste it(such as in conventional cases, if its 5 registers and 30 peak, it will still reserve 30 registers despite it being at 5, which then would waste 25 doing nothing)  Also SER happens first right?",hardware,2025-09-19 17:45:47,1
Intel,ngq1nfa,Impressive and thanks for enlightening me.,hardware,2025-09-28 21:38:03,1
Intel,nes2pt3,I don’t disagree. The performance figures seem good. The power figures may or may not be. I’m just nitpicking.  Edit: just noticed that they show the A19 having 99% P-core FP performance path 11% less power. That is weird and get’s to my point about power measurement strangeness.,hardware,2025-09-17 21:34:33,3
Intel,nete2h5,"The needlessly high clocks, agreed: Apple could've still improved perf with lower clocks.  >the need to have a powerful chip in laptops/desktops all sharing the same core design.  They previously kept this in check on the A16 and even A17 Pro, both sharing the same core designs as the M2 and M3 respectively. That doesn't seem too related, as every uArch should scale a few hundred MHz either down or up.",hardware,2025-09-18 02:02:53,1
Intel,nfckhxr,I posted links to reviews. I didn't copy the entire data for my own post,hardware,2025-09-21 01:50:21,0
Intel,net132z,"ISA matters much as well, or mostly the implementation of it. X86 and AMD dont go well together, and AMD is the very definition of incompetency. Both deserve to be sunseted by now. The fact that there is no proper ARM support on consumer platform is the only reason why X86 on consumer still exists. For servers, an ARM server is more power efficient, and only those really legacy stuff requires X86. Companies would really appreciate the cost savings and the ARM ecosystem more than the clusterfk of X86.",hardware,2025-09-18 00:48:07,1
Intel,nfb1vqa,"Yeah, I guess that wasn't clear. What I meant is that the IPC of the E-core is so much higher than Tiger Lake that it's performance at 2.6GHz is surprisingly close to desktop Tiger Lake at 5GHz.  I know it's not fair to compare with Intel 10nm (later Intel 7) with TSMC N3P as it's 2 major nodes apart, but this goes way beyond that because these chips are using only around 2.5w for just the E-cores. TSMC claimed 25-30% power savings from N7 to N5 and another 30-35% from N5 to N3.  Using these numbers, we get these 4 E-cores using somewhere around 6-7w PEAK, but this is all-core turbo. Intel's CPU at it's most efficient 12w configuration won't hit it's 4.4GHz turbo required to win in single-core performance and is going to hit closer to its 1.2GHz base clock in all-core sustained performance at which point the E-cores not only have their giant IPC advantage, but double the clockspeed too.  All this again to make the point that a macbook with this will be blazing fast for consumer workloads and might finally be the laptop to usher in multi-day battery life.",hardware,2025-09-20 20:29:44,1
Intel,nfbgkad,"This is true, but there aren't a ton of alternatives short of taking a ton of time to compile/run everything myself. I don't think it's worth it for a hypothetical IPC comparison that it sees nobody else would even be that interested in seeing.",hardware,2025-09-20 21:49:05,1
Intel,nfd97ff,"Reordering of shaders has a cost, if for a given martial you just hit 10 rays you will not want to dispatch that shader with just 10 instances as the cost of dispatch and scdulaing will be higher than just inlining the evaluation, so you will merge together the low frequency hits into a single wave were you then use branching/fuction point calls.    You will also use this mixed martial uber shader to use up all the dregs that do not fit within a SMD group.      Eg you might have 104 rays hit a martial but that martial shader can only fit 96 threads into a SIMD group so has 8 remaining thread, you don't want to just dispatch these on there own as that will have very poor occupancy (with 88 threads worth of compute ideal) so you instead inline them within a uber shader along with a load of other overflow.",hardware,2025-09-21 04:45:15,1
Intel,nfclcjn,"Are you perhaps blind? The youtube link to the review is at the top of the post? What is wrong with you friend? Feeling a bit under the weather?  The video is in Chinese and before I updated the post with a youtube link, the previous source was from Bilbili, a platform that doesn't even work in most countries.   So I summarised the important points in it for people who didn't understand Chinese and couldn't infer anything from the graphs. No one's gonna ignore the link that is at the top of the post and read my summary before seeing that a link is available.   You're fighting imaginary demons here. Go to your samsung subreddit and whine about Geekbench or something.",hardware,2025-09-21 01:55:46,2
Intel,nev2rja,"But we were discussing CPU uArches, and on the question of ""Does AMDs single core suck up 31W"" the answer is ""NO"", the SoC power is 31W, and that is an issue that can be solved independently of the CPU uArch.  This just means that the massive inefficiencies are not inherint to the CPU uArch, but are a Problem of SoC design.  7W on a single core isnt super efficient, but its far lower than 20-31W. The question is if AMD is able to strip away a large part of those ~20W overhead, but that is not contingent on questions about CPU uArches.  And thats the point of the 1 Core vs 2 Core comparison. To demonstrate that it isnt the CPU that sucks up 20W, but the SoC, which can be solved much differently.",hardware,2025-09-18 10:28:02,6
Intel,nfcmlir,Enjoy your scratch-prone iPhone!,hardware,2025-09-21 02:03:39,1
Intel,ng0a1u9,Your one brain cell is trying really hard. See if you can wake up its buddy.,hardware,2025-09-24 20:06:41,1
Intel,n133hqd,"Same price as B580 with lower performance, 4GB less vram and 128 bit bus.  A round of applause for Nvidia",hardware,2025-07-03 06:56:28,385
Intel,n14594r,"GB207 being slower than AD107 is pathetic, what's the point of these x07 dies again? They're not thst mich smaller than recent x06 dies.  They're spaffing design cost on these barely different dues.",hardware,2025-07-03 12:22:36,31
Intel,n133i3r,"Now that we have a third-party review, it pretty much confirms what Inno3D said the other day, it's definitively slower than the RTX 4060 by about 5-7%.",hardware,2025-07-03 06:56:35,39
Intel,n13b3g4,The RTX 5050 is 2.5% slower than the Arc B580.   It's also a 50 series card that costs $250.,hardware,2025-07-03 08:09:36,36
Intel,n146i3h,> the system used a Ryzen 7 9800X3D  If the B580 only wins by 2.6% with this CPU then it's going to lose when you use something weaker because of that CPU overhead problem with Intel.,hardware,2025-07-03 12:30:30,29
Intel,n13bh59,It's actually surprising that it's that close to a 4060 considering the 5050 only has 2 GPCs as opposed to the 4060's 3,hardware,2025-07-03 08:13:18,15
Intel,n14m3pz,IDK but that performance is actually not bad. It should've just been cheaper.,hardware,2025-07-03 13:58:15,10
Intel,n13fsw1,The only positive about this is they're likely going to make a 5040 with a cut down chip that should pretty easily fit <75W.,hardware,2025-07-03 08:55:13,8
Intel,n17bpa4,"Honestly for people who don't need a lot of GPU power, not the worse.   I have two work computers, one with a 4090 and an old one with a 3090Ti.  These GPUs sit idle just taking up space, would have made more sense to get the something less performant.",hardware,2025-07-03 21:51:56,3
Intel,n13ktwe,272mm\^2 for B580 vs \~150 or less for RTX 5050 and perf within 5% lmao,hardware,2025-07-03 09:44:20,10
Intel,n150h4y,"I think to lose in sales, it would have had to lose a little more convincingly.  Yes, it's worse. But not enough to make up for features and brand-loyalty / nvidias massive reputation with gamers who are not into tech.  This thing will sell. It will sell A LOT. Because Average-Kevin who just wants to play Fortnite and some League will have a great time with it. His YT videos will get upscaling, his shitty mic-quality will get fixed (mostly) by Nvidia Broadcast... and he DGAF why we think it's the wrong move.",hardware,2025-07-03 15:08:37,2
Intel,n1t7zvj,"Does it actually fall behind the b580 or is it due to vram bottleneck in certain instances?  Either way, this should open up people’s eyes about the b580. The b580 is just marginally above the 5050, which is a terrible product, with 4gb more vram… even at its 250 dollar price point, it’s never been a good deal. Not to mention how it’s going for 400 these days.",hardware,2025-07-07 14:18:35,2
Intel,n148e4b,"People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.  Edit: r/hardware is dumb CPU overhead is a thing and budget GPU buyers need to know which of these two cards is effected the most by it. Everyone already knows that you are going to be GPU limited on top tier CPU's its not valuable information for a budget card.",hardware,2025-07-03 12:42:05,7
Intel,n13o0yi,Looks like people already forgot that testing the B580 with a top end CPU gives unrealistic results for actual budget buyers.  Every trick in the book for a nvidia sucks article.,hardware,2025-07-03 10:13:39,8
Intel,n13vnqm,Don’t make Jensen sad,hardware,2025-07-03 11:16:41,2
Intel,n132z0y,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-07-03 06:51:39,1
Intel,n147qg1,Will it also fall behind them in the Steam hardware survey?,hardware,2025-07-03 12:38:08,1
Intel,n15wi0t,the 3060 is still a better value than this card bc its about same speed but 12gb vram and higher bus,hardware,2025-07-03 17:38:26,1
Intel,n16kpem,"Maybe a product for non-gamers who want/need multiple display setup, like me. Even then, if you bother to spend that amount of money for 5050, better add some more to get 5060, and if bother to get 5060 just better get 16GB one. Furthermore, you may feel why not add bit more to get 5070 series rather than 5060 16GB. That's how these things are priced.",hardware,2025-07-03 19:35:55,1
Intel,n1at07e,Value engineering to extract as much wealth as possible while unable to match performance conditions.,hardware,2025-07-04 13:00:03,1
Intel,nbicg0f,When you gimp the memory bus of course it is not as good,hardware,2025-08-30 15:52:42,1
Intel,n13okls,"No 75 watt gpu, pin connectior, yeah this is death of arrival.",hardware,2025-07-03 10:18:30,-2
Intel,n153l0f,"Trying to convince people to buy a 8gb card in 2025 should be illegal. Yet, people will line up to buy it because of the Nvidia logo...",hardware,2025-07-03 15:23:12,-1
Intel,n135r6a,"It's a Nvidia Rx 7600, actually good recommendation over that card if it released for the same price in India. Rx 7600 is awful compared to rtx 5050 in pretty much everything.",hardware,2025-07-03 07:17:43,-9
Intel,n163uxd,Humiliation for the RTX 5050 is well deserved. It's good this gets curb stomped by the Intel B580.  More market share for Intel because AMD also introduced a useless 8 GB card.,hardware,2025-07-03 18:13:13,-1
Intel,n1481lj,">128 bit bus  Seems like this isn't the main issue with 5050's performance. 4060 has lowest memory bandwidth out of all 3, yet it's the fastest card. The die is just too cut down.",hardware,2025-07-03 12:39:59,51
Intel,n137c11,Unfortunately it will still sell like hotcakes in pre-builts to people who don't know much about computers.,hardware,2025-07-03 07:32:56,93
Intel,n13dk9q,"Their brand name is so strong that despite being worse in basically every way, including having poor drivers which was one of their major selling points in the past, they will still sell super well",hardware,2025-07-03 08:33:33,31
Intel,n139o7n,"I just looked at PCPartPicker and the cheapest B580 on there, in the US, is $299 (and the brand is Onix). The Intel Limited Edition is $340. If you can get the 5050 at MSRP it would have a decent price advantage.  I mean, I wouldn't get a 5050 but in the US at least, the $249 price for the B580 isn't real.",hardware,2025-07-03 07:55:44,31
Intel,n147v98,"Where I live none of the main sellers stock the B580, a round of applause for Intel.  The B580 isn't faster on the CPU's owned by the people who are in the market of a budget GPU.  People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.",hardware,2025-07-03 12:38:56,8
Intel,n13ev3s,Intel apparently hasn't shipped any GPUs last quarter. The shortage is already been seen as prices of b580 are going up.,hardware,2025-07-03 08:46:08,6
Intel,n169a4v,"> with lower performance,  even on lower end cpu? Holy crap nvidia wtf",hardware,2025-07-03 18:39:52,1
Intel,n1521le,Because it will sell a shit load. Probably will be one of the highest selling Nvidia GPUs this generation,hardware,2025-07-03 15:15:58,18
Intel,n1sr0i1,"At these small die sizes, yields can be really high. High enough that if they wanted to have supply for a 5050, they might not have enough GB206 dies that can be cut down to 20/36 SMs.  The 5060M is probably catching the bulk of the bottom-level GB206 dies at 26/36.  There's also the memory difference, with the 5050 having gddr6 while the rest of the lineup uses gddr7. With dies this small, that cost difference in memory could actually make it worth taping out a new chip with controllers for the cheaper stuff. Gddr6 is dirt cheap at this point.  You could absolutely then argue that there's no reason for the 5050 to be so cut down that it needs a new, even smaller die, and that Nvidia can eat some margins on a budget card to give it better memory. And I'd agree. But I'm sadly not anybody who can make that decision.  Personally, I can't wait to see what becomes of the cut-down GB207s. The 5050 is a ""golden"" fully-enabled die. There will be some with only 18, or 16, or even fewer working SMs. Hell it could lose an entire GPC and be down to half-working. Those will be some absolutely sad little GPUs.",hardware,2025-07-07 12:41:55,3
Intel,n13lkxr,> It's also a 50 series card that costs $250.  I'll believe that when I see it in stock for 250. MSRP is meaningless if the product isn't available for that price.,hardware,2025-07-03 09:51:23,32
Intel,n13m4a0,And will be even worse since it's vram is 8gb. In many many games the b580 would Perform even better,hardware,2025-07-03 09:56:22,1
Intel,n14glix,Yeah it would be interesting to test with a more real-world CPU pairing.,hardware,2025-07-03 13:29:04,13
Intel,n151kyi,"Yeah, that was my question, did that ever get fixed? It seems like nobody remembers that anymore",hardware,2025-07-03 15:13:49,6
Intel,n152cbk,"TBH, they should have made THIS card a 75W SKU. It probably easily can be, without losing much performance. That would make it a very viable upgrade path for low-end systems. And kind of make the price more attractive, because it would require zero additional changes.",hardware,2025-07-03 15:17:22,6
Intel,n13qh8j,Margins go crazy,hardware,2025-07-03 10:35:09,19
Intel,n15brqh,4x the performance with MFG though.,hardware,2025-07-03 16:01:21,-1
Intel,n14yvrm,In mkst cases comparison would make sense since you testing gpu's not anything else. But when you toss in b580 complaints about using too powerfull cpu make much more sense,hardware,2025-07-03 15:01:03,1
Intel,n15bkf0,I have a system with a 9950x3d and a 1060. I’m thinking of getting a 5050 to go with it.,hardware,2025-07-03 16:00:24,-1
Intel,n13wkrr,> Every trick in the book for a nvidia sucks article.  That's just a standardised testing suite.   Someone will have low-end CPU testing once this gets any distribution.,hardware,2025-07-03 11:23:28,7
Intel,n13qrff,They're using the same cpu for the 5050 too. What a weird comment to make.,hardware,2025-07-03 10:37:36,0
Intel,n1t9745,"Don’t forget how everyone is shitting on the 5050, calling it the worst nvidia card ever, but were praising intel non stop for the b580.  Even using purely stats from the article, b580 is 3% better with 4gb more VRAM than “worst gpu ever”, so I don’t see how this translates to a win for the b580. Being better than a terrible product by a hair doesn’t exactly make it good.",hardware,2025-07-07 14:24:40,1
Intel,n13s2zk,Same can be said for the 5050 no? Both have CPU overheads unlike AMD which any mid-range AMD GPU can easily outperform Nvidia's 5090 at 1080p.   The 12GB VRAM however are better suited for people who are stuck at older Gen3/Gen4 PCIE speeds.,hardware,2025-07-03 10:48:35,-4
Intel,n18avh8,"It'll run nearly every game in existence at 1080p with playable framerates with the right settings. IDK why it would be a product for non-gamers. Calling it that means the B580, 4060 and 7600 are products for non-gamers, too.  Only game I could think of it wouldn't run properly would be oblivion remastered, which has terrible performance on anything.",hardware,2025-07-04 01:18:02,2
Intel,n14lclc,3050 released with the exact same specs and msrp. The 70w variant only came out 2 years after. The first variant was in top 10 of steam's hardware chart.,hardware,2025-07-03 13:54:24,9
Intel,n17ei31,"This is not a card for 1440p, as it would have issues with getting even 60 fps there, and in 1080p 8gb is mostly enough.  There is a difference between choking on VRAM with 80 base fps (5060ti) and 50 base fps (5050). Yes, second case is still annoying, but you might want to dial down the settings anyway, and then you would be fine.",hardware,2025-07-03 22:07:05,4
Intel,n139moa,Nvidia RX7600? That a new GPU? 😂,hardware,2025-07-03 07:55:19,1
Intel,n18a77s,"Funnily enough the 8gb 9060 XT is the best brand-new alternative at $300 and below, hands-down, and that includes the B580. It's x16 interface means it's less restricted by the vram compared to the 5060/Ti 8gb. The B580 still has overhead issues, VR issues, game issues and is rarely available at MSRP.  But people trash it anyway and praise the B580 without actually looking at its performance. In the benchmark linked by the OP alone it loses to the 4060 lol",hardware,2025-07-04 01:13:47,1
Intel,n14h57q,"Performance is fine I'm sure it'll perform just as good as the 3060 maybe slightly if you lower the power to 75W, call it a 5030 and sell it for $150. But these are different times and Nvidia doesn't do reasonable things like that anymore.  Bottom line, it's just the price and TDP that sucks.",hardware,2025-07-03 13:32:03,-1
Intel,n13vxvy,TBF a pre-built you're just looking at:   * What's the price?  * Does it have a graphics card or not?  * Overall package/size,hardware,2025-07-03 11:18:47,21
Intel,n15185y,"or even people who have a baseline knowledge, but think ""dlss and framegen"" will make this abomination better than its competition",hardware,2025-07-03 15:12:09,1
Intel,n14gfbw,"It's not so much the brand name but their near monopoly in the prebuilt market. There will be 1 prebuilt with a 9060XT 8 or 16GB for every 20 prebuilts with a 5050/5060. It's so bad that even the prebuilts with the 16GB 9060XT will be outsold by prebuilts with a 5050 at the same price range, as companies like to pair trash GPUs with ""high"" end CPUs like a 13700F to clear stock but the ""i7"" allows them to mark up the price.",hardware,2025-07-03 13:28:07,16
Intel,n13pika,"It was in stock at newegg monday for 3+ hours at $249.99, I picked one up after seeing a post that was 3 hours old on /r/buildapcsales   I've stopped into microcenter a couple of times and they said they get them, but I just haven't been lucky enough to be there when they were restocked.  EDIT: it is back in stock as of 11:40 EST at Newegg",hardware,2025-07-03 10:26:45,29
Intel,n13wagd,"In Germany you can find Arc B580 for €269-283 low-end, and B570 for €218-235.",hardware,2025-07-03 11:21:22,9
Intel,n13m7wm,Her in Canada they are $360 CAD. Cheap cheap. And ways to find. Might be a issue in your country perhaps,hardware,2025-07-03 09:57:18,9
Intel,n13rke5,"Went on Shopee and the lowest I could find is $280, which is not bad considering there's usually taxes which increases prices of GPUs by 10-20% from global USD MSRP.",hardware,2025-07-03 10:44:24,3
Intel,n14na3u,">the $249 price for the B580 isn't real.  Yes it is, it just gets sold out as soon as it's restocked - https://www.nowinstock.net/computers/videocards/intel/arcb580/",hardware,2025-07-03 14:04:18,2
Intel,n14edfd,Doesn't it just need a 7600?,hardware,2025-07-03 13:16:41,5
Intel,n13m638,Not in Canada. Prices are exactly as launch. Could be your countries problem,hardware,2025-07-03 09:56:50,13
Intel,n146tdx,"Eh, they recently shipped a new batch at MSRP to Newegg.  Don’t know what’s going on with the partner cards. Probably the typical shenanigans",hardware,2025-07-03 12:32:28,2
Intel,n14t8qw,You forgot about the smaller dies.,hardware,2025-07-03 14:33:57,0
Intel,n151f6c,"The chip is tiny, so a single wafer -> more chips. The demands on power delivery, cooling etc. are nothing to speak of. And the GDDR6 memory is cheap and readily available.  If anything will ever be in stock, it's this card.",hardware,2025-07-03 15:13:04,7
Intel,n1krj48,5060 has been in stock at MSRP everyday since launch. This should be the same. Even the 5060ti 8gb and 5070 are at MSRP.,hardware,2025-07-06 03:12:56,4
Intel,n144vvx,MSRP has always been meaningless because it stand for suggested retail price.,hardware,2025-07-03 12:20:16,3
Intel,n151x7q,"But if the recent LTT video is halfway accurate, AMD would murder both of them in that scenario. The 9060 XT 8GB might want to have a word with those two cards.",hardware,2025-07-03 15:15:24,-2
Intel,n14n60t,negative margins for intel,hardware,2025-07-03 14:03:43,10
Intel,n15ha0f,The B580 showed we need to test in the systems the cards will end up being used in. The restricted PCIe lines on the lower tier cards i.e. only use 8 also means they need to be tested on older systems.,hardware,2025-07-03 16:27:40,8
Intel,n15gzg8,You really believe you are an average 9950x3D owner? Really?,hardware,2025-07-03 16:26:15,5
Intel,n19kdp4,"Out of curiosity, what is your use case in such a setup? I find myself cpu bottlenecked quite often on a 7800x3D but i use a 4070S, when i used 1070 GPU was bottlenecking me.",hardware,2025-07-04 06:45:36,1
Intel,n14a1ho,"But Intel Arc drivers have a really bad overhead, while nvidia does not, so in gpu bound games the 5050 will have around the same performance but the b580 will lose a lot of it, so it's not a weird comment, it's a very important point.",hardware,2025-07-03 12:51:45,26
Intel,n188xgl,>in 1080p 8gb is mostly enough.  The narrative was pushed so much that this fact became an unpopular opinion.,hardware,2025-07-04 01:05:47,4
Intel,n13aat8,nvidia Rx 7600 equivalent or Rx 7600 alternative by nvidia. Ig people can't even get a silly joke nowadays,hardware,2025-07-03 08:01:46,-10
Intel,n18u6nu,The 12 GB of RAM in the B580 will age more gracefully for a truly low end gamers. On purpose of 8 gb is retro gaming.,hardware,2025-07-04 03:21:07,0
Intel,n15lctf,"Hate to be that guy but to call this a 30-class card is insane. The 1030 was 25% the performance of the 1060, and the 1630 was around 45% the performance of the 1660. This card is 80% the performance of a 5060, which should probably be called the 5050.  To take it a step further, the GT 1030 was 13% of the 1080ti, the 5050 is 20% of the 5090. The 1050 was 25% of the 1080ti, so it would be reasonable to call this a 5040 or I could even be satisfied with 5040ti.",hardware,2025-07-03 16:47:05,35
Intel,n15k9s6,That would be an interesting card,hardware,2025-07-03 16:42:01,1
Intel,n14itr9,I think a big thing about prebuilts are the aesthetics. Does it look good? is the cables managed? Is the RGB controllable?  Slapping a PC together is easy but building something that looks beautiful through the side panel is not as easy.,hardware,2025-07-03 13:41:11,8
Intel,n1kqz69,The 4060 is 2% and DLSS did plenty of work for the even slower 3060. DLSS is going to be great for this card. Frame gen will be good in games this card can run at good base framerates.,hardware,2025-07-06 03:08:56,2
Intel,n15e4ii,"cool, it's in stock now",hardware,2025-07-03 16:12:35,1
Intel,n16ve1o,A 2-year old CPU ushering a new platform at the time of B580's launch. Far from a budget user's upgrade timeline-wise.,hardware,2025-07-03 20:28:40,5
Intel,n1881kg,"For a brand-new truly budget build, AM5 is still restrictive pricewise to people eyeing GPUs of this level. The 3050 and 6600 were well-known inclusions in $500 builds, building with a 7500F is going to cost $280-300 just for the cpu, mb and ram alone. AM4 or LGA1700 otoh can be as low as $180-200.",hardware,2025-07-04 01:00:16,2
Intel,n17vhk5,Depends on the game  The 7600 can see some serious performance deficits in some games like Spider Man  https://youtu.be/00GmwHIJuJY&t=521  and Hogwarts' minimum FPS is affected  https://youtu.be/00GmwHIJuJY&t=468,hardware,2025-07-03 23:42:58,1
Intel,n1454en,I mean tariffs in the US are a thing...,hardware,2025-07-03 12:21:46,7
Intel,n1595nj,"It wasn't always meaningless. Overall deviation from MSRP used to be smaller, and it wasn't hard to find the basic models at MSRP once upon a time.",hardware,2025-07-03 15:49:09,9
Intel,n15k42w,more like we need to test with different processors. Like with high-end to see absolute maximum and some lower grade to see how good/bad driver overhead is,hardware,2025-07-03 16:41:16,1
Intel,n16tjab,I never said I was,hardware,2025-07-03 20:19:36,2
Intel,n14c896,"I'm looking into this now, and you are correct. I am just reading this for the first time.",hardware,2025-07-03 13:04:21,8
Intel,n195igm,Lol no. For truly low-end gamers the B580 will perform worse because of its CPU overhead that still hasn't been fixed more than half a year after release.,hardware,2025-07-04 04:42:40,5
Intel,n4q5tdd,"I mean not really. The b580 is low end enough to the point where it'll become obsolete in performance long before it runs out of VRAM.  It's the equivalent of saying buying a 16gb 7600xt in 2025 is ""more futureproof"" than buying a 12gb 3060 in 2025.",hardware,2025-07-23 15:14:46,1
Intel,n53yfmn,an 8 GB card is obsolete at day one for modern games.,hardware,2025-07-25 16:12:47,1
Intel,n15s14a,"It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup. You can find those numbers on Gamer's nexus latest shrinkflation video on this topic (https://www.youtube.com/watch?v=caU0RG0mNHg). Considering those things, the 5060 is essentially a 5050 and the 5050 is a 5030.  The GT 730 had 13.3% as many cores as the full GK110 die, with a 50W TDP.  The GT 1030 had around 10.3% as many cores as the full GP102 die, with a 30W TDP though this one was impressive.  The GXT 1630 had around 11% as many cores as full TU102 die, with exactly 75W TDP.  The RTX 5050 has 10.7% as many cores as the full GB202 die (RTX PRO 6000 Blackwell), and around 11.7% vs the 5090 (which is the number you'll se on Gamer's Nexus video, he rounds it up to 12%, I think that was a mistake on his part? I'm not gonna try and correct him though), but it has a TDP of 130W somehow, just 15W less than the 5060, that's absurd.  Hence why I'm saying it SHOULD be a sub 75W TDP card, and priced way below that. My theory is that it was OC'ed past its efficiency curve to make it at least moderately better than the 3060 and still be able to call it a XX50 card.",hardware,2025-07-03 17:17:52,-1
Intel,n9995zt,Nah most people who buy prebuilts just want a good working PC with minimum effort,hardware,2025-08-17 22:53:54,1
Intel,n176oya,Yep. That's a problem for that country. Countries with normal leadership have much more viable options,hardware,2025-07-03 21:25:30,5
Intel,n17ibhv,Price is only driven by demand and supply. See how the crazy expensive GPU still sell like a hot cake during the COVID lockdown period?,hardware,2025-07-03 22:28:19,1
Intel,n14d3md,It's well known by Arc owners.,hardware,2025-07-03 13:09:23,10
Intel,n53mmej,"The 16gb 7600xt  is a giant pile of crap even with 16 GB , so a 12 GB 3060 is the better choice.  If you go to techpowerup you'll see that the b580 is 17% in relative performance than the ""12 GB 3060 """,hardware,2025-07-25 15:18:15,1
Intel,n84aj5f,What if you’re playing at 720p?,hardware,2025-08-11 14:46:01,1
Intel,n17rxfu,"> It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup.  That is a absolute dog shit methodology to use.   The 1080 Ti was 471 mm², 5090 is 750 mm².  Those two products are not the same tier. And can therefor not be used as equal reference points. When it comes to die size the top Pascal card is closer to the 5080 than 5090.",hardware,2025-07-03 23:22:30,8
Intel,n53r1fq,The point is more VRAM =/= aging better. The b580 having 12gb isn't saving it from becoming obsolete in a few years.,hardware,2025-07-25 15:38:30,1
Intel,n1glwbn,"They re not the same price tier either bro... They could have called the 5090 a 5095ti if that makes you happier.  The guy you quoted is right.   Also comparing the performance diff between the 1030 vs 1060 because guess what the 5060 should have been the 5050 looking at die size and memory bus.   Not sure why people keep defending these naming schemes, do you think the engineers use them internally lol? Its bullshit that marketing comes up with",hardware,2025-07-05 12:34:50,2
Intel,n17vpx6,"Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍 Doesn't matter if die sizes get bigger, that's up to Nvidia. Just because the halo product got bigger doesn't mean they can get away with moving their product stack (below the XX90) one tier up in prices.   I guess it is a dog shit methodology to use, only Gamer's Nexus uses it and I'm sure #1 Nvidia apologist u/Alive_Worth_2032 knows more than him.   It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree.  And it's a shit methodology to use if you're an Nvidia executive.  If you can't see that Nvidia is putting all the effort into binning high end chips for AI cause it's 90% of their income, then I don't know what to tell you. Obviously it's a good move for them, and it works, but there's 0 reason for a gaming customer to defend them for it. We used to get much more out of their chips in the gaming cards for more reasonable prices, why's it wrong to want that?   You can't just draw a conclusion based on relative performance between 2 underpowered products, you compare them to the ""best"" Nvidia COULD give us (which is the full top die for each generation) and go down from there.  Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm^2. That's not even an excuse.",hardware,2025-07-03 23:44:19,-2
Intel,n180tf6,"> It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree. And it's a shit methodology to use if you're an Nvidia executive.  You can chose one methodology.   You can decide on comparing what hardware you get at a certain price point.   Or you can ignore price/bom and look only at arbitrary model numbers as if they mean something.  You cannot do both at the same time.  Personally I prefer to look at die area. The 5050 today is roughly comparable to the 1050 Ti, comparing it to the top cards that are not comparable is irrelevant.  Pascal did not have a analogue for the 5090, period.  >Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍  That changes nothing. The Pascal Titan are using the same die as the 1080 Ti. The whole tier of die that is used in today's consumer top SKUs and for the RTX 6000 Blackwell, DID NOT EXIST back then.  >Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm2. That's not even an excuse.  What are you even trying to say? Ampere also did not have a analogue to the 5090. The 3090 Ti, even it is sitting almost a tier below the 5090. The 2080 Ti and 5090 are unmatched in other generations.",hardware,2025-07-04 00:15:04,10
Intel,n18i4wk,"You’re still missing the point by focusing purely on die area and pretending model numbers are arbitrary. Model numbers mean something, nvidia knows that so that’s why they kept them consistent for so long (unlike AMD that's hella inconsistent). Because they rely on the perception of tier consistency from generation to generation, even if they’ve worsened the specs behind the scenes.  You say you “prefer to look at die area,” but that’s irrelevant unless you’re building the chips yourself. Customers don’t game on silicon real estate they game on actual performance and hardware capabilities. And the cuda core count + bandwidth vs top-die approach directly shows how much nvidia is offering relative to what they could offer if they weren't prioritizing AI margins.  So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  > Pascal did not have a 5090 analogue  That’s just semantics. Every generation has had a full-fat top die, whether branded as “Titan,” “RTX 6000,” or whatever, which where I drew the comparison. Comparing lower-tier cards to the top die is how you reveal how much of the architecture's potential is being offered to gamers.  And yes, 3090 and 3090 Ti still preserved proportionally higher specs vs top-tier dies. What changed now is not just die sizes, it’s NVIDIA reserving most of the silicon for AI and throwing scraps to gaming.  > You can choose one methodology...  Sure, and I chose one: compare the lowest-tier GPU to the top die, which accurately shows how nvidia has been shrinkflating consumer value over time. You're welcome to look at BOM and TDP too the 5050 still loses. It should be sub 75W and priced accordingly.  I think it's okay to call it an XX40 series card even if we keep the TDP at 75W but the XX30 series would need to disappear. What I don't agree with is using the die area excuse to justify shrinkflation just because the 5090 is such a halo product.",hardware,2025-07-04 02:03:25,0
Intel,n18jqvu,"> So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  Do you even listen to your own madness?  You realize that what you are saying. Is that the lower end 5000 series would be ""better"". If Nvidia removed the 5090 entirely. And renamed the 5080 the 5080 Ti, and made the 5070 Ti into the new 5080.  Suddenly, you would praise the lower end cards for being ""better"". Purely because the high end is less powerful. Nothing else changed, no one is getting more hardware for their money.  Jesus Christ the mental gymnastics you people go trough.   The hardware Nvidia gives you for your money, is all that matters. How large the top die is and what core configuration it has. Is irrelevant for the value for cards further down the stack.",hardware,2025-07-04 02:13:15,9
Intel,mxmawxn,End of Life is not the correct term for this. End of manufacturing is,hardware,2025-06-13 19:40:01,128
Intel,mxm2d1j,Was about to shit on Intel for such a terrible product lifecycle time and how its GPU division was not going to do well if a GPU only has a ~2 years of updates until I read the article...,hardware,2025-06-13 18:57:57,111
Intel,mxmb1bk,"End of life typically means end of support, not end of manufacture, or am I wrong?  Anyhoo I blame the article for bad wording",hardware,2025-06-13 19:40:37,28
Intel,mxm07wf,"> This announcement marks the beginning of the end for a model that arrived just two and a half years ago, and it offers partners a clear timetable for winding down orders and shipments. Customers should mark June 27, 2025, as their final opportunity to submit discontinuance orders for the Arc A750.",hardware,2025-06-13 18:47:32,13
Intel,mxmcw6w,"looks like Intel didnt fire enough of its incompetent staff. EOL usually means end of support/drivers/Developmemt.  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",hardware,2025-06-13 19:49:53,8
Intel,mxwxs3r,"Why did anyone buy these garbage Arc cards to begin with? The performance/$ was never ever ever ever ever not even for a single second better than comparable Nvidia and AMD cards.  I've never understood the point of the Arc cards. It was like a company in 2020 saying ""we're officially into console gaming and just created the PlayStation 2 for only $399!""",hardware,2025-06-15 14:35:14,2
Intel,mxsyrub,But did it really even live to begin with? 🤔,hardware,2025-06-14 21:08:51,1
Intel,mxm95g9,is this the shortest life cycle of recent GPUs? AMD was notorious for that... wasn't expecting Intel to top that....,hardware,2025-06-13 19:31:14,-7
Intel,mxpuga3,"So, for the distribution process this is also known as End-of-Sale. Since this is a B2C business, they can't really control when their resellers reach the end of their stock, but at that point Intel has stopped selling the product.  EoM and EoL are both dates that usually do not coincide with the End of Sales.",hardware,2025-06-14 10:09:38,10
Intel,mxmbvmp,"The article is poorly written, and the headline misleading.   The card is discontinued meaning no more orders will be accepted. It says nothing about software support.   Admittedly the miscommunication is Intel's fault because they specifically use EOL in their notification, but I also put this somewhat on the article writers because they didn't do a good job of clarifying that intel meant discontinued. They could have used more appropriate wording in the headline but instead chose to follow intel's lead likely knowing it would sow confusion, but lead to more clicks.",hardware,2025-06-13 19:44:48,118
Intel,mxm3esb,"Least likely reddit user behavior, actually reading the article probably puts you in the top 5 :)",hardware,2025-06-13 19:03:03,49
Intel,mxml6tk,"Don't worry, Iris Xe GPUs are still ""supported by the driver"", but their last fix was in 2023.  Intel doesn't notify when an architecture is actually dead, you're just left stranded for years until they make it finally official.",hardware,2025-06-13 20:31:06,12
Intel,mxmdlmm,"The incompetence source is actually intel themselves ...  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",hardware,2025-06-13 19:53:22,22
Intel,mxn4bgv,> Anyhoo I blame the article for bad wording   Intel chose the wording in its own notice.,hardware,2025-06-13 22:09:46,9
Intel,nbpqm8r,"Bruh did you even read the article?  Btw it costs half of a 3060 and it outperforms it, no idea why you are saying it is a garbage product lmao shitty redditors don't know the difference between end of manufacturing and end of life/service.  The article is garbage, but it was talking about the end of manufacturing, not the end of the service/life, meaning it will still het driver support.",hardware,2025-08-31 19:30:36,1
Intel,mxm9ioh,"No because they are still supporting it, just ending manufacturing of new cards.",hardware,2025-06-13 19:33:05,22
Intel,mxn1rnm,"For a website that's predominantly text-based, a shocking amount of its users can't read for shit",hardware,2025-06-13 21:55:47,13
Intel,mxmkvoz,"The job of a journalist is to provide the translation and context for their readers, not copypasta and regurgitate headlines that laypeople will immediately misunderstand.",hardware,2025-06-13 20:29:35,31
Intel,mxmj7un,That's just for the hardware manufacturing.,hardware,2025-06-13 20:21:23,18
Intel,mxmaq6o,"ah, i thought it meant end of driver support.",hardware,2025-06-13 19:39:05,5
Intel,mxmevd8,We all did. unconventional use of EOL,hardware,2025-06-13 19:59:39,12
Intel,mxnw8ja,"Unfortunately it’s been like that for decades with Intel, I can count numerous times in the past decade we’ve had this same conversation about their processor lines being EOL.",hardware,2025-06-14 00:53:32,5
Intel,mxn4nhz,End of Sale,hardware,2025-06-13 22:11:38,6
Intel,nmklnox,"b580 or save and get the 9060xt 16GB. Do not get a card with 8GB.   Yes, benchmarks will show the 8GB cards ""winning"" sometimes but when you run out of VRAM, they will flat out not work and many games will crash or they will become a stuttery, unplayable mess. Getting more VRAM doesn't increase your performance necessarily, it's a simple check of whether you have enough or you don't have enough. If you don't have enough, things get real bad.   In your situation, I'd recommending just holding out until you can afford the slightly more expensive 16GB 9060xt but if you really don't want to or can't for some reason just get the b580.",buildapc,2025-11-01 18:18:15,5
Intel,nmkf8tz,How much is 9060 xt 16 gb in your country?,buildapc,2025-11-01 17:45:27,3
Intel,nmkfw8k,"Save some bucks and grab the b580, paired with a good cpu it is very good for the price. 9060 / 9070 xt 16 gb would be your next upgrade as of today. Bf6 on 9600x and b580 is perfect",buildapc,2025-11-01 17:48:47,2
Intel,nml0lcc,Save for a 9060 XT 16GB.,buildapc,2025-11-01 19:36:37,1
Intel,nmlc4x7,Out of thos 3 5060,buildapc,2025-11-01 20:38:10,1
Intel,nmlxfqc,What you say makes sense. But would it be the same in 1080p games? Do you have an example of games that require more than 8GB in that resolution?,buildapc,2025-11-01 22:36:13,1
Intel,nmkhjpz,"This, the cheapest 16GB is about $350 here in the US which is about $80 more, if you’re planning to move to a 1440p in the near future you might think about this.",buildapc,2025-11-01 17:57:13,1
Intel,nmlxv7p,"At the moment it is not an option, I play in 1080p, I want to buy another m.2 SSD and also 16gb more RAM to have 32gb. I'm also going to buy another more comfortable chair... I want to spend just enough on the graphics without biting off more than I'm going to chew.",buildapc,2025-11-01 22:38:47,1
Intel,nmm3el4,"The thing with 8GB is that a lot of it is going to be consumed by background processes and apps. For example, I have a 16GB card. 3GB of that is used by windows, chrome, and other applications. That leaves me with 13GB, it would leave someone with an 8GB card with only 5GB, so that alone is a big issue. If you use multiple monitors, that will also consume even more VRAM.   Briefly checking, Alan Wake 2, Red Dead Redemption, Starfield, RE4 Remake, Cyberpunk and Hogwarts Legacy will get dangerously close to 8GB and can surpass it based on settings and background apps at 1080p. There are others, these are just a few popular examples.   In some games, you will merely get texture pop-in. In other games, it will become extremely stuttery (Cyberpunk). In other games, the game will straight up crash and not work if it passes the VRAM limit (Stellar Blade).",buildapc,2025-11-01 23:10:59,2
Intel,nmklz8g,"Hi there! Thanks for the comment.  We ask that posts and comments be in English so they can be understood by as many people as possible. Translations on Reddit are client-side, and not all apps or browsers support auto-translate. Currently many users (and moderators) aren’t able to read your {{kind}.  Could you please submit a new comment in English?  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2025-11-01 18:19:50,1
Intel,nmmz19n,Alright. I would choose the 9060 XT 8GB then.,buildapc,2025-11-02 02:14:58,1
Intel,nmma78u,"I understand your point, although we are talking specifically about the Intel Arc B550 12gb. I have also seen reviews of this graph where it raises more FPS in 1440p than in 1080p, as if the resolution made it a bottleneck.  I understand that in 1080p the load goes more towards the CPU and in 1440p the load falls on the GPU, however the difference in performance catches my attention. I have an R5 5600x and in theory it should not generate a bottleneck but this graph seems to be quite ""CPU dependent"" in 1080p resolution.",buildapc,2025-11-01 23:49:35,1
Intel,nmmlhu4,"Your CPU will mostly hold you back if you are playing an extremely CPU-intensive game (rare) or you are already pushing very high frames. In general, GPU is most important for gaming, although this varies based on resolution. At 1080p, the CPU is a lot more important than it is at 4k, where the GPU will virtually always be the limiter (unless you're running an ancient CPU with a 5090 or some absurd scenario). Still, even at 1080p the GPU is going to be a limiter much more often than the CPU.     Arc B580 no? Because that's what you said in your title. Arc will be okay as I mentioned, but it won't be as good as a 9060xt. 9060xt is ~20-50% better on average, which is quite significant. That being said, if you absolutely cannot afford the 16GB version of the 9060xt then I would just go with the arc because running into VRAM limits will be disabling.",buildapc,2025-11-02 00:55:50,1
Intel,njpvdqi,"They should work fine together. They did have some issues with overhead with lower end CPUs previously, but that got fixed. I haven't heard about the ReBAR issue specifically, though.",buildapc,2025-10-16 00:45:50,1
Intel,nmzzudv,"No clue, but I can find B580's for sale online, and black friday is a scammy ""holiday"". I'm personally fine waiting on a 780 if they ever come.",buildapc,2025-11-04 03:25:02,2
Intel,nn0046d,The terabyte was selling and was in the 1800 range now it is in the 2400 range in the “promotion”   so it gave me anxiety not knowing if it will turn around or not to wait,buildapc,2025-11-04 03:26:46,1
Intel,nn011ja,"I don't understand, I'm sorry. I can't find much of relevance searching ""terabyte intel b580"", nor do I understand your 1800 to 2400 range.  The b580 has been on sale for me in USA lately, and just recently went out of stock certain places. B780 is supposed to release first qtr next year but has been delayed with very little news. So I have little hope on that timeline.  Edit: [https://www.amazon.com/gp/product/B0DNMH4KQM?smid=ATVPDKIKX0DER&th=1](https://www.amazon.com/gp/product/B0DNMH4KQM?smid=ATVPDKIKX0DER&th=1)",buildapc,2025-11-04 03:32:37,1
Intel,nn01vhi,I'm from BR so I mentioned this price range and the store (this store is popular here),buildapc,2025-11-04 03:38:01,1
Intel,nmxi7cj,"yea, thats below msrp and even a copy of Arc Raiders. Game will run pretty great on that card  if you can spare the money, I would get the 5070",buildapc,2025-11-03 19:21:19,32
Intel,nmxiizr,"Def worth it if you want more performance. Its about 35-40% faster in raw performance (yes ik only 12gb of VRAM but thats enough in 98% of games even at 1440p, and even in the future just turn down graphics a tad and itll be fine)",buildapc,2025-11-03 19:22:56,10
Intel,nmxivvh,yeah that is a massive upgrade over the 9060 xt plus free game,buildapc,2025-11-03 19:24:42,6
Intel,nmxk692,Absolutely,buildapc,2025-11-03 19:31:01,6
Intel,nmxpbzr,5070,buildapc,2025-11-03 19:56:25,5
Intel,nmxrrci,"Is this a new build where you can pull money out of another part to give to GPU budget, or a solo GPU upgrade?   Because I always suggest maxing GPU budget, and skimping a bit on RAM and CPU in order to do so.   But if it's just the GPU, then your budget is your budget and there's no wiggle room.",buildapc,2025-11-03 20:08:04,4
Intel,nmyqkm3,"Do it, that’s a great value. It’s a great card, Nevermind the naysayers, I absolutely love mine. I paid $549 and it was worth every penny.",buildapc,2025-11-03 23:03:22,4
Intel,nmxsz8j,yes,buildapc,2025-11-03 20:13:53,2
Intel,nmxx72o,"If you stick to 1080p with the 5070, yeah.",buildapc,2025-11-03 20:34:08,-2
Intel,nmy75oj,"All right cool, good to hear. Just need to stop reading opinions about the 5070 I think",buildapc,2025-11-03 21:22:08,8
Intel,nmxvja0,"Good question. It's a solo GPU upgrade going into an existing budget build. It might actually bottleneck the GPU slightly but I am just going to try it. b450m/Ryzen 5 5600g, 16gb DDR4 3200, RM750e power supply. Taking out an RX6600. So yes I'm already skimping on the other components! haha",buildapc,2025-11-03 20:26:11,3
Intel,nmyyz0y,Nice great to hear. I’m going for it 🫡,buildapc,2025-11-03 23:49:07,2
Intel,nmy80wj,"I have a 9060xt 16gb. In battlefield 6, a new AAA game at medium/high settings 1440p, I barely ever reach 9gb of vram usage.  The 12 gb of vram from a 5070 will last a pretty long time. I would say raw performance matters more, where 5070 destroys 9060.  You also get dlss4 and mfg which are nice perks",buildapc,2025-11-03 21:26:24,11
Intel,nmxwu0b,"What Res/refresh is your monitor? Because at 1080p and high refresh >240Hz it could bottleneck, but 1440p <120Hz I doubt it will.",buildapc,2025-11-03 20:32:23,3
Intel,nmyy4f1,Eventually you might wanna go to 32gb of ram since 16 is becoming the minimun in a ALOT of games and the prices are likely going to only rise from now on,buildapc,2025-11-03 23:44:27,2
Intel,nmxyj48,I just bought an AOC Q27G40XMN which is 1440p 180hz. Good to hear it should be ok on that.   I'm also going to try it out on my living room TV so that would be 4k w/ upscaling. My understanding is that would shift more of the work to the GPU from the CPU so even less chance of bottlenecking.,buildapc,2025-11-03 20:40:33,3
Intel,nmz1fih,Oh boy I really wish I spent just a little more to go from 16 to 32 a while back. You don’t think they’re going back down any time soon?,buildapc,2025-11-04 00:02:57,2
Intel,nmxzg4e,"You are correct, the more detailed each frame, the less total frames per second the CPU needs to generate which prevents the CPU bottlenecking.",buildapc,2025-11-03 20:44:55,3
Intel,nme5uos,"The only thing I would look into. Is the 9060XT 16GB. Depending on your region, it may only be an additional 100 euro or so. Other than that, this is a solid build. Also, I'm not sure how Intel drivers are with Linux. Someone else may be able to comment on that.",buildapc,2025-10-31 16:15:01,1
Intel,nme6qkb,"I'd pick up the 7600x and get a separate air cooler. Stock coolers are terrible, and you should be able to get a pretty decent one for 20-30€.",buildapc,2025-10-31 16:19:23,1
Intel,nme777t,"These parts are very solid and I think that you'll be very happy with the performance! Especially if you plan on everyday computing with some light gaming/production work.  The only recommendation I would make is to add an aftermarket cooler that performs better than the stock cooler. Nothing wrong with the stock cooler though. I have the stock cooler of a 3900X on my Plex server and it definitely gets the job done with no concern! :)  If you would like to consider an aftermarket cooler, the [Thermalright Peerless Assassin](https://www.amazon.com/Thermalright-Peerless-Assassin-120-Cooler/dp/B09LGY38L4?th=1) has incredible performance at a great price.  Regarding your fan question, that's ultimately up to you. Some people want maximum airflow, some people can't stand the noise that can come with a tower full of fans spinning, some people don't care either way. But yeah, I would recommend 2 fans in the front for intake and one for exhaust. Which I guess is the default configuration. It's a solid little case, but fan options are real limited.",buildapc,2025-10-31 16:21:41,1
Intel,nme77rn,"Thanks for the advice!  I checked out the ASRock Challenger Radeon RX 9060 XT 16GB. It needs a PCI.E 5.0 slot, or am I reading that wrong? I believe me motherboard only supports up to PCI.E 4.0.   I will check whether I can upgrade the motherboard",buildapc,2025-10-31 16:21:46,1
Intel,nmeca3h,Added it! Thanks!,buildapc,2025-10-31 16:46:47,1
Intel,nme93ai,"Don't need PCIe 5. It just supports it, which is kind of useless at the moment for gamers. More so people who are going to be running local AI on their machines.",buildapc,2025-10-31 16:31:05,2
Intel,nme7xp0,"PCI-E 5 is backwards compatible with 4, 4 is compatible with 3, etc. If you run a PCI-E 5 card in a PCI-E 4 slot, you'll just have slightly reduced performance, if any reduction at all. Most GPU's never completely utilize the power available within the slot. Check out some of the benchmark videos comparing the performance difference when gaming.",buildapc,2025-10-31 16:25:22,1
Intel,nmecdis,I have changed the GPU! Thank you for your advice!,buildapc,2025-10-31 16:47:14,1
Intel,nmd3tsa,"Yes, its viable but 110$ sounds a bit much",buildapc,2025-10-31 13:01:12,27
Intel,nmd4qo3,"My kids have 10400 with a 1660 super. Runs great, although nothing heavier than Genshin Impact and Minecraft.",buildapc,2025-10-31 13:06:24,7
Intel,nmd5c8m,All the other comments are correct but i wanna ask. How old is that monitor? You can probably get a used 1080p one for pretty cheap,buildapc,2025-10-31 13:09:45,6
Intel,nmd826k,They had a 6500XT on Woot for like 80$,buildapc,2025-10-31 13:24:52,2
Intel,nmd9hjs,Ripoff for $110,buildapc,2025-10-31 13:32:47,2
Intel,nmuzfvh,I am using one and it's good enough. I am not gaming on 4k 240hz but I am happy with what the gpu does.,buildapc,2025-11-03 11:04:47,2
Intel,nmd4w94,its better than what you have and for 50 you can't go wrong really...,buildapc,2025-10-31 13:07:16,2
Intel,nmd6gbp,"$50 is fine, make the trade, saves you headache. AMD has dropped game optimization support for 5000 and 6000 series Radeon cards. 1660 super came out same time as the 5000 series and NVIDIA is keeping support up for now.",buildapc,2025-10-31 13:15:56,4
Intel,nmd4qlm,I mean it is more than double your current performance with more vram but the rx 5700xt can be commonly had for around 100$ used and is almost 50% faster than the 1660 super and with 8gb of vram instead. Although at 720p it might be overkill for your use case given the CPU you have. Not a bad deal at 50$ tbh especially since the gtx 1050 is basically worthless at this point.,buildapc,2025-10-31 13:06:23,1
Intel,nmd7h96,"That's a bit much for a 1660s, most 10 series 50, 60 and 70 rang cards aren't expensive now. But I would look for RTX2060, where I live used ones start from 90-100euro. At 720p it will be a much better boost, plus DLSS4",buildapc,2025-10-31 13:21:39,1
Intel,nmd8ioo,"I ran a 10400F + 1660 Super until a few months ago. It was struggling with UE5 games so much that I just got a new PC outright.  In any case, the lowest-end GPU I'd recommend in 2025 is an RX 6600 XT or an RTX 2060 Super. Anything less and you're basically settling for playing only old games.  ALSO VERY IMPORTANT: That 720p monitor (768p I assume) most likely DOES NOT have a digital input like HDMI or DisplayPort. The 1660 Super ONLY has digital outputs, maybe DVI on certain models. Make sure that the GPU and Monitor are compatible, because I sure didn't account for that back in 2019 when I got an RX 570, and had to ALSO spend money on a 1080p monitor with HDMI support instead of keeping my old 1366x768 VGA monitor.",buildapc,2025-10-31 13:27:24,1
Intel,nmd8vb4,"Yes as long as you are 1080p gaming and do not expect to run the absolute newest and most demanding games of the past couple years. I’ve been using a gtx 1660ti since 2021 and I love it. It also overclocks to the moon perfectly stable. I’ve got +150 on the core, +1300 on the memory at 110% power limit. It will happily go higher, but I stopped there as I wanted the temp under load to stay under 75c.  Edit: that overclock worked out to about 10% more fps in games for me  Also, not sure if your 1050ti requires a power connector but the 1660s does so make sure you have one. Your power supply should be fine though, as that card usually draws 100w at most.",buildapc,2025-10-31 13:29:21,1
Intel,nmdaaf8,My 1660 super still feels good enough with today's games. Other than Indiana Jones. All other games run well enough for me. Busy with ARC Raiders and BF6 at the moment.,buildapc,2025-10-31 13:37:11,1
Intel,nmdlknk,"16 series is fantastic for last gen, mostly useless for current gen.  But its much much more balanced in line with your CPU than a 1050ti is and that trade seems like a fine deal as long as everything works",buildapc,2025-10-31 14:35:41,1
Intel,nmdy9pj,"Honestly I was using a 1660 and a 1080 for way longer than I should have but it runs like all games on really really decent settings as long as you aren't playing modern AAA games. If you've got the extra money, a newer card is better of course though",buildapc,2025-10-31 15:38:26,1
Intel,nme5xsc,"It depends on what you're expecting. It'll run most games. RT only games won't run. It's a decent card and my older brother still uses it in his build for some racing games and shooters. But if you're looking to play all the latest games like indiana jones and doom the dark ages, I'd save a bit more money and get something newer with more vram.",buildapc,2025-10-31 16:15:26,1
Intel,nmfiiyu,"It's not.  At the very least I would get the 2060. It's only 20% more performance in raster, but, crucially, it can run DLSS - which is gonna be another 20-30% more performance, even at that resolution.",buildapc,2025-10-31 20:23:24,1
Intel,nmfta1o,If you're curious about how much more performance you'll be getting compared to your old 1050ti check out this video. It's a straight really good upgrade for you at $50. You'd probably be able to play at 1080p at a mostly stable 60fps depending on the game and settings. Especially older games no problem.  https://youtu.be/N8i-neXsMH8  It's perfectly viable. Contrary to what a lot of people say you don't have to always have the newest and best tech if your expectations aren't sky high. And it's still a far better deal for you at the moment than buying an entire new PC.  And here's a 1660 super running Arc Raiders. It runs amazing for how well the game looks.  https://youtu.be/h_31FmStPuY,buildapc,2025-10-31 21:22:35,1
Intel,nmgfmbt,Decent card for 1080p gaming in older last gen games. It's too slow for modern releases.,buildapc,2025-10-31 23:43:47,1
Intel,nmgi0g3,It will be a good boost in performance for 50$.,buildapc,2025-10-31 23:59:38,1
Intel,nmia8hx,"if the deal is legit and you are sure that there wont be any funny business, i'd say that its a no brainer   you will see a noticeable increase in your PC's output. if [https://gpu.userbenchmark.com/Compare/Nvidia-GTX-1660S-Super-vs-Nvidia-GTX-1050-Ti/4056vs3649](https://gpu.userbenchmark.com/Compare/Nvidia-GTX-1660S-Super-vs-Nvidia-GTX-1050-Ti/4056vs3649) is correct, you would be facing 94% increase in your graphical output   congratulations beforehand",buildapc,2025-11-01 09:17:23,1
Intel,nmd5yq2,50$ if i exchange my gtx 1050ti,buildapc,2025-10-31 13:13:13,9
Intel,nmd7e96,i wasnt buying a new monitor cuz my 1050ti will not support the high res.. my games will lag more than it lags now.. after getting a new gpu i will upgrade my monitor.. been using my AOC monitor since 2010.,buildapc,2025-10-31 13:21:11,3
Intel,nmk78k6,"Literally a worse deal, and worse card than the 1660 super lol.",buildapc,2025-11-01 17:04:33,1
Intel,nmv9n6c,"personal satisfaction matters more than showing off by buying latest hardwares only to play less demanding games. for me i will be happy with 1660 super cuz i play games like hunt showdown, abi, siege, valo and csgo. mainly hunt showdown. and i have a 60hz monitor which i will upgrade to a 1080p after getting this gpu. so yeah rock with whatever satisfies you.",buildapc,2025-11-03 12:26:23,1
Intel,nmd7i8k,i will never install anything amd related in my system. i have had a very bad experience with amd stuffs,buildapc,2025-10-31 13:21:48,0
Intel,nmd5vru,nah bro.. wont be using amd card again in my system.. i have bad luck with amd cards,buildapc,2025-10-31 13:12:46,2
Intel,nmia8ir,"UserBenchmark is the subject of concerns over the accuracy and integrity of their benchmark and review process.  Their findings do not typically match those of known reputable and trustworthy sources.  As always, please ensure you verify the information you read online before drawing conclusions or making purchases.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2025-11-01 09:17:23,1
Intel,nmd7ifu,"for 50, a 1660 super is excellent value, so I'd say go for it.",buildapc,2025-10-31 13:21:50,13
Intel,nmd9pvm,"I feel you, my 1050ti stopped pulling new games at 1080p. Ended up playing KCD at 720p in the end",buildapc,2025-10-31 13:34:03,3
Intel,nmdack3,"Still, it's easier on the eyes and it's nice for Media consumption as well but if you are satisfied it doesn't matter",buildapc,2025-10-31 13:37:31,1
Intel,nmka8ti,"In game they perform pretty similar, and it’s new not used. Some titles like Cyberpunk it actually performs better",buildapc,2025-11-01 17:19:44,1
Intel,nmdc380,"Well they just shot themselves in the foot with dropping gaming optimization support, no matter how rabid their fans become. I can't wait though, gonna low-ball the hell out of FBM posts and get some secondary cards for Lossless Scaling.",buildapc,2025-10-31 13:46:50,2
Intel,nmf3laz,"I mean sure, over a decade ago they weren't great, but now they make the best cpus lol, especially for gaming",buildapc,2025-10-31 19:04:08,1
Intel,nmd7os2,ok will finish the deal then.,buildapc,2025-10-31 13:22:49,1
Intel,nmdnzdx,I stopped using my GTX 760 which is pretty much the same performance as 1050 TI in  2020 I was getting 30 fps most games  idk how you made it 5 more years lol,buildapc,2025-10-31 14:47:54,2
Intel,nmdwg79,How is that viable I just bought a almost brand new 3050 for 140,buildapc,2025-10-31 15:29:41,3
Intel,nmdwsjz,"Bro I wouldn't pay that and, however trading your old GPU and $50 isn't bad because realistically your gonna get -20-30 tops. And you can find 1660 supers on marketplace all day for 100 or less",buildapc,2025-10-31 15:31:19,0
Intel,nmdzou8,"Don't expect much a performance difference.... yeah its better, but it still wont handle even slightly modern titles very well. You'd be much better off saving up a little bit more money and find9ng something newer.",buildapc,2025-10-31 15:45:15,-5
Intel,nmdpqg0,"Sheer willpower, I've not played too many games I wanted, earliest being RDR2. I actually wanted to get a new GPU just when miners peaked, then 'Rona happened, then I hit financial low. Used market was also horrid. Only with 40 series and now 50 series am I able to buy a GPU. Last year I almost rage bought a 4060 lol Now I'm aiming for a 5070",buildapc,2025-10-31 14:56:40,3
Intel,nmfla2n,"Viable because the RTX 3050 is an absolute shit deal of a card. It’s slightly lower than a 1660 Super on average. Its only upsides are DLSS, Frame Gen, and longer driver support.  For $50, the 1660 Super is a steal, and OP is able to get rid of their old GPU in the process too. It’s two birds with one stone.",buildapc,2025-10-31 20:38:15,4
Intel,nmfkghw,6GB or 8GB?  The 1660 Super is better than a 3050 6GB anyways.,buildapc,2025-10-31 20:33:49,1
Intel,nmek6wt,"I went from a 1050ti to a 1660 super a few years ago. And it gave me plenty of extra performance. My wife still uses that card to game at 1080p, and she has had no complaints so far. You just have to be ready to lower your settings.  Biggest downside is the lack of dlss.",buildapc,2025-10-31 17:25:35,2
Intel,nmes92s,What does she play?,buildapc,2025-10-31 18:05:54,2
Intel,nmffy1l,"When she first build the pc she played a lot of Hogwarts legacy. Which ran great. She also plays satisfactory.  I also used it to play games like Cyberpunk, HZD and Metro Exodus on (I think) at least medium. And I used a 1440p monitor.  I know a 1660 super is far from the newest or best cars. But it still performs ok in most modern titles as long as you dont expect to much and lower the settings. And I think for 50, it can be a decent deal.",buildapc,2025-10-31 20:09:36,2
Intel,nkwlod3,"Try it. Some of the older games are probably fine, newer probably not.  In any case a dedicated gaming desktop PC would definitely be a much better experience. But you don’t have to go crazy. Around $1200 is the sweet spot IMO. Check the sidebar for resources and guides.",buildapc,2025-10-23 04:41:56,4
Intel,nkwmrv9,"[https://www.youtube.com/watch?v=4ptRLMB4m2I](https://www.youtube.com/watch?v=4ptRLMB4m2I) he tested your exact CPU & iGPU and overall it looks playable depends on what your standard of playable means (with 1080p, around 40fps, no 60fps unfortunately). Most of the games you mentioned has upscaling tech like FSR/XESS that you can use in the graphic settings of each game to further improve the performance (basically playing the game in less than 1080p then upscale it to 1080p).     But if what you want is a 1080p stable 60fps for now and for the future, you definitely need something more than what you have right now. 1080p 60fps is easy to achieve with less than 700USD budget. Even more lower if you are confident in buying used parts",buildapc,2025-10-23 04:50:48,3
Intel,nkwyclp,"If you’re hesitant about buying games in case they don’t run well, you could always do a month of Xbox Game Pass and use any games on there to benchmark.",buildapc,2025-10-23 06:35:07,2
Intel,nl002wk,"I say build the PC, as a Gen Xer who hadn't touched PC gaming in 20 years I just built a gaming PC.  Had a blast doing it.  Just grab an awesome case and a mid tier GPU for starters.  I bought a Radeon 6800 XT GPU off of a friend and started from there.  My setup isn't top tier but it games incredibly.  Indiana Jones and Ghosts of Tsushima look fantastic.",buildapc,2025-10-23 18:18:27,2
Intel,nkwlrzy,We don't know your exact situation; actually test the games.,buildapc,2025-10-23 04:42:45,1
Intel,nkwlvko,"You might be able to play a couple of titles with the the specs you listed (call of duty world at war for example, maybe some older resident evil games) but it would be cutting it close and it would be low/medium settings",buildapc,2025-10-23 04:43:34,1
Intel,nkwo8of,"Most single player games are likely to be playable   I'd say install Steam and buy a few - you download them.. . but keep them in a steam account that you can use again on a new computer   So you kinda buy to keep  GOG is another store you can buy games in.. . and is probably a better option for things like ""The Witcher 3""",buildapc,2025-10-23 05:03:03,1
Intel,nkwumuq,"Treat yourself to the PC, you’ve earned it!",buildapc,2025-10-23 05:59:58,1
Intel,nkxh4e6,"Intel arc on the laptop runs older games pretty decently. Check the youtube link below for list of games. I think yours might be able to perform slightly better than this due to better cpu and possibly better graphics.  [https://youtu.be/iCY0tuXUOME?si=nPlnrH6olaDjL3S3](https://youtu.be/iCY0tuXUOME?si=nPlnrH6olaDjL3S3)  If youre not satisfied, you can always build/buy a gaming pc. If you take this route, just make sure that youre paying the correct price fot the parts youre getting. Often times you will see pre-built pcs, or shops scamming their customers by selling them ancient hardware for the same price as a new pc.",buildapc,2025-10-23 09:44:52,1
Intel,nkxn5v2,"since you said you have enough money to get a PC, do that, treat yourself with a decent setup, it would be a million times better than playing on a laptop, especially the games you mentioned, if you can share your budget im sure we can figure out a good PC for those games, you won't need anything crazy i promise haha, all mid range pc combinations are able to run all those very well",buildapc,2025-10-23 10:39:43,1
Intel,nkxu7qf,"When it comes to pc game, there are so many variables.  Graphics setting, resolution, upscaling, genre of game and perceived smoothness of different FPS all contribute to how well a game run for you.  It is really hard to tell without trying.    For an integrated gpu, yours is by no means weak, but it still cannot compare to modern discrete gpu.  Generally, anything in or before ps4 era(i.e released before 2020 ) should run fine with minimum graphics setting at 1080p with fsr / xess upscaling. It should safely run at 30fps or above (i.e. as smooth or smoother than ps4 game)  Anything newer you will have to either watch benchmark on youtube, try the demo or benchmark tool of the game(some games offer this).  Steam also support refund of games played in less than 2 hours and purchased less than 2 weeks.  But don’t do that too much or you will be denied the refund.",buildapc,2025-10-23 11:34:20,1
Intel,nkylr9h,Those are pretty good specs for a laptop i would think it would be fine but i only play on desktop. Laptop components arent as good because its harder to keep them cool,buildapc,2025-10-23 14:15:30,1
Intel,nl24jl2,"I think personally you should build a pc! As long as your budget allows it you can get something for around 600 bucks on facebook marketplace, or you can build with used parts off of facebook marketplace.   If you want recommendations I’ll give you some:  Ryzen 5 5900x (180ish used)  RX6700xt (200ish bucks used)  Asus b450-f motherboard (50 - 100 bucks used)  32gb ddr4 ram whatever you’d like, probably Corsair though. (80-100ish)  PSU 650W (something reputable like Corsair) (90 bucks)  You can find good air coolers for the cpu for under 50 bucks just do research.   You’re looking at 680 bucks on the high end but definitely can find parts to fit a cheaper budget. Feel free to message me and I’d be glad to help you find something.  Edit: this is what you should expect as far as performance, you should expect with these exact specs to be able to play currently all games on high settings, competitive or first person shooters tend to run better than single player games in my experience, I’d expect 120+ frames on 1080p, or even higher than that if you drop your settings. As far as single player games, you should be able to max out most games and get a solid 60fps which is fine for single player.",buildapc,2025-10-24 01:11:10,1
Intel,nkwly0y,"I am just hesitant, I don’t want to spend money on games and then find out they are not working",buildapc,2025-10-23 04:44:07,3
Intel,nl013fo,Can I ask how much did you pay?,buildapc,2025-10-23 18:23:24,1
Intel,nkwlvg3,I wanna make sure they work before spending money on them,buildapc,2025-10-23 04:43:33,1
Intel,nkwmakp,"But you said you have the money to get a gaming PC. So… if they don’t work well, you’ll just get the pc, right? That’s what your post suggests to me.",buildapc,2025-10-23 04:46:56,5
Intel,nkwsefw,Subscribe to PC Gamepass for a month ($17) and you'll have a bunch of different games to try and see how it fares. My guess is the integrated graphics won't be up to snuff for anything but indie games.,buildapc,2025-10-23 05:39:42,1
Intel,nkytstg,"Steam let's you refund with I think under 2 hours of playtime or less than 2 weeks, of it doesn't work and you don't get the gaming PC just refund the purchase",buildapc,2025-10-23 14:55:07,1
Intel,nl1pc4v,"The nice thing about building a desktop, is that you can upgrade it over time.  Cpu, motherboard and ram need to be bought together, but a decent power supply can last for years as can your case and monitor. I'm an old guy who has been building PCs for myself and kids for around 20 years.   Get a mid tower case with good air flow and a quality PSU.  I'd look at website recommendations for the best bang for your buck AMD BUILD for your cpu and motherboard recommendations , get 2 sticks of 16 gig ram., a 1 tb M2 ssd as a system drive, and dump the rest into the GPU.   27 inch monitors are fairly cheap now, and you can get whatever keyboard and mouse you want.",buildapc,2025-10-23 23:39:08,1
Intel,nkwpba3,"If you purchase the games on Steam, they are good about giving refunds if you can't play the game.    I think you will find mixed results, with some games/game areas being fine and others painfully lacking.    A lot depends on your own personal tolerance for slow/low res gaming.",buildapc,2025-10-23 05:12:18,3
Intel,nkwmmx3,Spend money on what?,buildapc,2025-10-23 04:49:43,1
Intel,nkwmlm2,You got a point. I think you are right. I have money for low/medium budget PC. I think I should actually try the game.,buildapc,2025-10-23 04:49:25,3
Intel,nkyyz4r,"Hello, your comment has been removed. Please note the following from our [subreddit rules](https://www.reddit.com/r/buildapc/wiki/rules):  **Rule 3 : No piracy or grey-market software keys**  > This includes suggesting, hinting, or in any way implying to someone that piracy, or violation of license agreements is an option.   > If a license key is abnormally cheap (think $5 - $30), it is probably grey market, and thus forbidden on /r/buildapc.    ---  [^(Click here to message the moderators if you have any questions or concerns)](https://www\.reddit\.com/message/compose?to=%2Fr%2Fbuildapc&subject=Querying mod action for this comment&message=I'm writing to you about %5Bthis comment%5D%28https://old.reddit.com/r/buildapc/comments/1odu6ga/-/nkxhg3c/%29.%0D%0D---%0D%0D)",buildapc,2025-10-23 15:20:16,1
Intel,nkwmpiv,"Just try one to start. One of the older titles. If you don’t game much you may find it takes a while to get through a big story game.  I’m in my 30s and RDR2 took me months, lol.  The first of the new Tomb Raiders (from like 2013?) is great, cheap, and very easy to run.",buildapc,2025-10-23 04:50:17,5
Intel,nl7ve0g,"If you buy a game on steam, they have a refund policy so if you play less than 2 hours and its been less than 2 week since perchance you get full refund no question. So  look up how to refund games and then try them all.",buildapc,2025-10-24 22:50:56,1
Intel,nlq15me,"I think people here are a bit too quick to say to scrap builds, but I think starting over isnt a terrible idea.  You can upgrade to the upper end of AM5, but at least in the US market, their prices are inflated to hell. For the price of something mid-line like a 5600X, you're 1/3 of the way to an AM5 upgrade.  That said, a 5600X hypothetical would still be a [big boost](https://duckduckgo.com/?t=fpas&q=5600x+vs+1600+ryzen&ia=videos&iax=videos&iai=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYNzmmyRm3FA) for you, so you'd have to evaluate that.   Technichally speaking, your current PC does hit the minimum specs for Arc Raiders, although how the minimum speca actually translate is anyone's guess now.  You left out your PSU which determines what GPUs you can add. ""Entry level"" cards now are like the b580, 9060XT, 5060TI, and if you go cheaper than that EBay refurbs become something to consider.   You didn't list resolution, target framerate, or budget, so I'm mostly going over high level stuff. To toss out rough figures:  AM5 Upgrade: $400+ not including GPU  GPU: $250-400+   Full Build: Likely $800+ for anything decent, $1000 is the ""typical"" list I see here.",buildapc,2025-10-27 21:54:42,1
Intel,nlqmst4,"Just wanted to chime in to add that a b580 would probably not be a good idea without a cpu upgrade to zen 4, 5 or at least a high-end zen 3. The Intel graphics cards make the CPU work somewhat harder than normal, so you won't get the full expected performance with a low-end zen 3 cpu (much less with a zen 1 cpu).",buildapc,2025-10-27 23:55:34,3
Intel,nm2sw7a,Thank you for your reply.  It's helped massively!   I decided to upgrade to Ryzen 7 5700x and a b580 as they happened to both be on sale.,buildapc,2025-10-29 20:57:59,2
Intel,nlqw4zo,"good catch, appreciated.",buildapc,2025-10-28 00:47:58,2
Intel,nloj08r,The uplift in raster would be around 15 %. Wait until after Christmas and get a 5070 Ti (if it really has to be nVidia).,buildapc,2025-10-27 17:19:50,1
Intel,nloji1i,"Ahh 50% is what I thought.. you put 15% in your fist comment mate, that’s what confused me   Thanks",buildapc,2025-10-27 17:22:15,1
Intel,nlomqvt,You can get the 5070 Ti for £679.99 unless you really want the aesthetics of that particular model,buildapc,2025-10-27 17:38:05,1
Intel,nlqlh1d,Do it,buildapc,2025-10-27 23:48:05,1
Intel,nloj6cc,Is that all? Isn’t it like 35%-40% ?,buildapc,2025-10-27 17:20:40,1
Intel,nlok3kt,"Yeah, sorry. For some reason I read ""5070"".",buildapc,2025-10-27 17:25:07,2
Intel,nlon1ld,Thanks mate I’ll have a look now,buildapc,2025-10-27 17:39:31,1
Intel,nlojbog,"Close to 50 % with a 5070 Ti.     Edit: Sorry, I misread your post. Yeah, the 5070 Ti is a decent upgrade.",buildapc,2025-10-27 17:21:23,2
Intel,nlon6o8,https://www.scan.co.uk/products/palit-nvidia-geforce-rtx-5070-ti-gamingpro-s-16gb-gddr7-ray-tracing-graphics-card-dlss-4-8960-core-2,buildapc,2025-10-27 17:40:11,1
Intel,nlonzw2,Also this one https://www.overclockers.co.uk/gainward-geforce-rtx-5070-ti-phoenix-s-16gb-gddr7-pci-express-graphics-card-gra-gnw-05399.html  You can get free delivery with code PCPPFREEDEL,buildapc,2025-10-27 17:44:05,1
Intel,nloohju,"If you really want that MSI one, I think they are doing a promo for a £100 Steam voucher for any purchase so if you get it for £730 then it technically works out cheaper if you can use it  Edit: scratch that, it’s only £40 for the 5070 Ti but worth shopping around if you can make use of their Steam voucher promo  https://uk.msi.com/Promotion/RTX50-100STEAM/graphics-cards",buildapc,2025-10-27 17:46:27,1
Intel,nlosh5t,"There pre order mate, the only one in stock for sub £740 is the MSI one with scan and I can get it for tomorrow. Thank you",buildapc,2025-10-27 18:05:51,1
Intel,nloq0ux,"That’s sweet mate, can I buy it off scan then claim the £40 back through msi?   At £730 with arc raiders and a £40 rebate it’s quite a good deal?  Thanks",buildapc,2025-10-27 17:53:53,2
Intel,nlotcwq,"Yeah I’m pretty sure you can, I would double check the T&Cs and make sure that the model you buy is eligible.   I’ve found Scan’s shipping cost to be quite expensive though.  If you do get it, I think you have to upload the serial number as proof, I think MSI wants the one on the actual GPU itself not the one on the box.  So take a picture of it before you install it.   I didn’t with my motherboard and couldn’t be bothered to take my pc apart lol, lucky it was a random game and not something I actually wanted.",buildapc,2025-10-27 18:10:17,1
Intel,nlouzad,"Heads up, Very is doing the MSI Ventus 3X OC for £707 with code COMPUTING10  Edit: I don’t think that one is eligible for that steam code promo actually oops",buildapc,2025-10-27 18:18:29,1
Intel,nkh7ll7,"1. Do you still have the AMD compatible mounting hardware that came with the AIO to be able to mount it on the different socket?  2. Avoid Asrock motherboards. They have been having issues this year with specific boards with specific BIOS versions frying specific CPUs.  3. 7600X is a great bang for buck CPU.  4. In BF6, the RTX 9070 averages 103fps whereas the 5070 averages 88fps. IMO it's worth the 10% price premium to get the RX 9070. The extra 4GB of VRAM will also help with longevity and 1440p||4k gaming.",buildapc,2025-10-20 15:05:17,1
Intel,nkt2ojr,"1. I didn’t, but now i do- thanks! 2. I did some research on this. Apparently it’s a non issue for my CPU that im getting. 3. Let’s goo! 4. Honestly I’m still on 1080p so i feel like 12GB VRAM is good?",buildapc,2025-10-22 16:53:38,1
Intel,nktj7xk,"In BF6 at 1080p the RX 9070 is faster.  RX 9070: Avg=144fps : 1%low=108  RTX 5070: Avg=139 : 1%low=99  According to techpowerup, across all games at 1080p, the 9070 averages 152 fps while the 5070 averages 143 fps.",buildapc,2025-10-22 18:11:17,1
Intel,nkal8rm,What will you use this PC for?,buildapc,2025-10-19 11:59:57,2
Intel,nkako8l,We don’t know what you want to do… so how should somebody give you feedback?,buildapc,2025-10-19 11:55:27,1
Intel,nkhnenl,Just gaming and maybe some light editing,buildapc,2025-10-20 19:39:29,1
Intel,nlgg0lo,"Because of high popularity of 5700X3D and 5800X3D -- and that they are now both out of production, it's likely their used prices have increased.  As a result, since you aren't on AM4 already, unless you're getting a killer deal on a used 5700X3D/5800X3D, recommend you go with 9600X + B850 + usual 32 GB 6000 CL30 kit. Should be right around 400 pounds.",buildapc,2025-10-26 10:43:31,35
Intel,nlgg2hk,"The 5700x3d and 5800x3d are no longer the value deals that they were. They're essentially end of life so have gone up (unless you can find a deal!).    If you want cheap performance and also to keep your DDR4 RAM, i think the best deals currently are i5-14600/K/KF and a B760 DDR4 motherboard. That's going to be the best bang for your buck.    However, i don't see the AM5 parts that much more expensive. Obviously, if you're looking at 9950x3D etc then you're paying a lot. But you could get a Ryzen 5 7600/7600X/9600X that'll perform better than your 7700K for pretty cheap and a B650 board for a good price. The only spanner in the works is the price of RAM. So, in this context, going DDR4 board on last gen systems could save you a lot.",buildapc,2025-10-26 10:44:00,8
Intel,nlgfy4d,"AM5 is a better idea, it gives you an upgrade path for the future. Something like Ryzen 5 7600 / 7600x / 9600x are all going to be far far better than your i7 7700K.   [https://uk.pcpartpicker.com/list/r93dTM](https://uk.pcpartpicker.com/list/r93dTM)",buildapc,2025-10-26 10:42:51,11
Intel,nlgho01,"Am5 any day. The price gap even for used am4 to am5 is so small, that am5 makes so much more sense. The am4 x3d are hugely overpriced, even used. A ryzen 7600x is already equal in performance to a 5700x3d (thats like 5% slower then a 5800x3d). You pay maybe 80€ more for going AM5 but have a much better outlook for an upgrade in some years. There were some edge cases were i would have recommended going am4 used (like 16 cores or a lot of cheap ram), but with ram prices skyrocketing, thats not really an option anymore. I would go am4 if you do a really budget gaming build like with a r5 3600, or you get a REALLY good deal on a am4 prebuilt. Am5 will have a much longer lifespan, with cpu upgrades coming and reusing ddr5....",buildapc,2025-10-26 10:58:52,4
Intel,nlgh43r,I wouldn't buy new and get am4 that's for sure. Since your upgrading peice by peice you should get the best CPU you can afford. Get a 7600X or 9600x on AM5 and you might even get a second CPU upgrade out of it.  AM4  might not even be fast enough for an 9060,buildapc,2025-10-26 10:53:48,3
Intel,nlgkzvu,"AM5, a 7600x more or less is the same as a 5700x3d except in instances when the L3 cache matters and it's 1% lows but 7600x 1% aren't even that bad.",buildapc,2025-10-26 11:28:54,3
Intel,nlgoaia,"Hmmmmm  Am5 , because you have better upgrade potential later. 9600x or 9700x.   Intel option , if you can get a cheap used 12400 or 12700k then maybe go for those.  Even 13th gen 13400 will beat your 7700k.",buildapc,2025-10-26 11:56:09,3
Intel,nlgigb2,Am5 no question,buildapc,2025-10-26 11:06:03,2
Intel,nlgmxfq,"Brother! I just upgraded from a 7700 and 1070 myself. Served me well for 8 years lol.   I went with a b850/9600x/9060xt and it is badass. You can sometimes save a little with a 7600x as it is only maybe 5% slower and a bit hotter. Went from 30ish fps in helldivers on low settings to 80+fps native, and 120-200 with fsr frame gen stuff turned on. It is a night and day difference vs my old unit... but its 4 generations of hardware newer so of course it is.   In my area for $280us they have a b650/7600x cpu combo with a free aio water cooler on Newegg. Killer deal.   5800x3d is great if you can find one. The 7800x3d is around $360us just for the chip new and actually available. Used 5800x3ds go for 350+ on ebay... not sure why people wouldnt just buy the better chip new for the same price... i guess they dont want to update an old motherboard.",buildapc,2025-10-26 11:45:19,2
Intel,nlgp6rc,Def AM5 if you don't have an AM4 Plattform right now,buildapc,2025-10-26 12:03:04,2
Intel,nlgrs2w,"It's a false economy to buy in to am4 at this point, go am5.",buildapc,2025-10-26 12:22:18,2
Intel,nlgzmj7,"If you are spending the money, you should go with the current socket system.  7800x3d is a good target",buildapc,2025-10-26 13:15:35,2
Intel,nlgget0,"If it's just performance as the i7 you can get AM4 non-x3d or LGA 1700 and still maybe get a gpu upgrade with whats left. If you were considering x3d chances are you'll spend roughly the same for entry-level am5.   You don't really need anything fancy like a 9600x and B850, get a 7500f + b650 and it'll still be a huge upgrade. Even 16gb ram would be alright.",buildapc,2025-10-26 10:47:15,1
Intel,nlh49q9,9600x+B850 board.   B650 boards are on end of life. Production stoped and updates soon also.  Go B850 for future path to upgrades following Zen6 and maybe Zen7,buildapc,2025-10-26 13:44:22,1
Intel,nlhax3a,Unless you have an AM4 board and the DDR4 RAM already you should go AM5.  AM4 is just not worth it anymore with the scarcity of x3d CPUs and DDR4 RAM prices climbing.  The budget nature of AM4 is pretty much done.,buildapc,2025-10-26 14:22:41,1
Intel,nlhcewg,"Jump to am5. You’re not already on am4, you have to buy a new motherboard for that, though you can re use memory. But the 7600/9600 areas fast as the 5800x3D and a lot cheaper.",buildapc,2025-10-26 14:30:50,1
Intel,nlhi7c3,"Entirely depends on what your new vs used pricings are locally. I would not even consider any AM4 X3D rigs here, they cost more than a new set of B650+7500F+32GB RAM.",buildapc,2025-10-26 15:01:46,1
Intel,nlhmtl0,5800x3D is pointless for new builders as it's insanely priced just go AM5 and grab a 7600x/9600x,buildapc,2025-10-26 15:26:04,1
Intel,nlhu2yd,You're not going to find a 5800x3d at a decent price unless you have some friend who's still holding onto AM4 for some reason.,buildapc,2025-10-26 16:03:25,1
Intel,nlhuk8b,"Do you have DDR4 RAM already? Then AM4 should be fine. The cost of RAM is soaring right now, so either buy RAM immediately or just stick to 5800x3d. It'll be great for years to come, and then just upgrade to AM6.   I would have recommended AM6, just as most will, but that was before the leaping RAM prices.",buildapc,2025-10-26 16:05:54,1
Intel,nliixy0,"5800X3D & 5700X3D are out of production and thus, the prices are only getting higher. Including used prices.   Just a quick search on ebay and sold listings are showing an average of $300-350, with some at $400 or more as a bundle with motherboard and RAM.   For not much more money, at around $550, you can get a 9600X, a B850 motherboard and 32GB of DDR5.   If RAM prices weren't skyrocketing like they currently are, you could have gotten a basic AM5 setup for around $450 a few weeks ago. Some of the price hike can be explained by RAM chips being purchased for AI, due to many many data centers popping up all over major cities. But also can be explained by the upcoming holiday season in a few short weeks. Its a common trend now - you can use a price tracker like CamelCamelCamel or PCPartpicker to look at the pricing trend.   Its not really ideal to move to AM4, especially buying brand new parts - the cost difference is in AM5's favor even with the DDR5 price hike. Any better CPU upgrade down the line would require you to jump to a new platform anyway, which means CPU/MOBO/RAM at a minimum.   Hell, if you're willing to wait a few weeks to see what the holiday ""sales"" bring. (more people are becoming aware that a few weeks before the holiday season price hikes tend to happen so that things like Black Friday and Christmas sales make you believe you're getting a good deal)  Starting off with something like the 9600X build I listed below will give you far more longevity, and the socket itself will have 1 or 2 more CPU generations so you'll still have an upgrade path even if something like AM6 comes out. You can adjust your country (for currency accuracy) on the top right if the UK isn't correct.   [PCPartPicker Part List](https://uk.pcpartpicker.com/list/hZMRRV)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 9600X 3.9 GHz 6-Core Processor](https://uk.pcpartpicker.com/product/4r4Zxr/amd-ryzen-5-9600x-39-ghz-6-core-processor-100-100001405wof) | £175.99 @ Box Limited  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE 66.17 CFM CPU Cooler](https://uk.pcpartpicker.com/product/GpbRsY/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se) | £36.95 @ Overclockers.co.uk  **Motherboard** | [Gigabyte B850 GAMING WIFI6 ATX AM5 Motherboard](https://uk.pcpartpicker.com/product/3f8Pxr/gigabyte-b850-gaming-wifi6-atx-am5-motherboard-b850-gaming-wifi6) | £169.99 @ AWD-IT  **Memory** | [Silicon Power XPOWER Zenith Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://uk.pcpartpicker.com/product/ypRwrH/silicon-power-xpower-zenith-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdg) | £119.99 @ Amazon UK   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **£502.92**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-10-26 18:06 GMT+0000 |",buildapc,2025-10-26 18:04:42,1
Intel,nlm8n48,"youre a bit late , RAM prices are through the roof right now. My suggestion would be stay at am4 for now and wait till AM5 components come down in prices, i dont know when, but they will eventually",buildapc,2025-10-27 08:32:52,1
Intel,nlgr53l,"That’s amazing, I’ll shop around for some parts and look at the lists others have supplied in this thread, I’d be thrilled if I can get it as low as £400. Thanks so much for this advice.",buildapc,2025-10-26 12:17:39,2
Intel,nlgs8wt,"I’m not especially determined to keep my DDR4 RAM other than the cost of buying DDR5. I’ll have a look at DDR4 options but I’m out of the loop on the 13/14th gen Intel issues. In my ignorance, I’m overly cautious and would probably prefer AMD, and most comments are suggesting the 9600X in that avenue.  Thanks for another perspective, I’ll learn more about the Intel issue and look into DDR4 options too.",buildapc,2025-10-26 12:25:39,3
Intel,nlgrehj,Thanks so much. That PCPP list is cheaper than I was hoping so I’m very happy with that. Thank you,buildapc,2025-10-26 12:19:32,1
Intel,nlgsnb6,The future upgrade path was also on my mind. I’ve already got the best the LGA1151 socket offers and it’s been frustrating that I can’t upgrade without replacing basically everything! Starting at the low end of AM5 means I can upgrade to better CPUs a few years from now without replacing the motherboard. Thank you for the comment.,buildapc,2025-10-26 12:28:33,2
Intel,nlgt9v5,"Haha, it’s good to hear directly from people who have recently made similar jumps. You hear barely anyone still talking about hardware this old and the best ways to upgrade.   The combination you mentioned seems almost ubiquitous in this thread, and I’ll probably go for the very same. I hadn’t really thought to look at prebuilt or combo deals but I’ll browse around here. It would be nice to move to watercooling too, just maybe not right now. Thanks for the comment!",buildapc,2025-10-26 12:33:03,2
Intel,nlni6ah,"Don’t I know it. But with consideration of all the other comments here, and the fact I’ll be spending probably £400 even without RAM, I might as well make it last a bit longer by going for a newer socket. Thanks for the comment still :)",buildapc,2025-10-27 14:16:48,1
Intel,nlh0cmw,"The Intel issue really only applies to i7 and i9. However, i wouldn't even suggest a 13th gen part because the memory controller on them was borked. They fixed that for 14th gen.",buildapc,2025-10-26 13:20:09,2
Intel,nlhk1rj,All the AM5 processors will likely suit your gaming needs while still giving you a bit of an upgrade path if necessary.,buildapc,2025-10-26 15:11:26,1
Intel,nlhr0vo,"Any benchmarks you see for 13/14 Gen make sure to take note of the RAM it uses.  Most are benchmarked using DDR5 and DDR4 will have lesser performance, especially if the DDR4 you have is  DDR4-2133 or something rather than later production DDR4-3200 or higher.",buildapc,2025-10-26 15:47:54,1
Intel,nlgwzz2,"In general Am4 is not bad per se, a lot of people on am4 (me included) gonna sit out the launch of am6. Those who started at its launch 2017 got a hell of an upgrade out of that socket. Problem is just its really bad value at its price compared to am5....   Oh. And dont forget to sell your old stuff, you will still get money for it....",buildapc,2025-10-26 12:58:45,1
Intel,nln7cth,"No worries brother. I hadn't built a pc in 15 years so figuring all this stuff out again was a fun but painful update. It is surprising how badly connected info is on all this stuff, especially how even a budget gaming rig for around $1k today absolutely smashes hardware that was near the top of the line pre-pandemic.   I went with a peerless assassin 120se air cooler... I helped a buddy replace his water cooler in his pre-built when the little pump died on it... made me a little sour on water cooling too. With a little 65w 9600x processor its only about asthetics as you could cool that with a $20 air cooler if you wanted.   Goodluck and happy hunting!",buildapc,2025-10-27 13:16:45,1
Intel,nlhr7lk,"It applies to all Raptor Lake CPUs with a TDP of 65W or higher, which includes the i5s.  However, it was significantly more common on the i7s and i9s as you mentioned.",buildapc,2025-10-26 15:48:52,3
Intel,nlh58bv,"Thanks, not sure I’ll get much for a CPU/motherboard that is definitely faulty somewhere but maybe someone else will be able to diagnose it better and get something out of it.",buildapc,2025-10-26 13:50:09,1
Intel,nlhw0za,"If we want to be pedantic about it, it applies only to the *600 class i5s as the 400 and 500 are not Raptor lake.    I do not think i ever saw any evidence of failing 14600 class CPUs and with the BIOS updates, these will pretty much be guaranteed to be fine. Voltages are good on these parts.",buildapc,2025-10-26 16:13:25,2
Intel,nkpcmqc,"9060xt 16gb or 5060ti 16gb  https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html",buildapc,2025-10-22 01:24:24,2
Intel,nkpdz44,Basically without a cpu upgrade you'd be holding back most modern gpus you'd put in your system.   I guess you could get a GPU as a future proof. Just remember you won't be getting all of the preformance.,buildapc,2025-10-22 01:32:37,0
Intel,nkpib4f,"I was afraid of this, but understood. I'm open to pushing the budget around if needed. The 1440p was not something I originally planned, but after making the swap last year I love it. Again with an eye to future proofing, I get that I need to rethink the budget. If we were talking closer to $500, does that extra $100 do enough to be in a more reasonable ballpark?",buildapc,2025-10-22 01:59:02,2
Intel,nkpfr3f,"You're laying it on a bit thick, it's not even an actual bottleneck on the card. A 9060XT isn't close to overkill",buildapc,2025-10-22 01:43:24,3
Intel,nkpek3t,I doubt it. They’re aiming for 60fps and are fine with less. And want to spend less than $300. It’s not like they’re trying to put in a 5090.,buildapc,2025-10-22 01:36:13,1
Intel,nkplc9f,"Yes $500 is much easier, but that card will work fine for 1440p, it's just that some people don't like buying used.   9060 XT or 7800 XT are both fine for 1440p. 7800 has more raw power but 9060 has better software support. Maybe 16gb 5060ti but they can be hard to find sometimes.",buildapc,2025-10-22 02:17:31,1
Intel,nkpg282,No no no. It's more his cpu won't be able to get the most out of a card like that. The card is overkill for his cpu.,buildapc,2025-10-22 01:45:18,0
Intel,nkpes19,I disagree. His cpu is very outdated and will bottle neck any modern GPU that isn't super budget tier.,buildapc,2025-10-22 01:37:32,0
Intel,nkpr2d9,"Got it. Yes I have no issue buying used/was frankly expecting I would. This is all good to know, thank you!",buildapc,2025-10-22 02:53:29,1
Intel,nkpfeke,"https://youtu.be/TXKyQYiLro8?si=x9qS64Z2w4HIJG3Z  There’s outliers, but not many?  And here’s for the 5060/9060: https://youtu.be/NqRTVzk2PXs?si=0Qp-3x-2sdNJeJKo",buildapc,2025-10-22 01:41:18,1
Intel,nkpiu19,"Appreciate the back and forth on this. Frankly I just don't think I can upgrade GPU and CPU at the moment, especially if I need to up the GPU budget by $200. I think $500 is probably swing-able (just not what I was planning). But I imagine even with that push I can't reasonably do both. At the risk of sounding silly...the idea of getting a GPU now that will be limited by CPU, and then upgrading the CPU in a couple years, seems feasible and reasonable to me. I don't mind not being able to push things in terms of gaming, or having to wait a couple years to fully use the GPU's potential.   Again I've been fine with my 1060 for all these years...so it's not like I'm trying to max things out.",buildapc,2025-10-22 02:02:11,1
Intel,nkqt024,"I have an R5 5600 and 5070.... Yes, there is some bottleneck... At 1080p(the card is usually 95%).  At 1440p? Minimal bottleneck, the card goes 100%, but, very rarely there is a stutter, which is alleviated with a bit of tweak on the settings.   To say it would be a bottleneck for a 9060xt or 5060ti is a stretch.",buildapc,2025-10-22 08:16:59,1
Intel,nkpivnh,yeah. I bought a 9060xt 16gb to go with my r5 3600 after watching their video.  I don't regret it at all.,buildapc,2025-10-22 02:02:27,4
Intel,nkpj5c9,This is fair. It's all up to the buyer. I just like giving people as much info as possible before their purchase. All that matters in the end is if the setbacks are worth it for you or not.,buildapc,2025-10-22 02:04:06,2
Intel,nkpku6f,"The ~~5060/~~ 9060xt’s are $350. Or, are you using CAD $?",buildapc,2025-10-22 02:14:30,2
Intel,nkqtdlp,"I have an R5 5600+ rtx5070.  There is a little bottleneck, but you will not really notice it. Especially, if you play a little with settings on 1440p or dsr.  You shouldn't worry too much... That said, 9070 or 5070 /  9060xt or 5060ti, up to you.  Please  note that, the 9070 is faster than the 5070 and you might need to dial it back a little to minimize the CPU bottleneck.  Hope that helps.",buildapc,2025-10-22 08:21:01,1
Intel,nkqti5i,Don't shoot the messenger. Don't believe me? Look it up yourself. These aren't my stats lol.,buildapc,2025-10-22 08:22:18,1
Intel,nkpjsc0,"Of course, and appreciate the info/clarity! Again my biggest hurdle here is just being so out of the game that I have no idea what I'm looking at. The comments here are extremely helpful, even if they are just helping me realize my original plans may not make sense, or I need to be realistic about what I may still be compromising!",buildapc,2025-10-22 02:08:01,1
Intel,nkpr70w,"USD. But, and I realize I didn't make this clear in the OP, I also don't mind buying used if needed.",buildapc,2025-10-22 02:54:18,1
Intel,nkqttsr,Look I don't want you to get the wrong idea. I'm not trying to crap on the Ryzen 5600. I've simply pointed out a fact. One that op should be aware of.,buildapc,2025-10-22 08:25:39,2
Intel,nkpkbu8,Exactly. Most people don't upgrade all at once anyways unless it's a new build. It's a very normal thing.,buildapc,2025-10-22 02:11:22,2
Intel,nkprlx2,The Powercolor Reaper 9060xt 16gb for $350 at Newegg seems like the right move if you can swing it. Otherwise you’re back to the overwhelming mess of cross referencing past generation GPU performance and used prices.,buildapc,2025-10-22 02:56:59,2
Intel,nkpydkf,"Thanks. Yes I think I can, I realize my budget may have been a little unrealistic - $350 should be entirely doable. Thanks for the suggestion/tip here! That said - if we were looking in the $400-$450 range, anything you'd recommend instead?",buildapc,2025-10-22 03:43:42,2
Intel,nkq4hw7,"There’s too many variations of used options out there for me to definitively say. Looking at some charts and some prices, maybe a 4070 super? Less VRAM but still performs quite a bit better than the 9060xt and has DLSS. Older generations of AMD cards won’t have FSR4 access (right now?) which looks better than FSR3. Could be fine for you, just mentioning it though.   https://tpucdn.com/review/zotac-geforce-rtx-5070-solid/images/average-fps-2560-1440.png",buildapc,2025-10-22 04:29:34,2
Intel,njgpccd,"I am such a big supporter of building PCs but right now, for your budget if you’re in the USA, I would consider this pre built. https://www.bestbuy.com/product/ibuypower-y40-gaming-desktop-pc-intel-core-i7-14700f-nvidia-geforce-rtx-4070-12gb-32gb-ddr5-ram-2tb-nvme-black/J3R75JY7PQ  $824 and with what I think are better specs.  Edit: This is sold out, I apologize. I just got my brother one yesterday.",buildapc,2025-10-14 15:36:03,1
Intel,njh94p5,"Looks roughly right, though you could make a few tweaks with the current pricing:  [PCPartPicker Part List](https://pcpartpicker.com/list/4zL2v4)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $149.99 @ Amazon  **CPU Cooler** | [Cooler Master Hyper 212 Spectrum V3 71.93 CFM CPU Cooler](https://pcpartpicker.com/product/jgbRsY/cooler-master-hyper-212-spectrum-v3-7193-cfm-cpu-cooler-rr-s4na-17pa-r1) | $14.99 @ Amazon  **Motherboard** | [ASRock B650M-HDV/M.2 Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/Dq4Zxr/asrock-b650m-hdvm2-micro-atx-am5-motherboard-b650m-hdvm2) | $109.99 @ Amazon  **Memory** | [PNY XLR8 Gaming 32 GB (2 x 16 GB) DDR5-6000 CL36 Memory](https://pcpartpicker.com/product/dVgZxr/pny-xlr8-gaming-32-gb-2-x-16-gb-ddr5-6000-cl36-memory-md32gk2d5600036xr) | $77.98 @ Amazon  **Storage** | [Silicon Power UD90 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/4kpzK8/silicon-power-ud90-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-sp01kgbp44ud9005) | $54.97 @ Silicon Power  **Video Card** | [Intel Limited Edition Arc B580 12 GB Video Card](https://pcpartpicker.com/product/Kt62FT/intel-limited-edition-arc-b580-12-gb-video-card-31p06hb0ba) | $249.99 @ B&H  **Case** | [Rosewill FBM-X3 MicroATX Mid Tower Case w/650 W Power Supply](https://pcpartpicker.com/product/pGJBD3/rosewill-fbm-x3-microatx-mid-tower-case-w650-w-power-supply-fbm-x3-650-g) | $89.99 @ Newegg   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$747.90**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-10-14 12:57 EDT-0400 |  The 7600X is a bit better than the 8400F, so I'd definitely take it instead. The Peerless Assassin is fantastic, but a bit overkill, the $15 Hyper 212 is still going to handle a 6-core Ryzen just fine. I've used the case/PSU bundle a few times, it's a solid deal for a basic but nice tempered glass case with fans and the PSU is serviceable. Altogether it comes in at about $750 with the B580, which is probably the best new option that comes in under your target price, but if you think you can stretch your budget by another $50 the [9060XT 16GB](https://pcpartpicker.com/product/DRYfrH/xfx-swift-oc-radeon-rx-9060-xt-16-gb-video-card-rx-96tsw16bq) is a stronger card. You can also check the used market, the $300 mark can find you an RTX 3080 10GB or RX 6800XT sometimes.",buildapc,2025-10-14 17:14:29,1
Intel,njienrv,Thanks for the recommendation! Looks like a really good build so it’s unlucky that it sold out. I’ll keep an eye out for pre-builts now too; I had prematurely ruled them out of consideration before. Thank you and hope your brother enjoys the build!,buildapc,2025-10-14 20:39:56,1
Intel,njifw6a,Thank you so much! I think I’ll spend that extra $50 for the 9060XT then. This is super helpful!,buildapc,2025-10-14 20:45:59,1
Intel,nissmda,Everything,buildapc,2025-10-10 16:42:53,22
Intel,niuogxx,"Upgrade your cpu to 14600k, add 16gb same model if possible and the gpu is up to you bc the 14600k can handle any gpu.",buildapc,2025-10-10 22:37:00,3
Intel,nisqkgr,If you only play esports games then upgrade cpu. Also  ram doesn’t cost too much so you might aswell upgrade that.,buildapc,2025-10-10 16:32:31,5
Intel,nisugfi,CPU,buildapc,2025-10-10 16:51:56,2
Intel,niu8byh,"A CPU upgrade will do the most for the games you play, even something like a i5-14400 would go a long way all by itself. Upping the ram could help some if you can get the same kit you already have for a reasonable price",buildapc,2025-10-10 21:04:23,2
Intel,niu2evz,"Used 16 gigs of ram, update bios and get 14 gen Intel cpu",buildapc,2025-10-10 20:33:30,1
Intel,niu6z4q,"You can get good CPU and GPU for a ""budget"" price if you checkout Facebook marketplace. With these upgrade, you can get more FPS.",buildapc,2025-10-10 20:57:11,1
Intel,niuy8xz,bottlenecked by cpu. You'll double frames getting i7 12700k/kf for sure. those games you mentioned are cpu intensive,buildapc,2025-10-10 23:38:41,1
Intel,nj68etq,Gaming chair,buildapc,2025-10-12 21:43:44,1
Intel,njfku1n,[use this website](https://www.logicalincrements.com/),buildapc,2025-10-14 11:46:26,1
Intel,njk1e3k,"Honestly, save up money so you can actually buy a new higher end PC sometime in the next few years. You won't regret it. It's a better investment than trying to upgrade your current one.",buildapc,2025-10-15 02:23:26,1
Intel,nisvfnh,"Jesus, you have like the lowest end hardware for the generation you bought into.... you can upgrade litterally everything.",buildapc,2025-10-10 16:56:37,1
Intel,nit8a87,So i Dont have that much budget but i want to be able to not worry about having low fps also my monitor is 165hz so i dont need that much power but i just want to know what can i keep because changing everything is a lot of money for me right now   Thank You for Your response!,buildapc,2025-10-10 17:58:30,1
Intel,nisz7za,You didn't have to brutalize him like that,buildapc,2025-10-10 17:14:51,4
Intel,nium0m0,Damn I was gonna say that,buildapc,2025-10-10 22:21:55,1
Intel,niu6p1o,"This is the only reasonable comment, why is everyone else acting like FPS games require 1500 dollars of hardware",buildapc,2025-10-10 20:55:44,6
Intel,nistet8,I remember buying my first RAMsticks. Way cheaper about 8 years ago.,buildapc,2025-10-10 16:46:50,1
Intel,nitaqpo,"It's possible his motherboard is older and only supports DDR4. In that case, the memory will be expensive and not worth upgrading.  \-   Added later, I did notice he included the motherboard name, and based on a quick search, that is a DDR4 motherboard.",buildapc,2025-10-10 18:10:51,1
Intel,niugzka,Ok I would look for a i5 14400 and some ram but when I have more budget what would you recommend me to upgrade later on,buildapc,2025-10-10 21:52:15,1
Intel,niszwzl,Just being honest haha not gonna sugarcoat it. Man’s needs a new cpu and ram which requires a new motherboard and a new gpu will of course help a lot.  Literally nothing he can keep  7800x3d/9800x3d  Corsair 6000mhz ram have kits relatively cheap  Hell a 5060 is a lot faster then his Intel gpu but I’d opt for a 5070 minimum Any am5 suitable board,buildapc,2025-10-10 17:18:11,-1
Intel,niuavpp,I saw that too I was like 🤧,buildapc,2025-10-10 21:18:19,2
Intel,nisuc5t,I’m pretty new to pc gaming,buildapc,2025-10-10 16:51:23,1
Intel,nitpity,yes its ddr4,buildapc,2025-10-10 19:26:35,1
Intel,niu6ch8,You don't need a fucking 7800x3d and 5070 for fortnite and valorant,buildapc,2025-10-10 20:53:55,9
Intel,nitrdas,All good. I wasn't really judging. Just made me laugh for real  Your response will help him. Gotta start somewhere,buildapc,2025-10-10 19:36:15,2
Intel,nivya9t,Someone’s awfully angry haha the guy didn’t post anything regarding budget. I also mentioned 5060 was a huge upgrade and just said what I would get.   If he’s gonna go through the trouble of upgrading his CPU and ram which would require a new motherboard anyways why not get something that’ll last him years. No one “needs” half the pc parts they get but if you keep your pcs for 6+ years then yes a 7800x3d would be a much better choice.   It’s already dropped in price and likely to go on sale during Black Friday. Also gives the guy a chance to upgrade his monitor over the next year or 2 if he so chooses.  You see I don’t mind going to work saving my money and spending just over $1000 for a pc I’ll get years of use out of. Others like you might like buying a new mid range mobo/cpu and then upgrade the gpu later when it becomes problematic and then upgrade the cpu and mobo again in this constant cycle or low tier parts. Everyone’s different lol,buildapc,2025-10-11 03:45:54,1
Intel,niv01nu,BUT IT WOULD BE  NICE TO HAVE n why do you think those will be the only games he ever plays? Lol,buildapc,2025-10-10 23:50:25,-1
Intel,niv11v9,Because they're the primary games he mentioned? You're recommending he drop well over a grand on new parts like it's the obvious thing to do. An A750 with a better CPU will play most games fine at 1080p just fine.,buildapc,2025-10-10 23:57:00,2
Intel,niv62s2,So he should build a system that can play every game? *Every game?*,buildapc,2025-10-11 00:30:38,1
Intel,nmjxf4y,i4-14700 and Arc B580.. not saying any of those choices are wrong... but the i7 is a high end CPU paired with a budget GPU. i5-14600K and 9060XT could've been better but ok... and also the Arc B580 can't really play on 4K without HEAVY XeSS.,pcmasterrace,2025-11-01 16:14:26,3
Intel,nmkeyx2,"It is a pretty questionable HTPC build tbh. You definitely should’ve allocated more money to the GPU.   14700 is a pricey chip, I’m sure the 14600k is cheaper and you easily could’ve power limited it in bios. Alternatively a 14400 would’ve been fine too.  As well I went ddr4 with my build and it was quite a bit cheaper.   Sf750 for this build is super overkill. It’s a great PSU, I have 2 of them, but they are very pricey.   Just the cost of fans you probably could’ve gotten a 9060xt as well.",pcmasterrace,2025-11-01 17:44:02,2
Intel,nmk0lh8,"This is not a gaming PC, it’s a media centre PC. For gaming, I use an RTX 4070 Super. The 65 watt i7 was chosen because it’s a cool running CPU, averaging 30–32 degrees in this build. Also, to semi future proof it for if I ever do turn it into a gaming PC. The Arc B580 was chosen for it’s great through-hole design that helps with airflow, which is beneficial given this SFF PC’s restrictive layout. That said, I have another build with a ARC B580 that games at 1440p just fine, so this PC would still be perfectly capable for gaming.",pcmasterrace,2025-11-01 16:31:03,6
Intel,nmkm09v,"It’s pointless to invest more money into the GPU at this stage, given that this is a media centre PC and not a gaming rig. The 65 watt i7-14700 was chosen not only because it’s a cool-running CPU, averaging 30–32 degrees, but also to semi future-proof the system in case I decide to convert it into a gaming PC later on. If that happens, I’d be using a higher-end GPU and wouldn’t want to be limited by an i5. The Arc B580 was selected because it’s more than capable for my current needs and features a through-hole design that helps with airflow. As for the PSU, while a 750-watt PSU isn’t strictly necessary, it’s 2025 and with GPUs continuing to demand more power, I wouldn’t go any lower. With the way this build is set up, I’d only need to upgrade the GPU in a few years, and that’s only if I choose to turn it into a gaming PC. The fans, been using Noctua fans for many years and I wouldn’t touch anything else.",pcmasterrace,2025-11-01 18:19:59,1
Intel,nmk3piz,Lol sorry mb. But you should mention that somewhere,pcmasterrace,2025-11-01 16:46:59,1
Intel,nmkoln6,What does media center pc mean in your context? For me my HTPC is being used like a gaming pc for my TV.   Also Intel is very good with idle power draw but make sure you’ve limited the boost wattage. For me I’ve limited my 14500 to 95w on short bursts. The 14700 should have a higher base boost power.,pcmasterrace,2025-11-01 18:33:33,1
Intel,nmkq92m,"I mean just watching movies, listening to music, streaming and video encoding. No gaming.  I also have a 14500, a 14700K, and another 14700 in three other PCs. Those are gaming systems, and I’ve found that when set to Intel default settings, they average around 60–65 degrees during gaming without me needing to enforce power limits, since Intel default settings enforce them automatically.  Edit: 60–65 degrees for the non-K CPUs. My 14700K runs more like 70–75 degrees. Honestly, I much prefer the non-K 14700, since in my benchmarks it performed only 3–5% lower, yet ran cooler and used less power. Not like anyone overclocks these days anyway.",pcmasterrace,2025-11-01 18:42:19,1
Intel,nmhduxm,"Welcome to the PCMR, everyone from the frontpage! Please remember:  1 - You too can be part of the PCMR. It's not about the hardware in your rig, but the software in your heart! Age, nationality, race, gender, sexuality, religion, politics, income, and PC specs don't matter! If you love or want to learn about PCs, you're welcome!  2 - If you think owning a PC is too expensive, know that it is much cheaper than you may think. Check [http://www.pcmasterrace.org](http://www.pcmasterrace.org) for our famous builds and feel free to ask for tips and help here!  3 - Consider supporting the folding@home effort to fight Cancer, Alzheimer's, and more, with just your PC! [https://pcmasterrace.org/folding](https://pcmasterrace.org/folding)  4 - Do you need a new PC? We're giving away a high-end PC build in a WORLDWIDE constest: https://www.reddit.com/r/pcmasterrace/comments/1nnros5/worldwide_giveaway_comment_in_this_thread_with/  We have a Daily Simple Questions Megathread for any PC-related doubts. Feel free to ask there or create new posts in our subreddit!",pcmasterrace,2025-11-01 03:43:10,1
Intel,nmerzpv,Meanwhile AM4 still getting new CPU's,pcmasterrace,2025-10-31 18:04:33,2642
Intel,nmem9lw,"AMD have clarified their original stupid as fuck statement mate. Apparently what the utterly inept fuckwads in their marketing department were MEANT to say, according to the engineers, is that any NEW software tech that's developed from now on, will only be available for 7000 series and up GPU's. RDNA1 and RDNA2 GPU's will continue to receive the same game ready style drivers as they always have.   It just means that if AMD develop anything new, like FSR4, it won't be made backward compatible with older cards. A lot of us have been saying for years that AMD's entire marketing team should be fired, and replaced with people from the engineering department, because this level of fuck up is literally so common that their nickname these days is ""Advanced Marketing Department"", and not in a good way.",pcmasterrace,2025-10-31 17:36:00,3214
Intel,nmev636,"neither, whip out your voodoo and start blasting some quake.",pcmasterrace,2025-10-31 18:20:58,115
Intel,nmem1jh,"check toms hardware,they just cant communicate properly,your hardware is fine you will get updates and game support",pcmasterrace,2025-10-31 17:34:53,578
Intel,nmf9ssy,"My 6700xt hasnt actually gotten any of the new features in the last few years anyway. All this did was make it official. As long as my games play the way they always have though, who cares",pcmasterrace,2025-10-31 19:36:54,68
Intel,nmf0s2e,"It’s 2025, there are no ethical billion dollar companies.",pcmasterrace,2025-10-31 18:49:46,59
Intel,nmelym2,"Neither, backlog is the way.",pcmasterrace,2025-10-31 17:34:28,57
Intel,nmeljgi,"Turn back and chill with your older, but still perfectly adequate, hardware.  My 3050 is still going strong, I just can't play with top settings.",pcmasterrace,2025-10-31 17:32:18,109
Intel,nmerk8m,I don’t think the 5070 or 5070ti are melting. So that’s where I’m at.,pcmasterrace,2025-10-31 18:02:22,38
Intel,nmeoaf1,This is if you have fomo. I have a base model 4070 carrying me just fine after using a 1080ti sense 2019.,pcmasterrace,2025-10-31 17:46:04,6
Intel,nmevy57,"I dunno, I'll take old drivers over potential house fire.",pcmasterrace,2025-10-31 18:25:00,25
Intel,nmf8eac,Probably Lisa Su right now:  ![gif](giphy|W3a2OU6RGyv7OmJaw0),pcmasterrace,2025-10-31 19:29:24,5
Intel,nmfi83p,my 13 year old 7950 still works and runs games.  support doesnt mean shit,pcmasterrace,2025-10-31 20:21:48,4
Intel,nmfxny9,at least minimum support means shit still works.  It just means that it won't get better. The performance doesn't really degrade.  Melting Cables on the other hand means you're fucked.,pcmasterrace,2025-10-31 21:48:15,12
Intel,nmexs17,2070s.. *there is still fight left in me*  Gotta hold on for another year.. or until next gen of gpu’s arrive..,pcmasterrace,2025-10-31 18:34:25,7
Intel,nmgzn7o,I don’t understand what kind of support you’re looking for on a 5 year old card? Idk about windows but on Linux cards even older than that work fantastic.,pcmasterrace,2025-11-01 01:59:45,3
Intel,nmi5lex,"doesn't amd's case only apply to the windows drivers? on linux the drivers are open source, so realistically the support ends whenever the drivers stop being maintained by the community.",pcmasterrace,2025-11-01 08:25:28,3
Intel,nmia5lz,Thank God the 5070 TI is fine had it for about 6 months now and no issues at all The burning connector is she seems to only be with the 5080 and 5090s mostly,pcmasterrace,2025-11-01 09:16:28,3
Intel,nmiuc2f,"Are 5090 still melting? I have seen like 5 the entire year, reddit would make me think there 10 gpus a day burning",pcmasterrace,2025-11-01 12:25:18,3
Intel,nmlr1yz,https://preview.redd.it/v5r80zlrwpyf1.jpeg?width=1179&format=pjpg&auto=webp&s=12a01e28df4e2a710fb7d1d511a3b78987bd5bd7,pcmasterrace,2025-11-01 21:59:09,3
Intel,nmf55yh,"Drivers are still supported on 6000 and 5000 series cards, just no new game optimization for old cards.",pcmasterrace,2025-10-31 19:12:24,6
Intel,nmf5i11,I had such high hopes for celestial and battle mage,pcmasterrace,2025-10-31 19:14:10,2
Intel,nmfaqi9,The statement of AMD wasn't true though. Seems to be an old problem with their marketing people fucking up stuff.,pcmasterrace,2025-10-31 19:41:56,2
Intel,nmfbesp,Intel: Best I can do is layoff half the company,pcmasterrace,2025-10-31 19:45:33,2
Intel,nmfqqm7,Amd because I don't want to bother with proprietary drivers,pcmasterrace,2025-10-31 21:08:02,2
Intel,nmfxtaa,"AMD has good open source drivers, i don't see a problem.",pcmasterrace,2025-10-31 21:49:09,2
Intel,nmg033i,its simply not true.,pcmasterrace,2025-10-31 22:03:01,2
Intel,nmg0mdk,Hey! Thats not fair! Don't forget: games flagging 'updated/current' AMD drivers as a problem asking the user to rollback the driver until the problems are fixed!,pcmasterrace,2025-10-31 22:06:21,2
Intel,nmhdoui,*Red Sun in the Sky Plays*,pcmasterrace,2025-11-01 03:41:50,2
Intel,nmi7049,"Dont forget the w11 market manipulation to fuel the AI bubble, as well as DRM chips. lol... at this point its sad how latestage capitalism isn't for the market, and only for the profit.",pcmasterrace,2025-11-01 08:41:35,2
Intel,nmjsvtf,https://preview.redd.it/60sslsa23oyf1.jpeg?width=1080&format=pjpg&auto=webp&s=66010cbb44e829162bdbe445a7bdb2b9b850a1a6,pcmasterrace,2025-11-01 15:50:52,2
Intel,nmg6sps,NVIDIA by a mile and it's not close. DLSS alone should make the choice an easy one. If you are able to not fuck up plugging in the GPU power cable you've only got a 99.9999% chance of being just fine.,pcmasterrace,2025-10-31 22:46:19,4
Intel,nmgabl4,"I mean, my 5700 xt is older than covid and still getting updates.",pcmasterrace,2025-10-31 23:09:22,4
Intel,nmhs8dn,I will never regret 7900XTX. It's a beast.,pcmasterrace,2025-11-01 05:55:52,3
Intel,nmi3zn0,Lol I use AMD on Linux.  Long live my RX480.,pcmasterrace,2025-11-01 08:07:28,3
Intel,nmid8uq,"People like to shit on Nvidia but I've never had an issue with the hardware, and the software and support is buttery smooth.  Amd experience is like pulling teeth...",pcmasterrace,2025-11-01 09:50:07,3
Intel,nmix5jz,meanwhile AMD GPU users of 13 year old cards are still getting regular updates. [On](https://www.phoronix.com/news/AMDGPU-More-GCN-1.0-SI) [Linux](https://www.phoronix.com/news/Linux-6.19-AMDGPU-Analog).,pcmasterrace,2025-11-01 12:45:33,2
Intel,nmeuffv,my rx 5700 driver always crashing at radom at cyberpunk 2077 and Battlefield 6. it becames anoying at the point that my next gpu will be a Nvidia,pcmasterrace,2025-10-31 18:17:07,3
Intel,nmftmwf,"The lies against AMD spreading fast, wow.",pcmasterrace,2025-10-31 21:24:40,5
Intel,nmf11dl,NVIDIA. Much more stable drivers and the melting cables are almost always user error and less than 1% of cards shipped. Use the adapter. Don't buy into the freakout. 99% of people have zero issues. I had to switch off my 7900xtx to a 5080 and it's night vs day on stability and quality.,pcmasterrace,2025-10-31 18:51:08,7
Intel,nmgttxj,AMD already clarified they're NOT dropping support for RDNA 1 and 2.,pcmasterrace,2025-11-01 01:19:33,3
Intel,nmhfmfu,"Y’all are dumb. And it shows.  Tom’s Shitware misinterpreted the translation from German website and AMD then had to clarify that they aren’t dropping support. RDNA1 and 2 will still get updates, will still work with future games.   https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games   (Unironically had to be shared by tom’s shitware again)",pcmasterrace,2025-11-01 03:57:24,2
Intel,nmgkg59,"Welp, Arc is now out the window now that Nvidia is an Investor of Intel and encouraging Intel to use their low end Chips as iGPUs.  Your only real hope is AMD.",pcmasterrace,2025-11-01 00:16:08,2
Intel,nmeoa0h,Wait until he hears about zeus bolt,pcmasterrace,2025-10-31 17:46:01,2
Intel,nmesvgb,Arc,pcmasterrace,2025-10-31 18:09:06,2
Intel,nmg8gqo,I love how kids think this is a brand new phenomenon.  They've never heard of Terascale...  Plus AMD themselves have clarified their previous statement so there's that. You all can calm down now...,pcmasterrace,2025-10-31 22:57:12,2
Intel,nmja8de,"For those who use Linux, you'll never have driver problems with AMD, lmao.",pcmasterrace,2025-11-01 14:08:07,2
Intel,nmfdzy5,"When 9060xt showed up its ugly 8 GB head, Steve said that AMD should just shut the fuck up. In fact, most of the video (titled 9060xt) was a huge rant about their (AMD's) marketing department. They really need a Spanish Inquisition style cleansing.",pcmasterrace,2025-10-31 19:59:14,2
Intel,nmevqd4,I just wait awhile between GPU upgrades I used a 2080 up until this year it still worked perfectly fine but I had to start dropping seetings to medium or so to keep 120+ FPS so I just got a 5080 and will probably upgrade in another 2 or 3 generations.,pcmasterrace,2025-10-31 18:23:53,1
Intel,nmexts4,"Brother, you have a 7900gre",pcmasterrace,2025-10-31 18:34:40,1
Intel,nmf93ux,Why not both with AsRock and Sapphire?,pcmasterrace,2025-10-31 19:33:10,1
Intel,nmfa5iy,I'm praying my MSI RTX 3080 Suprim X card doesn't die until NVIDIA figures out how to make a proper card again. Still trucking along fine on medium/high settings.,pcmasterrace,2025-10-31 19:38:46,1
Intel,nmfcont,What's even happening with intel arc at this point ? I remember seeing some stuff about intel moving away from discrete gpus. Theres also been reports of them partnering with nvidia for something.,pcmasterrace,2025-10-31 19:52:19,1
Intel,nmfid8o,The only Nvidia cards that are having melting cable issues are ones with TDPs above 350W.   Cards below 330W don’t have these issues.,pcmasterrace,2025-10-31 20:22:33,1
Intel,nmfidxc,Why is AMD only supporting GPU for short time?,pcmasterrace,2025-10-31 20:22:39,1
Intel,nmfihwm,"Me, using an Intel Arc card I got on sale: ""Ha. I am superior to neither of these.""",pcmasterrace,2025-10-31 20:23:15,1
Intel,nmflh9k,"gamers are no longer my friend, there is only AI",pcmasterrace,2025-10-31 20:39:19,1
Intel,nmfls1v,buy a 5060 or 5070... they dont burn cables,pcmasterrace,2025-10-31 20:40:54,1
Intel,nmfmpd9,![gif](giphy|3oz8xLlw6GHVfokaNW)  Me with my 7900XTX,pcmasterrace,2025-10-31 20:45:50,1
Intel,nmfofqd,Never had a cable melt on any of my cards.,pcmasterrace,2025-10-31 20:55:10,1
Intel,nmfox6f,average closed source drivers L,pcmasterrace,2025-10-31 20:57:47,1
Intel,nmfpsyk,"I have a 4090 with an ATX 3.1 power supply, my cable has been good for two years and I game on it pretty much every day.",pcmasterrace,2025-10-31 21:02:41,1
Intel,nmfrzy5,https://preview.redd.it/4fh35v40kiyf1.jpeg?width=6240&format=pjpg&auto=webp&s=8d0b4fc823e9a4d83a926370927cbe6c14ad0b60  The secret ending,pcmasterrace,2025-10-31 21:15:14,1
Intel,nmftp6y,I’ve had no problems with my 7800xt,pcmasterrace,2025-10-31 21:25:02,1
Intel,nmfuwzz,I'd buy 5090 if not melting cables.,pcmasterrace,2025-10-31 21:32:07,1
Intel,nmfvd9v,Amd for the win,pcmasterrace,2025-10-31 21:34:46,1
Intel,nmfxhzq,Ironically the best new feature I ever got for my 3070 was FSR framegen. I don’t really understand the expectation of new features that were never promised.,pcmasterrace,2025-10-31 21:47:16,1
Intel,nmg20rn,All of this was reported inappropriatly by toms hardwareas well.  I absolutely loved in 2010s era but the fact that anybody gives it any attention to anything they publish in 2025 is beyond me.,pcmasterrace,2025-10-31 22:15:12,1
Intel,nmg25jn,"What percentage of cables do you really think has melted bro? 0.0001%? Meme aside, stupid shit like that is not worth even considering as a factor for a purchasing decision.",pcmasterrace,2025-10-31 22:16:03,1
Intel,nmg2e7u,Not just the support the performance and industry support for amd is abysmal to say the least,pcmasterrace,2025-10-31 22:17:35,1
Intel,nmg4dbv,Stable drivers > Day 1 Game Ready drivers with issues  I'd rather have a working outdated driver than troubleshooting and rolling back a new poorly coded driver,pcmasterrace,2025-10-31 22:30:21,1
Intel,nmg5pxe,Arc is effectively cancelled.,pcmasterrace,2025-10-31 22:39:14,1
Intel,nmg5zh7,My 4090 is fine,pcmasterrace,2025-10-31 22:40:58,1
Intel,nmg64rs,And then there is UE5.  Sad days. Really sad days...,pcmasterrace,2025-10-31 22:41:56,1
Intel,nmg8907,"don't forget Nvidia's shitty drivers. They somehow managed to fuck up my latest driver install on my 4070 mobile and now I can't play unreal engine games, OpenGL games, or Vulkan games without crashes/not working at all. I also can't easily install drivers because Nvidia driver installer has a problem with Intel Hyperthreading and throws a 7z.crc error every time you try installing drivers, you have to go to msconfig and limit number of processors to 2 when doing a driver update",pcmasterrace,2025-10-31 22:55:47,1
Intel,nmgd1xz,"Wait, there's people updating drivers? I've been stuck on a version from 2021, granted it's an RX 580",pcmasterrace,2025-10-31 23:27:02,1
Intel,nmggpu8,I have a 5070ti what's  with the melting port thing ? Just had mine for like a month,pcmasterrace,2025-10-31 23:50:59,1
Intel,nmgia14,"I wouldn't get your hopes up for Intel, [discrete GPUs may be on the chopping block](https://www.notebookcheck.net/Intel-Arc-Celestial-dGPU-seems-to-be-first-casualty-of-Nvidia-partnership-while-Intel-Arc-B770-is-allegedly-still-alive.1118962.0.html) following their NVIDIA partnership.  Not confirmed or anything, but it is also notable Tom Petersen was on a podcast a week ago and [refused to talk about Celestial](https://videocardz.com/newz/intels-tom-petersen-explains-xe-and-arc-gpu-naming-spaghetti-declines-to-discuss-b770-and-celestial-updates). And even if discrete GPUs still do happen with Celestial, will they keep going after that?",pcmasterrace,2025-11-01 00:01:27,1
Intel,nmgieb9,"For anyone tempted by big blue, my ARC B580 has been fantastic in both Windows and Linux.",pcmasterrace,2025-11-01 00:02:16,1
Intel,nmgpjhy,Intel Arc is fire and more people should give it a chance.   The B580 gave me an absolutely beautiful experience.,pcmasterrace,2025-11-01 00:50:37,1
Intel,nmgqc7l,"I have felt like such the fucking outsider when it comes to like 99% of hardware gripes...  Oh anything past a 30 series card is gunna melt!   - Really? Cause my 3060ti litterally never complains regardless of what Im doing  Old Ryzen chips and board are fucking trash and never last  - Really my First gen Ryzen 5 2600x has litterally never failed me after being ran nonstop for years, which is understandable horrible...  You wont be able to sustain decent thermals without XYZ AIO etc etc etc...  - ./looks at temp 35c with a stock cooler and only 2 intake fans. ' Uh ok? '",pcmasterrace,2025-11-01 00:55:55,1
Intel,nmgqdne,Arc = Nvidia now,pcmasterrace,2025-11-01 00:56:10,1
Intel,nmgt0mj,I mean there is a 3rd path with Intel arc.,pcmasterrace,2025-11-01 01:13:53,1
Intel,nmgt0ok,"If you had asked me when I bought the 3090 that it would be the last card I’d ever buy I would’ve laughed, but here we are. Not only would I have to upgrade my PSU, but I’d risk the melting cables",pcmasterrace,2025-11-01 01:13:54,1
Intel,nmgt96s,"Sitting here with my Intel GPU and AMD CPU, wondering how soon I'll get fucked over",pcmasterrace,2025-11-01 01:15:34,1
Intel,nmgtmh7,"Me, as a shareholder of both: 💀",pcmasterrace,2025-11-01 01:18:06,1
Intel,nmh25o8,"That's called the middle, walk the untreaded path or walk back",pcmasterrace,2025-11-01 02:17:23,1
Intel,nmh283o,"Intel ARC is dead. Termination of Intel ARC would have been part of the Nvidia deal. If it was not, Nvidia was freely  helping ntel to gain a solid market share of gpus by temporarily providing a good GPU until ARC becomes a solid product. Listen!! INTEL ARC IS DEAD!¡",pcmasterrace,2025-11-01 02:17:52,1
Intel,nmh2iv0,"Amd support is only relevant if you are in windows, come to the dark side, we have unlimited driver support.",pcmasterrace,2025-11-01 02:20:02,1
Intel,nmh2zbk,If have an AMD RX 480 and I still receive driver updates every few months. I bought my card on 2017 I think I'm alright.,pcmasterrace,2025-11-01 02:23:14,1
Intel,nmh39j4,High regards,pcmasterrace,2025-11-01 02:25:14,1
Intel,nmh76mu,*0-5 years old cards...   The Ryzen Z2A was just released with the white Xbox Ally. It uses RDNA 2... In late 2025... When they already released RDNA 4.,pcmasterrace,2025-11-01 02:53:14,1
Intel,nmh9z20,Someone explain plz,pcmasterrace,2025-11-01 03:13:38,1
Intel,nmhaq5i,"I know intel had problems with their cpus, and i stopped buying them becaue of it.  But has anyone heard of any problem with their GPU's ? I'm interested in them but don;t want to guy them and find out their GPU's rot too...",pcmasterrace,2025-11-01 03:18:54,1
Intel,nmhbqg2,"This is Intel's chance for a comeback, if they can secure it. There's a gap in supply to a growing demand.",pcmasterrace,2025-11-01 03:26:27,1
Intel,nmhck3l,Intel Arc is a shovel next to the rock telling you to dig your way out but the shovel is only 10 dollars.,pcmasterrace,2025-11-01 03:32:48,1
Intel,nmhcp93,"The alternative is, you don't really need the highest end model",pcmasterrace,2025-11-01 03:33:56,1
Intel,nmhfbtu,RX6000 can't do 60 real FPS 4K in 2025,pcmasterrace,2025-11-01 03:55:01,1
Intel,nmhgyh6,"minimum support means nothing, i'm still very happy with my rx570",pcmasterrace,2025-11-01 04:08:24,1
Intel,nmhl876,"melting connectors is an issue for people who can spend $1500-4000 on gpu. If you can pay $1500+ then  amd have no issues with support at this price range (but much cheaper actually). rx580 was supported 7 years, then moved to legacy (which is still decent). Pointless post in many ways",pcmasterrace,2025-11-01 04:46:14,1
Intel,nmhm5zu,My 4070 is sick and y'all are haters,pcmasterrace,2025-11-01 04:54:50,1
Intel,nmhmk78,Support for Radeons can be provided by the community since their drivers are open source. I'm playing SWTOR on high on a 12-year-old Radeon.,pcmasterrace,2025-11-01 04:58:29,1
Intel,nmhmlmw,It's that intel that added one more pin to its every new gen CPU to make it incompatible with previous?,pcmasterrace,2025-11-01 04:58:51,1
Intel,nmhqp1j,so full amd cpu and graphics set. Cause intel melting things too.,pcmasterrace,2025-11-01 05:39:41,1
Intel,nmhrn2k,Man how I wish they would release something like a B780 which competes with the 9070 XT and 5070 Ti because apparently the B770 will compete with the 9060 XT 16GB and 5060 Ti 16GB. Might be wrong though.,pcmasterrace,2025-11-01 05:49:27,1
Intel,nmhxcd2,My 5700xt is running fine…. 2019 card.,pcmasterrace,2025-11-01 06:52:32,1
Intel,nmi0g43,"AMD with ryzen - yes we have supported the am4 platform for 8 years and we're not done yet.  AMD with Radeon - noooo, just because it's still in production and is one of our best selling cards doesn't mean we can support a card from 2 generations ago, that's too much to ask.",pcmasterrace,2025-11-01 07:27:39,1
Intel,nmi4hwr,pc enthusiast in a nutshell.,pcmasterrace,2025-11-01 08:13:05,1
Intel,nmi4l2a,"Yay even more reason to move to Linux, you didn't have to do this AMD! My RX 6900XT will be glad to run on open source drivers",pcmasterrace,2025-11-01 08:14:03,1
Intel,nmi7en6,Im still using my rx580. It's 10 years old,pcmasterrace,2025-11-01 08:46:09,1
Intel,nmi7jcp,"I'm waiting for the B770, says it will be close to the 5070 but cheaper with 16GB VRAM...",pcmasterrace,2025-11-01 08:47:35,1
Intel,nmi9qea,"My 4670 still be getting updates. Not an AMD card though, but an ATi one.",pcmasterrace,2025-11-01 09:11:51,1
Intel,nmia3l4,Sure that intel will find its own arc to piss off consumers too.  At this point we can only hope the positives are more than the negatives.,pcmasterrace,2025-11-01 09:15:50,1
Intel,nmieuq4,"Don't buy the beta GPU (aka RTX 4090 / RTX 5090). People are paying for R&D reason to refine for future RTX 6080 reason, then RTX 6090 issue will be refined for RTX 7080",pcmasterrace,2025-11-01 10:06:39,1
Intel,nmiixfy,"Gamers? Employed adult gamers, maybe. Dude's back there gaming on a P1000 🤷💐",pcmasterrace,2025-11-01 10:47:50,1
Intel,nmire07,"""minimum support"" lol  How many new features are being adding to Nvidia's 2-3 generation old cards?",pcmasterrace,2025-11-01 12:02:41,1
Intel,nmjdady,"You could switch to linux, nyuknyuk.",pcmasterrace,2025-11-01 14:25:58,1
Intel,nmjiaxd,"It's kind of sad at this point that if I want to use nVidia, I literally can't because the cable's unsafe.",pcmasterrace,2025-11-01 14:54:20,1
Intel,nmjlt0b,I chose amd for cpus and nvidia for graphics,pcmasterrace,2025-11-01 15:13:28,1
Intel,nmjs380,Bring in the melting cables !,pcmasterrace,2025-11-01 15:46:47,1
Intel,nmjxqzg,How about monopoly of the brothers...   Amd blocking rocm on all but their top cards means you gotta buy an Nvidia card if you want your own LLM runner.   Hurray for unchecked capitalism...,pcmasterrace,2025-11-01 16:16:09,1
Intel,nmkhpua,There's AMD cards with 12VHPWR connector too. And let's not start on Nvidia's backwards compatibility...,pcmasterrace,2025-11-01 17:58:05,1
Intel,nmkjvbo,4090 performance,pcmasterrace,2025-11-01 18:09:04,1
Intel,nml61po,Very few people are dealing with melting cables.,pcmasterrace,2025-11-01 20:06:10,1
Intel,nmlev2o,It is MIND BOGGLING that a 5 trillion dollar company can’t develop a cable and insist on a new standards that accommodate the cable. Nobody buying a GPU is gonna balk at a new PS standard. You’re telling me an electrical engineer straight out of college couldn’t do the math on this one in under a year for under a million bucks?  5Trillion is 5 milllion million.,pcmasterrace,2025-11-01 20:52:13,1
Intel,nmllv22,"My rx5700xt is fine, problem is new games where devs don’t give a shit about you. Remember PS1-2 games where devs was literaly making magic?",pcmasterrace,2025-11-01 21:30:22,1
Intel,nmuhwsz,https://preview.redd.it/36mqxnyv10zf1.png?width=1920&format=png&auto=webp&s=d9b1e032c5fe4745a9758d21634f5591bd5d7d1d,pcmasterrace,2025-11-03 08:05:39,1
Intel,nmupnno,"I think the shit AMD has now done is either a mistake or they are going on the dark side.  After all, they allegedly ""leaked"" the fsr 4 that can work on GPUs such as 6000 series 7000 , but still all we can do is wait and see. That's our only power",pcmasterrace,2025-11-03 09:28:09,1
Intel,nmvxeqt,The question is - will you guys buy Intel Arc at all?,pcmasterrace,2025-11-03 14:45:47,1
Intel,nmx2oyz,One thing in common you’ve across every post about melting cables is the guy was a complete moron and either used some cheap extension or didn’t plug it in properly. I’ve had no issues with my 5090 and know plenty of people who have the same build with 5090s who also have had no issues.,pcmasterrace,2025-11-03 18:06:02,1
Intel,nmygz7h,"Its doesn’t say “I don’t know” or “I’m wrong” because that isn’t in the training data. It doesn’t say this because there is no negative penalty for making a wrong guess when it doesn’t know, and a positive reward if its guess is right. Therefore it makes sense to always guess if it doesn’t know.   Source: open ai released a paper on it, https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf",pcmasterrace,2025-11-03 22:11:29,1
Intel,nmevi6x,Now I'm confused cuz on Linux those gpus will be supported and why not on windows?,pcmasterrace,2025-10-31 18:22:44,1
Intel,nmetpx7,"Man oh man, the amount of vitriol that was in this sub earlier was absolutely barbaric.   From the comments you would think amd broke into their house and shot their pets and loved ones it was insane",pcmasterrace,2025-10-31 18:13:28,-1
Intel,nmex5qt,I hate that I read this with Leia's voice in mind. Angry upvote...  ![gif](giphy|Q9yOCHupg1mqrXhfAZ),pcmasterrace,2025-10-31 18:31:15,1
Intel,nmfi0ue,I will still choose amd. nvidia is the apple of gpus.,pcmasterrace,2025-10-31 20:20:42,1
Intel,nmiistn,"If you care about political stuff influencing your purchasing decisions, the US government bought a 10% stake in Intel with money that was supposed to help them set up domestic fabrication from the CHIPS Act. So 10% of every dollar you spend on them goes directly to what the government is doing right now. No that doesn't mean you get any profits from Intel's increasing share price on your tax returns.  I'll stick with AMD.",pcmasterrace,2025-11-01 10:46:35,1
Intel,nmixqej,I personally regretting switching to AMD... Not the perfs but drivers and soft,pcmasterrace,2025-11-01 12:49:29,1
Intel,nmlbcsv,https://preview.redd.it/ct42fn2lhpyf1.jpeg?width=843&format=pjpg&auto=webp&s=e6824c5de4fe7a49f4f722ef3058956086c40f1f,pcmasterrace,2025-11-01 20:34:00,1
Intel,nmml9na,Nvidia and Even Nexus said it's caused by people bending their cables.,pcmasterrace,2025-11-02 00:54:30,1
Intel,nmnjkqr,"the AMD hate is forced. just because they decided FSR 4 wasn't worth it on RDNA 1 and 2, doesn't mean the company is greedy and going to kill support for RDNA 4 in 2028 or whatever. Radeon is still far superior and option to Nvidia, period.",pcmasterrace,2025-11-02 04:27:29,1
Intel,nmep1fa,It’s really not that serious 😂,pcmasterrace,2025-10-31 17:49:44,-1
Intel,nmer1y4,Just buy a 5070ti.,pcmasterrace,2025-10-31 17:59:48,-1
Intel,nmevw6u,Amd cards are also melting...,pcmasterrace,2025-10-31 18:24:44,-3
Intel,nmfa319,"Here's me with my MSI 1070 just about holding it together worried as fuck about all the price scalping, melting cables and lack of choice that will be due in the next year or so, not to mention the crappy operating system choices now.",pcmasterrace,2025-10-31 19:38:24,0
Intel,nmgomrk,"This is miss information.  AMD isn't ending support for RNA 1 and 2, they just aren't going to bend over backwards to optimize the for that specific hardware, and honestly after how many years of optimization I don' think there anything left on the bone.  They didn't even move it to legacy support and you will still download the same launch day driver packages as people with RDNA3 and newer use to play games.   You want know how support AMD really is?  I have a RX590, it was originally released in 2018, which was a was process shrink of the 580, which was a tweaked/revised 480 originally released in 2016.  This architecture is getting close to 9 years old, and  AMD still released a new drivers for both windows 10 and 11 in August. (2025-08-04).     This whole thing is a nothing burger.",pcmasterrace,2025-11-01 00:44:33,0
Intel,nmepagg,Middle path that leads to Hell:  Intel Arc,pcmasterrace,2025-10-31 17:50:59,-2
Intel,nmfffcp,and if anything the melting were mostly the user's fault for not plugging it in correctly lol,pcmasterrace,2025-10-31 20:06:52,-1
Intel,nmeswj6,Should have been specific for the Melting Cables part which have been only on 5090's as far as I have seen.,pcmasterrace,2025-10-31 18:09:16,-1
Intel,nmeta70,The only cables I've seen melting are for the 90's.  Just get an 80 and you're fine.  Also use official cables for your PSU.,pcmasterrace,2025-10-31 18:11:12,-4
Intel,nmh6xsi,"Oh so now that AMD doelsit, we gotta downplay eh, where was everyone losing their minds like when Nvidia did it  Oh right, gotta dick ride AMD because dickriders only exist when it's Nvidia, AMD dickriders are just pro cosumer and therefore not dickriders  Still with the overused joke of melting cables which I suppose is as true as AMDs dogshit drivers",pcmasterrace,2025-11-01 02:51:30,-1
Intel,nmerqds,"Intel Arc whilst there's electric arcs all over, I see what you did there OP",pcmasterrace,2025-10-31 18:03:14,0
Intel,nmesu3n,"I'm not advocating for it, but my strategy was to buy a 5090 and get the 3 year Micro Center warranty in case the cable melts.",pcmasterrace,2025-10-31 18:08:55,0
Intel,nmf9mds,Used to be other way round,pcmasterrace,2025-10-31 19:35:55,0
Intel,nmfnq19,The 5080 does not melt cables. The minimum for 1440p is a 5070.,pcmasterrace,2025-10-31 20:51:19,0
Intel,nmfqmss,Only a couple super high end cards melt cables,pcmasterrace,2025-10-31 21:07:26,0
Intel,nmgb9w3,To be fair your gpu won’t be melting unless you own a 5090,pcmasterrace,2025-10-31 23:15:31,0
Intel,nmgtc3v,"AMD released their first AM4 CPU in September 2016. They have actively supported that socket for nearly a decade now, releasing yet another new AM4 CPU last year.  New versions of Radeon 6000 series GPUs were being released until October 2023 (6750GRE was the last). That's 2 years ago. Imagine buying a brand new product on its release date and within two years they're no longer receiving optimization updates.",pcmasterrace,2025-11-01 01:16:07,0
Intel,nmhvb5k,"Yeah those melting cables... such a worry. I'll never buy nvidia again, unless they are the best so I am getting another 5080 soon.  But those cables man, it's hard to sleep at night knowing that if someone breaks in, pulls out the cable a bit and runs games it might melt a bit.",pcmasterrace,2025-11-01 06:29:42,0
Intel,nmjd08j,"They barely support the latest games, especially on day 0 nothing but random driver timeouts on day 0 with Hell divers 2 The last of us part 1/2 and even past day 0 Cyberpunk having random driver timeouts for months, Dying light 2 had RGB laser show issue for over a year on RDNA3 and even random driver timeouts,  There always a day 0 game that has issues, and sometimes even older games.  It won't matter if they drop support or not they keep launching cards with broken drivers at launch, and they keep getting away with it because there is always that random guy defending AMD.  seriously tho fuck melting 12vhpwr connectors and cables, that issue is just as bad as AMD still struggling to have stable drivers for the latest games.",pcmasterrace,2025-11-01 14:24:20,0
Intel,nmlxey1,"Intel ARC can't save u mate. I heard the division is all but dead. And with nvidia working with intel, they said they would work on CPUs with integrated nvidia processors for GPUs and AI, but didn't say anything about graphics cards. I wish nvidia would throw intel some graphics bones so their GPU division would make gaming GPUs to compete with AMD since nvidia are so focused on AI now.",pcmasterrace,2025-11-01 22:36:05,0
Intel,nmifkvx,You are literally contradicting yourself.   Nvidia cables only melt when drawing a lot of power which is only done by high end XX80 and XX90 GPUs. Amd does not have a GPU in their lineup to compete with XX80 and XX90s so you literally have no choice but to go nvidia **meaning that your meme makes no sense**.   And if melting cables are truly stopping you... Just undervolt wtf.,pcmasterrace,2025-11-01 10:14:01,-3
Intel,nmf52qu,"I will never buy an AMD card. Their software and drivers are just garbage and their cards are always behind on features. They burned me a few. Times already, never again.",pcmasterrace,2025-10-31 19:11:57,-7
Intel,nmewxcz,the melting cables can be fixed with undervolting your cards,pcmasterrace,2025-10-31 18:30:04,-4
Intel,nmh1n56,https://preview.redd.it/88dgfffa1kyf1.jpeg?width=1262&format=pjpg&auto=webp&s=6aad526f1e9dfad1d91654624387a67eb5a610fd  Every month I steal my grandparents’ credit card and buy new PC parts for free. I don’t care about Nvidia or AMD I’m evil. 😈,pcmasterrace,2025-11-01 02:13:47,-1
Intel,nmh3quz,"Yeah I’ll take a Jerry rigged or underclocked nvidia GPU over a normal AMD GPU. I have an AMD GPU right now in my tower computer and even though my nvidia GPU in my laptop literally died, I’m still 100% going back to nvidia for my next tower pc GPU.",pcmasterrace,2025-11-01 02:28:42,-1
Intel,nmeuh3a,"Intel: lets pretend dx11 never existed, lets just optimize our drivers for the latest and lamest of titles (cybershit 77 onwards).   Btw many ppl buy midrange cards to enjoy older, well made titles at silky smooth frametimes and sky high fps, everything on ultra of course. Thats where intel sucks big time",pcmasterrace,2025-10-31 18:17:22,-6
Intel,nmi39km,"I mean, i am rooting for more competition, but which nvidia card burns the cables? Is it the top of the range? That one does not have an AMD counterpart anyways..",pcmasterrace,2025-11-01 07:59:29,-2
Intel,nmewwyl,"AMD makes cpus as their primarly source of income and their discrete desktop gpus as a side thing. Nvidia was a gpu company, now its a self proclaimed AI company.",pcmasterrace,2025-10-31 18:30:00,1395
Intel,nmfwclc,"""AM4 is practically dead!"" they said for about 10 years now. Not complaining really with my 5800X here.",pcmasterrace,2025-10-31 21:40:30,42
Intel,nmfq7xl,"I hate how people keep saying this. Yeah, TECHNICALLY it's receiving ""new"" CPUs, but they're all just cut down manufacturing rejects of the mainline CPUs with very limited releases. It really doesn't even count.",pcmasterrace,2025-10-31 21:05:03,35
Intel,nmfoy38,https://preview.redd.it/trx8l9vxgiyf1.jpeg?width=1280&format=pjpg&auto=webp&s=65a2ad415073653986377a822c0a62af2315c8d9,pcmasterrace,2025-10-31 20:57:55,13
Intel,nmh5zt1,You mean rebranded cpus of a rebranded cpu?,pcmasterrace,2025-11-01 02:44:41,3
Intel,nmfpv71,Ayyo what,pcmasterrace,2025-10-31 21:03:02,1
Intel,nmfr1c3,Meanwhile in Linux Ati cards still get driver updates,pcmasterrace,2025-10-31 21:09:43,3
Intel,nmfy3zw,New CPU? BAH Rebranded 2020 Release CPU,pcmasterrace,2025-10-31 21:50:58,1
Intel,nmgd6zd,Is there a new X3D coming out? 👀,pcmasterrace,2025-10-31 23:27:56,1
Intel,nmhcpny,Wait…new-new? Like 2025 CPUs? 2026? I haven’t done much research since I already assumed I was locked down to am4 forever.,pcmasterrace,2025-11-01 03:34:02,1
Intel,nmhpa7d,"AMD truly hates its GPU division, lol.",pcmasterrace,2025-11-01 05:25:12,1
Intel,nmiih3j,CPUs.,pcmasterrace,2025-11-01 10:43:23,1
Intel,nmu8k7r,AM4 x3d cpus are no longer produced though,pcmasterrace,2025-11-03 06:31:47,1
Intel,nmentcw,https://preview.redd.it/hctyj3saihyf1.png?width=447&format=png&auto=webp&s=6f5a431d766a026fa82917884724fa4fe09eb795  Basiclly what you said,pcmasterrace,2025-10-31 17:43:46,1961
Intel,nmern3f,Their marketing team is certainly one that gives the vibes of 100% nepo appointments.,pcmasterrace,2025-10-31 18:02:46,446
Intel,nmepn0f,"The Advanced Marketing department thing comes from that deranged weirdo at UBM, if memory serves, and the irony is that he makes that joke to imply that they are only good at marketing, but bad at creating products, which is the literal inverse of reality. AMD's products are pretty good, but jesus christ on a cracker is their market utterly horrendous",pcmasterrace,2025-10-31 17:52:42,161
Intel,nmetlhq,This makes perfect sense with redstone supposedly coming this year of course they were going to split the driver stack. Terrible communication aside this seems pretty obvious,pcmasterrace,2025-10-31 18:12:49,34
Intel,nmesc8m,I hope thats what AMD will actually do and not just PR corporate bullshit meant to save face. Still kinda sucks that 6000 series will never get official FSR 4 support unlike the 2000 series getting DLSS 4 which is a massive improvement.,pcmasterrace,2025-10-31 18:06:21,77
Intel,nmf5qfe,So kind of like microsofts feature support vs extended support  You don't get the new shiny but we fix what you already have  I suppose you'd only see over time if that's a big deal or not. Personally I do like products having at least some on no new features extended support. That way if a new feature brakes things it isn't one of the final patches you see(How long that should be I'm not sure),pcmasterrace,2025-10-31 19:15:23,12
Intel,nmft2ni,">this level of fuck up is literally so common that their nickname these days is ""Advanced Marketing Department"", and not in a good way.  https://preview.redd.it/2rt6ywm3liyf1.jpeg?width=960&format=pjpg&auto=webp&s=51347cbe45ec4ef3faa4e1aa7bace3d26a1eb9ea",pcmasterrace,2025-10-31 21:21:24,9
Intel,nmeuoic,I’ve seen this explainer in like 5 posts over the last day and people are STILL posting misinformation 🙄,pcmasterrace,2025-10-31 18:18:26,42
Intel,nmf0p1q,Why does AMD even have a marketing team when Reddit and Youtube does it for free and does it 500 times better?,pcmasterrace,2025-10-31 18:49:23,24
Intel,nmfwb5f,"Except they say “New features, bug fixes and game optimizations will continue to be delivered as required by **market needs**…”  Basically, Game Optimizations will be released on an inconsistent basis with new drivers, which is still a baffling decision considering RDNA2 marketed optimizations as a selling point 3 years ago.  I expect BARE MINIMUM 6 1/2 years of consistent game optimizations, not 5.",pcmasterrace,2025-10-31 21:40:16,10
Intel,nmezy28,"Honestly their marketing pissed me off when FSR4 was released and they were like "" sooooo we arent bringing that to the 7000xtx after all"".",pcmasterrace,2025-10-31 18:45:32,7
Intel,nmf5wub,"It's still pretty dumb: if eg. FSR4 or any new feature set can improve the performance of an older card, without an inherently crapton of additional massaging, it should be made to do so. And reportedly, FSR4 has already been shown to work on and improve performance of these older generation RDNA cards in modded drivers, so it has been demonstrated that the only obstacle in implementing it is a profit-seeking one. Though saying that is there not profit in providing value for customers on older hardware, doesn't that build the brand, etc. or are they just interested in pushing the next generation of eWaste into the bin sooner.",pcmasterrace,2025-10-31 19:16:19,4
Intel,nmfbg3e,I love your flair,pcmasterrace,2025-10-31 19:45:44,1
Intel,nmffndo,Pro tip to let the marketing department only to deal with product banners and social media accounts and leave press statements to engineers in any company,pcmasterrace,2025-10-31 20:08:02,1
Intel,nmfh6ay,How do they manage to fuck up their only job this badly every single time?,pcmasterrace,2025-10-31 20:16:09,1
Intel,nmfo92j,No no man. I just learned from this meme that AMD is not support graphics cards anymore. Keep up!,pcmasterrace,2025-10-31 20:54:10,1
Intel,nmfutsx,It's amazing how bad their marketing department is.,pcmasterrace,2025-10-31 21:31:35,1
Intel,nmfxl8j,"Advanced marketing devices is exactly how userbenchmark calls amd, but in the opposite meaning  As in their marketing by sponsoring every single youtuber on earth accomplished their desktop cpus to be bought by gamers more than intel",pcmasterrace,2025-10-31 21:47:48,1
Intel,nmg6dil,Oh damn. Thats absolutely not what i thought that announcement was. Thats totally reasonable. Were the marketing department about to do share buybacks or something. Because that level of incompetence would kill people in any blue collar job.,pcmasterrace,2025-10-31 22:43:31,1
Intel,nmg6ykz,"Yeah I saw the right side and was like wtf, they keep supporting 10+ year old cards  lol",pcmasterrace,2025-10-31 22:47:22,1
Intel,nmg7umx,Advanced Marketing Disaster,pcmasterrace,2025-10-31 22:53:10,1
Intel,nmgc1vr,You mean following a panic meeting they decided to throw the marketing team under the bus and backtrack,pcmasterrace,2025-10-31 23:20:32,1
Intel,nmgfmfo,"The answer ive been looking for, thank you!!!",pcmasterrace,2025-10-31 23:43:48,1
Intel,nmghy5e,> the utterly inept fuckwads in their marketing department   https://www.youtube.com/watch?v=tHEOGrkhDp0,pcmasterrace,2025-10-31 23:59:12,1
Intel,nmglpxb,They tried to replace the communications team with AI didn’t they?,pcmasterrace,2025-11-01 00:24:43,1
Intel,nmh3frx,I didn’t read your entire comment but I’d like to say fuck AMD and double-fuck NVDIA,pcmasterrace,2025-11-01 02:26:29,1
Intel,nmhfdaq,AMD wasting money on these marketing people smh.,pcmasterrace,2025-11-01 03:55:20,1
Intel,nmoqx8t,Yeah userbenchmark has been giving that marketing department WAY too much credit lmfao,pcmasterrace,2025-11-02 11:24:31,1
Intel,nmexwzy,">is that any NEW software tech that's developed from now on, will only be available for 7000 series and up GPU's  Well that's disappointing. It's way too soon. I'm sure it's not just due to hardware limitations, since 6000 and 7000 share similar hardware necessary for upscaling. They can't throw that ""we drop support because compatibility blah blah"" card on consumers.  Well on a positive note, it's good that 6000 series will still get FSR4 support thru mods, although that might be limited, as some online games could trigger anticheats.",pcmasterrace,2025-10-31 18:35:07,0
Intel,nmepgcq,That just sounds like BS from AMD.   Why come out and say something you already said. What this sounds like is AMD trying to back peddle a decision that would cost them future sales. The best part is if they get this out there now. Then oops don't update for games moving forward. Then no one will say a word. It will be done and over with.   Everyone knew FSR4 and the like wasn't coming to RDNA1/2 gpu's they said this before. They don't back port new stuff anyway.   So just more corpo speak for hey we screwed up now forget bout it look a shiny thing consumer look at the shiny.,pcmasterrace,2025-10-31 17:51:48,-13
Intel,nmf5yzg,Which I feel like they didn’t need to say? Like obviously not every feature is going to be supported an older hardware already right now FSR 4 is not available on the 7000 series though the rumour is they will have it eventually. Like why is this something the end user needs to be informed about?   I almost wonder if they were planning on pulling Support and then just decided to say that it was misinterpreted because they saw the reaction,pcmasterrace,2025-10-31 19:16:38,0
Intel,nmesu8q,"> RDNA1 and RDNA2 GPU's will continue to receive the same game ready style drivers as they always have.  Where did they say that. All I've found is:  >""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch""  Which isn't the same thing at all.",pcmasterrace,2025-10-31 18:08:56,-11
Intel,nmeouda,"""communications has been bad"" kinda sounds like ""cables aren't melting due to design flaw, rather because user error""",pcmasterrace,2025-10-31 17:48:46,-27
Intel,nmh4s2d,Oh take us back!,pcmasterrace,2025-11-01 02:36:07,6
Intel,nmeqaps,AMD Marketing communicating like idiots!   ![gif](giphy|clotJg9IqBitMcRJ61),pcmasterrace,2025-10-31 17:55:59,224
Intel,nmga2p8,They should just fire their whole marketing department and build a new one i guess,pcmasterrace,2025-10-31 23:07:43,12
Intel,nmffzui,"It's ironic because it's a sub for PC master race, but as a PC gamer it's like the biggest sub I avoid lol. You'll see something posted all over the sub about how it's bad, and then next week its vice versa.",pcmasterrace,2025-10-31 20:09:52,6
Intel,nmh2fca,"I'm pretty sure the clarification had a big asterisk of something along the lines ""we will continue updating them according to market demand"", i think they're still moving them to maintenance mode the same they did for Polaris and Vega which means they will continue necessary bug and security fixes, unlikely to recieve optimizations (tho still possible) but more importantly no new features will be added, this kinda sucks when you consider RTX 20 series which is older than RDNA2 is still receiving partial support for new DLSS features.",pcmasterrace,2025-11-01 02:19:20,5
Intel,nmf4ji2,the original message was slightly ambiguous but then literally every news outlet choose to interpret it in the very worst way possible and then add clickbait headlines on top of that misinterpretation. The resulting mess is only marginally AMD's fault.  They aren't saying anything now that wasn't in what they said originally.,pcmasterrace,2025-10-31 19:09:08,20
Intel,nmicw02,"""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch.""   Isn't the marketing department the one responsible to judge ""market needs""? Not looking good.",pcmasterrace,2025-11-01 09:46:16,2
Intel,nmnjz05,"this is proof that all the AMD hate is extremely forced and happens no matter what they do, they could launch an 24GB GPU for 400 dollars and people would point at it as a reason no to buy Radeon.",pcmasterrace,2025-11-02 04:30:04,1
Intel,nmhovii,The updates AMD meant: Games will be playable.   The updates PCMR expect: Future AI upscaling support on old cards.,pcmasterrace,2025-11-01 05:21:01,0
Intel,nmh2wfn,"It did recieve AFMF and Anti lag 2, but now they pulled the plug and new features won't be added even if they can work on these cards. People should care, nvidia is still shipping new DLSS features to 2018 and 2019 GPUs, we can't be normalizing AMD pulling the plug on feature updates for 3-5 years old cards.",pcmasterrace,2025-11-01 02:22:40,26
Intel,nmgksob,"Same, I don't give a shit about shiny new things I just want it to be stable.",pcmasterrace,2025-11-01 00:18:27,9
Intel,nmnfz43,Help yourself by installing fsr4 int8 with optiscaler. Is game changer for my rx 6800,pcmasterrace,2025-11-02 04:04:07,1
Intel,nmnkoou,"the cards function and gets updates, which is more than people are giving them credit for, proof that the HAte train never stops because people love to do free marketing at any chance for Nvidia, even if it hurts them in the eyes of people with brains. which is unreasonably hard to come across these days.",pcmasterrace,2025-11-02 04:34:55,1
Intel,nmgb5cv,Accurate,pcmasterrace,2025-10-31 23:14:43,12
Intel,nmgrhfv,Nvidia is worth $4 trillion right now. We're way past them _just_ being a multi-billion dollar company.,pcmasterrace,2025-11-01 01:03:24,14
Intel,nmk88ma,"There are no ethical companies , full stop",pcmasterrace,2025-11-01 17:09:33,2
Intel,nmnayfw,did you think there ever were?  Companies worth any meaningful amount are by definition unethical to reach that.,pcmasterrace,2025-11-02 03:30:48,0
Intel,nmeo0b6,"3050 isn't a bad card. It was just too expensive at launch. I was gifted one, and it is now in my kid's PC.",pcmasterrace,2025-10-31 17:44:41,53
Intel,nmeovnm,1070ti here,pcmasterrace,2025-10-31 17:48:57,8
Intel,nmi79ob,">older,  >My 3050  Brother",pcmasterrace,2025-11-01 08:44:37,1
Intel,nml3fpi,My FE 2060 Super is running fine for all the weird indie games I like anyways,pcmasterrace,2025-11-01 19:52:02,1
Intel,nmevd5m,"My 3050 does the job for what I need to just fine. Don't get me wrong, there is a temptation to upgrade to a 3070 (Before anyone comments, it's cheaper than a RX 6700 10GB on the used market( to go alongside my new 180Hz screen, but I'm unsure whether i'll pull the trigger for it or not.",pcmasterrace,2025-10-31 18:22:00,0
Intel,nmeuelw,"At least one 5070Ti did, but it’s certainly less common than the 5080/5090",pcmasterrace,2025-10-31 18:17:00,17
Intel,nmgc1ym,My buddy's 9070 uses the 12 pin and it hasn't melted either. I think it's partially the wattage of the higher end cards running into the very tiny safety margins that's the issue. The cables a have a lot more breathing room in the mid range.,pcmasterrace,2025-10-31 23:20:33,3
Intel,nmnl9p9,you need to educate then. its EVERY card with a 12VHPWR that is melting. including the 9070XTs that come with it.,pcmasterrace,2025-11-02 04:39:07,1
Intel,nmlbk0f,This is exactly me lol. 1080ti was an extreme beast. I should frame that beauty!,pcmasterrace,2025-11-01 20:35:04,2
Intel,nmnlbj4,7970 runs fine here too.,pcmasterrace,2025-11-02 04:39:28,1
Intel,nmgb7o9,You have the oldest gen card that supports dlss,pcmasterrace,2025-10-31 23:15:08,1
Intel,nmf8q8s,It better be ready to do another decade when it becomes affordable enough for me. Too early to hit retirement,pcmasterrace,2025-10-31 19:31:10,1
Intel,nmnlfx8,my HD 7970 is throwing a fit on modern linux updates so i don't think thats necessarily true.,pcmasterrace,2025-11-02 04:40:21,1
Intel,nmfb0dw,Losing day 1 game optimization is basically losing driver support. Security patches won't optimize the new games for your GPU,pcmasterrace,2025-10-31 19:43:25,0
Intel,nmijxo5,People != you.,pcmasterrace,2025-11-01 10:57:09,0
Intel,nmnlv56,the few people who shit on nvidia do so because they deserve it. the people who shit on AMD are doing so for clout and almost always never have an actual reason for it. spread misinfo and paint them out to be inferior and disgusting when its literally the other way around.,pcmasterrace,2025-11-02 04:43:25,-1
Intel,nmfowq3,Did you do some OC/UV?,pcmasterrace,2025-10-31 20:57:43,1
Intel,nmf4ti9,Mine played cyberpunk just fine.,pcmasterrace,2025-10-31 19:10:37,-1
Intel,nmi2jr1,"The cheapest 5080 here is 1050€. Fuck. That. Cheapest used is like 900.  I wish I could say Im glad I dont need a gpu because i have a 3080, but I couldnt even get a stable 60 on 1440p WITH dlss on freaking keeper.  Unoptimized games are actively sponsored by nvidia in secret, change my mind.",pcmasterrace,2025-11-01 07:51:25,2
Intel,nmf55gw,Just a 1% chance it might catch your house on fire. Who wouldn't take those odds.  And all so nvidia could save a a few extra cents.,pcmasterrace,2025-10-31 19:12:20,-5
Intel,nmh7s10,"They said they will optimize for games ""as needed"".",pcmasterrace,2025-11-01 02:57:34,2
Intel,nmi6qeu,YESS! I love fake competition! Now I don't have to choose as much because the entire market is led by 2 companies that are too busy chasing the next wall street fad so they don't have the incentive to make competent products!,pcmasterrace,2025-11-01 08:38:29,1
Intel,nmnm2t6,"not true. older cards can have issues. like my HD 7970 having a massive shit itself session after i update for like the past month, this is a 13 year old card so its not like its actually gonna affect anyone nowadays cause GCN 1.0 is long dead,",pcmasterrace,2025-11-02 04:44:57,1
Intel,nmg0hf7,"A and B are the last desktop GPUs (most likely), and C will be 1 (maybe) dGPU with a few APUs. Idk about Druid if that will be delayed or canceled",pcmasterrace,2025-10-31 22:05:30,2
Intel,nmhxg32,B580 is really good from my friend's experience. Would support team blue/green now onwards at least they dont need to have to have marketing to explain their products and support is there.,pcmasterrace,2025-11-01 06:53:41,2
Intel,nmgqr7x,"No, Intel and Nvidia are just working together for CPUs with integrated graphics.   Arc is here to stay, thankfully!",pcmasterrace,2025-11-01 00:58:33,1
Intel,nmhx0zh,Yeah AMD is like another Nvidia now charging - 50 for subpar feature set. Really hoping for a quick intel win here along with the CPU as well.,pcmasterrace,2025-11-01 06:49:00,1
Intel,nmjio2h,I mean I’m still getting updates to the drivers in my laptop’s 3060.,pcmasterrace,2025-11-01 14:56:20,0
Intel,nmf6l8z,"Linux drivers OpenSource, Windows drivers not..",pcmasterrace,2025-10-31 19:19:52,2
Intel,nmfp7oh,Finally someone got the refrence,pcmasterrace,2025-10-31 20:59:23,1
Intel,nmlbep7,https://preview.redd.it/t52c9zumhpyf1.jpeg?width=1024&format=pjpg&auto=webp&s=ea3d1c720034b8cd8a02f52dfc22082650fb90fb,pcmasterrace,2025-11-01 20:34:17,1
Intel,nmeujlx,This is the sweet spot. I'm considering upgrading my 3060ti for this.,pcmasterrace,2025-10-31 18:17:43,0
Intel,nmffjfb,Which ones? I haven't seen em posted here yet?,pcmasterrace,2025-10-31 20:07:27,2
Intel,nmg19x6,Same. Seriously considering just grabbing a 3060Ti and calling it good.,pcmasterrace,2025-10-31 22:10:29,1
Intel,nmg5n7t,"My ARC a750 has been ace, I still get driver updates every other week or so, can play the games I wanna play and even more intense games like Space Marine 2",pcmasterrace,2025-10-31 22:38:44,1
Intel,nmezmf7,"There has been a case of 5070ti cable melting, though. They have the same bad power delivery setup from what i know, so they can still melt. 5080s can do that too",pcmasterrace,2025-10-31 18:43:54,-4
Intel,nmip1eh,So your solution to a bad product is that you fix it for them. Got it.,pcmasterrace,2025-11-01 11:43:24,1
Intel,nmgggri,I haven't heard of any driver issues in years,pcmasterrace,2025-10-31 23:49:19,2
Intel,nmhplwb,AMD has free and open source drivers on Linux.   Immediately makes Nvidia worse,pcmasterrace,2025-11-01 05:28:29,2
Intel,nmhx9dd,I dont know why people would honestly buy AMD in 2025 unless its either way cheaper or more available in their region.,pcmasterrace,2025-11-01 06:51:36,1
Intel,nmepra0,"Why are you pissed? Amd clarified that they messed up with communication. 60 series cards will still get game ready drivers, just not new tech that comes out. So if/when fsr4 happens, 60 series cards won’t get it. That’s it.",pcmasterrace,2025-10-31 17:53:17,4
Intel,nmepwc7,"Cable issue is due to a well known, incredibly well documented design flaw that has been detailed by people like DerBauer. As to the AMD thing, as I said in my comment in this thread, AMD have already allowed the engineers to clarify the fucktastically stupid statement made by the marketing department. Your friend will be just fine, his card will still get game updates. You're zero for two at the moment.",pcmasterrace,2025-10-31 17:54:00,2
Intel,nmer4cl,They have already reversed this decision. They are not in fact ending game ready driver support. Just new features (think FSR4 / MFG) won't come to older cards.,pcmasterrace,2025-10-31 18:00:08,1
Intel,nmfss5c,Looking forward the bubble bursting finaly. Overvalued fucks. Gonna hurt us all but its better then it growing more. Privatize gains and socialize the loses.,pcmasterrace,2025-10-31 21:19:44,617
Intel,nmg5jf7,"Nvidia were planning on being an AI company for two decades, that is why they’re at the top now",pcmasterrace,2025-10-31 22:38:02,15
Intel,nmh4o0t,I don't know if it is self proclaimed when it has already become their primary source of profits. nVidia is an AI company that also sells gaming/studio GPU's.,pcmasterrace,2025-11-01 02:35:19,5
Intel,nmii8jk,Self proclaimed? They are 100% the market leader in AI hardware.,pcmasterrace,2025-11-01 10:41:02,5
Intel,nmib5ym,"AMD probably has decided for gaming GPUs, it's going to rely more on SOCs. It already has big contract from Sony for Playstation and  Microsoft for the current and next gen series. PS5 Xbox X/S will likely sell a combined of 150 million units at minimum.",pcmasterrace,2025-11-01 09:27:39,1
Intel,nmkdtfr,"was talking with an AMD rep at Kaseya DattoCon this year, mentioned how my company’s media servers run RTX Pro cards because we need the raw power and he said “yeah we’re okay with being number two in the GPU market”",pcmasterrace,2025-11-01 17:38:05,1
Intel,nmkvgu3,Wtf is the opposite of a discrete desktop GPU,pcmasterrace,2025-11-01 19:09:25,1
Intel,nmgoipe,"It is dead, nothing topped the 5800X3D on the AM4 side",pcmasterrace,2025-11-01 00:43:47,15
Intel,nmhcfji,"My girlfriend's GPU died, and she has literally been playing Outer Worlds 2 on an integrated gpu on an AM4 processor. Lowest possible settings of course but it doesn't make it any less funny.",pcmasterrace,2025-11-01 03:31:48,1
Intel,nmkma21,"I mean it's ""dead"" in the sense that there won't be another generation of chips released for it, but AM4 chips and mobo's are still plenty good for gaming.",pcmasterrace,2025-11-01 18:21:21,1
Intel,nmfxvfj,i swear people will say amd is great because they still produce and sell 3000Gs (aka 50$ dual core CPUs on AM4 that are e-waste and on par with a 2015 i5 4460 but with -30W TDP),pcmasterrace,2025-10-31 21:49:31,6
Intel,nmfy92v,"but when Intel refresh their old CPU, people say otherwise  Yeah bullshit AMD fanboy propaganda about AM4",pcmasterrace,2025-10-31 21:51:46,-5
Intel,nmfwh1i,Re-releases of old CPUs with slightly different clock speeds (binning) and names.  The last truly new AM4 CPU was the 5800X3D,pcmasterrace,2025-10-31 21:41:13,19
Intel,nmftjb1,"They're meh. XT series, not really much special just stick to the regular ones.",pcmasterrace,2025-10-31 21:24:05,1
Intel,nmhgp8d,"No, not really new. Just different Zen3 variants. Last real new CPU was the 5800X3D.",pcmasterrace,2025-11-01 04:06:14,1
Intel,nmg384j,"well, on linux amd have 10y+ support with raytracing drivers as far as the rx480  sadly it is linux only. not windows.",pcmasterrace,2025-10-31 22:22:55,156
Intel,nmfwnqu,![gif](giphy|9mtE009hcWPOesk8C4),pcmasterrace,2025-10-31 21:42:18,60
Intel,nmfklzh,"In the like 4 jobs I had in “tech” yeah pretty much.  I’ve mainly seen Nepotism and cronyism affect 3 major areas.   Marketing, HR (the cancer that spreads its own nepotistic disease), and the “head” of an already established mostly autonomous department where their whole schtick is to read the notes about “efficiencies” that they contribute nothing too besides being the one on camera during inter-department company calls. Take a guess where I was lol.   Nepo/crony hires are the worst because they are like the only ppl in the company that can get away with being totally inept.   This does not excuse AMD though since they are way past the point. I wonder if AI is being used to “streamline messaging” as sometimes it just words things wrong and if you don’t catch incorrect nuance or the unintended implications, things like this can happen.   But what the fuck do I know I grew up poor lol.",pcmasterrace,2025-10-31 20:34:38,96
Intel,nmfu69e,It feels like nVidia did the hiring process for their marketing.,pcmasterrace,2025-10-31 21:27:46,6
Intel,nmfwdke,So every marketing team ever?,pcmasterrace,2025-10-31 21:40:39,3
Intel,nmexxic,It is getting support with Redstone release,pcmasterrace,2025-10-31 18:35:12,27
Intel,nmfxu2o,"They arent going to stop fixing glitches on a gpu still in production  Vega still gets updates, its not pr",pcmasterrace,2025-10-31 21:49:17,9
Intel,nmeu67x,"It can’t, any more than RTX2000 can get all the features of the RTX5000 cards. The hardware is just different.",pcmasterrace,2025-10-31 18:15:48,-14
Intel,nmi07da,"Kind of, yes. With the GPU's that means AMD owners still get regular driver updates for new games.",pcmasterrace,2025-11-01 07:24:52,1
Intel,nmf9l80,Because the outrage is worth more clicks and karma than the clarification. People will be using the original info as ammo for a loooong time.,pcmasterrace,2025-10-31 19:35:44,33
Intel,nmjz2a4,"Tbf, this is the first time I've seen this clarification. AMD PR fucked up.",pcmasterrace,2025-11-01 16:23:05,1
Intel,nmk13pm,"the post you're replying to is the one misinforming people.  https://preview.redd.it/l9t6q6imaoyf1.png?width=613&format=png&auto=webp&s=4be9523b7481b01ed8448a1b9336c5951ddcc391  this is the real statement. notice how its says nothing about ""same driver treatment"" and still leaves the cards in maintenance mode?",pcmasterrace,2025-11-01 16:33:39,0
Intel,nmglov4,"Ok but that misinformation is the official announcement, maybe they shouldn't lie in advertising if they want people to tell the truth",pcmasterrace,2025-11-01 00:24:31,-5
Intel,nmfxckp,Are you the user benchmarks guy?,pcmasterrace,2025-10-31 21:46:21,10
Intel,nmfvblw,"I imagine the share holders would freak out if you told them you were getting rid of the entire marketing team, marketing is tangible to the average person compared to say QA or a documentation team.",pcmasterrace,2025-10-31 21:34:30,4
Intel,nmfye40,Because billion dollar companies down give a shit about reddit drama comments. That's not what matters,pcmasterrace,2025-10-31 21:52:37,3
Intel,nmi0e9n,"Lol at the idea that Reddit makes any difference to anything or anyone, anywhere. We're a very insular group here, with far less reach than certain peoples egos might like to imagine.",pcmasterrace,2025-11-01 07:27:03,1
Intel,nmfym5x,"According to userbenchmark, the marketing from Advanced Marketing Devices is mainly buying youtubers reviews to test at unrealistic scenarios where their tech barely wins and publishing fake info online about their shitty cpus",pcmasterrace,2025-10-31 21:53:59,1
Intel,nmgm21s,Especially funny since it's the most powerful AMD card in raw raster,pcmasterrace,2025-11-01 00:27:03,1
Intel,nmi0c29,"No it's not. It's like expecting ray tracing to work on a 10 series card. FSR4 CAN sort of kind of work, but not really. Why expect them to fudge it, when all that'll happen is people will hate wank themselves into a coma over it not working 100% properly.",pcmasterrace,2025-11-01 07:26:20,1
Intel,nmfdcko,"Meh, I just use optiscaler and use xess 2 and will use 3 since fsr4 doesn't work too well on 6800",pcmasterrace,2025-10-31 19:55:50,2
Intel,nmerfwj,Because communication issues exist and sometimes one team thinks they’re better than the other and will say dumb shit that the other team disagrees with,pcmasterrace,2025-10-31 18:01:45,32
Intel,nmes5i1,that message was yesterday,pcmasterrace,2025-10-31 18:05:23,2
Intel,nmevxxb,Thats exactly what this is lol BS.  They have been so scummy with software since the 9000 series.  I'm done with AMD GPUs I think.,pcmasterrace,2025-10-31 18:24:59,0
Intel,nmfl74s,The only one who's shocked is Hasans dog.,pcmasterrace,2025-10-31 20:37:50,29
Intel,nmfg7lm,At least they corrected it before Gamers Nexus had the chance to post yet another ragebait video,pcmasterrace,2025-10-31 20:11:00,16
Intel,nmi7yq5,"The problem with their marketing is that it's been this terrible and unreliable for the company for at least a decade... I mean, remember ""poor Volta""?",pcmasterrace,2025-11-01 08:52:18,1
Intel,nmhgjls,5 trillion*,pcmasterrace,2025-11-01 04:04:57,8
Intel,nmk6ztj,"All the same, pretending Intel’s not gonna be screwing folks just as hard as Nvidia or AMD is pure pipe dreams. It’s all dystopian these days.",pcmasterrace,2025-11-01 17:03:19,1
Intel,nmneqo2,"I’d argue if I could name some, I actually can’t off the top of my head. Just feels like there should be some I could name…",pcmasterrace,2025-11-02 03:55:43,1
Intel,nmndzvk,"No, I agree with you. My only stipulation is that in 2025 we should ALL know better.",pcmasterrace,2025-11-02 03:50:42,1
Intel,nmeovsg,there is no bad there is only bad pricing,pcmasterrace,2025-10-31 17:48:58,43
Intel,nmexcwz,"The 6GB one is what I wish the 8GB was. If the 8GB version was slot-powered and priced like the 6GB one, it may have been more attractive.",pcmasterrace,2025-10-31 18:32:16,6
Intel,nmhi1x5,That’s where my 3060ti is now.   Will run Minecraft and Goat simulator to his hearts content.,pcmasterrace,2025-11-01 04:17:45,2
Intel,nmiar4a,Just because it's not a 1080 doesn't mean that 3050 isn't an older card. It's two generations old.  It's objectively older than the current generation.,pcmasterrace,2025-11-01 09:23:04,1
Intel,nmftydf,"It is also not that common with 5080 it is just the overvolted garbage like Asus and aorus high-end shit with ""liquid metal"" and ""super vapour champer"" and ""ultimate performance"" I mean they do this for GPUs and motherboards you pay more for ""premium versions"" to only get instability    But 5090 tho this shit is flaming",pcmasterrace,2025-10-31 21:26:30,15
Intel,nmi7eph,One melting could be user error/a single faulty product (shit happens)  Multiple is likley a problem that could affect most/all cards,pcmasterrace,2025-11-01 08:46:10,2
Intel,nmgvd9k,"🤓Emm ""actually""  one did burn‚ but just one.",pcmasterrace,2025-11-01 01:30:17,-2
Intel,nmfblan,Do most new games even run on 5000 series cards?,pcmasterrace,2025-10-31 19:46:31,2
Intel,nmhxqrc,"The cards aren't losing game optimizations, only new features.",pcmasterrace,2025-11-01 06:56:59,1
Intel,nmfniyf,>Security patches won't optimize the new games for your GPU  Since when is it AMD's job to make sure a game runs and not the game developers?,pcmasterrace,2025-10-31 20:50:16,1
Intel,nmj3vvg,"nah, its weird because is each one a diferent problem, in black ops 6 i'm forced to play at minimum because memory leak, in battlefield 6 the driver just die at random, and cyberpunk just crash because the gpu",pcmasterrace,2025-11-01 13:29:27,1
Intel,nmgbd9v,None of them have started fires that spread beyond the connector melting,pcmasterrace,2025-10-31 23:16:08,7
Intel,nmf6bgf,">Just a 1% chance it might catch your house on fire.  Less than 1% globally and almost all user error and that isn't how statistics work.  If you are that concerned then you should also unplug every electronic in your home if you aren't using them. Turn off all the lights. Might as well flip the breakers. Also, cooking? You might as well forget about that. That's the leading cause of fires in the US.  Much higher chance you catch your house on fire by cooking bacon then with a properly installed and seated NVIDIA GPU.",pcmasterrace,2025-10-31 19:18:27,13
Intel,nmoofxd,Which is what I would expect.,pcmasterrace,2025-11-02 11:01:28,1
Intel,nmol3g8,"Wow, I've never heard of that GPU. But anyway, people are more concerned about the RX 6600 or RX 580/570, and they're safe for now.",pcmasterrace,2025-11-02 10:29:28,1
Intel,nmg2z0b,Interesting. Thanks for the info.,pcmasterrace,2025-10-31 22:21:17,1
Intel,nmjob0a,"1. that's only one generation old, and also not getting any new features (AFAIK).  2. AMD's GPUs are also still getting driver updates.",pcmasterrace,2025-11-01 15:26:51,1
Intel,nmf789t,Yeah I know that but umm you're right.,pcmasterrace,2025-10-31 19:23:14,0
Intel,nmic76w,"O we got the reference, just most people are too angry about something to do anything about it",pcmasterrace,2025-11-01 09:38:54,1
Intel,nmfftj0,1. 9070xt 2. If you didn't see then you are blind,pcmasterrace,2025-10-31 20:08:56,-5
Intel,nmgz3yr,"I had the bifrost model. Had several games that had weird graphical issues as they weren't optimized properly for yhr GPU. Just wasn't ideal, and while it was fairly priced, I just want a non jank experience.",pcmasterrace,2025-11-01 01:55:56,1
Intel,nmitz14,5070 5060 and 5050 do not melt,pcmasterrace,2025-11-01 12:22:36,0
Intel,nmj9vyv,"No one gives a shit about Linux. And to that point, the open source drivers are the reason why AMD performs better on Linux, because their own Windows drivers are just trash.",pcmasterrace,2025-11-01 14:06:06,-1
Intel,nmfupdc,They will forever focus on AI now. I think that cat is out of the bag forever. Gaming GPUs is a nice side hustle for them. The 1080TI will forever be the king to us boomer,pcmasterrace,2025-10-31 21:30:52,328
Intel,nmfvk4b,"You vastly underestimate how long rich people can keep a bubble going by just pumping more money into it.  Jeff Bezos, Mark Zuckerberg, Elon Musk, Bill Gates... they are all keeping the AI bubble alive.  And you can bet your ass that they won't run out of money any time soon.",pcmasterrace,2025-10-31 21:35:52,12
Intel,nmn684d,"Overvalued, horrible for creatives in all areas, and a serious burden on our energy security. Between crypto and AI, these industries can't end soon enough.",pcmasterrace,2025-11-02 02:59:23,2
Intel,nmfy373,And AMD don't make money from the AI race? Explain to me why their stock jumped after a deal made for that specific purpose then,pcmasterrace,2025-10-31 21:50:49,1
Intel,nmgjdfi,"AI gained them around ten years of profit in a single year, can't blame them for changing their game. Even though it sucks for all of us.  I doubt ai will go away. If the bubble pops, at most it'll become a niche thing that people do, like NFT junk. The bubble burst years ago but people are still clinging on to NFT going into new NFT scams and what not.",pcmasterrace,2025-11-01 00:08:50,1
Intel,nmhbx9h,"It is, but the AI drive will never stop. Remember, while LLMs are the current hype, there has never been a cessation to the ever growing demand for true general AI, or in other words, sentient machine intelligence.  Whether that is a good idea or not, is another matter altogether. But make no mistake - we will get there, and soon.",pcmasterrace,2025-11-01 03:27:55,1
Intel,nmkgnm3,Keep looking,pcmasterrace,2025-11-01 17:52:39,1
Intel,nmowdnm,">Looking forward the bubble bursting finaly.  The thing is, it won't. Datacenter GPUs are always more needed than gamer GPUs...",pcmasterrace,2025-11-02 12:10:36,1
Intel,nmgodkk,people been wanting the bubble to burst for 4 years now.  It ain't gonna burst,pcmasterrace,2025-11-01 00:42:51,-4
Intel,nmjlswl,lol,pcmasterrace,2025-11-01 15:13:27,0
Intel,nmhhrhn,They were planning that back in 2005? Was generative AI even a thing then?,pcmasterrace,2025-11-01 04:15:16,1
Intel,nmhwd3v,"i mean not everyone going to buy best one, as long as it good enough with reasonable price, then most people will buy that instead",pcmasterrace,2025-11-01 06:41:32,3
Intel,nmg8p1p,"You could argue about the 5600X3D as it offered a new value proposition, but yeah.",pcmasterrace,2025-10-31 22:58:42,4
Intel,nmh1dk5,"I still think it counts as a new release and a good thing overall, the last few SKUs were very clearly defective chips that failed to reach target performance of existing SKUs so they would've never made it to market had they not rebranded them so it's extra supply that lengthened AM4 lifespan.     However the XT refresh before that was a scam as they took perfectly functional CPUs, applied a useless overclock and sold them as a new model for a higher price when they could've just released them without the useless OC at regular street prices and everyone would be happy.",pcmasterrace,2025-11-01 02:11:54,0
Intel,nmhn9en,Ok I’m not missing much.,pcmasterrace,2025-11-01 05:05:07,1
Intel,nmg8vxu,"R300 series gained new drivers 2 months ago on Linux. 20+ year old cards, even the youngest and last of the series, Radeon 9550's are literally ancient yet there they are.",pcmasterrace,2025-10-31 22:59:58,52
Intel,nmhhioy,And SteamOS is... Linux. Really rooting for that to become the de facto gaming OS for the masses so the communities can stop bickering. I've been enjoying it.,pcmasterrace,2025-11-01 04:13:09,27
Intel,nmiufgt,And another W for Linux,pcmasterrace,2025-11-01 12:26:01,4
Intel,nmm24vr,Another reason to leave MS behind.,pcmasterrace,2025-11-01 23:03:39,1
Intel,nmfm5hc,"There's such a huge difference between marketers and """"""marketers"""""", the former are mostly numbers people, the latter are almost all ""idea people"" that contribute nothing positive",pcmasterrace,2025-10-31 20:42:54,32
Intel,nmi5fxm,Actually maybe they should just let AI do their marketing. At this point it probably would be an improvement.,pcmasterrace,2025-11-01 08:23:45,2
Intel,nmgkqyu,Source?,pcmasterrace,2025-11-01 00:18:09,4
Intel,nmk0u17,"youre just making stuff up now. they didnt even specify anything about ""same driver treatment"" let alone new features.",pcmasterrace,2025-11-01 16:32:16,1
Intel,nmevdtr,"I disagree, there was a world of difference when i tried the dlss 4 transformer model compared to dlss 3 when i was playing oblivion remastered on my 2060",pcmasterrace,2025-10-31 18:22:06,21
Intel,nmevmi3,"Except people have been getting it working, quit dick riding the billion dollar corp.",pcmasterrace,2025-10-31 18:23:21,5
Intel,nmfy7qs,We get dlss upscaler but not fg because thats a different thing  Fsr 4 UPSCALER could work on rx6000 without much issue (it alredy does with unofficial patches),pcmasterrace,2025-10-31 21:51:33,1
Intel,nmfg5vk,I mean it would have been great if they officially supported 6000. Technically there's no reason to exclude 6000 from the FSR4 backporting aside from them wanting to force consumers to buy their recent GPUs.  I say this because quality wise FSR4 is better than XeSS2 or FSR3 for that argument.,pcmasterrace,2025-10-31 20:10:45,1
Intel,nmex0wp,"that is true but when every statement that comes out of your communication team mouth is back tracked. One has to stand back and ask... is it a mistake or whats going on.   AMD gets enough free passes as is, if this was Nvidia reddit be torching them quicker then you could blink.",pcmasterrace,2025-10-31 18:30:35,-10
Intel,nmhmtby,https://preview.redd.it/xmt1f2k3vkyf1.jpeg?width=686&format=pjpg&auto=webp&s=877685b58fac8711ab505deba340b7d5a50a2307,pcmasterrace,2025-11-01 05:00:52,15
Intel,nmtfxme,steam?,pcmasterrace,2025-11-03 02:50:00,1
Intel,nmesuve,Till your house burns down cause you bought a 5090,pcmasterrace,2025-10-31 18:09:01,39
Intel,nmgaznm,"Bad products exist but they tend to be an active kind of bad like flashdrives claiming to be 1tb, but really being 1gb and if you write to them they keep deleting the oldest data   Or psu which light on fire and burn your house down",pcmasterrace,2025-10-31 23:13:41,1
Intel,nmi853x,"The 6500xt was straight up bad. It just did not work as it should have, and no pricing would have saved it. Or a connector that burns down your house. This saying makes no sense, like if a car was cheap enough you would still buy it even though it had no brakes???",pcmasterrace,2025-11-01 08:54:17,0
Intel,nmkan6l,"The 3050 I have is a compact ITX card, so when my kid inherits the 6700XT, I've got plans for that 3050.",pcmasterrace,2025-11-01 17:21:47,1
Intel,nmfwrz3,"It shouldn’t happen at all, I think in all my years in overclocking communities since before Reddit was even a thing, I can count on one hand the number of times I saw a 6 or 8 pin connector melt. There’s zero reason to even use the 12 pin except to save card makers a few pennies.",pcmasterrace,2025-10-31 21:42:59,12
Intel,nmhmx8c,Wow lol. A sapphire 9070 xt melted. Amazing.,pcmasterrace,2025-11-01 05:01:53,2
Intel,nmffyfa,"Wasn't the entire point of the thread we are on that game optimization _would actually_ continue as normal, and old cards just wouldnt be getting new features?   And yea. I play pretty much any game I want on decade old cards. When you arent trying to push frames at 4k ultra settings you'd be surprised what old cards can run lol.",pcmasterrace,2025-10-31 20:09:39,2
Intel,nmfi1fk,"Well maybe not 5000 series, but 6000 series are really powerful",pcmasterrace,2025-10-31 20:20:47,1
Intel,nmfs402,"There are certain optimisations that are entirely driver controlled especially within shaders. AMD writes driver level optimisations for shader compilation and usage, that increase performance.",pcmasterrace,2025-10-31 21:15:53,1
Intel,nmfodv7,Day 1 game drivers help devs to optimize their games for their gpus though... Why else would you think they'd exist?,pcmasterrace,2025-10-31 20:54:54,-1
Intel,nmg80bl,Have there actually been any fires at all? All I have ever heard of is a few melted cables.,pcmasterrace,2025-10-31 22:54:12,10
Intel,nmfrmqj,"connector has at most a 10% margin of safety, fuck off with that user error bullshit gamers nexus spewed out of his oportunistic mouth when literal EE's looked at connector and found some serious problems with design even after multiple revisions  fucking sapphire rx9070xt's with 12V high failure rate connector have melting problems and that card is nowhere near the 660w limit connector has even with transients getting into 500w range",pcmasterrace,2025-10-31 21:13:07,-1
Intel,nmumrr9,was rebranded with a slight chip refresh into the R9 280 and shows up as such in the drivers under its various names,pcmasterrace,2025-11-03 08:56:52,2
Intel,nmjp89q,"1. They are on the 50 series GPUs now, so wrong. Also they are getting better support for more games and a few more stuff with ray tracing. Small stuff but still nice to have.  2. Unrelated because I was only referring to whether Nvidia GPUs were getting updated. I did not say anything about AMD, nor did I intend to.",pcmasterrace,2025-11-01 15:31:50,0
Intel,nmfh38j,1. Thank you  2. Pcmr remains the most randomly hostile sub I follow,pcmasterrace,2025-10-31 20:15:41,3
Intel,nmg0kg9,"Just as with games consoles and the internet before it, the AI bubble will burst. But just as with those other two markets, AI will persist regardless. Just with less enthusiasm in the  support it retains.",pcmasterrace,2025-10-31 22:06:01,126
Intel,nmfv02w,Until the bubble burts lol. And when people realize how overplayed and overhyped AI is. Peole like them less and less luckily.   Ofc companies jump on it as they think they can save some money. Plot twist to unfuck whatever the hell ai is gonna do for them is gonna cost way more than they saved lol.,pcmasterrace,2025-10-31 21:32:37,72
Intel,nmguv3d,I hope they make AI actually AI then. Cause that shot barely works for me,pcmasterrace,2025-11-01 01:26:42,1
Intel,nmlh6zk,"OK, 1080ti you aren't wrong. I still have one alive and well in a rig with an accellero 3 cooler conversion.  But...  1080ti only happened because NVIDIA got spooked that AMD were about to drop some absolute fire. They believed it enough that they actually let some performance out of the bag and into the market... otherwise they would have released a crappier card on purpose, because that is legitimately what NVIDIA do without any real competition: hold back technology and dribble it out in ever crappier improvements year on year.  AI is cool, but apart from GPT doing my spreadsheets for me (which costs OpenAI far more than my subscription) , it's made every single product it's integrated in worse.  It's already at the point where a customer faced with the choice between AI assistant integrated VS vocally proclaiming no AI...they will pick the one without AI because it's just data harvesting guff.  It is a bubble. There will be no AGI. Customers are already tired of the lies, and companies aren't going to keep losing money to do my spreadsheets for me.",pcmasterrace,2025-11-01 21:04:40,1
Intel,nmglphc,Not gonna lie. AI is extremely useful. But the energy consumption is what is going to be the limiting factor. They need to focus on power efficiency if they want to prevent that burst but I bet they won’t.,pcmasterrace,2025-11-01 00:24:38,-3
Intel,nmijdio,But they won't burn all their money for a bubble though,pcmasterrace,2025-11-01 10:51:55,1
Intel,nmghgab,"C'est peut être la traduction Reddit qui est inexacte mais je ne comprends pas ce que vous voulez dire par ""bulle IA"". Je veux dire, c'est clair maintenant que l'IA va perdurer et que les besoins ne vont faire que croître d'années en années.   Aujourd'hui c'est tellement rentable que le marché PC Gamer représente une niche pour les entreprises qui fabriquent des GPU, et ce n'est pas prêt de changer.",pcmasterrace,2025-10-31 23:55:54,-6
Intel,nmixvtl,"Will we though? I can mostly understand how LLMs etc. came to be but true AI that will think for itself, upgrade itself and have its own opinions without pre-set conditions is still pure fantasy at least to me",pcmasterrace,2025-11-01 12:50:29,2
Intel,nmijjxt,There was no AI hype 4 years ago? What are you on lol? Bubble doesn't mean it's not useful. It means it's highly overvalued as compared to profits generated. It should burst within a year now.,pcmasterrace,2025-11-01 10:53:36,1
Intel,nmhwl4k,"Machine learning was a thing way before that, and CUDA came out around 2005.",pcmasterrace,2025-11-01 06:44:01,13
Intel,nmiani8,">The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.  https://en.wikipedia.org/wiki/Machine_learning",pcmasterrace,2025-11-01 09:21:58,5
Intel,nmgjyih,"It wasn't even widely released, so, meh  5700x3d was widely released and a slight down bin of 5800x3d. Tbh I think 5700x3d existed more as an official price cut (which is fine!)  5600x3d did come in at a lower price but is ultimately a cut down single CCD x3d chip. Not exactly new new, but interesting for those near a microcenter.",pcmasterrace,2025-11-01 00:12:46,4
Intel,nmidad4,In general Linux has awesome compatibility with old and exotic hardware.,pcmasterrace,2025-11-01 09:50:34,7
Intel,nmidah0,"I pulled out my radeon HD 7870 out of my first gaming computer, good to know my workstation made into a server will work with it lol",pcmasterrace,2025-11-01 09:50:36,2
Intel,nmijep0,"Gaming OSs won't catch on. Dual booting is just too unreliable for average people to be fucking with it. Hell I'm a very advanced user and I won't install 2 oses on the same drive. I have just had too many problems including one OS killing the other several times in just the last couple of years. I'm genuinely bothered that it seems you can't just disable NVME drives in the bios anymore. What I actually want is one or more distros to have official Radeon and nVidia support. I know the drivers basically work on all distros, but I want to know if they break they actually have a commitment to solve it.",pcmasterrace,2025-11-01 10:52:14,1
Intel,nmevh1b,How’s the 4x frame gen working for you?,pcmasterrace,2025-10-31 18:22:34,-14
Intel,nmfqib1,if people want to accept the performance decrease to get FSR 4 INT8 why don't they contact AMD to include the support?,pcmasterrace,2025-10-31 21:06:43,3
Intel,nmg3xhq,"Happens to tech sales, a lot more often than it should and other faucets of tech industry (and other industries for that matter but I’m not as familiar in those industries)  Part of the issue is marketing team that doesn’t really understand what’s being offered or told of them too",pcmasterrace,2025-10-31 22:27:30,1
Intel,nmtzcnu,Better than most certainly. They haven’t released Half Life 3 though… so…,pcmasterrace,2025-11-03 05:08:07,1
Intel,nmfgpa2,So it still just a pricing problem. They just need to lower the price by about $200000,pcmasterrace,2025-10-31 20:13:38,15
Intel,nmgb3t3,Nobody's house has burned down from a 5090   Some people have had some plastic smoke damage from the connector failing and melting,pcmasterrace,2025-10-31 23:14:26,2
Intel,nmi8b5s,Well hiring SB to make brakes sounds easy enough if the 500k car Cost me 5k.,pcmasterrace,2025-11-01 08:56:09,2
Intel,nmgbh1j,None at all   Its just melted cables   People imagine their entire house burning down because some plastic got squishy,pcmasterrace,2025-10-31 23:16:48,14
Intel,nmjuaoe,"1. That's my oversight. Honestly, I straight up forgot Nvidia even had a 40 series. Probably because it was so forgettable. lol  2. Unrelated how? My original comment was about both AMD *and* Nvidia not giving old cards new features, and your rebuttal was ""my 3060 gets driver updates"".  Driver updates, patches, and bug fixes are not new features, though. Adding official DLSS3 support to 30x0 GPUs would be a new feature.",pcmasterrace,2025-11-01 15:58:07,1
Intel,nmi3qk6,"It might burst but I very much doubt nvidia will come out a loser. They have their hands on almost everything related to AI, I'm not an expert but talked a bit with some peolle doing research on machine learning and there's almost no alternatives for nvidia in some cases.",pcmasterrace,2025-11-01 08:04:45,26
Intel,nmg4zdz,"Sure, it's a bubble, but it's a bubble like the Dot Com bubble was in the late 90s, early 2000s.  A lot of investors are going to lose their shirts, a lot of companies are going to go bankrupt and people will realize that not everything needs to be (or should be) AI, just as not everything needed to be a website or an online retailer.  But just as some websites and online retailers and the internet persisted and grew, so will AI grow and persist where it makes sense. So, bubble or not, it's here to stay.",pcmasterrace,2025-10-31 22:34:23,75
Intel,nmgyzkw,"Android/iOS apps are the new best thing  Cloud is the new best thing  BigData is the new best thing  Alexa and the other voice assistants are the new best thing  VR is the new best thing  NFTs are the new best thing  Chatbots are the new best thing  AIs are the best best thing  <- You are here    I forgot serverless, but too lazy to find out where it goes.",pcmasterrace,2025-11-01 01:55:06,10
Intel,nmhavan,"Nvidia is in a position that sure, when the bubble bursts they'll lose evaluation as AI demand decreases but they won't burst themselves. The bubble bursting will destroy any company that just slapped AI on their app description and hoped to get bought by VC/investors before they had to show how they made money. No different than the Dot Com bubble or Mobile App bubble or social media bubble. Some companies like OpenAI or Google or Anthropic will ""win"", they'll buy up any startup/smaller company that actually shows promise (or potentially has promise and would deny competitors the engineers working there) then the market will calcify. Nvidia, however, supplies whoever wins regardless so while there won't be bidding wars, demand isn't likely to decrease for a long while.",pcmasterrace,2025-11-01 03:19:56,1
Intel,nmg2enw,Its not overplayed and overhyped. Just because reddit hates it doesnt mean its going away anytime soon.,pcmasterrace,2025-10-31 22:17:39,-10
Intel,nmg6z0x,Yeah not happening,pcmasterrace,2025-10-31 22:47:27,0
Intel,nmhdv8s,"Data Centers use a lot of power, and there's more demand for data centers than before, but this isn't a specific to AI problem. I've ran local LMs (Llama primarily) on my own server, they're not significantly harder or demanding to run than a typical server. Per the UPS I have attached to it, that single server uses around 200-400W the average is around 1700kW/h per year, and this is a fairly inefficient homelab.   Image/Video generation on the other hand, requires a lot more processing power, as anyone who's worked VFX will tell you, but this isn't the bulk of work that LLMs are doing. It's also why AI was able to ""pop up"" on every site so quickly, as it's really not significantly more intensive for Cloud providers than most K8S systems. This isn't a _pro_ AI comment btw, just noting that the power consumption aspect is a bit of misinformation that companies like OpenAI are _themselves_ pushing as if they make it seem too intensive to ""get in"" at this point, they solidify themselves as the defacto winner of the AI bubble. It's also why they recently have started bringing up ""ethical concerns"" about data theft and wanting regulation on data scraping...because they've already scraped everything. They want regulations now because it helps solidify their position.",pcmasterrace,2025-11-01 03:43:14,3
Intel,nmjp5ql,Unless they are conviced it'll be worth it in the end.,pcmasterrace,2025-11-01 15:31:27,1
Intel,nmjp30e,"His words, not mine.",pcmasterrace,2025-11-01 15:31:02,1
Intel,nmig7if,That thing is going to be supported as late as 2040 by the Linux community. If not even longer.,pcmasterrace,2025-11-01 10:20:25,3
Intel,nmiupha,"yeah but you don't need a gaming specific OS, you just need your main OS to support your gaming needs. linux is getting there, fast. i recently switched and haven't looked back. all my games worked basically out of the box and i'm even getting more fps than on windows.",pcmasterrace,2025-11-01 12:28:07,2
Intel,nmirqbq,"on the same drive indeed.  most PC support multiple drives luckily.  just selecting the os from the bios or boot menu is so much easier, just need a shortcut for that.",pcmasterrace,2025-11-01 12:05:23,1
Intel,nmewhx5,"Its been nice actually ever since i upgraded to a 5060ti recently, such a game changer, although the tolerance of people for latency might vary from person to person. I havent felt the need for 4x though, just up to 3x for stalker 2 with ultra plus mod graphics.",pcmasterrace,2025-10-31 18:27:52,8
Intel,nmnkzmp,"they'd need to pay me to get a 5090, and even then i'd just turn around and sell it so i can get a 9070XT and the system it will go in. if its 6000 dollars new here in australia, i'll sell for 4500.",pcmasterrace,2025-11-02 04:37:06,1
Intel,nmnl678,"still more than it ever should have been, the fact there isn't and never was a lawsuit about this is depressing, Nvidia should have been broken up already",pcmasterrace,2025-11-02 04:38:24,1
Intel,nmjvaio,1. Fair.  2. That’s what is happening. [This driver posted on 9/10](https://www.nvidia.com/en-us/geforce/drivers/results/254394/) adds DLSS support for some games.,pcmasterrace,2025-11-01 16:03:20,1
Intel,nmin5i6,Their value is set to plummet. It's massively overinflated. They'll survive but it's perhaps not such as good idea to have shares as it was 12 months ago.,pcmasterrace,2025-11-01 11:26:48,13
Intel,nmibjd4,"Their stock value will probably crater though, since it's based on the fantasy that AI will just keep growing exponentially forever.",pcmasterrace,2025-11-01 09:31:44,1
Intel,nmg85ci,"Fun fact, the stock of amazon dropped from over 100$ to about 6 during the crash while all business metrics of amazon went up and up and up... For the company, it wasn't a huge issue because they didn't need to gather more capital on the markets urgently, but it just goes to show that even in a bubble crash, it doesn't mean that all the companies that drop weren't actually a fair value even before the drop",pcmasterrace,2025-10-31 22:55:08,21
Intel,nmijmge,"What's the .com bubble?   Sorry if it's a stupid question, I was 10 in the 2000 and not fully aware of how the internet worked (also, not from the US, we mostly had .it here)",pcmasterrace,2025-11-01 10:54:15,1
Intel,nmg56s3,Yeah and. I never said ai is going completely away lol.,pcmasterrace,2025-10-31 22:35:43,-3
Intel,nmg33qz,"It absolutely is. It's not without uses, it's a useful tool in narrower applications.  It's not the sci-fi level AGI that techbros and wallstreet are creaming themselves fantasizing about. The insane investments and valuations aren't from what the technology is or has a plan to develop into... it's solely from investors watching cyberpunk sci-fi movies and having wet dreams at the idea of replacing everyone.",pcmasterrace,2025-10-31 22:22:08,11
Intel,nmg2ko9,It 100% is overhyped lol. Studdy it more. Gother some information. Its half useless and gimmicky at best. Added value is diminishingly low.,pcmasterrace,2025-10-31 22:18:42,5
Intel,nmg3zt6,"Look at the amount of money going into AI, and then the amount of revenue being generated by AI. The difference is insane, and I see no realistic path to profitability.   It's all driven by speculation and a downright messianic belief that if they just crack ""AGI"" first they're going to be the rulers of a new world where everyone needs AI for everything.  At some point the investors are going to want to get paid and then  the whole house of cards collapses.",pcmasterrace,2025-10-31 22:27:55,1
Intel,nmg6hdz,"It's absolutely overhyped.  AI will be a useful too, but it's being treated as if it's gonna solve all the world's problems.",pcmasterrace,2025-10-31 22:44:13,1
Intel,nmi120o,"That's only half of the truth isn't it? Running them locally is only possible due to the fact that it was trained and made ready for deployment. Running them isn't the part that needs a lot of resources, it's the training.   And the bigger the model the more hardware you need to throw at it and therefore more power is consumed.   I agree with the rest you've said but this part is important too.",pcmasterrace,2025-11-01 07:34:34,1
Intel,nmjsh0t,It's been almost 10 years since we heard that the bubble was going to burst. The reality is that NVIDIA boxes no longer give a damn about gamers.,pcmasterrace,2025-11-01 15:48:46,1
Intel,nmkmwqn,"Agree with the idea that you just need a replacement for Windows, not 2 OSes. The elephant in the room is still the anticheat problem. Huge swathes of people play multiplayer games, and an OS that can't run Fortnite is still massively handicapped.",pcmasterrace,2025-11-01 18:24:35,2
Intel,nmisa4y,"I use clover on mine. Default boot is Linux, then Windows is there to play the incompatible games which for me is just GTA5.",pcmasterrace,2025-11-01 12:09:40,1
Intel,nmewmeg,So you weren’t using 4x FG on your 2060? Why not?,pcmasterrace,2025-10-31 18:28:30,-7
Intel,nmnml7a,"The reason nobody can effectively sue over it is that manufacturers have been replacing what people lose from the failures   To sue, you need to prove damages and that the company did not sufficiently cover them   Its like nintendo joycon drift   They just keep replacing peoples joycons for free... so there is nothing to sue over",pcmasterrace,2025-11-02 04:48:44,1
Intel,nmkeaj2,"That's updating an existing feature, though, as the 3060 already supports DLSS4.  An actual *new* feature would be something like Nvidia enabling DLSS frame generation on the 30 series; something it doesn't currently officially support.  It seems like it's kinda splitting hairs, but that's what the whole AMD thing is/was about: People thinking that ""no new features"" means the same thing as ""no updates"".  Like, the RX 6600 supports FSR3, so if FSR3 gets an update, that update will apply to the RX6600. Just like the DLSS update applied to your 3060.",pcmasterrace,2025-11-01 17:40:32,1
Intel,nmjlx9b,RemindMe! 5 years.,pcmasterrace,2025-11-01 15:14:06,1
Intel,nmj8u42,"It's was a tech bubble in the US in the late 90s. Basically startups were popping up and getting investors just to start online businesses or websites, so you'd have huge purchases or investments in new companies that were basically just ""noun"" + "".com"" (the ending to our website addresses in the US at the time).  You had ridiculous stuff pop up nearly everyday and get investment, and then in 2-3 years the investors (and the companies) realized they had no way to actually make money, so the bubble burst.  You can look up articles and videos on it by searching for ""Dot Com Bubble"", it's often discussed.  Ironically, many of the failed businesses' ideas are a profitable thing now, and similar websites and companies exist - but the fact was the infrastructure and demand just wasn't there 25 years ago.",pcmasterrace,2025-11-01 13:59:55,1
Intel,nmg6aem,"Yeah, but we're talking about Nvidia. So they'll still be in the AI game for as long as AI exists. So the bubble bursting will never bankrupt Nvidia, only companies buying from Nvidia. The point is that Nvidia is never leaving the AI game now, because they are THE source of the hardware for AI.",pcmasterrace,2025-10-31 22:42:57,28
Intel,nmg4b2z,"The overreaction are indeed crazy. But that being said, Nvidias stock is so absurdly high due to their profit margins and sales volume.  Companies arent going to stop buying Nvidia gpus anytime soon (unless AMD, Intel, or Cerebras potentially compete).  Look at their profit year by year! Companies arent going to stop buying gpus anytime soon. Their stock wont crash unless serious completion comes in  https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue",pcmasterrace,2025-10-31 22:29:57,1
Intel,nmg2zaz,"""Study it more"". No, I'm not in college doing a dissertation lol. Maybe re-educate yourself and you will realize why its not going anywhere.",pcmasterrace,2025-10-31 22:21:20,-3
Intel,nmjvfwa,Are you even listening to me?,pcmasterrace,2025-11-01 16:04:07,1
Intel,nmogi4x,"yeah, that's true, 100% a big downside, if you happen to play games where that matters. i didn't even know fortnite had kernel level anticheat, because i'm lucky enough that all the games i play don't require that.  i think it's a market saturation thing. if a significant enough part of the user base switched (steam deck is helping a lot there), the developers would have to adjust their anticheat solutions eventually. since kernel level anticheat still doesn't effectively prevent cheating anyway, i think it's just a current trend that will eventually go away again.",pcmasterrace,2025-11-02 09:43:54,1
Intel,nmex05q,"uh because its not supported?, or you thinking you have a gotcha moment because fg is obviously not supported on 3000 series and below.  I might have not been clear but the 3000 series and below can actually benefit from the upscaling of the transformer model.",pcmasterrace,2025-10-31 18:30:28,10
Intel,nmnq5fe,this shouldn't legally be enough to cover their asses. consumer protection needs to be better than that.,pcmasterrace,2025-11-02 05:17:00,1
Intel,nmjm385,I will be messaging you in 5 years on [**2030-11-01 15:14:06 UTC**](http://www.wolframalpha.com/input/?i=2030-11-01%2015:14:06%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/pcmasterrace/comments/1ol0uuv/help_me_intel_arc_youre_my_only_hope/nmjlx9b/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fpcmasterrace%2Fcomments%2F1ol0uuv%2Fhelp_me_intel_arc_youre_my_only_hope%2Fnmjlx9b%2F%5D%0A%0ARemindMe%21%202030-11-01%2015%3A14%3A06%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201ol0uuv)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,pcmasterrace,2025-11-01 15:14:59,1
Intel,nmg6mcw,Yeah and again. Never stated any of those things. I kinda actualy said what you said.,pcmasterrace,2025-10-31 22:45:08,4
Intel,nmg6qbc,"There's a lot of ""market human centipede"" going on though too. With these companies investing in the companies buying their products. It's not exactly a straightforward or healthy thing. Especially when the bulk of ""AI"" has no path to profitability without some ""magic"" tech breakthrough.  You ever hear that joke about two economists walking in the woods?  >The first economist says to the other “I’ll pay you $100 to eat that pile of shit.” The second economist takes the $100 and eats the pile of shit. > >They continue walking until they come across a second pile of shit. The second economist turns to the first and says “I’ll pay you $100 to eat that pile of shit.” The first economist takes the $100 and eats a pile of shit. > >Walking a little more, the first economist looks at the second and says, ""You know, I gave you $100 to eat shit, then you gave me back the same $100 to eat shit. I can't help but feel like we both just ate shit for nothing."" > >""That's not true"", responded the second economist. ""We increased the GDP by $200!""  It very much feels relevant lately.",pcmasterrace,2025-10-31 22:45:52,1
Intel,nmg32im,Never said it is going lol. I said i am looking forward it bursting. And it is overhyped.....,pcmasterrace,2025-10-31 22:21:54,6
Intel,nmexa0h,"So you originally disagreed with me, but now you agree that in fact the RTX2000 series can’t do everything the RTX5000 can do. Thank you.",pcmasterrace,2025-10-31 18:31:52,-6
Intel,nmnx6nk,"Less than 1% fail, and those which do, they replace   What else do you want from them",pcmasterrace,2025-11-02 06:22:31,0
Intel,nmjxmuq,Seeing that 2030 is in 5 years physically hurt me,pcmasterrace,2025-11-01 16:15:33,4
Intel,nmg7trr,"Riddle me this. Why does Reddit think it is so much smarter than those who literally work on Wall Street?  Reddit always thinks they are right when they are usually in the minority.  Remember the election last year and how it was literally impossible for Kamala to lose? Or that the switch 2 and MK8 would be a failure because it was a $80 game and a $500 console? Or even just look at this sub a few years back. People have been waiting for the Nvidia stock to ""burst"" for years now.  Get my point?",pcmasterrace,2025-10-31 22:53:00,1
Intel,nmey9hy,"When did I even say that, not my fault you automatically assumed that I was implying that the 2060 can do nvidia fg just because i said that DLSS 4 Upscaling worked with it.",pcmasterrace,2025-10-31 18:36:55,13
Intel,nmod2xu,a real product that doesn't melt and force you to get new units.    some of yall need brains. fuck me,pcmasterrace,2025-11-02 09:08:39,1
Intel,nme4zm5,"Great timing, if the Intel driver works and supports the older GPUs   :)",pcmasterrace,2025-10-31 16:10:46,2
Intel,nmdyzvq,Is that the channel ran by children?,pcmasterrace,2025-10-31 15:41:56,1
Intel,nme0r1j,I thought that was LTT?  (only half joking),pcmasterrace,2025-10-31 15:50:19,3
Intel,nmcuykv,"That's cool. Pun intended.  Vram also likes cold temperatures, you could get a copper plate and cut out a hole just big enough to slide it over the cold plate of the water block. Some thermal paste is enough to help make contact between cold plate and copper plate from the sides of the cold plate. A little bending on the outside so it can make contact with the vram chips, pads and you can maybe get another 10% performance uplift on top.",pcmasterrace,2025-10-31 12:07:24,12
Intel,nmcwlsu,"Interesting thought, but VRAM actually does not like cold temps, it gets ""cold bug"" and artifacts much sooner, it's actually better to keep VRAM around 30-40C.",pcmasterrace,2025-10-31 12:17:55,6
Intel,nme05l5,Samsung gddr6 does not like cool temps at all.,pcmasterrace,2025-10-31 15:47:29,1
Intel,nmd2pjo,"30-40° is water cooling range, there is some OC benefit but gddr can go higher much with cooler temps. Modern gddr chips are very well designed and can take some beating.",pcmasterrace,2025-10-31 12:54:51,2
Intel,nmdzhar,"In all seriousness, is it possible that a GPU would artifact and show signs of dying due to memory being too cold? I didn't have my heat turned on in my office and my GPU was artifacting, but I've had 0 issues since it warmed back up. I thought it might be a possibility, but I thought ""no way is THAT the problem""",pcmasterrace,2025-10-31 15:44:14,1
Intel,nmd6isx,30-40C on VRAM is a cheap ebay heat sink and a fan.,pcmasterrace,2025-10-31 13:16:20,3
Intel,nmebym6,"yes, it is, even in XOC, if you run too cold the CPU crashes",pcmasterrace,2025-10-31 16:45:13,1
Intel,nkj6azy,Congrats! Was thinking about getting one for my son's PC. Hope it works great. Happy Gaming.,pcmasterrace,2025-10-21 01:50:42,7
Intel,nkjwc8b,curious how does this compare with the gpu standard-- 5070 cards?,pcmasterrace,2025-10-21 04:47:08,2
Intel,nkk52qj,Congrats. Planning to upgrade my aging rx 580 4gb too this spring. Same card,pcmasterrace,2025-10-21 06:04:48,2
Intel,nkjt2l8,Genuinely never heard of this card till now. Turns out it’s popular for the power and price which is rare af these days lol,pcmasterrace,2025-10-21 04:21:06,3
Intel,nkj792p,"That’s a sick looking card, gonna look nice man. Congrats, what you gaming? I’ve seen bf6 run great on it.",pcmasterrace,2025-10-21 01:56:28,1
Intel,nkjf5y9,Noice. Solid choice,pcmasterrace,2025-10-21 02:44:44,1
Intel,nkjqdze,Dude post some gaming with it....how much did u pay?,pcmasterrace,2025-10-21 04:00:37,1
Intel,nkjvj38,Purple box? Did Intel switch to team purple??  /s,pcmasterrace,2025-10-21 04:40:31,1
Intel,nkoyop8,I've had mine for 9ish months. Really love it! XD,pcmasterrace,2025-10-22 00:01:13,1
Intel,nksl0ek,4060ti//4070,pcmasterrace,2025-10-22 15:28:49,1
Intel,nkoyev0,You won't be disappointed,pcmasterrace,2025-10-21 23:59:36,2
Intel,nkkdy92,It’s honesty what the 9060 and 5060 should be  250 for 12gb vram and solid 1080p to low 1440p gaming,pcmasterrace,2025-10-21 07:36:54,2
Intel,nmndchy,B580 my goat,pcmasterrace,2025-11-02 03:46:11,4
Intel,nmndyzs,B580 might become the ol' reliable of the 2020s,pcmasterrace,2025-11-02 03:50:33,1
Intel,nmneyks,B580 is a little warrior but can’t beat the raw horsepower of the 5080,pcmasterrace,2025-11-02 03:57:07,1
Intel,nmnf00s,B580 my future goat   Waiting for the B770 though,pcmasterrace,2025-11-02 03:57:22,1
Intel,nmnfgbv,"With AMD dropping additional support for the RX 6600, I wonder what type of support Intel will give us for their cards.",pcmasterrace,2025-11-02 04:00:34,0
Intel,nmne8ud,it seems that way. I was surprised when I saw them in stock again not long ago. I also believe intel is releasing a better battlemage card later as well.,pcmasterrace,2025-11-02 03:52:26,1
Intel,nmnfy5s,"Watch the 2080ti outlast the 6700xt and Arc B580 it competes on performance with lol.   Ok, I hope not in all seriousness, but it has already beat out the 6700xt for support and it would be funny.",pcmasterrace,2025-11-02 04:03:57,1
Intel,nmnewm1,"Its mostly speculation, but I really hope they release a high end card eventually.",pcmasterrace,2025-11-02 03:56:46,1
Intel,nmnf62f,"Yep, the B770... challenging the 4070/5070",pcmasterrace,2025-11-02 03:58:30,1
Intel,nmwsn9o,9060xt 36% faster on techpowerup.,pcmasterrace,2025-11-03 17:18:41,3
Intel,nmwo0ig,Ferrari vs fiat,pcmasterrace,2025-11-03 16:56:49,2
Intel,nmwtjix,rx 9060xt s b580 performance video.  [https://www.youtube.com/watch?v=4eMd3mdgpg0](https://www.youtube.com/watch?v=4eMd3mdgpg0)     also 4gb extra vram is nice to have. Amd still has the slightly maturer software,pcmasterrace,2025-11-03 17:22:56,2
Intel,nmwsueq,Clear winner I take it?,pcmasterrace,2025-11-03 17:19:38,1
Intel,nmwv4j7,"Faster, more VRAM, and fsr 4 better than Intel upscaling. Rt also should be faster.  Yeah.",pcmasterrace,2025-11-03 17:30:28,3
Intel,nmzj4va,1 stick of ram makes it run in single channel so thats definitely going to have a impact on fps.,pcmasterrace,2025-11-04 01:46:36,22
Intel,nmzjwdi,"1 stick of ram is the problem, get 2x8gb",pcmasterrace,2025-11-04 01:51:00,9
Intel,nmzk16d,The problem is that you are using one slot of 16GB ram. Buy a 2nd one and get 32GB ram (2x16gb) and that will help immensely.,pcmasterrace,2025-11-04 01:51:46,6
Intel,nmzniac,"One ram stick and underclocked CPU? 10700f says it boosts to 4.8ghz for single core, not sure on multicore MHz, probably 4.5ghz?  Run some software and benchmarks. CPU z to check pcie link speed, by running it's render test in windowed (if single monitor) and ram speed is correct. Can also run its CPU benchmark to see if near average of CPU. 3dmark Timespy demo. Cinebench r23 or superposition for more if needed.  B460 chipset Mobo? If so ram can't overclock and runs stock of CPU, 2666mhz which is slow af.",pcmasterrace,2025-11-04 02:11:40,4
Intel,nmzn5rm,i guess intel just sux then,pcmasterrace,2025-11-04 02:09:40,4
Intel,nmzymir,"Maybe that Intel CPU has just aged poorly. It's still on the ancient 14nm process after all aka Skylake+++.  Maybe it was slightly better for gaming when it first released, and just isn't good for more modern games?",pcmasterrace,2025-11-04 03:17:23,1
Intel,nn0ecmj,"As far as im aware, bf6 and arc raiders benefit from l3 cache much more then your average game (tbf most modern games are benefiting a lot from bigger caches with lower latency)  10700k has a puny 16mb of l3 cache thats also generally worse even if it was 32mb, while the 3600x has 32mb of l3 cache. Even if this doesnt make the hugest difference in highest fps, it will for sure effect the average fps *greatly*  Also ram not being dual channel prolly tears off 5-15 fps.  Vram usage is most likely because, 1. 2060 super has a faster memory bus 2. You are playing at 1080p while your friend is at 1440p, which uses a LOT more vram (i have used as high as 7gb more vram on 1440p vs 1080p, same settings/game. That is a extreme example keep in mind, you will see closer to 4gb difference)",pcmasterrace,2025-11-04 05:05:55,1
Intel,nmzmyuh,I'm aware that multi channel RAM will boost my FPS but a difference of 30-45 is way more than dual channel will fix,pcmasterrace,2025-11-04 02:08:34,-6
Intel,nn024js,"In one game (Conan Exiles), having 2666mhz or 3200mhz (EXPO) ram updated my FPS from like 50 to 70 in heavy demanding base. In case it helps.",pcmasterrace,2025-11-04 03:39:38,2
Intel,nmzvo7u,"And just to be sure, the graphics settings are IDENTICAL between the two runs you're comparing? Like, he doesn't have on upscaling while you have it off, or anything along those lines?",pcmasterrace,2025-11-04 02:59:29,1
Intel,nmzvem5,"I'm with you on this, I have a feeling that going dual channel is not going to magically close that gap. Not sure why people are down voting, anyone who has tested single vs multi channel or have watched benchmark comparisons would see that there's likely something else going on here. Can you swap RAM with your buddy real fast and test it to save yourself spending money on dual channel and still coming up short in performance?  To anyone down voting OP, or me, you can either test it yourself or look up results from trusted reviewers/testers: https://youtu.be/hr6p1tqeM3M?si=ZnXjQG6qhQ1JFVJR",pcmasterrace,2025-11-04 02:57:55,1
Intel,nmzx5kx,"the friend is on AMD while OP is on intel, so this is likely dual channel 3000-3600mhz memory vs single channel 2400 (or maybe even 2133) which could very well cause the difference imo",pcmasterrace,2025-11-04 03:08:24,6
Intel,nn07wit,That video is comparing ddr5 ram where dual channel doesn't make as much of a difference,pcmasterrace,2025-11-04 04:18:00,1
Intel,nn08nda,"Ah, good thing there's also results for that too! https://youtu.be/bDgDtz7ImGI?si=TrGA95Aa9KOI_ZGK",pcmasterrace,2025-11-04 04:23:17,1
Intel,nmqhfho,Also The Finals which is also from Embark. Crazy how good Embark is with UE5.,pcmasterrace,2025-11-02 17:29:24,263
Intel,nmr5pa1,Arc raider is a great example that UE5 can be good in the hands of good developers. I couldn't tell that it was a UE5 game.,pcmasterrace,2025-11-02 19:25:08,56
Intel,nmqnwl3,"Bf6 does have a flaw regarding high cpu usage. Which was detected in the beta and annoyingly carried over into the final product. There was a fix thankfully, but having to find that on youtube is rather poor. That aside it runs relatively well no crashes for myself unless my undervolt gpu wise is a tad too much. Sad how many titles aren’t done to a decent standard for the cost of them now.",pcmasterrace,2025-11-02 18:01:09,54
Intel,nmp8dji,"Optimised doesn't just mean high framerates, it means good performance in relation to the tech it's using. Both of those games aren't using intensive RT or PT nor is their general rendering quality pushing boundaries in any way so the fact they run well isn't impressive             E.g. If the original pac-man released now and ran at 120fps on today's budget gaming PCs that would still mean it has awful optimisation even though it runs well",pcmasterrace,2025-11-02 13:32:45,66
Intel,nmsdza5,"If I remember correctly Helddivers 2 was 30gb when I installed, it got less optimized over time which is kinda sad tbh",pcmasterrace,2025-11-02 23:06:35,3
Intel,nmp4ej4,HD2 is designed to run quickly on HDD hence the size.  Cod has had file management to reduce file size based on what you want for several years. BO6 mp was like less than 50 gbs total.,pcmasterrace,2025-11-02 13:07:27,20
Intel,nmqunkt,Optimization in BF6 is not as good as the other ones which look just as good maybe even better. I have to play this game with low everything or else I go over my 8GB vram limit and Ill start stuttering,pcmasterrace,2025-11-02 18:33:19,9
Intel,nmqy50j,"its because they didnt over complicate the graphics. neither of the games are truly breathtaking visually from the standpoint of just high detailed textures and lighting. they look amazing, but theres plenty of techniques that the devs used to make the game look good enough, but also run really well. zoom in really close on the textures on ultra settings in Arc and youll see its pretty pixelated, but for things like terrain who cares? stuff like that adds so much.",pcmasterrace,2025-11-02 18:49:35,2
Intel,nmrt8w8,I still feel bf6 ran better during beta on my machines,pcmasterrace,2025-11-02 21:16:47,2
Intel,nmpr2xx,I do think it is a bit dramatic to not use DLSS or upscaling in general nowadays. At 4k and even 1440p I think it is perfectly reasonable to expect some degree of upscaling. Why throw away performance for some relatively minor clarity improvements?   Beyond that I think it is important to remember that optimisation isn’t just making your gaming using cheaper rendering techniques.   I would call Avatar Frontiers of Pandora optimised even if it quite hard to run because it looks very good for the resources it uses and doesn’t have persistent stuttering or other issues along those lines.,pcmasterrace,2025-11-02 15:16:52,7
Intel,nmq99aj,call of duty isn't unoptimized,pcmasterrace,2025-11-02 16:49:15,3
Intel,nmrc00u,"Battlefield 6 is a polished turd. It may look good visually, but only playing a ton do you see how bad the gunfights feel when someone has slow internet. The laggy player benefits and kills you. Youll be playing and put a full mag into em and they seemingly turn around and one shot. It is really frustrating, specially in battle royale.",pcmasterrace,2025-11-02 19:55:45,3
Intel,nmqa0ho,HD2 runs good on older hardware because of the size.,pcmasterrace,2025-11-02 16:52:57,1
Intel,nmr251h,I'd like to think it's because they changed their direction but it also could be they just so happened to actually get a talented developer on board.  I've worked in a lot of places and no matter what the people up top want it's really just about if the worker can deliver.,pcmasterrace,2025-11-02 19:08:17,1
Intel,nmrjyn4,And then monetize the crap out of them.,pcmasterrace,2025-11-02 20:33:07,1
Intel,nmrxpkd,I wish I got arc raiders over BF6 lol,pcmasterrace,2025-11-02 21:38:37,1
Intel,nmv73fq,Both games do not use any of the new tech out and both are not rendering anywhere close to what other games are. TIL this day Battlefield 5 and 1 look decades better than 6.  Battlefield6 also still have the high CPU Bug. This game is a downgrade in graphics and doesn’t use RT/PT nor Lumen so it’s nothing groundbreaking. Same goes for Arc.  If this was Battlefront 2 or something then yes. So saying these games are proof of good optimization is a stretch to be fair.,pcmasterrace,2025-11-03 12:08:02,1
Intel,nmxegvr,"You do realize the only reason BF6 is ""optimized"" is because there's no ray tracing?   It's basically battlefield 4 with different color filters and way higher res textures.",pcmasterrace,2025-11-03 19:02:54,1
Intel,nmqm5jb,Battlefield 6 runs like shit with my 12400f and 5070,pcmasterrace,2025-11-02 17:52:34,1
Intel,nmrb2g6,This post is interesting because I need dlss for good fps in Bf6 but not in Helldivers. I've never had a single performance issue with hell drivers in the slightest.,pcmasterrace,2025-11-02 19:51:11,1
Intel,nmrtw86,"Arc raiders I don't really agree with, I think there's a lot of room for improvement there. I thought the finals ran far better. Bf6 though absolutely",pcmasterrace,2025-11-02 21:19:55,0
Intel,nmqhoay,"It seems that UE5 is finally maturing. With the Witcher 4 development influencing the 5.6 version, recent developers benefit from the improved tooling and fixed issues.",pcmasterrace,2025-11-02 17:30:37,-3
Intel,nmrz5ll,Explain it to me slowly; why do people have a problem with using DLSS?  It's just a tool developers use to increase performance.  It doesn't change the gameplay and any latency it adds is only going to be noticeable in edge cases.   The developers don't owe it to you that the game runs with your preferred settings.,pcmasterrace,2025-11-02 21:45:56,-1
Intel,nmqtelp,Don't forget Helldivers 2 used upscaling by default too. So people with great systems probably didn't even realize why it looked a bit smudgy unless they specifically went into the video options and turned off the upscaling.,pcmasterrace,2025-11-02 18:27:29,0
Intel,nms7efl,"BF6 sure, arc raiders…. Eeeeeehhhhh",pcmasterrace,2025-11-02 22:30:14,0
Intel,nmtev7l,"Battlefield 6 is a 70gb download and has an rtx *2060* as its minimum.  Y'all's standards are in the dirt. Though judging by your examples, they're also extremely inconsistent. Helldivers 2, for example, has lower requirements than either of those games.",pcmasterrace,2025-11-03 02:43:12,-5
Intel,nms8sx9,"It's a good combination of technical implementation and good design. Their games aren't huge in scope: The Finals and Arc Raiders, for example, use very deliberate sizes for the maps and number of players. If you tried to put the destruction in The Finals, with a 64 player server, it would take some crazy work to get it to work as smoothly. Same thing with Arc Raiders: the game looks great, but it is also very carefully designed not to be too heavy from the beginning.",pcmasterrace,2025-11-02 22:38:06,20
Intel,nmqsgu0,"Something very important to consider that people don't mention enough, is that it was made using a modded version of UE5 not base UE5.",pcmasterrace,2025-11-02 18:23:02,33
Intel,nmu2cvp,I was blown away arc raiders defaulted to high on my 1070ti (iric). It ran fine. I only played 1 night during server slam.   Bf6 beta ran fine but defaulted to low. I bumped textures and a couple things to medium and still seemed OK.,pcmasterrace,2025-11-03 05:34:09,6
Intel,nmr19ik,"Mhm, same. My poor 14400f keeps on dropping below 60 fps on some maps, despite my 9070xt chugging out 120+ frames constantly.   I know that I'm cpu bottlenecked as is, but it's just certian moments where it drops, at other times/on other maps it's constant 80-100 frames without issues.",pcmasterrace,2025-11-02 19:04:13,13
Intel,nmsdnmm,What's the fix?,pcmasterrace,2025-11-02 23:04:47,5
Intel,nmrz80s,"BF6's optmization also involves tiny render distance. Even having settings on Overkill and running it on an NVMe, things pop in like at 500m from you when flying.",pcmasterrace,2025-11-02 21:46:16,3
Intel,nmr4ois,"Yeah BF6 is decent, but has some issues. I've never had a game require so much tinkering to run correctly. I needed a BIOS update, a CFG file, and multiple other tweaks to get that game to run at its best. I also have to point out that the CPU optimisation needs some work outside of these tweaks, nothing BF6 does apart from the improved server-side physics should be reducing CPU performance by a large margin.",pcmasterrace,2025-11-02 19:20:14,1
Intel,nmpwf27,"You are 100% correct and PCMR is being PCMR as usual....Arc Raiders replaced Lumen for a MUCH less demanding and unimpressive version of GI. It has less light bouncing, it literally only Ray Traces light and no it's not as if it's difficult to replace it. It was a decision, nothing more.  Battlefield 6 I'm not even going to start there lolol...all I see is jaggy box buildings and terrible foliage. It's destruction model is o.k. at best but it does show on the CPU side of things. The actual rendering? I played games that looked better during covid.   I'm not going to knock the decisions as MP games shouldn't be intensive. But to act like the developers are more talented is a flat out lie...but I'll be downvoted too In the biggest mass hypocrisy ever..the OpTiMiZAtIOn crisis of 2025 where people who shouldn't be on p.c. are still running their COVID builds and trying to max out cutting edge games",pcmasterrace,2025-11-02 15:44:07,16
Intel,nmtmw73,"Agreed, I've been loving BF6 and while it looks good and runs well, it doesn't look *great* either. It hasn't wowed me graphically like other recent releases.   I'll be one of the few people wishing it had an RT option. The models and textures look pretty good but the lighting is its biggest weakness.",pcmasterrace,2025-11-03 03:36:19,1
Intel,nmphpv4,"lol, tell that to all the other crap releases these past few years.",pcmasterrace,2025-11-02 14:26:52,-8
Intel,nmrlgpc,"Ive been saying this too, arc raiders running well isnt impressive, the game looks like trash, and textures are from 2009, no wonder it uses no vram",pcmasterrace,2025-11-02 20:40:13,-4
Intel,nmp5sz2,"https://preview.redd.it/hklczreeguyf1.png?width=563&format=png&auto=webp&s=1fbc83a718239bb21fdfdd3fb62bed3f2aff07ae  Wish you could say that again with being ""less than 50 gbs total""",pcmasterrace,2025-11-02 13:16:28,29
Intel,nmql1v2,Except the fact that HD2 doesn't run well on anything other than high end hardware. They are catering to a make believe audience,pcmasterrace,2025-11-02 17:47:16,5
Intel,nmqyph9,Bf6 isn’t the greatest looking game which is fine. But performance wise it is amazing,pcmasterrace,2025-11-02 18:52:15,9
Intel,nmq8ty6,"I think people aren't against upscaling or frame gen. What people don't like is forcing it to get playable performance. For example, the 3060 can manage high settings on Indiana jones and get mostly 60fps at 1080p native. That's more than acceptable, but if I wanted to get MORE than 60fps I can CHOOSE to enable DLSS Quality for example, and if I'm getting 60+ fps all the time I can CHOOSE to enable some frame gen to get a smoother image. But games like Monster Hunter Wilds where by default the presets enable upscaling and frame gen only to get 60 fps, and that's frame gen FPS so in reality more like 30 fps or actual frames. That's the problem.  The technologies are great when it enhances the experience to a better level, not to achieve the bare minimum.",pcmasterrace,2025-11-02 16:47:08,14
Intel,nmr5mby,"The upscaling paradigm has become necessary because rendering 4x the pixels over 1080p is very intensive, yet developers want to use the extra power to improve visuals for real, not just improve resolution. Games running at various resolutions was a normal thing during the CRT era, fixed pixel displays made upscaling necessary and consoles having been doing it since the HD era. It's unrealistic to expect every developer to make their games run at native 4k 60fps, especially when they want to improve visuals at the same time.",pcmasterrace,2025-11-02 19:24:44,6
Intel,nmq95ko,"I think the default should be good performance at native res, enabling upscaling should be a bonus to performance, not what is required to get playable framerates with reasonable graphics settings.  I always boot a game up first at native first, see what the perf is, then if it's not great then i drop down to FSR 4 quality.  The ideal scenario is great performance while having FSR 4 set to native AA, such a crisp image",pcmasterrace,2025-11-02 16:48:44,5
Intel,nmr7yaw,"True, it‘s good tech. But let me tell you how many times I thought i saw an enemy in Battle Royale, which was in fact an upscaling artefact.",pcmasterrace,2025-11-02 19:36:01,1
Intel,nmvan4i,Yup. Avatar is optimized. It's just demanding because it pushes a lot of technologies.,pcmasterrace,2025-11-03 12:33:22,1
Intel,nmpw6qs,Go ahead and find that acceptable that's what they like to hear. What you find acceptable just becomes the new bare minimum for them and then when that feels normal they drop the bar again.,pcmasterrace,2025-11-02 15:42:57,-7
Intel,nmqfemp,"At the very least, give gamers a choice if they want performance, quality or a balance of both. Any game developer that does not do this, is in the wrong market.",pcmasterrace,2025-11-02 17:19:24,0
Intel,nmq2jk7,The cult like hate of frame gen on this sub is a bit overboard.,pcmasterrace,2025-11-02 16:14:59,-13
Intel,nmsbpdh,I have heard such assertions regarding every iteration of Battlefield thus far.,pcmasterrace,2025-11-02 22:54:02,0
Intel,nmwjbnt,"You do make a fair point, though games like Helldivers 2 don't even have any of those, and suffer from being a poorly optimized game. Also, Arc Raiders do have RT or RTX Global Illumination. Battlefield 6 is an odd one, but I guess that's understandable, considering developers wanted most systems to be able to run the game well enough, and considering it's a multiplayer-focused game so everyone wants to have high frames.",pcmasterrace,2025-11-03 16:34:21,1
Intel,nmr5x0v,Do a bios update and thank me later.,pcmasterrace,2025-11-02 19:26:10,4
Intel,nmr63tc,"UE5.6 will not be a silver bullet for performance. It will be better, but if UE5 games run shit for you now, they will still run like shit for you on 5.6 games.",pcmasterrace,2025-11-02 19:27:04,0
Intel,nmqt0bl,This games used a modded version of UE5 not base UE5. This mods weren't made by epic. So no not really.,pcmasterrace,2025-11-02 18:25:35,-5
Intel,nmsf623,"They don't. Only insane redditors that don't actually play games, instead preferring to analyse zoomed in screenshots and get mad over nothing.",pcmasterrace,2025-11-02 23:13:04,3
Intel,nmsc2kc,"People have grown so accustomed to the imperfections of antialiasing that the eventual elimination of them is perceived – ironically – as a visual deterioration.  Also, it is in fashion now to hate everything Nvidia did.",pcmasterrace,2025-11-02 22:56:03,1
Intel,nms5i95,Because it looks worse.,pcmasterrace,2025-11-02 22:19:47,0
Intel,nmtf6c2,It's should literally be the opposite,pcmasterrace,2025-11-03 02:45:10,1
Intel,nmuptj3,"Battlefield 6 is not a 70GB download if you select specific DLCs/parts to be installed (Campaign/Multiplayer/Redsec and its HD textures, my download is 43.7GB with RedSec and Multiplayer only being installed).  Also, just because Helldivers 2 has lower requirements, it doesn't mean it's optimized, it just means that you can try to run it, but because the game is unoptimized in terms of getting high frames/maintaining them to the point of crashing your game. In fact, Helldivers 2's system requirements in terms of AMD CPU are literally the same for Arc Raiders.  https://preview.redd.it/q2jf1w8tg0zf1.png?width=558&format=png&auto=webp&s=a703fac2461f3e96d4ebd8589bc383ef464b9044",pcmasterrace,2025-11-03 09:29:54,2
Intel,nmufgkg,So you’re saying requirements are still too high for what these games offer?,pcmasterrace,2025-11-03 07:40:24,1
Intel,nmqtkn3,"Lmao all big devs will use a modded version, as each game needs certain features that are not included in the base version. This is a standard for softwares across a lot of industries, not only gaming.",pcmasterrace,2025-11-02 18:28:16,141
Intel,nmqy04i,"It’s good because Embark consists of a lot of the engineers from DICE who created peak Frostbite (BF1).   Like, engines are fucking engines. The difference is the decade of experience in graphics programming of the engineering team.",pcmasterrace,2025-11-02 18:48:56,20
Intel,nmsdkal,What point are you trying to make here?,pcmasterrace,2025-11-02 23:04:16,3
Intel,nmr04kw,"People keep repeating that, and yet I cannot find any authoritative source on that.  And [Embark is actually pretty good at open-sourcing their stuff](https://embark.dev/) and being transparent about their development process.  I'm not trying to discredit, I'm just curious and want to read more about what they've done, and I can't find a single source.",pcmasterrace,2025-11-02 18:58:55,5
Intel,nmxefii,"All devs change UE, so saying it's a modded version doesn't make sense. UE (any version) is not meant to be used completely as is. It's a general engine after all.",pcmasterrace,2025-11-03 19:02:43,1
Intel,nn0fy6h,Every studio modifies UE. Every single one that uses it.,pcmasterrace,2025-11-04 05:18:37,1
Intel,nmr4vy6,"Try a bios update, I was having the same problem on my system and that fixed it. Make sure XMP (or the intel equivalent) is enabled, and check out the user.cfg fix.",pcmasterrace,2025-11-02 19:21:13,5
Intel,nmr3esa,"The issue i mainly found was high temps to start with. Once i applied the fix that was sorted and it probs helped in other ways. Have a look on the fix, see what results it does",pcmasterrace,2025-11-02 19:14:18,3
Intel,nmthehi,"You tell the game manually how many cores you want to use.   >Create a ""User.cfg"" file with Notepad at the root of your Battlefield 6 folder with these lines:      Thread.ProcessorCount 8     Thread.MaxProcessorCount 8     Thread.MinFreeProcessorCount 0     Thread.JobThreadPriority 0     GstRender.Thread.MaxProcessorCount 16  >Change 8/16 to the number of cores and threads your CPU has, you can find this in Task Manager under the CPU tab of Performance at the bottom.   >This fix also worked in the beta, my CPU (5800X3D) now sits at like 20% in the menus compared to 50%, and much lower in actual matches which prevents fps instability.   (not my post but the bot says I'm not allowed to mention the thread where it was posted)   Others are using software to move the game away from the non X3D cores (on AMD, not sure if the same thing happens to Intel p/e-cores) because BF6 doesn't use core parking correctly.  JayzTwoCents tried [video ](https://youtu.be/Czd6eqSr-qg) that but warned using a tool like process lasso could get you banned because of EAs rootkit checking what software is allowed to talk to their games.   He found a different tool [video ](https://youtu.be/m3YVDB0Ymi8) and a short benchmark didn't show much difference. So ymmv",pcmasterrace,2025-11-03 02:59:29,3
Intel,nmtneb0,https://youtu.be/pGvyXENXFUk?si=HoiSkTz628DeYwtQ,pcmasterrace,2025-11-03 03:39:44,1
Intel,nmtlcfm,Not to mention you could get a life ban for simply having some controller app running in the background  I went to fire it up the battle royal and it detected DS 4 windows and flagged it like I fucked up big time..luckily it picked it up because I imagine if it was running in the background it would be a ban...BF 6 is way too sketchy for me lol..secure boot bricking builds and life time bans all for the servers to be plagued by cheaters in shitty P.C. builds anyway..not worth the headache..such a creepy fucking game,pcmasterrace,2025-11-03 03:25:47,-2
Intel,nmqvcan,"I don’t know why you are being downvoted, when you are just giving actual facts. Both are great games, but they do not look as nearly as good as a UE5 game with PT, like Alan Wake 2 or Stalker.    PCMR is so weird nowadays, they want perfect games upon release, which need to look extremely good and that run as smooth as butter. But at the same time, they have a PC from 7 years ago, and they hate on FG, MFG and RT for no reason. Just stick to console at this point.",pcmasterrace,2025-11-02 18:36:33,14
Intel,nmr8wt3,"Dont forget they are still targeting ps5 and old xbox as the lowest hardware floor. They have to make sure old hardware runs it and its looks nice. Arc Raiders looks very nice in high settings, i have a hard time deciding between the graphics vs more fps. Love the graphics in very high with DLSS4. It’s either 70 fps with my 2070 super with good graphics or 90-100 fps in lower settings. They did a good job for this ancient hardware",pcmasterrace,2025-11-02 19:40:42,3
Intel,nmpp2xu,"I'm talking about the misuse of the word not whether the games are better or worse than contemporaries, learn to read",pcmasterrace,2025-11-02 15:06:26,14
Intel,nmpejp6,"Oh ok, not sure what I was smoking.  Regardless, I’m pretty sure that has to do with the optimization required to make sure it loads quickly across 2 gens of consoles + PC.",pcmasterrace,2025-11-02 14:09:25,-7
Intel,nmrdev7,Brother HD2 runs like shit on high end hardware. Trust me lol,pcmasterrace,2025-11-02 20:02:36,6
Intel,nmrau28,I haven’t played in a minute but is it actually that bad? My 4070 Ti isn’t exactly top of the line and it did ok.  I believe they said recently they’re looking at getting rid of the stuff that makes it run better on HDD because the game size has increased significantly.,pcmasterrace,2025-11-02 19:50:03,1
Intel,nmr24t0,To me a game that I can only play on low is not optimized the game eats too much VRAM while on the other hand arc raiders runs like a dream with high settings,pcmasterrace,2025-11-02 19:08:15,-6
Intel,nmrt42t,Shocked when it came out to see all the gameplay kids looking so utterly bland.,pcmasterrace,2025-11-02 21:16:08,-6
Intel,nmr5thk,"Is it not the same thing as using a variety of resolutions back in the CRT era though? Devs want to improve visuals, not just resolution. Upscaling has become necessary as a result. Upscaling on consoles has been a thing since the HD era.",pcmasterrace,2025-11-02 19:25:42,0
Intel,nmsbhhl,"How many times you saw an enemy that turned to be an antialiasing artifact? The issue that we had since... err... Skyrim, I guess?",pcmasterrace,2025-11-02 22:52:50,1
Intel,nmpwg6m,Do you also think it’s unacceptable to unload the scenes that are happening behind you? Like that’s also dropping the bar from the entire game world being rendered at the same time.,pcmasterrace,2025-11-02 15:44:16,2
Intel,nmpzey3,"Dlss and frame gen are a nice bonus - but shouldn’t be the default.  Not using them when they offer genuine benefits in a lot of situations just seems silly.  I use both even though technically the image quality is inferior - to keep gpu power draw down, and spare the 12vhfpwr  from burning my house down.",pcmasterrace,2025-11-02 15:59:10,0
Intel,nmq8c49,"Because the latency increase from enabling is noticeable.  It's fine for singleplayer, but in multiplayer it will negatively impact your experience.",pcmasterrace,2025-11-02 16:44:37,11
Intel,nmq3uy8,"Hell, frame gen criticism I get, upscaling at some point starts becoming a dislike a great technology for no real reason.",pcmasterrace,2025-11-02 16:21:43,1
Intel,nmtsfsy,I have a lot to learn.  Went from 50-60fps to 90+ after doing bios update.  Thank you for helping me.,pcmasterrace,2025-11-03 04:14:28,5
Intel,nmr3oi9,Every developer uses a moded version of UE5. This is not at all unique to embark or DICE,pcmasterrace,2025-11-02 19:15:35,6
Intel,nmqutub,"Many ""mods"" made by CD Project RED have been included in UE5 over the years and these developers have definitely benefited from that.",pcmasterrace,2025-11-02 18:34:07,1
Intel,nmtoh09,"Arc uses a GI model used with UE 4 ""RT"" titles..I believe it was Nvidea who developed it for UE 4, I could be wrong though...but it's literally plug and play on UE 5 and not anything but a menu selection",pcmasterrace,2025-11-03 03:47:01,1
Intel,nmufpv4,"I'm saying they all have very average requirements, both his examples of well-optimized games and poorly-optimized games, and are very similar to each other.",pcmasterrace,2025-11-03 07:43:05,1
Intel,nmqwmia,"Some big devs definitely, all of these are the cases were UE5 worked. However lots of big devs haven't done that, expecting for base UE5 to just work and smaller devs can't afford to (with the exception of Expedition 33 apparently).   It is important to take this in consideration so smaller devs don't use base UE5 and expect it to just work perfectly. Since so far no base UE5 game has worked without technical problems.  UE5 made most gamedev companies cheap out on engine devs thinking that they could just use base UE5. And this generally hasnt worked (or well just not worked so far).",pcmasterrace,2025-11-02 18:42:31,-13
Intel,nms9136,XMP Is the Intel equivalent. AMD version is called EXPO.,pcmasterrace,2025-11-02 22:39:22,4
Intel,nms95ne,Where is this fix. I feel like it’s happening to me too,pcmasterrace,2025-11-02 22:40:04,1
Intel,nmwtz7e,Secureboot is best done with a fresh install of Windows in UEFI mode.,pcmasterrace,2025-11-03 17:25:01,1
Intel,nmrdaq4,"To put it frankly, this sub is fucking retarded in all honesty. The amount of verifiably false or incorrect takes I've seen here that are taken as gospel is ridiculous.",pcmasterrace,2025-11-02 20:02:03,15
Intel,nmsct6r,Why do they need to look good? They're multiplayer online games.,pcmasterrace,2025-11-02 23:00:08,2
Intel,nmqhcns,Downloading something on pc that is supposed to be for console is a prime example of a company NOT optimizing…,pcmasterrace,2025-11-02 17:29:01,17
Intel,nmrj3f7,"I have game too. I get ~130fps at 1440p on Ultra. I should be getting more, especially with how old the engine is",pcmasterrace,2025-11-02 20:29:03,0
Intel,nmrefa2,I mean consoles have never had a choice. You get what the devs put on you and that's about it... Recently they've had modes for 30fps or 60fps but that's it.,pcmasterrace,2025-11-02 20:07:20,2
Intel,nmscl1x,"Not that often honestly… I think its the fact that upsampling ghosting occurs with movement, the antialiasing my eye has had time to get accustomed to.   Also, lets be honest, what happens the most is there is a piece of fucking paper or debris flying around, and I mistake that for an enemy.",pcmasterrace,2025-11-02 22:58:53,1
Intel,nmpwvh5,That's either a bad faith argument that you know isn't the same or you genuinely think you've made a point there. Either way any further discussion will be a waste of time.,pcmasterrace,2025-11-02 15:46:23,0
Intel,nmse67r,"You can't get something for nothing. If you want better performance, some compromise is necessary. You can either lower the native res, lower fidelity settings, or use dlss. Dlss is just the best compromise because its basically not noticeable.",pcmasterrace,2025-11-02 23:07:38,1
Intel,nmq8qdt,"It just looks worse, in many games lower details in native look better than DLSS on quality if you have 1440p or smaller resolution display   Like in cyberpunk it's for me a massive difference, DLSS image just isn't sharp  On 4k DLSS quality is great tho",pcmasterrace,2025-11-02 16:46:38,3
Intel,nmtsq81,"If it helps you feel any better, it took me a week to figure that out and I’ve been PC gaming for over 16 years.",pcmasterrace,2025-11-03 04:16:37,6
Intel,nmrdvfs,In this case they are using a heavily forked version of UE5 made by Nvidia called NvRTX.,pcmasterrace,2025-11-02 20:04:46,1
Intel,nmr3pxy,"Base ue5 doesn't work because you also end up not spending any time on optimizing the game. Nanite is not an optimization tool their official documentation says as much.   Quality game devs will always work Dev magic.   You also have games like helldivers 2, looks dated, runs worse than every single well optimized ue5 game. I have 400 hours in that game but everytime I turn it on I cant imagine why it took so long for people to complain about performance. The game ran terribly since launch. But it wasn't ue5 so most people did not complain until now.",pcmasterrace,2025-11-02 19:15:46,19
Intel,nms85pd,">Some big devs definitely, all of these are the cases were UE5 worked.  This is very, very incorrect. One of the most controversial games insofar as optimization goes, was Jedi:Survivor, and if you think that's just using Unreal Engine out of the box, you'd be insane.  Virtually all AAA games using UE5 will tweak and change it to their needs, and there's a ton of reason why games end up under-performing when released: it can range from exceeding the technical scope, to production issues like deadlines and mismanagement. It's all always about technical competence.",pcmasterrace,2025-11-02 22:34:27,4
Intel,nmtpqeb,If you want more perspective in the matter Arc is using a GI that was used with UE 4...it's not all that difficult as the groundwork has already been laid out...it was an early version of Ray Tracing developed by Nvidea for UE 4...they also just turn nanite off on top of whatever physics model the have..  They literally just turned the settings down on the engine side of things before development.,pcmasterrace,2025-11-03 03:55:37,1
Intel,nmtndmk,https://youtu.be/pGvyXENXFUk?si=HoiSkTz628DeYwtQ,pcmasterrace,2025-11-03 03:39:37,2
Intel,nmwy31l,Lol..this is what I mean,pcmasterrace,2025-11-03 17:44:28,1
Intel,nmryyr0,"I'd say it originates from the basic phenomenon of ""I want it tailored to me"" that you find in any customer base.  I'd say if PC components were far cheaper you would have the reverse of people hating on non-graphically perfect games like BF6 due to their inferior graphics.",pcmasterrace,2025-11-02 21:44:59,2
Intel,nmrais9,What?,pcmasterrace,2025-11-02 19:48:29,1
Intel,nmrvn7i,On what difficulty? On SuperHelldive my 4080 and 13600k go down to under 60fps on an ultrawide at 1440p. Doesn't matter what graphic settings i use.,pcmasterrace,2025-11-02 21:28:29,4
Intel,nmrsqxi,"You also don’t have a choice in PC games apart from the choice to upgrade hardware to hit your target resolution. Crazy how resolution scale sliders aren’t hated, but DLSS and FSR is hated for making upscaling better.",pcmasterrace,2025-11-02 21:14:21,-1
Intel,nmpxejj,"No, but seriously, you think we are accepting less but what do you think optimisation in games is? It’s almost never just getting the exact same visual fidelity for better performance. You usually need to make some sort of sacrifice just that hopefully that sacrifice is small enough that you won’t notice. With something like DLSS I certainly think you make a very tiny sacrifice, especially at higher internal resolution. Specifically going to 4K from 1440p DLSS quality mode is quite frankly so good looking that I don’t really get the people complaining. Even at lower internal resolutions, it looks quite good.",pcmasterrace,2025-11-02 15:49:01,2
Intel,nmsif8i,100,pcmasterrace,2025-11-02 23:30:56,1
Intel,nmymd4a,Console for all my life.  Stepped into PC about a year ago.  It's different but so much more fun.,pcmasterrace,2025-11-03 22:40:35,2
Intel,nmymeh1,Thanks again brother,pcmasterrace,2025-11-03 22:40:48,2
Intel,nmtoqp7,Yea it was used in UE 4 too in those RT titles...not that impressive and easy to plug in  Also it's just the lighting model really..you can turn off nanite in UE 5 pretty easily too  They were choices not really developmental procedures lol,pcmasterrace,2025-11-03 03:48:52,1
Intel,nmthv5b,"Helldivers 2 looking dated?  Please say sike. It’s not trying to be the most giga realistic thing out there, but it looks pretty damn good.",pcmasterrace,2025-11-03 03:02:32,-6
Intel,nn07ez7,Night and day difference for me. Thanks G,pcmasterrace,2025-11-04 04:14:38,1
Intel,nmsaupg,"On top of Covid bringing a lot of people in that were once on consoles. A lot of people aren't use to it but if I went to the Steam forums in 2016 complaining about game optimization in current games with a 460 I'd be laughed off the forums and if I came there expecting 120 fps in Metro on my 7950 I'd be told to get help...  GPU prices are high yea..I'll accept a Nvideas a monopoly post any day of the week...but to be toxic to developers because your old ass shit won't run their game is a level of low I don't care to be grouped up with.they are at a 12% profit margin and make the least, Steam makes more than them no matter how hard they try..you have one company sitting on 5 trillion dollars selling a 60 series GPU with 8 gb Vram for 300 bucks and it's the developers fault because your mind can only line up cause and effect with what's in your direct vision. If you were sitting on a 3060 ti this year you were suppose to upgrade..plain and simple.  Games are seeing almost the same average frames as they always have on flagship models. There was always games that ran better because that's what the developer chose...but bashing software pushing devs so one day mis ranged can run Nanite and Path Tracing isn't the answer..they are walking so Battlefield 6 can run. On top of it all Developers pick framerate targets YEARS before release..they have NO idea how much the next mid tier gaming GPU is going to cost or how much Vram it's going to have....they just go based on consoles and then port.",pcmasterrace,2025-11-02 22:49:23,1
Intel,nmryxsf,"You say those 50 extra GB's are apparently due to optimization required for 2 gens of consoles + PC. If it were actually optimized, anything related to console would not be included with the PC version.",pcmasterrace,2025-11-02 21:44:51,0
Intel,nms0ioe,i usually play at 6 sometimes up to 7 or 8 or down to 5,pcmasterrace,2025-11-02 21:52:58,2
Intel,nms1izr,It’s been so long since release and the game is still badly optimised 💔🥹(also ignore my flair I haven’t changed it in a loooooong time),pcmasterrace,2025-11-02 21:58:12,1
Intel,nmrxjym,"Again.. Is due to optimization. Hardware that SHOULD run 1080p games is now actually running 720p games. They're using the easy way out instead of using upscaling to give a better experience, they're using upscaling to give the bare minimum experience.",pcmasterrace,2025-11-02 21:37:52,2
Intel,nmttpvw,It looks pretty dated. I love the game but I am not in denial.,pcmasterrace,2025-11-03 04:23:55,2
Intel,nmsnubb,"I mean, if the architecture of the game is built around it it seems like it would be a little silly to have 2 completely different builds ostensibly, especially when it’s crossplay/crossplat",pcmasterrace,2025-11-03 00:00:45,1
Intel,nmsyg82,"at high difficulties is when people start getting more issues, HD2 basically destroys any CPU it comes upon.",pcmasterrace,2025-11-03 01:03:16,2
Intel,nmry5iq,"Some devs are doing that for sure, but most devs are just pushing tech forward at a time where nobody can keep up buying new hardware. I just got a new GPU to replace a 6 year old 20 series GPU, and I no longer have many complaints with how games run.",pcmasterrace,2025-11-02 21:40:50,1
Intel,nmttsyr,Can you give an example of a game it looks like?,pcmasterrace,2025-11-03 04:24:32,1
Intel,nmsqsyu,"There are common parts, yes. But it is also a 100% certain there are different builds for all platforms, it is a choice (and a bad one technically) to leave all the unnecessary stuff in.",pcmasterrace,2025-11-03 00:17:47,1
Intel,nmsknbo,This is always the case lol  People NEED to upgrade...had some dude telling me Framgen is trash in another PCMR thread..turns out he had a 6600 and is apparently happy with it...but can't help but to shit on my Borderlands 4 experience saying it runs like shit on my rig..he's telling me this when his card plays games at 45 fps from 5 years ago at 1080p  Its irritating really and its no longer productive..it's just trolling,pcmasterrace,2025-11-02 23:43:14,0
Intel,nmi90kk,i bet CES 2026,pcmasterrace,2025-11-01 09:03:53,1
Intel,nmi9j2h,Probably 2026. Intel will not announce any new products ahead of their competition. Let alone not mentioning it in their earnings call,pcmasterrace,2025-11-01 09:09:34,1
Intel,nmopqdd,Common embark W,pcmasterrace,2025-11-02 11:13:34,10
Intel,nmpkyza,"TF you mean ""even if you didn't reported them"". Are there games that refund gear only if you report?",pcmasterrace,2025-11-02 14:44:37,11
Intel,nmq3e4g,This is built into tarkov! Just ask Prapor!,pcmasterrace,2025-11-02 16:19:20,2
Intel,nmrcna2,"Can someone explain what "" refunding gear lost to cheaters "" means ?  Never played arc",pcmasterrace,2025-11-02 19:58:54,2
Intel,nmsivct,thats a pretty nice thing!,pcmasterrace,2025-11-02 23:33:26,1
Intel,nmvgcg4,"I haven't got this game but want it, how would that work if your backpack is full?",pcmasterrace,2025-11-03 13:10:02,1
Intel,nmqdxpa,But I was told kernel level anti cheat stopped cheaters,pcmasterrace,2025-11-02 17:12:07,-1
Intel,nmqib1y,sometimes ABI won't refund your gear even if you report a blatant cheater. most of the time they refund it even if you do not report them (happened to me once because I thought my killer was a legit player).,pcmasterrace,2025-11-02 17:33:45,6
Intel,nmszhxx,"I believe Delta Force is one of the those games. I could be mistaken though, but I personally haven’t received anything after encountering a wall hacker.",pcmasterrace,2025-11-03 01:09:44,3
Intel,nms8niy,"When you run a map and die, all the gear you brought in is lost.  Get killed by cheater you get that gear back.",pcmasterrace,2025-11-02 22:37:14,5
Intel,nmqlpck,"It’s strong, but there’s a reason they keep updating it?",pcmasterrace,2025-11-02 17:50:23,8
Intel,nmsfjqy,"Nothing stops cheaters. If anyone believes anything other than the perpetual cat-and-mouse game, that's on them.",pcmasterrace,2025-11-02 23:15:07,8
Intel,nmr1dn8,"Because they can't be assed to funnel money and effort into researching other options?  We have hardware-level cheats now that kernel anti-cheats outright are unable to detect. Despite the flaws in Valve's anti-cheating strategies, I'd say they're on the right path with the whole ""we trained an AI to know the difference between normal and abnormal gameplay and watch the games are they're being played"" and ""we have a non-transparent system using data like the value of your account's games and verified information with us to determine a trust value and match players of similar trust together to lessen the likelihood of average players from being matched with uncaught cheaters."" Those kinds of things are very clearly going to become more and more necessary and this shit ain't cutting it.",pcmasterrace,2025-11-02 19:04:44,5
Intel,nmhsapu,"As a kind of Hail Mary thing, you could try to clear the CMOS to try to get the system at least bootable to BIOS, and try the flash again.   There are a very few mb that include a bios recovery feature, sort of a factory reset. I can't say yours is one, but you might want to check the manufacturer support docs.",pcmasterrace,2025-11-01 05:56:34,1
Intel,nmkftwy,"I believe I tried that? Removing the CMOS battery for 15ish minutes should’ve trained it, right?   I don’t know where the pads to short are otherwise",pcmasterrace,2025-11-01 17:48:27,1
Intel,nm95zuu,verify game files,pcmasterrace,2025-10-30 20:20:01,2
Intel,nma7udg,I installed the latest driver in the nvidia app and cleared my directX cache in disk cleanup and the issues gone for me,pcmasterrace,2025-10-30 23:42:30,1
Intel,nm9jxb5,"I have this issue, verifying does not resolve.  You can alt tab back into the game and proceed to play with this error hanging in the background - played 3 games before it crashed to desktop. Just seems like a day 1 issue.",pcmasterrace,2025-10-30 21:28:59,1
Intel,nm9xitz,Having this issue as well. 4080S with a 13900k. Restart and game file verify no joy.,pcmasterrace,2025-10-30 22:44:26,1
Intel,nmc9fwq,Steam's Negative feedback is primarily about how aggressive and overpriced the storefront and in-game transactions are. So I don't see this as an actual win.,pcmasterrace,2025-10-31 09:09:55,7
Intel,nmc68co,"For now, let's check again in six months",pcmasterrace,2025-10-31 08:37:33,6
Intel,nmclr88,The whole “dont like it don’t buy it” mentality is bullshit when they shove it into our faces as much as possible. They obviously want us to like it and buy it.,pcmasterrace,2025-10-31 11:00:52,6
Intel,nmeb5au,Every advertiser in the world wants you to like and buy their shit. Does that mean you do just because they shove it in your face as much as possible?,pcmasterrace,2025-10-31 16:41:15,1
Intel,nmlw8iv,They shove it into your face because people are buying it?,pcmasterrace,2025-11-01 22:29:02,0
Intel,nmqchsg,"Yeah, as someone who doesnt buy skins I don't even notice them being shoved in my face. It's really not a big deal.  It's just these people have no self control apparently.",pcmasterrace,2025-11-02 17:05:05,1
Intel,nmfr8sl,"Maybe those kids parents should teach them better. Or stop giving them access to the credit card.  But of course, blame me for thinking people should have a modicum of willpower and not just cave at the first shiny thing that's dangled in front of them.",pcmasterrace,2025-10-31 21:10:55,2
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,160
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,81
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,45
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,15
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,5
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,4
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,5
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,4
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,4
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,You’re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asf😭,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,45
Intel,mz2vfon,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",AMD,2025-06-22 01:09:53,19
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,15
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,16
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,5
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,13
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,8
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,5
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,4
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,4
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,6
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,17
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,7
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,5
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,4
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,5
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"👍   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,19
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,16
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,6
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,38
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-7
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,7
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,6
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,80
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,10
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,23
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,15
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-13
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-15
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,6
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,6
Intel,m84dadg,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,8
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,4
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,57
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,20
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-13
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,47
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-3
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,23
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,1
Intel,lgze3vw,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-7
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-11
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,12
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,17
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,5
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,5
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,5
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-3
Intel,lfkw8g2,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,5
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,12
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,11
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,10
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,5
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,9
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,4
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,31
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,21
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,6
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,3
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,7
Intel,lf385p0,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,5
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-1
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,8
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,4
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,21
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,12
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,4
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,12
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,-1
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,3
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,9
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,226
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,24
Intel,kxiush3,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,111
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,18
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,29
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,120
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,69
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,8
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,38
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,35
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,58
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,13
Intel,kxiah6c,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",AMD,2024-04-01 05:59:50,24
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,21
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,4
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,3
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,25
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,6
Intel,kxp7mvs,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,4
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,4
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,4
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,11
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,15
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,3
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,6
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,20
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,3
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,2
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,4
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,4
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,3
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,0
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-9
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-24
Intel,kxk9iir,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-2
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-5
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,46
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,30
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,15
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,5
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,31
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,13
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,13
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-2
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,18
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,7
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,4
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,8
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,3
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,6
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,30
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,8
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,8
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-4
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,12
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,3
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-4
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,25
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,7
Intel,kxjbu8k,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,5
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,3
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,4
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,13
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,5
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,5
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,4
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-14
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,9
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,12
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,0
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-2
Intel,kyy38w2,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,32
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,3
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,5
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,7
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,43
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,28
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,4
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,15
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,3
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,7
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,8
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,7
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,10
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,1
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,1
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,2
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,2
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,10
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,22
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,19
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,3
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,6
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,5
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-11
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,0
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,28
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,31
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,1
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,4
Intel,kyhsjnw,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-2
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,9
Intel,kxj49ms,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,7
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,6
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,16
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,12
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,-1
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,13
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,10
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,6
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting 😂🤣,AMD,2024-04-01 14:23:58,2
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,8
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,9
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,6
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,8
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-1
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,3
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,8
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-2
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,2
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,3
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,2
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-7
Intel,kxih401,Oh then just ignore my comment 😅,AMD,2024-04-01 07:20:10,-1
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,8
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-11
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-14
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,4
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,2
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,7
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,12
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,3
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,27
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,6
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,6
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,37
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,11
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,23
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,16
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,16
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,6
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,12
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,6
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,15
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,7
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,4
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,7
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,0
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,17
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,2
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,6
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-10
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-7
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-3
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-5
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-6
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,4
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,6
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,20
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,8
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,8
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,9
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,8
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-1
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,7
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,2
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-2
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-1
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,4
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,5
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,12
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,5
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,1
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,2
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,3
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,kcvx2pq,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,AMD,2023-12-11 10:20:41,10
Intel,kcvsq1w,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",AMD,2023-12-11 09:20:24,14
Intel,kcvzwca,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,AMD,2023-12-11 10:58:26,2
Intel,kcyc7u2,That 7900xtx sale number is insane,AMD,2023-12-11 21:59:22,2
Intel,kcytq9l,That just shows that most people that buy GPU's don't know a thing about them.,AMD,2023-12-11 23:54:41,1
Intel,kcwedyi,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",AMD,2023-12-11 13:30:14,16
Intel,kcvzjgq,best discounts were 6750xt 6800 and 7800xt,AMD,2023-12-11 10:53:41,1
Intel,kdazjvv,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,AMD,2023-12-14 10:36:15,1
Intel,kcvv71l,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",AMD,2023-12-11 09:54:52,5
Intel,kcwe3k6,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",AMD,2023-12-11 13:27:46,5
Intel,kcw3vwl,"They're not out of stock there, duh",AMD,2023-12-11 11:47:20,6
Intel,kcyhmsr,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,AMD,2023-12-11 22:33:46,3
Intel,kd0h0lm,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-12-12 07:54:01,1
Intel,kcxlwiu,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",AMD,2023-12-11 19:18:18,3
Intel,kcxu0yw,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",AMD,2023-12-11 20:09:16,1
Intel,kcx65jb,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",AMD,2023-12-11 16:46:01,2
Intel,kcw55l4,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",AMD,2023-12-11 12:01:31,1
Intel,kcy5bwz,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",AMD,2023-12-11 21:17:17,1
Intel,keln136,the card is pretty bad if you missed that somehow,AMD,2023-12-23 12:50:02,1
Intel,kcw5qf2,AMD probably ships leftover to countries in which they know it will sell,AMD,2023-12-11 12:07:48,3
Intel,kdhrs0l,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",AMD,2023-12-15 17:44:32,1
Intel,kemomla,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",AMD,2023-12-23 17:01:05,1
Intel,kb07k5w,"As you notice the photoshop version differs, so you can't compare them really",AMD,2023-11-27 18:28:41,4
Intel,kao517l,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),AMD,2023-11-25 07:14:52,2
Intel,k74d0ev,So basically PC games are never going to tell us what the specs are to run the game native ever again.,AMD,2023-10-30 18:21:26,537
Intel,k751wfu,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",AMD,2023-10-30 20:52:50,52
Intel,k748hf1,The true crime here is needing FSR to reach these requirements.,AMD,2023-10-30 17:53:41,192
Intel,k74fig7,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",AMD,2023-10-30 18:36:48,34
Intel,k74fyp7,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),AMD,2023-10-30 18:39:33,17
Intel,k74c3qr,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,AMD,2023-10-30 18:15:55,66
Intel,k7407mp,"well, at least the chart is easy to read, not a complete mess",AMD,2023-10-30 17:03:14,19
Intel,k7466bs,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,AMD,2023-10-30 17:39:45,12
Intel,k741zrd,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,AMD,2023-10-30 17:14:11,56
Intel,k748t1r,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,AMD,2023-10-30 17:55:39,54
Intel,k76jevo,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",AMD,2023-10-31 02:51:26,6
Intel,k76we4z,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",AMD,2023-10-31 04:46:34,8
Intel,k75qe4v,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,AMD,2023-10-30 23:33:24,4
Intel,k74d5ad,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",AMD,2023-10-30 18:22:16,23
Intel,k748nsg,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",AMD,2023-10-30 17:54:45,13
Intel,k74kjmk,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",AMD,2023-10-30 19:07:25,17
Intel,k7533wp,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",AMD,2023-10-30 21:00:16,3
Intel,k77mpgx,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,AMD,2023-10-31 10:35:56,3
Intel,k74hphk,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,AMD,2023-10-30 18:50:08,12
Intel,k773lbl,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",AMD,2023-10-31 06:11:46,7
Intel,k74eavi,I hope they will bundle this game with CPUs/GPUs,AMD,2023-10-30 18:29:20,2
Intel,k74fxtg,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,AMD,2023-10-30 18:39:23,2
Intel,k77022h,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,AMD,2023-10-31 05:27:42,2
Intel,k78gl5o,This game better look like real life with those specs. I does looks beautiful!,AMD,2023-10-31 14:43:10,2
Intel,k75as90,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",AMD,2023-10-30 21:48:53,5
Intel,k748ctr,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,AMD,2023-10-30 17:52:56,4
Intel,k74mjvt,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,AMD,2023-10-30 19:19:43,2
Intel,k74oqng,I love it how absurd these things are these days.,AMD,2023-10-30 19:33:13,2
Intel,k74awed,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,AMD,2023-10-30 18:08:29,3
Intel,l06s6dc,Does anyone know if it supports SLI or crossfire?,AMD,2024-04-18 19:16:23,1
Intel,k73yt0i,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,AMD,2023-10-30 16:54:36,2
Intel,k74kcre,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",AMD,2023-10-30 19:06:17,1
Intel,k74mb9y,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,AMD,2023-10-30 19:18:13,1
Intel,k74oabx,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,AMD,2023-10-30 19:30:24,1
Intel,k74sd5w,Why in the f*ck is upscaling included on a specs page?,AMD,2023-10-30 19:55:26,1
Intel,k77tobm,no more software optimization and full upscaling  bleah,AMD,2023-10-31 11:50:04,1
Intel,k749m34,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,AMD,2023-10-30 18:00:34,-1
Intel,k74d7ll,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",AMD,2023-10-30 18:22:41,0
Intel,k74phj0,"I've never even heard of this game, nor care about it, but these system requirements offend me.",AMD,2023-10-30 19:37:49,0
Intel,k79vqgg,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,AMD,2023-10-31 20:00:03,0
Intel,k744khv,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",AMD,2023-10-30 17:29:53,-1
Intel,k79zrsa,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",AMD,2023-10-31 20:25:07,0
Intel,k7c9frd,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,AMD,2023-11-01 08:20:56,0
Intel,k75roib,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,AMD,2023-10-30 23:42:07,-2
Intel,k741y61,Nice,AMD,2023-10-30 17:13:54,-1
Intel,k765iq2,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",AMD,2023-10-31 01:14:49,0
Intel,k77fyxm,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",AMD,2023-10-31 09:08:01,0
Intel,k77zvvn,Native gaming died or what ? Wtf they turning pc gaming into console gaming,AMD,2023-10-31 12:44:58,0
Intel,k76r1ve,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",AMD,2023-10-31 03:53:47,-1
Intel,k74pme9,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,AMD,2023-10-30 19:38:39,1
Intel,k74qrim,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,AMD,2023-10-30 19:45:38,1
Intel,k74t7ty,"Omg, it would be a graphic master piece or  bad optimized thing.",AMD,2023-10-30 20:00:39,1
Intel,k74ybxr,First time I see matches recommendations for nv and amd GPUs...,AMD,2023-10-30 20:31:22,1
Intel,k756ha4,Rip laptop rtx 3060 6gb,AMD,2023-10-30 21:21:24,1
Intel,k759rkx,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",AMD,2023-10-30 21:42:14,1
Intel,k75e3m3,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,AMD,2023-10-30 22:10:21,1
Intel,k75ej2l,*NATIVE* resolution gang ftw!,AMD,2023-10-30 22:13:11,1
Intel,k75k1fl,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",AMD,2023-10-30 22:50:05,1
Intel,k75n4rl,Looks capped at 60fps?,AMD,2023-10-30 23:11:06,1
Intel,k75ops0,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",AMD,2023-10-30 23:21:57,1
Intel,k75qeie,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,AMD,2023-10-30 23:33:28,1
Intel,k760or5,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",AMD,2023-10-31 00:42:35,1
Intel,k77465m,Is it using UE5?,AMD,2023-10-31 06:19:31,1
Intel,k77bz45,"Ubisoft, rip on launch.",AMD,2023-10-31 08:09:13,1
Intel,k77htgp,guessing no DLSS3 then?,AMD,2023-10-31 09:33:36,1
Intel,k77l01d,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,AMD,2023-10-31 10:15:17,1
Intel,k77twmu,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",AMD,2023-10-31 11:52:12,1
Intel,k77x5ay,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",AMD,2023-10-31 12:21:45,1
Intel,k77z2e6,Farewell 1660ti… looks like it’s time for an upgrade,AMD,2023-10-31 12:38:12,1
Intel,k78p32u,well my 3300x is now obsolete for these new AAA games...,AMD,2023-10-31 15:37:57,1
Intel,k798boq,4k ultra right up my alley 😏,AMD,2023-10-31 17:36:35,1
Intel,k799x2x,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,AMD,2023-10-31 17:46:17,1
Intel,k7ec5fz,7900xtx will do 4k 120fps with FSR 3 then I guess?,AMD,2023-11-01 18:22:39,1
Intel,k7fkb1o,What must one do to achieve a higher rank than an enthusiast? A demi-god?,AMD,2023-11-01 22:56:10,1
Intel,k7gz4pf,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",AMD,2023-11-02 05:23:56,1
Intel,k7k2rqe,Is vrr fixed with frame gen then?,AMD,2023-11-02 20:32:54,1
Intel,k859q58,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",AMD,2023-11-07 00:16:09,1
Intel,k8kv1d8,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",AMD,2023-11-10 00:27:13,1
Intel,kaeixym,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,AMD,2023-11-23 05:18:15,1
Intel,k74h55l,They recommend upscaling even at 1080p.. disgusting ew,AMD,2023-10-30 18:46:41,217
Intel,k75hnu4,pretty much this we all knew they would start using upscaling as a crutch.,AMD,2023-10-30 22:34:04,14
Intel,k76ze7a,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,AMD,2023-10-31 05:19:49,5
Intel,k74f7tr,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",AMD,2023-10-30 18:35:00,27
Intel,k74e6u4,My thoughts too…,AMD,2023-10-30 18:28:39,5
Intel,k778snb,Thanks to all of you that were screaming dlss looks better than native lmao.,AMD,2023-10-31 07:23:19,5
Intel,k79niu3,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",AMD,2023-10-31 19:09:38,1
Intel,k77adqn,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,AMD,2023-10-31 07:46:18,2
Intel,k74x8ab,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",AMD,2023-10-30 20:24:48,-1
Intel,k74zbiw,Nope we as a community abused a nice thing,AMD,2023-10-30 20:37:19,1
Intel,k755j7t,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",AMD,2023-10-30 21:15:25,-6
Intel,k75o59y,Didn't take them long to make upscaling worthless.,AMD,2023-10-30 23:18:03,-1
Intel,k78qsdu,i smell a burgeoning cottage industry of game spec reviewers!,AMD,2023-10-31 15:48:28,0
Intel,k7hhgqi,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",AMD,2023-11-02 09:36:54,0
Intel,k7k4fzn,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",AMD,2023-11-02 20:43:01,0
Intel,k74js59,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",AMD,2023-10-30 19:02:44,-2
Intel,k78m5tc,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,AMD,2023-10-31 15:19:10,-1
Intel,k74pa1g,yeah this is the new standard,AMD,2023-10-30 19:36:32,1
Intel,k77dlbv,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",AMD,2023-10-31 08:33:14,24
Intel,k78n190,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,AMD,2023-10-31 15:24:48,1
Intel,k77nqtn,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",AMD,2023-10-31 10:47:59,-6
Intel,k74z5qq,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",AMD,2023-10-30 20:36:21,12
Intel,k76jxv9,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",AMD,2023-10-31 02:55:18,2
Intel,k76rqab,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,AMD,2023-10-31 04:00:06,2
Intel,k75qnpb,The actual true crime here is even having fsr to begin with. It should just have dlss,AMD,2023-10-30 23:35:12,-6
Intel,k76ju0a,Seems like they tried to cover every basis with these.,AMD,2023-10-31 02:54:33,7
Intel,k75ao0c,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",AMD,2023-10-30 21:48:08,9
Intel,k74htwr,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",AMD,2023-10-30 18:50:52,14
Intel,k74jsww,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,AMD,2023-10-30 19:02:51,3
Intel,k760d0a,"GCN support is over, RDNA1 is the lowest currently supported arch.",AMD,2023-10-31 00:40:29,6
Intel,k74k4hi,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",AMD,2023-10-30 19:04:51,2
Intel,k77v39i,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",AMD,2023-10-31 12:03:13,0
Intel,k74efc2,What do you mean forced raytracing?,AMD,2023-10-30 18:30:05,12
Intel,k77dids,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,AMD,2023-10-31 08:31:59,6
Intel,k74fapr,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",AMD,2023-10-30 18:35:29,16
Intel,k7gd959,It's actually 960p :(,AMD,2023-11-02 02:12:51,2
Intel,k78nhhp,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",AMD,2023-10-31 15:27:45,0
Intel,k74go33,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,AMD,2023-10-30 18:43:49,6
Intel,k78s16k,We'll see in a year.,AMD,2023-10-31 15:56:02,-1
Intel,k744q6u,AFAIK  &#x200B;  It is with RT,AMD,2023-10-30 17:30:51,22
Intel,k745n5v,Seems pretty good to me given it is at 4K with RT,AMD,2023-10-30 17:36:31,18
Intel,k743fzm,"Likely includes the RT features , its also 4k",AMD,2023-10-30 17:23:02,7
Intel,k743sfv,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",AMD,2023-10-30 17:25:06,-11
Intel,k74aacw,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",AMD,2023-10-30 18:04:43,33
Intel,k74c9q7,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",AMD,2023-10-30 18:16:57,24
Intel,k78pyma,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,AMD,2023-10-31 15:43:22,1
Intel,k771jw5,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",AMD,2023-10-31 05:46:01,1
Intel,k7aj8ac,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",AMD,2023-10-31 22:34:23,2
Intel,k77bs4b,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,AMD,2023-10-31 08:06:28,1
Intel,k75qm0a,Exactly! It even got xess so Intel users also can use xess,AMD,2023-10-30 23:34:53,2
Intel,k75k4vk,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",AMD,2023-10-30 22:50:44,3
Intel,k74wzhj,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",AMD,2023-10-30 20:23:22,5
Intel,k7708p1,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,AMD,2023-10-31 05:29:56,4
Intel,k77n6u7,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,AMD,2023-10-31 10:41:33,3
Intel,k75ya7o,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",AMD,2023-10-31 00:26:50,3
Intel,k7a5u7g,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",AMD,2023-10-31 21:03:27,0
Intel,k77o6pg,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",AMD,2023-10-31 10:53:04,1
Intel,k755rf9,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",AMD,2023-10-30 21:16:52,6
Intel,k75jput,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",AMD,2023-10-30 22:47:54,4
Intel,k761p0l,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",AMD,2023-10-31 00:49:12,1
Intel,k75i4pq,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,AMD,2023-10-30 22:37:13,2
Intel,k77cvge,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",AMD,2023-10-31 08:22:22,-1
Intel,k755zaf,"Mirage is PS4 game, Avatar is PS5",AMD,2023-10-30 21:18:15,6
Intel,k74a84y,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",AMD,2023-10-30 18:04:20,3
Intel,k75qyxk,Timed epic exclusivity? Aww man.,AMD,2023-10-30 23:37:17,3
Intel,k74vid8,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,AMD,2023-10-30 20:14:32,0
Intel,k74dkaf,You... are.. joking... right..?,AMD,2023-10-30 18:24:49,2
Intel,k770p8h,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,AMD,2023-10-31 05:35:28,3
Intel,k7ftdl4,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,AMD,2023-11-01 23:57:41,3
Intel,k74azia,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,AMD,2023-10-30 18:09:00,0
Intel,k749l36,Try ubisoft achievements :),AMD,2023-10-30 18:00:24,-1
Intel,k7a8c78,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",AMD,2023-10-31 21:19:36,1
Intel,k767klo,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",AMD,2023-10-31 01:28:47,1
Intel,k783twl,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,AMD,2023-10-31 13:15:34,0
Intel,k76sj05,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),AMD,2023-10-31 04:07:38,3
Intel,k76mhsf,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",AMD,2023-10-31 03:14:45,4
Intel,k747o6w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-30 17:48:49,1
Intel,k74w3s3,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",AMD,2023-10-30 20:18:07,0
Intel,k75if4b,yup that is why I will play 1440 UW native with a 7900XTX.,AMD,2023-10-30 22:39:08,1
Intel,k74vwla,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,AMD,2023-10-30 20:16:56,1
Intel,k75qeu1,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",AMD,2023-10-30 23:33:32,2
Intel,k75p425,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",AMD,2023-10-30 23:24:38,1
Intel,k75qigc,Very similar performance I guess,AMD,2023-10-30 23:34:13,1
Intel,k77bjls,Ye,AMD,2023-10-31 08:03:05,0
Intel,k7813wv,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,AMD,2023-10-31 12:54:43,0
Intel,k77obao,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",AMD,2023-10-31 10:54:29,1
Intel,k77o9a4,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",AMD,2023-10-31 10:53:51,1
Intel,k77udwp,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-31 11:56:37,1
Intel,k78piiq,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,AMD,2023-10-31 15:40:37,1
Intel,k7e7bz2,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,AMD,2023-11-01 17:53:25,1
Intel,k7eem4z,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,AMD,2023-11-01 18:37:35,0
Intel,k7fs4ss,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",AMD,2023-11-01 23:49:18,0
Intel,k7k3mjx,"with AFMF it works , didnt test with FSR3 now.",AMD,2023-11-02 20:38:03,1
Intel,k8l2h6h,Reading before raging :),AMD,2023-11-10 01:18:48,1
Intel,k74j51i,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",AMD,2023-10-30 18:58:47,67
Intel,k774q01,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,AMD,2023-10-31 06:26:48,3
Intel,k74rg7l,For 30fps even lol,AMD,2023-10-30 19:49:50,2
Intel,k74tk9h,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",AMD,2023-10-30 20:02:46,0
Intel,k779kpw,Or be happy your shitty video card is still supported.,AMD,2023-10-31 07:34:39,0
Intel,k78mpu0,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,AMD,2023-10-31 15:22:45,0
Intel,k77deta,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,AMD,2023-10-31 08:30:28,8
Intel,k77kqzi,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,AMD,2023-10-31 10:12:09,1
Intel,k74w3jk,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,AMD,2023-10-30 20:18:05,13
Intel,k74frkt,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,AMD,2023-10-30 18:38:21,10
Intel,k74k70d,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",AMD,2023-10-30 19:05:17,2
Intel,k7a00mh,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",AMD,2023-10-31 20:26:38,1
Intel,k74w35a,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",AMD,2023-10-30 20:18:01,0
Intel,k77ddfz,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,AMD,2023-10-31 08:29:53,7
Intel,k79hiyj,But it does in many ways.,AMD,2023-10-31 18:33:00,7
Intel,k77ktee,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",AMD,2023-10-31 10:13:00,1
Intel,k78mx3z,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",AMD,2023-10-31 15:24:03,-2
Intel,k792qlt,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,AMD,2023-10-31 17:02:03,5
Intel,k77jb2i,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",AMD,2023-10-31 09:53:38,3
Intel,k776y63,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",AMD,2023-10-31 06:57:06,11
Intel,k75f8fi,speaking facts my guy,AMD,2023-10-30 22:17:50,0
Intel,k77ezjy,You shouldn't have downvote dood wtf redditors ?,AMD,2023-10-31 08:54:03,0
Intel,k79hyhx,People with AMD cards dislike upscaling more because FSR sucks ass lol.,AMD,2023-10-31 18:35:41,4
Intel,k77ebzi,"Uhm, me btw...",AMD,2023-10-31 08:44:25,4
Intel,k77kv35,"> Who actually plays games at native these days, if it has upscaling?  I do.",AMD,2023-10-31 10:13:35,2
Intel,k75rfzg,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",AMD,2023-10-30 23:40:30,2
Intel,k7kxjkt,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",AMD,2023-11-02 23:49:47,3
Intel,k7xpols,onto absolutely nothing.,AMD,2023-11-05 15:20:55,1
Intel,k785u5t,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",AMD,2023-10-31 13:30:13,10
Intel,k75w2l1,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,AMD,2023-10-31 00:12:04,6
Intel,k76yitl,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,AMD,2023-10-31 05:09:41,1
Intel,k77agw9,The game is raytracing only with a ridiculous ammount of foooliage.,AMD,2023-10-31 07:47:35,6
Intel,k77h1xr,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",AMD,2023-10-31 09:23:11,5
Intel,k78n7c0,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,AMD,2023-10-31 15:25:54,3
Intel,k74jbjo,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,AMD,2023-10-30 18:59:54,3
Intel,k77uxf6,False.  guy blocked me lmao,AMD,2023-10-31 12:01:42,1
Intel,k770n5h,"I thought that was on Linux, though I might be wrong",AMD,2023-10-31 05:34:47,0
Intel,k75wi30,1070 doesn't have hardware RT though.,AMD,2023-10-31 00:14:59,4
Intel,k74g227,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),AMD,2023-10-30 18:40:06,23
Intel,k78amg9,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,AMD,2023-10-31 14:03:39,0
Intel,k74qlaz,Completely agree.,AMD,2023-10-30 19:44:35,3
Intel,k7hr3n6,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,AMD,2023-11-02 11:28:02,1
Intel,k7476pu,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),AMD,2023-10-30 17:45:53,3
Intel,k75aybf,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",AMD,2023-10-30 21:50:00,7
Intel,k74gjdn,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",AMD,2023-10-30 18:43:00,13
Intel,k74gjiu,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,AMD,2023-10-30 18:43:01,1
Intel,k74d1es,"Derp, derp.",AMD,2023-10-30 18:21:37,0
Intel,k7gqdq3,Sounds like John on Direct Foundry Direct every week.,AMD,2023-11-02 03:55:28,1
Intel,k7721zf,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",AMD,2023-10-31 05:52:19,3
Intel,k77id8y,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",AMD,2023-10-31 09:41:01,5
Intel,k75quo4,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,AMD,2023-10-30 23:36:30,3
Intel,k75wl3m,Avatar is RT only. There is no non RT mode.,AMD,2023-10-31 00:15:31,9
Intel,k77ar4b,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,AMD,2023-10-31 07:51:38,4
Intel,k76ly3i,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",AMD,2023-10-31 03:10:28,2
Intel,k78qssi,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",AMD,2023-10-31 15:48:32,3
Intel,k75m1nd,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,AMD,2023-10-30 23:03:44,7
Intel,k75t1dz,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,AMD,2023-10-30 23:51:23,1
Intel,k74c5po,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",AMD,2023-10-30 18:16:15,12
Intel,k74ejc7,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",AMD,2023-10-30 18:30:46,3
Intel,k762gt6,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,AMD,2023-10-31 00:54:19,0
Intel,k7795r7,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,AMD,2023-10-31 07:28:35,6
Intel,k8xdnpw,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,AMD,2023-11-12 13:53:39,1
Intel,k77azgf,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",AMD,2023-10-31 07:54:57,0
Intel,k7ad42q,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",AMD,2023-10-31 21:51:09,-1
Intel,k771qsz,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",AMD,2023-10-31 05:48:28,1
Intel,k7baowx,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",AMD,2023-11-01 01:58:28,1
Intel,k76iowi,This is what I am thinking too.,AMD,2023-10-31 02:46:08,1
Intel,k7al3b9,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",AMD,2023-10-31 22:48:01,1
Intel,k78r7ik,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",AMD,2023-10-31 15:51:02,0
Intel,k7eeuc1,Let's hope so. 30fps is far from recommended for FSR 3,AMD,2023-11-01 18:38:59,1
Intel,k7kbe2u,Yeah I heard it works with that hoping it works with fsr3 now,AMD,2023-11-02 21:25:15,1
Intel,k74jv5m,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",AMD,2023-10-30 19:03:14,42
Intel,k770h31,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",AMD,2023-10-31 05:32:44,0
Intel,k74va9v,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",AMD,2023-10-30 20:13:10,-9
Intel,k78mlfi,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,AMD,2023-10-31 15:21:57,5
Intel,k776zix,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,AMD,2023-10-31 06:57:37,4
Intel,k77ebsi,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",AMD,2023-10-31 08:44:20,-1
Intel,k82kpsv,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",AMD,2023-11-06 14:19:02,1
Intel,k792d5j,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",AMD,2023-10-31 16:59:46,2
Intel,k77dtn2,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",AMD,2023-10-31 08:36:45,3
Intel,k77togb,Assassins creed origins and odyssey side quests/collectibles oh my god,AMD,2023-10-31 11:50:06,2
Intel,k74jzj8,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,AMD,2023-10-30 19:04:00,0
Intel,k74m5ig,I appreciate your insights and opinions. Thank you.,AMD,2023-10-30 19:17:14,-1
Intel,k775lx7,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",AMD,2023-10-31 06:38:51,1
Intel,k77sn6f,console games started upscaling way before PCs .,AMD,2023-10-31 11:40:07,4
Intel,k79hu3v,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,AMD,2023-10-31 18:34:55,4
Intel,k79vmwl,you forgot who owns one of the most popular engines out there?,AMD,2023-10-31 19:59:27,-1
Intel,k7abo7k,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",AMD,2023-10-31 21:41:33,-5
Intel,k77b8ol,downvoted by devs lol,AMD,2023-10-31 07:58:40,5
Intel,k79ib8i,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,AMD,2023-10-31 18:37:52,0
Intel,k78h0ux,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",AMD,2023-10-31 14:46:05,-1
Intel,k792yre,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,AMD,2023-10-31 17:03:29,0
Intel,k79ll9w,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,AMD,2023-10-31 18:57:46,2
Intel,k78xgtm,Why is there a dialog message about unsupported hardware when you try and run a 390X?,AMD,2023-10-31 16:29:37,3
Intel,k77blcl,It can do software based RT just like every other modern GPU out there.,AMD,2023-10-31 08:03:49,2
Intel,k74gdb3,Well then no wonder rx 5700 can't manage 30 fps lol,AMD,2023-10-30 18:41:59,28
Intel,k74m0rw,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",AMD,2023-10-30 19:16:25,9
Intel,k74ihd8,Avatars uses a Lumen like software RT solution.,AMD,2023-10-30 18:54:49,19
Intel,k78qd8n,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",AMD,2023-10-31 15:45:52,2
Intel,k74lfw8,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",AMD,2023-10-30 19:12:53,13
Intel,k75797r,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),AMD,2023-10-30 21:26:16,2
Intel,k74khy7,"Yeah, thatsl happened.",AMD,2023-10-30 19:07:09,0
Intel,k7a6im2,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",AMD,2023-10-31 21:07:50,1
Intel,k75wfsk,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",AMD,2023-10-31 00:14:32,1
Intel,k77chzy,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",AMD,2023-10-31 08:16:52,2
Intel,k77pinh,You didn't understand. It's another Swiss knife engine,AMD,2023-10-31 11:07:56,0
Intel,k76pny2,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",AMD,2023-10-31 03:41:22,1
Intel,k75nvkv,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,AMD,2023-10-30 23:16:13,-1
Intel,k77ap82,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,AMD,2023-10-31 07:50:56,-2
Intel,k78n2dv,The PS5 has a 6700.,AMD,2023-10-31 15:25:00,1
Intel,k7i9v2e,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",AMD,2023-11-02 13:58:02,1
Intel,k74jbph,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",AMD,2023-10-30 18:59:55,1
Intel,k8xdwme,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,AMD,2023-11-12 13:55:46,1
Intel,k77fln1,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,AMD,2023-10-31 09:02:53,0
Intel,k7ahuzb,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,AMD,2023-10-31 22:24:31,1
Intel,k78sexp,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,AMD,2023-10-31 15:58:21,1
Intel,k74o0rt,TAA,AMD,2023-10-30 19:28:44,52
Intel,k74rhkm,Temporal anti aliasing.,AMD,2023-10-30 19:50:04,8
Intel,k78mepd,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",AMD,2023-10-31 15:20:46,3
Intel,k78zoea,"Ahh, welcome to r/FuckTAA",AMD,2023-10-31 16:43:15,1
Intel,k7fo2qt,Even 4K looks blurry with some implementations of TAA,AMD,2023-11-01 23:21:50,1
Intel,k775ulz,they're giving you the bare minimum until your upgrade!,AMD,2023-10-31 06:42:13,0
Intel,k74lgqm,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,AMD,2023-10-30 19:13:01,-5
Intel,k7551yk,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",AMD,2023-10-30 21:12:24,11
Intel,k74xjh2,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",AMD,2023-10-30 20:26:40,13
Intel,k7549zz,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",AMD,2023-10-30 21:07:34,3
Intel,k78jvij,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",AMD,2023-10-31 15:04:19,6
Intel,k79h6tb,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,AMD,2023-10-31 18:30:53,1
Intel,k77gqlw,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",AMD,2023-10-31 09:18:51,2
Intel,k78mn3v,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",AMD,2023-10-31 15:22:16,4
Intel,k7kwbcl,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,AMD,2023-11-02 23:41:35,1
Intel,k783kme,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",AMD,2023-10-31 13:13:39,-1
Intel,k7adm22,No I havent. AMD is bigger than Epic.,AMD,2023-10-31 21:54:30,2
Intel,k7aqfmz,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",AMD,2023-10-31 23:27:21,5
Intel,k77exua,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",AMD,2023-10-31 08:53:22,0
Intel,k79j5la,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",AMD,2023-10-31 18:43:01,6
Intel,k78wlj1,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,AMD,2023-10-31 16:24:13,5
Intel,k79ipwc,It's software ray tracing which isn't accelerated by hardware.,AMD,2023-10-31 18:40:20,2
Intel,k79sb21,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,AMD,2023-10-31 19:39:06,1
Intel,k79a73q,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,AMD,2023-10-31 17:47:59,2
Intel,k74o26m,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,AMD,2023-10-30 19:28:58,20
Intel,k778s32,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",AMD,2023-10-31 07:23:06,1
Intel,k75bea7,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,AMD,2023-10-30 21:52:53,5
Intel,k75j5le,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,AMD,2023-10-30 22:44:05,7
Intel,k74nnyo,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",AMD,2023-10-30 19:26:34,4
Intel,k7a88v5,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",AMD,2023-10-31 21:19:00,1
Intel,k77pvj3,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,AMD,2023-10-31 11:11:45,3
Intel,k78lq5t,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",AMD,2023-10-31 15:16:17,3
Intel,k76zwtm,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,AMD,2023-10-31 05:25:55,2
Intel,k77ddap,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",AMD,2023-10-31 08:29:49,7
Intel,k77dhd7,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",AMD,2023-10-31 08:31:33,5
Intel,k77h10q,Both excellent 👌 Down the Rabbit Hole was another solid one.,AMD,2023-10-31 09:22:50,1
Intel,k7b776f,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",AMD,2023-11-01 01:31:29,1
Intel,k78ut1x,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",AMD,2023-10-31 16:13:03,0
Intel,k74xlfo,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,AMD,2023-10-30 20:27:00,26
Intel,k74uq1m,I have to turn it off in borderlands 3. Ugh.,AMD,2023-10-30 20:09:47,2
Intel,k78zmsb,r/FuckTAA,AMD,2023-10-31 16:42:58,0
Intel,k7n69qz,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",AMD,2023-11-03 12:46:56,1
Intel,k7gyunv,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",AMD,2023-11-02 05:20:45,1
Intel,k7bw0wl,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,AMD,2023-11-01 05:13:29,-1
Intel,k78p5rj,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",AMD,2023-10-31 15:38:25,2
Intel,k7kx549,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,AMD,2023-11-02 23:47:05,1
Intel,k7c5sa3,epic is tencent...,AMD,2023-11-01 07:26:14,-1
Intel,k79evb5,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",AMD,2023-10-31 18:16:40,0
Intel,k79vqds,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",AMD,2023-10-31 20:00:02,1
Intel,k79dbl9,You don't need RT hardware to do software RT. That's what I'm saying.,AMD,2023-10-31 18:07:06,3
Intel,k74wirt,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",AMD,2023-10-30 20:20:36,19
Intel,k755cwh,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",AMD,2023-10-30 21:14:19,18
Intel,k74ogkv,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),AMD,2023-10-30 19:31:29,15
Intel,k751p3z,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,AMD,2023-10-30 20:51:38,10
Intel,k75za9c,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",AMD,2023-10-31 00:33:28,9
Intel,k76k5hm,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,AMD,2023-10-31 02:56:53,3
Intel,k77u7zx,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,AMD,2023-10-31 11:55:06,2
Intel,k77anl2,Yes hah,AMD,2023-10-31 07:50:17,1
Intel,k78kxsq,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",AMD,2023-10-31 15:11:02,1
Intel,k75bm3k,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",AMD,2023-10-30 21:54:17,1
Intel,k75jy4k,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,AMD,2023-10-30 22:49:28,7
Intel,k75cf9l,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",AMD,2023-10-30 21:59:30,10
Intel,k74s4b6,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",AMD,2023-10-30 19:53:57,4
Intel,k78qh5r,Oh man the hair... its so crazy how much upscaling kills the hair..,AMD,2023-10-31 15:46:32,0
Intel,k74vpzd,i mean they already arent the smartest bulbs considering they went team green.,AMD,2023-10-30 20:15:50,-8
Intel,k7bwwni,Reading comrehension dude.,AMD,2023-11-01 05:24:02,1
Intel,k77rq8u,"I do, but thanks for your interest.",AMD,2023-10-31 11:31:01,0
Intel,k781oqh,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,AMD,2023-10-31 12:59:12,1
Intel,k77gylo,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",AMD,2023-10-31 09:21:55,0
Intel,k77prws,Nice. It’s on the list. Thanks man,AMD,2023-10-31 11:10:40,0
Intel,k79111l,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,AMD,2023-10-31 16:51:38,1
Intel,k74y62b,Probably because TAA is (unfortunately) more prevalent than it ever was.,AMD,2023-10-30 20:30:23,12
Intel,k77crxd,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",AMD,2023-10-31 08:20:57,4
Intel,k7ky7yi,Oh my bad if I read that wrong,AMD,2023-11-02 23:54:13,1
Intel,k79nw3z,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,AMD,2023-10-31 19:11:53,1
Intel,k74xr8d,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,AMD,2023-10-30 20:27:57,-9
Intel,k78nlj0,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",AMD,2023-10-31 15:28:29,1
Intel,k78nx4p,imagine being mad that 4 year old cards arent high end,AMD,2023-10-31 15:30:32,2
Intel,k78qsw0,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,AMD,2023-10-31 15:48:33,0
Intel,k75i9u4,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",AMD,2023-10-30 22:38:10,1
Intel,k77gx0m,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",AMD,2023-10-31 09:21:18,1
Intel,k74smha,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",AMD,2023-10-30 19:57:03,0
Intel,k78qwba,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",AMD,2023-10-31 15:49:08,0
Intel,k74wqog,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",AMD,2023-10-30 20:21:55,2
Intel,k7bxbe9,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",AMD,2023-11-01 05:29:04,1
Intel,k77y4x2,"It's for textures, not object edges from FSR use, lol. Two completely different things",AMD,2023-10-31 12:30:16,4
Intel,k793lf6,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",AMD,2023-10-31 17:07:23,-1
Intel,k77g0i6,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",AMD,2023-10-31 09:08:37,11
Intel,k77ubrs,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",AMD,2023-10-31 11:56:04,-1
Intel,k8f5yw4,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",AMD,2023-11-08 22:27:09,1
Intel,k79oe6d,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,AMD,2023-10-31 19:14:56,1
Intel,k753gel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",AMD,2023-10-30 21:02:23,11
Intel,k78lli0,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,AMD,2023-10-31 15:15:24,4
Intel,k7au2y4,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",AMD,2023-10-31 23:54:23,1
Intel,k75wc3v,5700 was probably the lowest AMD card they had to test with.,AMD,2023-10-31 00:13:51,1
Intel,k74tcrl,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",AMD,2023-10-30 20:01:29,5
Intel,k7byrit,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,AMD,2023-11-01 05:47:35,1
Intel,k7a8jw6,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,AMD,2023-10-31 21:20:59,1
Intel,k7kn3dl,AMD CAS is aimed to restore detail,AMD,2023-11-02 22:40:21,1
Intel,k7k5dha,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",AMD,2023-11-02 20:48:40,1
Intel,k7cl7iw,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,AMD,2023-11-01 10:59:12,2
Intel,k74ugbm,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",AMD,2023-10-30 20:08:12,0
Intel,k7bzcr6,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",AMD,2023-11-01 05:55:17,1
Intel,k7cntlm,Yeah but a 3060ti didn't,AMD,2023-11-01 11:27:22,1
Intel,k78mcrt,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",AMD,2023-10-31 15:20:25,2
Intel,k74zgnn,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",AMD,2023-10-30 20:38:09,2
Intel,k1rzmke,Confirmed Can it run CP2077 4k is the new Can it run Crysis,AMD,2023-09-22 22:22:20,586
Intel,k1s9f6d,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,AMD,2023-09-22 23:31:10,585
Intel,k1rtgmz,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,AMD,2023-09-22 21:41:53,306
Intel,k1tezss,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",AMD,2023-09-23 05:11:42,24
Intel,k1spmgk,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,AMD,2023-09-23 01:30:07,19
Intel,k1ryede,3080 falling behind a 3060? what is this data?,AMD,2023-09-22 22:14:11,124
Intel,k1say8p,wake me up when we have a card that can run this at 40 without needing its own psu.,AMD,2023-09-22 23:42:17,46
Intel,k1seqym,5090 here I come!,AMD,2023-09-23 00:09:51,26
Intel,k1sf8id,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",AMD,2023-09-23 00:13:23,40
Intel,k1samn8,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",AMD,2023-09-22 23:39:57,29
Intel,k1se7w7,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,AMD,2023-09-23 00:06:00,28
Intel,k1tcwd4,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",AMD,2023-09-23 04:49:38,6
Intel,k1swxw6,It's a good thing nobody has to actually play it native.,AMD,2023-09-23 02:26:41,18
Intel,k1sd0w5,Well good thing literally nobody is doing that…,AMD,2023-09-22 23:57:14,9
Intel,k1tv5m1,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,AMD,2023-09-23 08:24:44,9
Intel,k1svrss,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,AMD,2023-09-23 02:17:32,16
Intel,k1u5n8e,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,AMD,2023-09-23 10:40:09,4
Intel,k1t06iw,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",AMD,2023-09-23 02:52:43,10
Intel,k1sta67,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,AMD,2023-09-23 01:58:23,15
Intel,k1ru0dy,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",AMD,2023-09-22 21:45:23,24
Intel,k1sh29m,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",AMD,2023-09-23 00:26:40,19
Intel,k1rwvmw,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,AMD,2023-09-22 22:04:05,22
Intel,k1s5ads,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,AMD,2023-09-22 23:01:20,8
Intel,k1t5pl6,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",AMD,2023-09-23 03:40:25,7
Intel,k1sde74,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",AMD,2023-09-22 23:59:57,16
Intel,k1sbtem,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",AMD,2023-09-22 23:48:27,5
Intel,k1sr9s2,This doesn’t seem right…. 3080 performing worse than a 2080TI?,AMD,2023-09-23 01:42:55,2
Intel,k1t2kii,How tf is a 2080 ti getting more fps than a 3080?!?,AMD,2023-09-23 03:12:42,2
Intel,k1t66da,I'm not seeing the RTX A6000 on here...,AMD,2023-09-23 03:44:41,2
Intel,k1tglq7,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,AMD,2023-09-23 05:29:29,2
Intel,k1u9v0r,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",AMD,2023-09-23 11:28:08,2
Intel,k1ufe40,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",AMD,2023-09-23 12:22:22,2
Intel,k1witvs,Does anyone actually play this game? Its more of a meme game imho,AMD,2023-09-23 20:46:05,2
Intel,k1xd78i,4.3 fps 😂😂😂,AMD,2023-09-24 00:13:50,2
Intel,k1sczh9,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",AMD,2023-09-22 23:56:57,10
Intel,k1rumnu,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",AMD,2023-09-22 21:49:26,11
Intel,k1shlno,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,AMD,2023-09-23 00:30:36,4
Intel,k1t2byj,Lol the 3060ti is better than a 7900xtx...crazy,AMD,2023-09-23 03:10:40,3
Intel,k1vuadd,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,AMD,2023-09-23 18:12:06,3
Intel,k1s8b2u,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,AMD,2023-09-22 23:23:07,10
Intel,k1t61f2,ThE gAMe iS PoOrLY OpTImiSed!!!,AMD,2023-09-23 03:43:26,4
Intel,k1ttlda,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,AMD,2023-09-23 08:04:31,3
Intel,k1sgjyj,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",AMD,2023-09-23 00:22:53,2
Intel,k1sxb06,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",AMD,2023-09-23 02:29:34,2
Intel,k1sydqu,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",AMD,2023-09-23 02:38:06,2
Intel,k1tbdbm,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,AMD,2023-09-23 04:34:09,2
Intel,k1twz2u,"""4090 is 4 times faster than 7900XTX""",AMD,2023-09-23 08:48:39,2
Intel,k1uouoc,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",AMD,2023-09-23 13:40:46,2
Intel,k1wadup,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,AMD,2023-09-23 19:52:54,2
Intel,k1tl3qo,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),AMD,2023-09-23 06:20:29,2
Intel,k1s8g0f,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",AMD,2023-09-22 23:24:08,1
Intel,k1s8o3g,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",AMD,2023-09-22 23:25:45,1
Intel,k1sg5kw,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,AMD,2023-09-23 00:20:00,2
Intel,k1skakr,Native resolution is a thing of the past. DLSS looks better than native anyway.,AMD,2023-09-23 00:50:07,2
Intel,k1txff8,Is this with DLSS + FG?,AMD,2023-09-23 08:54:33,1
Intel,k1smbmr,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",AMD,2023-09-23 01:05:06,-2
Intel,k1s3u4i,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",AMD,2023-09-22 22:51:01,2
Intel,k1t2qkw,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,AMD,2023-09-23 03:14:12,0
Intel,k1sgzlr,No one is using these settings.,AMD,2023-09-23 00:26:07,1
Intel,k1spqtc,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",AMD,2023-09-23 01:31:01,0
Intel,k1shcs4,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",AMD,2023-09-23 00:28:49,1
Intel,k1sj5mo,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",AMD,2023-09-23 00:41:56,1
Intel,k1ulwx6,cyberpunk sucks don't worry about it,AMD,2023-09-23 13:17:47,1
Intel,k1rvukw,3080ti?,AMD,2023-09-22 21:57:22,1
Intel,k1sljwz,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",AMD,2023-09-23 00:59:20,1
Intel,k1sqr9g,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,AMD,2023-09-23 01:38:55,1
Intel,k1tt1b6,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",AMD,2023-09-23 07:57:19,1
Intel,k1tuc3v,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",AMD,2023-09-23 08:14:11,1
Intel,k1uq4ce,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,AMD,2023-09-23 13:50:25,1
Intel,k1s9ert,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),AMD,2023-09-22 23:31:05,-2
Intel,k1rzvbb,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",AMD,2023-09-22 22:23:58,-17
Intel,k1sfn3w,What body part do you think will get me a 5090?,AMD,2023-09-23 00:16:18,0
Intel,k1txo0m,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",AMD,2023-09-23 08:57:42,0
Intel,k1u7ngf,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",AMD,2023-09-23 11:04:00,0
Intel,k1uxont,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",AMD,2023-09-23 14:44:13,0
Intel,k1s5mej,That not even proper path tracing.,AMD,2023-09-22 23:03:45,-2
Intel,k1s6qt1,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,AMD,2023-09-22 23:11:53,-5
Intel,k1sb5wk,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",AMD,2023-09-22 23:43:51,-8
Intel,k1svwfa,Native is a thing of the past when dlss produces better looking image,AMD,2023-09-23 02:18:33,-3
Intel,k1sdfih,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,AMD,2023-09-23 00:00:13,-3
Intel,k1s7xzc,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,AMD,2023-09-22 23:20:30,-10
Intel,k1slr2s,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,AMD,2023-09-23 01:00:47,-1
Intel,k1sfvki,"Rather than having 2-3fps, I would go and see a video of path tracing.",AMD,2023-09-23 00:17:59,0
Intel,k1sihxn,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",AMD,2023-09-23 00:37:09,0
Intel,k1su9sh,cool idc i wont play that shit at such shit settings,AMD,2023-09-23 02:05:55,0
Intel,k1syk9g,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,AMD,2023-09-23 02:39:33,0
Intel,k1tpxos,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,AMD,2023-09-23 07:18:17,0
Intel,k1tvoqm,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",AMD,2023-09-23 08:31:41,0
Intel,k1tytwk,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,AMD,2023-09-23 09:12:53,0
Intel,k1tzugl,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,AMD,2023-09-23 09:26:15,0
Intel,k1uhusa,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",AMD,2023-09-23 12:44:21,0
Intel,k1v2ghf,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,AMD,2023-09-23 15:16:08,0
Intel,k1v8o7m,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",AMD,2023-09-23 15:55:59,0
Intel,k1vfxae,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,AMD,2023-09-23 16:41:57,0
Intel,k1vjs6i,"Looks like 4K is mainly just for videos, not gaming for the time being.",AMD,2023-09-23 17:06:09,0
Intel,k1vnxlu,How can you bring out such technology and no hardware can process it properly.  an impudence,AMD,2023-09-23 17:32:09,0
Intel,k1wm3gk,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,AMD,2023-09-23 21:06:13,0
Intel,k1ysyyz,You could just you know... disable path tracing and get 70 fps or go below 4k,AMD,2023-09-24 08:12:13,0
Intel,k1z173o,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",AMD,2023-09-24 09:58:57,0
Intel,k25idkz,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",AMD,2023-09-25 16:06:18,0
Intel,k1rv0mr,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,AMD,2023-09-22 21:51:56,-33
Intel,k1sbmt5,"so it is settled, Devs have zero optimization in games.",AMD,2023-09-22 23:47:08,-13
Intel,k1t5ilx,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",AMD,2023-09-23 03:38:40,-2
Intel,k1t8qzc,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",AMD,2023-09-23 04:08:39,-3
Intel,k1syqs1,Once again proving RT is useless,AMD,2023-09-23 02:41:00,-6
Intel,k1sfuie,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,AMD,2023-09-23 00:17:45,325
Intel,k1sw4a8,"""only gamers get this joke""",AMD,2023-09-23 02:20:15,0
Intel,k1tqj3o,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",AMD,2023-09-23 07:25:29,-4
Intel,k1uey5o,Starfield is peaking around the corner.,AMD,2023-09-23 12:18:19,0
Intel,k1ume3r,starfield as well,AMD,2023-09-23 13:21:33,0
Intel,k1smpf3,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,AMD,2023-09-23 01:07:57,412
Intel,k1sm1vi,The new Crysis,AMD,2023-09-23 01:03:02,18
Intel,k1t68ad,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",AMD,2023-09-23 03:45:09,40
Intel,k1tduwg,How long until a $200 card can do that?,AMD,2023-09-23 04:59:33,9
Intel,k1tl6bz,Most improvement is probably going to AI software more than hardware in the next few years.,AMD,2023-09-23 06:21:18,12
Intel,k1sui5w,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",AMD,2023-09-23 02:07:40,34
Intel,k1tggmb,GPU need to have its own garage by then,AMD,2023-09-23 05:27:53,5
Intel,k1sjd2a,Yep! Insane how fast things change.,AMD,2023-09-23 00:43:26,16
Intel,k1th7kz,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",AMD,2023-09-23 05:36:13,26
Intel,k1tksc7,Yeah rtx 12060 with 9.5 gb Vram will be a monster,AMD,2023-09-23 06:16:50,12
Intel,k1toc97,I'm willing to bet hardware improvement will come to a halt before that.,AMD,2023-09-23 06:59:10,5
Intel,k1tp7w0,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",AMD,2023-09-23 07:09:42,8
Intel,k1u5t5c,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,AMD,2023-09-23 10:42:10,2
Intel,k1u9trt,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,AMD,2023-09-23 11:27:46,2
Intel,k1umihs,But then the current gen games of that era will run like this. The cycle continues,AMD,2023-09-23 13:22:31,2
Intel,k1vcazz,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,AMD,2023-09-23 16:19:04,2
Intel,k1t8b6k,Who knows what new tech will be out in even 4 years lol,AMD,2023-09-23 04:04:25,2
Intel,k1tl2z8,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",AMD,2023-09-23 06:20:14,1
Intel,k1u0yj3,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",AMD,2023-09-23 09:40:52,1
Intel,k1u6uez,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",AMD,2023-09-23 10:54:31,1
Intel,k1sby3m,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,AMD,2023-09-22 23:49:24,312
Intel,k1rw4fg,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",AMD,2023-09-22 21:59:09,101
Intel,k1sbwfh,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",AMD,2023-09-22 23:49:03,11
Intel,k1t5qx0,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",AMD,2023-09-23 03:40:46,-4
Intel,k1tf1z9,I think that most of the progress will go together with software tricks and upscalers.,AMD,2023-09-23 05:12:21,4
Intel,k1s07rb,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",AMD,2023-09-22 22:26:17,127
Intel,k1rz345,That's VRAM for you,AMD,2023-09-22 22:18:45,130
Intel,k1tc7vb,we are counting decimals of fps its just all margin of error.,AMD,2023-09-23 04:42:40,13
Intel,k1s03of,If you buy a card with low ram that card is for right now only lol,AMD,2023-09-22 22:25:32,3
Intel,k1v5pv8,Wake me up when a $250 GPU can run this at 1080p.,AMD,2023-09-23 15:37:12,8
Intel,k1uiun2,Well DLSS isn't best. DLAA is,AMD,2023-09-23 12:52:43,6
Intel,k1svcks,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,AMD,2023-09-23 02:14:15,25
Intel,k1si3ng,Can’t you just disable TAA? Or do you just have to live with the ghosting?,AMD,2023-09-23 00:34:15,5
Intel,k1v2dce,TAA is garbage in everything. TAA and FSR can both get fucked,AMD,2023-09-23 15:15:33,3
Intel,k1svesl,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",AMD,2023-09-23 02:14:43,6
Intel,k1v301u,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",AMD,2023-09-23 15:19:41,2
Intel,k1v2lmw,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",AMD,2023-09-23 15:17:04,2
Intel,k1sv2p6,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",AMD,2023-09-23 02:12:07,7
Intel,k1vdbm6,Imagine buying a 4090 and then using upscaling.,AMD,2023-09-23 16:25:29,-6
Intel,k1scblp,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,AMD,2023-09-22 23:52:08,17
Intel,k1sfzkt,PC gaming is in Crysis.,AMD,2023-09-23 00:18:47,5
Intel,k1y7rss,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,AMD,2023-09-24 04:15:05,-1
Intel,k1t8lmy,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,AMD,2023-09-23 04:07:13,2
Intel,k1rxtnn,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",AMD,2023-09-22 22:10:20,8
Intel,k1u60lu,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",AMD,2023-09-23 10:44:39,14
Intel,k1sizpa,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",AMD,2023-09-23 00:40:43,-7
Intel,k1u5vqx,Yeh because native 4k looks worse than dlss + RR 4k,AMD,2023-09-23 10:43:02,9
Intel,k1xi8dd,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",AMD,2023-09-24 00:50:15,2
Intel,k1u5mhq,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,AMD,2023-09-23 10:39:55,11
Intel,k1sshhs,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",AMD,2023-09-23 01:52:13,9
Intel,k1sr5lk,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",AMD,2023-09-23 01:42:00,2
Intel,k1t41gi,VRAM,AMD,2023-09-23 03:25:30,3
Intel,k1uiuuv,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",AMD,2023-09-23 12:52:46,1
Intel,k1zcvhv,still better than 90% of games in 2023,AMD,2023-09-24 12:08:19,0
Intel,k1rvtrp,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,AMD,2023-09-22 21:57:13,19
Intel,k1s0kpr,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",AMD,2023-09-22 22:28:43,-1
Intel,k1sbnjt,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",AMD,2023-09-22 23:47:17,-7
Intel,k1u6o55,All graphics you see on your computer screen is fake,AMD,2023-09-23 10:52:26,3
Intel,k1t62rv,Path tracing is more demanding than Ray tracing,AMD,2023-09-23 03:43:46,5
Intel,k1u17zi,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",AMD,2023-09-23 09:44:22,3
Intel,k1s9p4d,I guess between the 3090 and the 4070,AMD,2023-09-22 23:33:11,0
Intel,k1sjydp,Yep hahaha,AMD,2023-09-23 00:47:40,2
Intel,k1v42p3,RemindMe! 7 years,AMD,2023-09-23 15:26:41,2
Intel,k1s3vi6,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",AMD,2023-09-22 22:51:16,18
Intel,k1s45n5,"This is an absurd, fanboyish thought lmao",AMD,2023-09-22 22:53:15,14
Intel,k1u3kdb,dont let novidia marketing see this youll get down voted into oblivion,AMD,2023-09-23 10:14:19,2
Intel,k1s2le1,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,AMD,2023-09-22 22:42:21,-8
Intel,k1sylo6,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,AMD,2023-09-23 02:39:51,3
Intel,k1sltal,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",AMD,2023-09-23 01:01:15,2
Intel,k1u6rsu,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,AMD,2023-09-23 10:53:38,2
Intel,k1t37ky,Fanboyism of both kinds is bad,AMD,2023-09-23 03:18:18,1
Intel,k1sbir3,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",AMD,2023-09-22 23:46:20,-1
Intel,k1u85m7,Since it needs more than 10gb vram,AMD,2023-09-23 11:09:41,3
Intel,k1tq0gr,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,AMD,2023-09-23 07:19:12,5
Intel,k1rw9dl,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,AMD,2023-09-22 22:00:02,29
Intel,k1sl1fh,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",AMD,2023-09-23 00:55:34,7
Intel,k1rx0ka,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",AMD,2023-09-22 22:04:58,-14
Intel,k1u2f5k,It's settled that you have no idea what you talking about,AMD,2023-09-23 09:59:44,4
Intel,k1sczqa,Its full path tracing u cannot really optimize this much.,AMD,2023-09-22 23:57:00,10
Intel,k1tpj68,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",AMD,2023-09-23 07:13:34,3
Intel,k1s9h3d,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",AMD,2023-09-22 23:31:33,3
Intel,k1sprwd,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,AMD,2023-09-23 01:31:15,143
Intel,k1tds1v,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",AMD,2023-09-23 04:58:42,28
Intel,k1u7ebf,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,AMD,2023-09-23 11:01:03,18
Intel,k1vo73s,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",AMD,2023-09-23 17:33:46,5
Intel,k1u7ms4,Amd had a tessellation unit. It went unused but it was present,AMD,2023-09-23 11:03:47,2
Intel,k1srvji,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",AMD,2023-09-23 01:47:34,166
Intel,k1sqn2i,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",AMD,2023-09-23 01:38:00,33
Intel,k1tzlza,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",AMD,2023-09-23 09:23:04,5
Intel,k1uatm9,hobbies quaint consist aromatic political hat aback boat relieved crush   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 11:38:16,5
Intel,k1u8wkg,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",AMD,2023-09-23 11:17:47,16
Intel,k1tdixd,"Oh damn, I forgot about those cards.  I wanted one so badly.",AMD,2023-09-23 04:56:03,3
Intel,k1u3mme,Remember when companies tried to sell physics cards lol,AMD,2023-09-23 10:15:05,3
Intel,k1thwg4,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,AMD,2023-09-23 05:43:55,15
Intel,k1tjl5e,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",AMD,2023-09-23 06:03:08,2
Intel,k1umriw,I remember when people had a dedicated PhysX card.,AMD,2023-09-23 13:24:29,2
Intel,k1un2sg,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,AMD,2023-09-23 13:26:56,16
Intel,k1tob83,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",AMD,2023-09-23 06:58:48,5
Intel,k1txgpk,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,AMD,2023-09-23 08:55:01,6
Intel,k1ugyqs,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",AMD,2023-09-23 12:36:34,3
Intel,k1ubuqx,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",AMD,2023-09-23 11:48:42,2
Intel,k1uc6ox,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",AMD,2023-09-23 11:51:55,7
Intel,k1uln5g,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",AMD,2023-09-23 13:15:37,5
Intel,k1ull2c,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,AMD,2023-09-23 13:15:10,2
Intel,k1tocdn,25 years,AMD,2023-09-23 06:59:12,6
Intel,k1u1mrk,"The software needs to run on hardware, right now it eats through GPU compute and memory.",AMD,2023-09-23 09:49:38,6
Intel,k1tqm89,And sadly ended up with 450 Watt TDP to achieve that performance.,AMD,2023-09-23 07:26:36,12
Intel,k1tvo5h,This. We'd be lucky to see more than 3 generations in the upcoming decade.,AMD,2023-09-23 08:31:28,4
Intel,k1u1jnl,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",AMD,2023-09-23 09:48:32,3
Intel,k1ucdje,Not with that attitude,AMD,2023-09-23 11:53:45,2
Intel,k1ugj3a,Love how it's still gimped on memory size 😂,AMD,2023-09-23 12:32:43,7
Intel,k1u3901,"They just need to render the minimum information needed , and let the ai do the rest",AMD,2023-09-23 10:10:17,1
Intel,k1u9l5i,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",AMD,2023-09-23 11:25:11,-5
Intel,k1u27rs,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",AMD,2023-09-23 09:57:10,4
Intel,k1ufc12,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",AMD,2023-09-23 12:21:51,2
Intel,k1t3hpt,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",AMD,2023-09-23 03:20:42,154
Intel,k1tek93,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",AMD,2023-09-23 05:07:01,10
Intel,k1uoawi,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",AMD,2023-09-23 13:36:30,3
Intel,k1su9mw,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,AMD,2023-09-23 02:05:53,-19
Intel,k1sbp7z,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",AMD,2023-09-22 23:47:36,86
Intel,k1sxtj8,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",AMD,2023-09-23 02:33:36,6
Intel,k1svx2t,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,AMD,2023-09-23 02:18:41,5
Intel,k1sbvoe,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",AMD,2023-09-22 23:48:54,18
Intel,k1slgjk,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,AMD,2023-09-23 00:58:39,11
Intel,k1tf7yk,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",AMD,2023-09-23 05:14:13,3
Intel,k1sad2c,So rendering at 960p? Oof...,AMD,2023-09-22 23:38:00,5
Intel,k1sd3lh,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",AMD,2023-09-22 23:57:47,-5
Intel,k1rxn27,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,AMD,2023-09-22 22:09:06,-17
Intel,k1s6fmd,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,AMD,2023-09-22 23:09:38,-24
Intel,k1tzl0f,Dont use useless raytracing and you wont have any problems lol,AMD,2023-09-23 09:22:43,-1
Intel,k1vpidg,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",AMD,2023-09-23 17:42:04,7
Intel,k23ed9f,AW2 looks insane. Can't wait to play soon,AMD,2023-09-25 04:14:36,2
Intel,k1si0t0,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,AMD,2023-09-23 00:33:40,24
Intel,k1s5qaj,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",AMD,2023-09-22 23:04:33,32
Intel,k1sor0g,My 3080 in shambles,AMD,2023-09-23 01:23:27,4
Intel,k1u5gbh,"It looks better than native even with fsr quality, the Taa in this game is shit",AMD,2023-09-23 10:37:48,-1
Intel,k1t69ca,You dont need to increase raw performance. You need to increase RT performance.,AMD,2023-09-23 03:45:25,31
Intel,k1vko89,Imagine!,AMD,2023-09-23 17:11:45,6
Intel,k1vz5q2,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,AMD,2023-09-23 18:42:28,5
Intel,k22igme,Only people who don't give a shit about high quality graphics don't care about ray tracing.,AMD,2023-09-25 00:09:28,3
Intel,k1tamw3,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,AMD,2023-09-23 04:26:51,3
Intel,k1s8m60,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",AMD,2023-09-22 23:25:22,4
Intel,k1v3dh2,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",AMD,2023-09-23 15:22:07,6
Intel,k1u1pck,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,AMD,2023-09-23 09:50:33,7
Intel,k1sl137,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",AMD,2023-09-23 00:55:30,2
Intel,k1u5xqe,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,AMD,2023-09-23 10:43:42,-4
Intel,k22yxwq,The Evangelical Church of Native,AMD,2023-09-25 02:08:44,1
Intel,k1vk8iw,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",AMD,2023-09-23 17:09:00,2
Intel,k22yk42,Nice but ive been playing video games since the 70s and its Shit end off..,AMD,2023-09-25 02:05:53,2
Intel,k1s3cm8,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,AMD,2023-09-22 22:47:37,-14
Intel,k1ssze4,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",AMD,2023-09-23 01:56:05,-4
Intel,k1tpcbe,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,AMD,2023-09-23 07:11:12,11
Intel,k1srkr7,"Yeah, sure, now go back to play starfield.",AMD,2023-09-23 01:45:16,6
Intel,k1siya9,"There literally is a /s, what else do you need to detect sarcasm?",AMD,2023-09-23 00:40:26,3
Intel,k1ycket,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",AMD,2023-09-24 05:03:07,1
Intel,k1wclz4,"I know, which makes path tracing even worse off imo.",AMD,2023-09-23 20:06:58,1
Intel,k1v47c6,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,AMD,2023-09-23 15:27:32,2
Intel,k1vh7xa,haha this is gold 🥇,AMD,2023-09-23 16:50:10,2
Intel,k1sakbd,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",AMD,2023-09-22 23:39:29,-7
Intel,k1s5318,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",AMD,2023-09-22 22:59:52,6
Intel,k1t4yk0,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",AMD,2023-09-23 03:33:41,-3
Intel,k1sakwq,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",AMD,2023-09-22 23:39:36,-9
Intel,k1ryjta,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,AMD,2023-09-22 22:15:11,2
Intel,k1stpfv,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,AMD,2023-09-23 02:01:36,39
Intel,k1w9mwj,Funnily Crysis still have better destructible environments than Cyberpunk has tho,AMD,2023-09-23 19:48:19,1
Intel,k1vonge,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",AMD,2023-09-23 17:36:37,7
Intel,k1txp0j,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,AMD,2023-09-23 08:58:05,16
Intel,k1tg0xs,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",AMD,2023-09-23 05:23:00,6
Intel,k1syqzz,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",AMD,2023-09-23 02:41:04,4
Intel,k1u23r0,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,AMD,2023-09-23 09:55:42,3
Intel,k1u67ug,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,AMD,2023-09-23 10:47:01,14
Intel,k1t2x0x,LOL I remember this exact scene also.,AMD,2023-09-23 03:15:45,33
Intel,k1ua0tu,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",AMD,2023-09-23 11:29:52,9
Intel,k1t6cm4,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",AMD,2023-09-23 03:46:15,18
Intel,k1tlbna,"I'd like to see ray tracing addon cards, seems logical to me.",AMD,2023-09-23 06:23:01,12
Intel,k1tyf7o,"TBH, I could probably run some of the old games I have on CPU without the GPU.",AMD,2023-09-23 09:07:31,3
Intel,k1ub8fu,glorious squash wild file crawl ancient crowd racial soft north   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 11:42:32,3
Intel,k1uhy6a,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,AMD,2023-09-23 12:45:10,3
Intel,k1tcex2,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,AMD,2023-09-23 04:44:40,3
Intel,k1tsmml,What gpu you have,AMD,2023-09-23 07:51:58,2
Intel,k1tyt5q,Now it even runs fine on a Ryzen 2400G.,AMD,2023-09-23 09:12:37,2
Intel,k1t9hds,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",AMD,2023-09-23 04:15:43,39
Intel,k1vaqdn,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",AMD,2023-09-23 16:09:02,10
Intel,k1tf4z4,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,AMD,2023-09-23 05:13:17,3
Intel,k1tt5y9,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",AMD,2023-09-23 07:59:01,18
Intel,k1w7xof,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",AMD,2023-09-23 19:37:48,4
Intel,k1tyzar,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,AMD,2023-09-23 09:14:51,2
Intel,k1v3dv1,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",AMD,2023-09-23 15:22:12,2
Intel,k1uaqi2,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,AMD,2023-09-23 11:37:22,6
Intel,k1uke2m,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,AMD,2023-09-23 13:05:30,2
Intel,k1udkaf,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:05:15,5
Intel,k1tl1au,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",AMD,2023-09-23 06:19:42,25
Intel,k1t9pys,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",AMD,2023-09-23 04:17:58,17
Intel,k1u3wuq,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",AMD,2023-09-23 10:18:38,1
Intel,k1sx1p7,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",AMD,2023-09-23 02:27:32,41
Intel,k1t81d2,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,AMD,2023-09-23 04:01:49,4
Intel,k1szqei,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",AMD,2023-09-23 02:49:05,14
Intel,k1syew7,It's significantly better than raster. It kills fps but the quality is great,AMD,2023-09-23 02:38:21,9
Intel,k1ug9ty,Better graphics needing more expensive hardware is hardly a hot take.,AMD,2023-09-23 12:30:24,17
Intel,k1u52xg,"Yes, better graphics costs performance. SHOCKING",AMD,2023-09-23 10:33:12,13
Intel,k1t1ej8,People do it!,AMD,2023-09-23 03:02:50,7
Intel,k1tqio6,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,AMD,2023-09-23 07:25:19,11
Intel,k1t7tj4,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",AMD,2023-09-23 03:59:48,3
Intel,k1sx4o4,Sounds like most nvidia fanboys,AMD,2023-09-23 02:28:11,14
Intel,k1u2qx4,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",AMD,2023-09-23 10:03:52,1
Intel,k1u3no0,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,AMD,2023-09-23 10:15:26,-1
Intel,k1y7ifv,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,AMD,2023-09-24 04:12:33,-1
Intel,k1ss6lq,What? I thought amd was always more tuned to raster as opposed to reflections,AMD,2023-09-23 01:49:57,2
Intel,k1staby,Which is why nvidia is rabidly chasing AI hacks,AMD,2023-09-23 01:58:25,23
Intel,k1sv3ph,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,AMD,2023-09-23 02:12:20,4
Intel,k1sm72w,Say that to the people playing upscaled games at 4k (540p) on PS5.,AMD,2023-09-23 01:04:07,-2
Intel,k1s6hg3,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",AMD,2023-09-22 23:10:00,29
Intel,k1s7ntd,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,AMD,2023-09-22 23:18:28,11
Intel,k1slxqu,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",AMD,2023-09-23 01:02:10,12
Intel,k1s73qo,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",AMD,2023-09-22 23:14:27,-14
Intel,k1ttfgf,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,AMD,2023-09-23 08:02:21,12
Intel,k1tby96,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,AMD,2023-09-23 04:39:58,2
Intel,k1sb10i,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",AMD,2023-09-22 23:42:52,13
Intel,k1sf5sr,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",AMD,2023-09-23 00:12:51,2
Intel,k1u66ws,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),AMD,2023-09-23 10:46:42,5
Intel,k1t46jg,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",AMD,2023-09-23 03:26:44,-1
Intel,k1u6zio,You can already have 120fps on $1000 PC,AMD,2023-09-23 10:56:12,8
Intel,k1vnjmf,It improves both looks and fps so it is a win win,AMD,2023-09-23 17:29:44,0
Intel,k1scklw,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",AMD,2023-09-22 23:53:55,7
Intel,k1s9sl2,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,AMD,2023-09-22 23:33:53,5
Intel,k1s6x1t,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,AMD,2023-09-22 23:13:07,3
Intel,k1s97ev,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",AMD,2023-09-22 23:29:38,1
Intel,k1s30i9,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,AMD,2023-09-22 22:45:15,-8
Intel,k1s8eym,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,AMD,2023-09-22 23:23:56,-1
Intel,k1wd23h,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",AMD,2023-09-23 20:09:52,1
Intel,k1save0,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",AMD,2023-09-22 23:41:42,4
Intel,k1sama1,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,AMD,2023-09-22 23:39:52,0
Intel,k1tastr,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",AMD,2023-09-23 04:28:29,20
Intel,k1tf175,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",AMD,2023-09-23 05:12:08,10
Intel,k1v57gq,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,AMD,2023-09-23 15:33:56,5
Intel,k1uehqr,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,AMD,2023-09-23 12:14:02,3
Intel,k1uvdfc,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,AMD,2023-09-23 14:28:22,3
Intel,k1uvhzl,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",AMD,2023-09-23 14:29:15,2
Intel,k1vxcz9,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",AMD,2023-09-23 18:31:17,2
Intel,k1u94ng,moving data between the two is the issue,AMD,2023-09-23 11:20:15,9
Intel,k1ud2dh,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:00:31,8
Intel,k1tzket,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",AMD,2023-09-23 09:22:31,2
Intel,k1tcnbk,Game physics doesn't seem to be a focus anymore though,AMD,2023-09-23 04:47:04,17
Intel,k1vx974,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,AMD,2023-09-23 18:30:38,2
Intel,k1y74dq,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,AMD,2023-09-24 04:08:48,0
Intel,k1zf2ze,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",AMD,2023-09-24 12:27:53,0
Intel,k1u0kty,"Yes, but not after Nvidia bought Ageia.",AMD,2023-09-23 09:35:56,5
Intel,k1y5xsz,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",AMD,2023-09-24 03:57:42,1
Intel,k1usgsa,I like how you said: static image  because in motion upscaling is crappier,AMD,2023-09-23 14:07:47,-2
Intel,k1t97dw,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",AMD,2023-09-23 04:13:02,-3
Intel,k1t7en2,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",AMD,2023-09-23 03:55:55,5
Intel,k1t22gi,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",AMD,2023-09-23 03:08:23,1
Intel,k1u3zfu,"It *is* way slower, you're just compensating it by reducing render resolution a ton",AMD,2023-09-23 10:19:33,-9
Intel,k1ubnzq,If it was bloodborne i am guilty of that myself,AMD,2023-09-23 11:46:49,5
Intel,k1uerlc,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:16:37,6
Intel,k1y7wcn,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,AMD,2023-09-24 04:16:20,1
Intel,k1tarlv,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",AMD,2023-09-23 04:28:09,5
Intel,k1tfawo,Rasterisation is a “hack” too,AMD,2023-09-23 05:15:07,39
Intel,k1sxetf,If it works it works....computer graphics has always been about approximation,AMD,2023-09-23 02:30:22,36
Intel,k1t8lwj,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",AMD,2023-09-23 04:07:17,19
Intel,k1tqzxo,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",AMD,2023-09-23 07:31:22,5
Intel,k1vlzcm,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",AMD,2023-09-23 17:20:00,0
Intel,k1s7up5,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,AMD,2023-09-22 23:19:50,-10
Intel,k1sony7,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",AMD,2023-09-23 01:22:48,-5
Intel,k1sot43,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,AMD,2023-09-23 01:23:54,-4
Intel,k1s8jx2,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,AMD,2023-09-22 23:24:55,4
Intel,k1ubk56,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",AMD,2023-09-23 11:45:47,2
Intel,k1snovf,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",AMD,2023-09-23 01:15:25,3
Intel,k1sd914,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,AMD,2023-09-22 23:58:55,8
Intel,k1ubcdm,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",AMD,2023-09-23 11:43:38,-2
Intel,k1tno35,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",AMD,2023-09-23 06:50:58,9
Intel,k1ucpbc,Get that fps with a 5120*1440 240Hz monitor,AMD,2023-09-23 11:56:59,1
Intel,k1u928y,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",AMD,2023-09-23 11:19:32,-2
Intel,k1vo11q,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",AMD,2023-09-23 17:32:45,1
Intel,k1s3igv,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,AMD,2023-09-22 22:48:46,4
Intel,k1ugb6v,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",AMD,2023-09-23 12:30:45,1
Intel,k1wecws,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",AMD,2023-09-23 20:18:14,1
Intel,k1sbp25,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,AMD,2023-09-22 23:47:34,-5
Intel,k1sj47l,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",AMD,2023-09-23 00:41:39,-7
Intel,k1tbdg8,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,AMD,2023-09-23 04:34:12,8
Intel,k1tgkh1,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",AMD,2023-09-23 05:29:06,3
Intel,k1ueofm,8600gt in SLI man you are Savage!🔥,AMD,2023-09-23 12:15:48,2
Intel,k1v8q15,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",AMD,2023-09-23 15:56:19,3
Intel,k20zqc9,there there’s no way there will ever be another 8800 GT. you got so much for your money.,AMD,2023-09-24 18:32:27,3
Intel,k1v3f8y,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",AMD,2023-09-23 15:22:27,2
Intel,k1vqgag,Seemed like a perfect use case for the sli bridge they got rid of.,AMD,2023-09-23 17:48:01,3
Intel,k1y719d,Why would it need to send the data to the other card? They both feed into the same game.,AMD,2023-09-24 04:07:58,2
Intel,k1uqeyb,Big up the Vega gang,AMD,2023-09-23 13:52:34,3
Intel,k1tgpce,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",AMD,2023-09-23 05:30:35,40
Intel,k1u8rwh,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",AMD,2023-09-23 11:16:24,9
Intel,k1tffr8,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,AMD,2023-09-23 05:16:37,16
Intel,k1toby8,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,AMD,2023-09-23 06:59:04,3
Intel,k1w7uac,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",AMD,2023-09-23 19:37:13,0
Intel,k1thrmi,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",AMD,2023-09-23 05:42:24,3
Intel,k1t66vz,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",AMD,2023-09-23 03:44:48,3
Intel,k1ueiu5,We are guilty of the exact same sin.,AMD,2023-09-23 12:14:20,2
Intel,k1tzi21,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-09-23 09:21:41,0
Intel,k1u5aaz,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",AMD,2023-09-23 10:35:43,3
Intel,k1u0whe,would you care to explain ? Kinda interested to here this,AMD,2023-09-23 09:40:08,1
Intel,k1scoi2,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,AMD,2023-09-22 23:54:43,6
Intel,k1tcqbd,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",AMD,2023-09-23 04:47:55,7
Intel,k256x6l,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,AMD,2023-09-25 14:57:47,2
Intel,k1s8ne8,Turn on path tracing. Embrace the PowerPoint.,AMD,2023-09-22 23:25:36,5
Intel,k1ssjdy,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,AMD,2023-09-23 01:52:37,11
Intel,k1vpcr3,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",AMD,2023-09-23 17:41:03,0
Intel,k1s8f3n,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",AMD,2023-09-22 23:23:57,-1
Intel,k1wloml,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",AMD,2023-09-23 21:03:39,1
Intel,k1tpscr,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",AMD,2023-09-23 07:16:33,6
Intel,k1sen3q,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",AMD,2023-09-23 00:09:03,5
Intel,k1sdc8m,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",AMD,2023-09-22 23:59:34,0
Intel,k1tcqfl,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",AMD,2023-09-23 04:47:57,2
Intel,k1ttjgv,Shame you don't. They did it for a reason.,AMD,2023-09-23 08:03:50,2
Intel,k1v42qp,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,AMD,2023-09-23 15:26:42,3
Intel,k1wmal2,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",AMD,2023-09-23 21:07:26,2
Intel,k1w2bdg,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,AMD,2023-09-23 19:02:32,4
Intel,k1wmeoj,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",AMD,2023-09-23 21:08:09,0
Intel,k1zjknf,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",AMD,2023-09-24 13:05:06,0
Intel,k1ue2iz,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,AMD,2023-09-23 12:10:02,3
Intel,k1tkit9,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",AMD,2023-09-23 06:13:48,5
Intel,k1t75sa,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",AMD,2023-09-23 03:53:39,4
Intel,k1v732r,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",AMD,2023-09-23 15:45:58,6
Intel,k1ub5y8,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,AMD,2023-09-23 11:41:49,12
Intel,k1t4gtg,Or a bag of fake tricks.,AMD,2023-09-23 03:29:16,13
Intel,k1u716i,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",AMD,2023-09-23 10:56:45,0
Intel,k1tnk4a,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",AMD,2023-09-23 06:49:39,0
Intel,k1szwwn,Have you tried Metro Exodus Enhanced?,AMD,2023-09-23 02:50:34,6
Intel,k1ttno1,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,AMD,2023-09-23 08:05:18,3
Intel,k1wlywq,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",AMD,2023-09-23 21:05:25,1
Intel,k1u602a,What is this? A reasonable comment in this dumpster fire of a sub?,AMD,2023-09-23 10:44:29,3
Intel,k1sg0o3,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,AMD,2023-09-23 00:19:01,2
Intel,k1tgd2j,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",AMD,2023-09-23 05:26:46,3
Intel,k1v2w6l,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",AMD,2023-09-23 15:18:58,3
Intel,k1wpc9k,Except it was just a rebadged 8800GTX/Ultra,AMD,2023-09-23 21:27:07,2
Intel,k1wso05,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",AMD,2023-09-23 21:49:06,1
Intel,k1tlekn,Pixel art go brr,AMD,2023-09-23 06:23:58,2
Intel,k1uefox,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,AMD,2023-09-23 12:13:29,3
Intel,k1tws4k,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",AMD,2023-09-23 08:46:03,0
Intel,k1shejh,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",AMD,2023-09-23 00:29:11,3
Intel,k1tykau,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",AMD,2023-09-23 09:09:21,2
Intel,jws0ze9,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",AMD,2023-08-18 21:33:20,70
Intel,jwrrxaq,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),AMD,2023-08-18 20:34:56,55
Intel,jwtujwf,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,AMD,2023-08-19 06:46:11,12
Intel,jws0ogv,I wonder if this will get added to Mangohud and Gamescope.,AMD,2023-08-18 21:31:17,10
Intel,jws656f,This is quality. Great work.,AMD,2023-08-18 22:08:17,8
Intel,jwtl7yn,Doesn’t capframeX uses presentmon as its monitoring tool?,AMD,2023-08-19 04:58:08,5
Intel,jwse5d4,I can finally see if it really is the ENB taking down my Skyrim gamesaves,AMD,2023-08-18 23:05:28,3
Intel,jwt3rjk,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",AMD,2023-08-19 02:18:52,9
Intel,jwv7vh1,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),AMD,2023-08-19 15:06:20,2
Intel,jwx068e,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",AMD,2023-08-19 21:46:34,2
Intel,jx6nzqo,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,AMD,2023-08-21 21:00:38,2
Intel,jwt26ko,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",AMD,2023-08-19 02:06:15,7
Intel,jwvd88w,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,AMD,2023-08-19 15:41:34,3
Intel,jwsaw5e,Thanks Intel! I will try this out at least since I hate MSI afterburner.,AMD,2023-08-18 22:42:00,4
Intel,jwss1oh,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,AMD,2023-08-19 00:48:31,2
Intel,jwsaaac,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",AMD,2023-08-18 22:37:37,-2
Intel,jwteu8t,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,AMD,2023-08-19 03:54:41,0
Intel,jwscc3e,And AmD gives far more than Nvidia.,AMD,2023-08-18 22:52:16,-11
Intel,jwwf6wi,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,AMD,2023-08-19 19:37:09,1
Intel,k25hh4a,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,AMD,2023-09-25 16:00:53,1
Intel,jwsaffm,People have reported that cpu usage in Radeon software is not very accurate.,AMD,2023-08-18 22:38:38,36
Intel,jwsejpm,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,AMD,2023-08-18 23:08:23,6
Intel,jwsu2it,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",AMD,2023-08-19 01:03:33,3
Intel,jwubtsm,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",AMD,2023-08-19 10:34:05,2
Intel,jws5mkd,Just use afterburner as OSD.,AMD,2023-08-18 22:04:42,2
Intel,jwtvx7w,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,AMD,2023-08-19 07:03:38,1
Intel,k3rohbn,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",AMD,2023-10-06 20:51:17,1
Intel,jwrzqfw,You beat me to it :),AMD,2023-08-18 21:25:03,4
Intel,jx08btc,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",AMD,2023-08-20 15:17:16,10
Intel,jwu03xe,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,AMD,2023-08-19 07:58:51,7
Intel,jx8l1n5,Technically it should be possible to add in MSI afterburner because it's open source,AMD,2023-08-22 06:01:19,1
Intel,jwtnlcl,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",AMD,2023-08-19 05:23:42,2
Intel,jwyec42,It was a pet project of one of the Intel engineers.   6/10 is not bad!,AMD,2023-08-20 03:57:36,4
Intel,jx6uru1,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,AMD,2023-08-21 21:43:19,1
Intel,jwtopeu,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,AMD,2023-08-19 05:36:25,6
Intel,jwus2bx,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",AMD,2023-08-19 13:12:58,2
Intel,jwwr3eh,I hate afterburner and RTSS. This is way better,AMD,2023-08-19 20:49:19,1
Intel,jwst65i,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",AMD,2023-08-19 00:56:51,9
Intel,jwtohd3,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",AMD,2023-08-19 05:33:51,-9
Intel,jwt37au,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",AMD,2023-08-19 02:14:23,1
Intel,jwsd0ai,"Well, everyone uses RTSS anyway and it gives you basically everything.",AMD,2023-08-18 22:57:07,1
Intel,jx7hc39,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,AMD,2023-08-22 00:20:25,1
Intel,jwscka1,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",AMD,2023-08-18 22:53:52,9
Intel,jwxcu6w,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,AMD,2023-08-19 23:14:33,1
Intel,jwsf4i1,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",AMD,2023-08-18 23:12:39,3
Intel,jwsnfu5,Afterburner fucks with my settings in adrenaline,AMD,2023-08-19 00:13:59,5
Intel,jwu3toe,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",AMD,2023-08-19 08:48:58,4
Intel,jwv9n3n,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",AMD,2023-08-19 15:18:05,3
Intel,jxi33zm,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,AMD,2023-08-24 02:22:09,1
Intel,jwsus0t,I get downvoted for asking a valid question?,AMD,2023-08-19 01:08:51,2
Intel,jwtt73f,Thank you for continuing to contribute Nothing to this conversation.,AMD,2023-08-19 06:29:18,8
Intel,jwslkce,Clearly not more than this beta of presentmon,AMD,2023-08-18 23:59:56,1
Intel,jwsd4sf,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,AMD,2023-08-18 22:58:01,-2
Intel,jwsfkyz,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",AMD,2023-08-18 23:16:00,11
Intel,jwxd4ky,Where are you seeing this?,AMD,2023-08-19 23:16:32,1
Intel,jwsfr15,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",AMD,2023-08-18 23:17:13,1
Intel,jwsno08,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",AMD,2023-08-19 00:15:40,14
Intel,jwuero7,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,AMD,2023-08-19 11:08:44,4
Intel,jwu8j4q,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,AMD,2023-08-19 09:51:11,3
Intel,jwv8abu,Great news to gamers though,AMD,2023-08-19 15:09:05,2
Intel,jwvbwid,ah sorry I meant NVK,AMD,2023-08-19 15:32:34,1
Intel,jwsuuv1,"I don't know, I didn't downvote you.",AMD,2023-08-19 01:09:27,6
Intel,jwsec7c,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,AMD,2023-08-18 23:06:52,6
Intel,jwt2g8b,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",AMD,2023-08-19 02:08:23,3
Intel,jwxvkn7,It’s where you oc in adrenaline,AMD,2023-08-20 01:27:06,1
Intel,jwuly2q,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",AMD,2023-08-19 12:21:17,4
Intel,jwv200x,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",AMD,2023-08-19 14:26:34,3
Intel,jwtzdkr,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",AMD,2023-08-19 07:49:04,4
Intel,jwuvpd6,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",AMD,2023-08-19 13:41:23,2
Intel,jwv1po6,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",AMD,2023-08-19 14:24:33,6
Intel,jwuapdo,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",AMD,2023-08-19 10:19:39,2
Intel,jx3nm3i,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",AMD,2023-08-21 06:56:43,1
Intel,jwv7qel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",AMD,2023-08-19 15:05:22,2
Intel,jwucdfz,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",AMD,2023-08-19 10:40:52,3
Intel,jwuwjmm,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",AMD,2023-08-19 13:47:45,1
Intel,jx4di9f,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",AMD,2023-08-21 12:07:55,1
Intel,jwuithf,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",AMD,2023-08-19 11:51:44,4
Intel,jwuq603,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",AMD,2023-08-19 12:57:40,3
Intel,jwuozk8,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",AMD,2023-08-19 12:47:47,2
Intel,jwvi4ei,Congratulations on the new gaming laptop. Don’t let people get you down and not everyone has to have the very best to be happy so if it makes you happy then I’m happy for you.,AMD,2023-08-19 16:13:46,25
Intel,jwwunye,"Congrats the 30 series isn't half bad. Laptops aren't bad but gaming desktops add more umph to their performance. My first gaming rig was a gaming desktop with 64gbs ddr4 3200mhz, rtx 3060 12gb, 850w platinum psu, b660m gigabyte motherboard and a intel i5 13600kf. Couldn't have had a better first rig.my current build now is a hellhound raedeon rx 7900xt,gigabyte  z790x ax gaming motherboard,  850w platinum psu, 64gbs ddr5 6000mhz,Samsung 980 1gb ssd, intel i5 13600kf(watercooled).best upgrade I made for 1440p gaming.",AMD,2023-08-19 21:10:15,2
Intel,jx1t2ia,"Congratz!  Also, what's that wallpaper? Looks awesome",AMD,2023-08-20 21:29:18,2
Intel,jwvupca,I don't understand the stickers. Ryzen CPU Check. Both Radeon and GTX graphics? Is one discrete and the other dedicated? Just needs an Intel sticker to make it more confusing.,AMD,2023-08-19 17:32:54,1
Intel,jwv4u8q,"That is not a ""gaming rig"" not even close my friend. ;)",AMD,2023-08-19 14:46:02,-9
Intel,jwvgjd6,3050ti?  Gaming rip?  No,AMD,2023-08-19 16:03:09,-5
Intel,jwwhi65,"5 or so years ago i built my first real gaming rig. Few years later i bought new laptop, because i already had a gaming pc i wanted something capable to game but didnt want to spend too much money for high end. So i bought Lenovo legion with same gpu - 3050ti.   What i soon realized was that i love the comfort of being able to browse the net and game from the couch, being able to put my legs up, put my back against a big pillow etc... Despite having much more capable pc, i was lazy to play on it and was using gaming laptop instead 90% of the time.   Problem is, 3050ti in the last year or so really starting to show its limitations, most of new games are straight up unplayable anymore. Even on lowest settings it doesnt run newest games at playable standards, probably because of only 4gb of vram. Hogwartz, Rachet and Clank, Remnant 2 all pretty much unplayable unless you drop resolution to 720... I really regret not putting few hundreds more for something like 3070. (unrelated, but why stepping up laptop gpu tiers are so expensive, for example 3060 to 3070 is like 300e here for essentially same system otherwise, 3070 to 3080 is like 4-500e extra more, its even more expensive overall than desktop gpu tiers for way lesser performance improvements...).   So my advice would be, if you can get at least 3060, its worth it to pay up a bit more, ideally 6600m or 4060, those few hundreds increase in price will be worth in a few years.  ALSO: not sure if OP knows, but these laptops come with shittiest ram, upgrading it to a dual channel dual rank is pretty much a must as it nets 10-20% more performance.",AMD,2023-08-19 19:51:52,1
Intel,jwxgjhs,There is nothing wrong with laptops I'd build my own if there was a market readily available for it like a desktop.,AMD,2023-08-19 23:39:57,1
Intel,jwz7u2u,No!,AMD,2023-08-20 09:40:51,1
Intel,jx2ecvu,"This isn't exactly what I'd imagine hearing the words ""gaming rig"" but I'm glad you like it. Happy gaming!",AMD,2023-08-21 00:03:02,1
Intel,jz3n7rj,Ignore the haters dude. Congratulations on your new gaming laptop. Gaming is something that is supposed to make us happy not cater to snobs. Not everyone needs 4k 60fps. If it satisfies your requirements it's the best rig for you. I have a Ryzen 5 2500u and I game on that so I am super happy for you.,AMD,2023-09-04 14:46:53,1
Intel,jwvkdlj,"People take things too seriously, and tend to forget that every enthusiast was once a novice.  This is OP's first gaming setup; they deserve to feel that excitement.",AMD,2023-08-19 16:28:13,9
Intel,jwvnufp,who even buys the 'very best' laptop anyway?,AMD,2023-08-19 16:50:14,2
Intel,jwvc7sf,Lenovo Ideapad Gaming 3,AMD,2023-08-19 15:34:40,3
Intel,jx1ui6e,"It's just the default one that came with the laptop, theres a red and a blue version probs from AMD and Intel lol.  You should be able find them online.",AMD,2023-08-20 21:39:12,2
Intel,jwvwcww,Its a ryzen cpu with integrated radeon graphics and an nvidia dedicated gpu. Its completely normal and not really confusing.,AMD,2023-08-19 17:43:04,11
Intel,jwwk8it,APU + dGPU,AMD,2023-08-19 20:08:00,1
Intel,jwvby00,"You know what I mean.   It's my very first PC/Laptop set up so I'm definitely calling it a rig, it's way more powerful than anything I've owed before it.",AMD,2023-08-19 15:32:51,14
Intel,jwv71nz,"It games, it's a gaming rig. :)",AMD,2023-08-19 15:00:53,9
Intel,jwvgp6w,you do realise not every wants or needs a 4070 to play games they like,AMD,2023-08-19 16:04:16,9
Intel,jwyz4ai,I upgraded the ram. Has 16gb ddr 5 dual channel now,AMD,2023-08-20 07:45:41,2
Intel,jwvm2xc,"Thanks dude! I think people forget that not everone needs the same kinda thing from a computer which is why they sell multiple models.  This is great for what I need it for, and I feel like a kid at Christmas, happy to be able to dip my toe into PC gaming, finally!",AMD,2023-08-19 16:39:11,9
Intel,jwvpsjs,You get my point at least you should,AMD,2023-08-19 17:02:04,5
Intel,jwvh0km,"Just to let you know , these laptops have different versions , one I have is something like Ar15mhqp or some shit like that   It is less powerful than yours but decent enough",AMD,2023-08-19 16:06:23,4
Intel,jwvc5tx,That is okay if you see that way and congrats.,AMD,2023-08-19 15:34:19,-6
Intel,jwv9ikl,"There's nothing ""rig"" about a laptop. Laptops aren't built to be rigged.",AMD,2023-08-19 15:17:14,-5
Intel,jwvboff,"No, it's a notebook.  Also have a low end gpu dude.  Not a game rig. It's nice but is not that.",AMD,2023-08-19 15:31:06,-6
Intel,jwvib92,"I know, but the same laptop or any other with 6800m or other AMD gpu would have been a much better choice lol",AMD,2023-08-19 16:15:00,-8
Intel,jwxg0r9,All that matters is that your happy with it,AMD,2023-08-19 23:36:26,3
Intel,jwvhehi,Glad you’re enjoying it. People something think that unless you get something with everything maxed out then you couldn’t be happy lol,AMD,2023-08-19 16:08:55,2
Intel,jwvcmin,"Yeah I'm just happy to be able to start playing around on some games on PC now. I've wanted to play flight simulator on pc instead of my xbox for ages now becuase of all the addons,and now i'll have access to those :D",AMD,2023-08-19 15:37:25,4
Intel,jwvc36g,"3050ti isn't really low end. And it will do for everything I need it for. I have an Xbox too for the most modern stuff.  People think that everyone has to have 4k 60fps in every game these days and not everyone needs or wants that, especially in a first machine.",AMD,2023-08-19 15:33:49,7
Intel,jwvlvjc,"for you maybe, this fits my needs perfectly.",AMD,2023-08-19 16:37:52,7
Intel,jwzgnvu,"Yup I am more than happy!  I've only ever had and played with integrated graphics on any other laptop I've had so this is a huge step for me.  I will play most brand new stuff on my Xbox for the best ecperience but stuff like KF2 I can now play a 1080p ultra and get 100s frames, could only play that before at 900p low.",AMD,2023-08-20 11:29:53,3
Intel,jwvho43,Well you say that right now but one day a game you really wanna play will come and your pc will be shitting itself trying to run it. If I had the money I would buy the best rig.   But if you don’t have a lot of money like me it’s all about price/performance,AMD,2023-08-19 16:10:41,0
Intel,jwvcxpz,That is the important that you are happy but still the reality is one.  Just enjoy the notebook.,AMD,2023-08-19 15:39:35,-6
Intel,jwwilm3,"not too similar but I can recommend grabbing dirt rally 2.0 on a steam sale with all DLC for 8€ / $10 or whatever it is, generally games that simulate some things are pretty much at home on PC",AMD,2023-08-19 19:58:19,1
Intel,jwvcs7g,"Yes, the 3050 ti is a low end card.",AMD,2023-08-19 15:38:30,4
Intel,jwvljzf,I have an Xbox too so I'll be able to play it on there anything that is out of spec.,AMD,2023-08-19 16:35:47,1
Intel,jww7sx0,I have a friend playing Baldurs Gate 3 on a 980/ You don't know what low end is apparently lmfao,AMD,2023-08-19 18:51:09,5
Intel,jwvcxzc,It's better than the 1650 and the 2050 though. And they were the other choices.,AMD,2023-08-19 15:39:38,5
Intel,jwwudqt,And that is normal the game don't requiere a big gpu to run the game is a mid / low graphics.  Besides the 980 was the top of that generation. The 3050 is the low end of that particular generation.,AMD,2023-08-19 21:08:34,1
Intel,jwvw154,"Dont let the haters bring you down dude, if you are happy, then enjoy your new laptop!",AMD,2023-08-19 17:41:04,5
Intel,jwwuzrc,"Both cards are the lowest version of his generation.  Again the notebook is nice but is a low end ""gaming"" notebook and again the point is that you ENJOY IT and play many games.",AMD,2023-08-19 21:12:17,2
Intel,jwwv6vy,He needs to know what he have but also I congrats OP for your purchase. The point is that you enjoy it.,AMD,2023-08-19 21:13:29,1
Intel,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,11
Intel,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,3
Intel,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
Intel,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,1
Intel,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
Intel,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
Intel,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
Intel,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
Intel,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
Intel,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
Intel,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
Intel,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
Intel,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
Intel,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
Intel,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
Intel,nmoztyk,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Intel,2025-11-02 12:36:20,2
Intel,nmquye0,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Intel,2025-11-02 18:34:43,1
Intel,nmzufnv,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Intel,2025-11-04 02:52:11,1
Intel,nlbgoss,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Intel,2025-10-25 14:51:22,13
Intel,nlawueh,That naming scheme really is complete and utter dogshit,Intel,2025-10-25 12:59:00,6
Intel,nlp3wrh,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Intel,2025-10-27 19:03:55,1
Intel,nlrilyu,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Intel,2025-10-28 02:55:34,1
Intel,nlz6ywu,I'm looking forward to check how those series will perform!!,Intel,2025-10-29 09:04:56,1
Intel,nlbj26i,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Intel,2025-10-25 15:03:33,16
Intel,nlbihha,look forward to new APUs,Intel,2025-10-25 15:00:35,2
Intel,nlhde6w,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Intel,2025-10-26 14:36:10,2
Intel,nlpqfuu,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Intel,2025-10-27 20:58:10,1
Intel,nmbs4xl,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Intel,2025-10-31 06:11:07,1
Intel,nldt8zb,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Intel,2025-10-25 22:15:21,0
Intel,nlclyxd,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Intel,2025-10-25 18:23:40,-9
Intel,nlguu28,You’ll be waiting till 2027 on the amd side.,Intel,2025-10-26 12:44:06,3
Intel,nmbs0u8,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Intel,2025-10-31 06:09:58,1
Intel,nlq5vco,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Intel,2025-10-27 22:20:54,1
Intel,nlg1l4u,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Intel,2025-10-26 08:16:14,7
Intel,nlp4s9i,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Intel,2025-10-27 19:08:23,2
Intel,nlpqrfz,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Intel,2025-10-27 20:59:49,1
Intel,nlcna8j,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Intel,2025-10-25 18:30:27,9
Intel,nlp4f4m,Really? This should be good for Intel in 2026.,Intel,2025-10-27 19:06:31,1
Intel,nmbt7jh,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Intel,2025-10-31 06:22:00,1
Intel,nlqexwa,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Intel,2025-10-27 23:11:50,1
Intel,nmbsirj,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Intel,2025-10-31 06:15:03,1
Intel,nld2j3h,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Intel,2025-10-25 19:51:15,-3
Intel,nlqjpxg,👍,Intel,2025-10-27 23:38:19,1
Intel,nld2x4w,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Intel,2025-10-25 19:53:17,7
Intel,nld43st,You are right. I meant 'Gorgon Point'.,Intel,2025-10-25 19:59:34,1
Intel,nln841i,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Intel,2025-10-27 13:21:07,1
Intel,nlp48zs,My brain hurts and I’m still confused,Intel,2025-10-27 19:05:38,1
Intel,nmbrwqs,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Intel,2025-10-31 06:08:49,1
Intel,nkxsilf,"I think him saying ""unreleased products"" could mean it's still coming.",Intel,2025-10-23 11:21:44,27
Intel,nkz30mp,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Intel,2025-10-23 15:39:54,9
Intel,nkzp226,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Intel,2025-10-23 17:25:52,5
Intel,nl8a9mb,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Intel,2025-10-25 00:22:50,1
Intel,nkxmfto,Intel is not serious with dGPUs,Intel,2025-10-23 10:33:35,-5
Intel,nl0j2ic,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Intel,2025-10-23 19:51:32,6
Intel,nlec4sq,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Intel,2025-10-26 00:09:57,2
Intel,nm6dcp4,B50 is interesting once the software is there (planned Q4).,Intel,2025-10-30 11:58:35,1
Intel,nkxz8vy,"Yes, they are literally just playing. It's all a game lol",Intel,2025-10-23 12:08:42,12
Intel,nkz3604,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Intel,2025-10-23 15:40:37,2
Intel,nlbmybg,They are as serious as AMD.,Intel,2025-10-25 15:23:34,1
Intel,nl1d1w8,This interview happened during the quiet period so I don't think he could talk about the future,Intel,2025-10-23 22:26:07,8
Intel,nlbmm1g,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Intel,2025-10-25 15:21:47,2
Intel,nlc5u9q,Where did you hear that it was cancelled?,Intel,2025-10-25 17:01:38,1
Intel,nkzrbdu,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Intel,2025-10-23 17:36:29,3
Intel,nkzb6g5,There has been no update regarding celestial dGPUs internally.,Intel,2025-10-23 16:19:27,3
Intel,nl9n96c,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Intel,2025-10-25 06:15:43,1
Intel,nlc8fdp,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Intel,2025-10-25 17:14:34,0
Intel,nlcandy,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Intel,2025-10-25 17:25:50,1
Intel,nlbpug0,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Intel,2025-10-25 15:38:24,1
Intel,nlc7sgw,Ex-Intel coworkers/acquaintances.,Intel,2025-10-25 17:11:23,1
Intel,nl34or5,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Intel,2025-10-24 05:10:43,2
Intel,nl3q53x,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Intel,2025-10-24 08:35:10,2
Intel,nlafjvl,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Intel,2025-10-25 10:48:26,1
Intel,nlcioic,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Intel,2025-10-25 18:06:26,1
Intel,nlbtqtn,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Intel,2025-10-25 15:58:32,3
Intel,nlcbiv4,"So, no news story has come out stating that?",Intel,2025-10-25 17:30:18,2
Intel,nlc8k5a,"Yes, through my ex colleagues",Intel,2025-10-25 17:15:14,1
Intel,nleb7hi,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Intel,2025-10-26 00:04:10,1
Intel,nlbuyry,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Intel,2025-10-25 16:04:56,2
Intel,nlchyo2,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Intel,2025-10-25 18:02:48,2
Intel,nmbst69,Why would Intel tell you? It would just stall selling all current ARC cards.,Intel,2025-10-31 06:17:59,1
Intel,nlc2d7k,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Intel,2025-10-25 16:44:12,3
Intel,nlc7zs4,BMG was 0 margin product,Intel,2025-10-25 17:12:23,2
Intel,nlc3nqx,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Intel,2025-10-25 16:50:50,2
Intel,nlci8uq,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Intel,2025-10-25 18:04:14,3
Intel,nlck9s5,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Intel,2025-10-25 18:14:45,1
Intel,nled55i,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Intel,2025-10-26 00:16:21,1
Intel,nmbspxb,Actually after the Nvidia deal there's definite reason for Intel to cancel future ARC development.  They have a partner that makes better GPU than them. Why would they continue? If the Nvidia deal is successful I expect even their iGPU will disappear.  In no sense it makes sense to develop a line that's redundant with what your partner is doing.,Intel,2025-10-31 06:17:05,1
Intel,nkl7ghk,Think pretty easy naming. Any X infront just automatically means better iGPU,Intel,2025-10-21 12:17:01,36
Intel,nklikpx,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Intel,2025-10-21 13:28:12,18
Intel,nkkzyp1,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Intel,2025-10-21 11:20:48,14
Intel,nkodq7t,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Intel,2025-10-21 21:59:26,8
Intel,nkkoc9f,Naming for these chips are terrible,Intel,2025-10-21 09:31:57,10
Intel,nkmeuem,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Intel,2025-10-21 16:22:11,1
Intel,nkzbmky,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Intel,2025-10-23 16:21:35,1
Intel,nkld363,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Intel,2025-10-21 12:54:21,17
Intel,nkuhiob,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Intel,2025-10-22 20:56:33,2
Intel,nl9xb0y,"If the game could be 50–60% stronger, that would be That would be a killer",Intel,2025-10-25 07:53:56,1
Intel,nkljsi3,"That’s been true for the past generations, but it looks like it will change this generation",Intel,2025-10-21 13:35:30,6
Intel,nl9xpor,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Intel,2025-10-25 07:57:55,1
Intel,nkkrh6o,Still better than Ryzen 365 AI pro MAX+,Intel,2025-10-21 10:04:31,53
Intel,nkm3cl3,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Intel,2025-10-21 15:22:49,3
Intel,nkv8b22,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Intel,2025-10-22 23:24:43,1
Intel,nkm4j1i,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Intel,2025-10-21 15:28:57,3
Intel,nkksh88,Yea putting ai the model name is disgusting 😂,Intel,2025-10-21 10:14:17,23
Intel,nkl1j6x,I can't wait for the Ryzen 688S AI Pro MAX+++,Intel,2025-10-21 11:33:19,13
Intel,nkmi2u0,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Intel,2025-10-21 16:38:27,3
Intel,nkmgpw7,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Intel,2025-10-21 16:31:44,3
Intel,nkpnp7f,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Intel,2025-10-22 02:32:12,3
Intel,nklj3is,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Intel,2025-10-21 13:31:20,-3
Intel,nknc85p,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Intel,2025-10-21 18:59:33,2
Intel,nklps98,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Intel,2025-10-21 14:09:54,0
Intel,nko4b5d,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Intel,2025-10-21 21:10:14,4
Intel,nklngwf,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Intel,2025-10-21 13:56:45,12
Intel,nkns6e4,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Intel,2025-10-21 20:13:23,3
Intel,nkoestf,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Intel,2025-10-21 22:05:28,5
Intel,nkmk3eq,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Intel,2025-10-21 16:48:08,2
Intel,nkpsi6y,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Intel,2025-10-22 03:02:49,3
Intel,nkof5d7,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Intel,2025-10-21 22:07:26,1
Intel,nkq1gt0,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Intel,2025-10-22 04:06:17,2
Intel,nm4gnwa,Same core counts too,Intel,2025-10-30 02:20:15,1
Intel,njdkfg9,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-10-14 01:35:52,1
Intel,njo3me8,"To summarize from the article for some folks:  6300 puts this GPU as 33.4% faster than the 140V, 71.3% faster than the 890M, and 54.3% of the 8060S. Just over half of Strix Halo's top config.  Bear in mind though, that this benchmark favors ARC GPUs compared to gaming results. The 140V and 890M are roughly equal and this benchmark puts the 140V as 28.4% faster.",Intel,2025-10-15 18:57:43,52
Intel,njp2gv2,"Ugh, who is naming these products?!?!? Between the internal code names (which are now publicly used) and the actual product names, it's a mess. As bad as monitor naming.",Intel,2025-10-15 21:57:08,22
Intel,njq65e2,"Sick, Panther actually sounds good, and Lunar/Arrow on mobile already sounded good to me. Please keep pumping those iGPU numbers Intel Arc engineer bros.  AutoCAD on integrated Intel graphics WHOOOOOO  AMD step it up next gen please, Intel is no longer a static target in graphics, kthx.",Intel,2025-10-16 01:51:35,9
Intel,njoj14y,This sound more realistic and not as good as the 50% better Intel announced.   For Intels sake I am hoping the 50% is real,Intel,2025-10-15 20:14:46,6
Intel,njpl7pm,... with early drivers and probably no decent support yet. Its also probably an engineering sample? 😉,Intel,2025-10-15 23:47:19,3
Intel,nkgif8b,Wow pantherlake is looking enticing. Should I wait for novalake? Will novalake have same battery life like lunar lake or pantherlake?,Intel,2025-10-20 11:51:27,1
Intel,nkzvqp0,"It's going to cost like $10,000, right?",Intel,2025-10-23 17:57:21,1
Intel,njo7pwi,"The top lunar lake has 8Xe2 cores, and this has 12Xe3 cores. 30% faster seems… bad? Like why not 50%+ given 50% more cores and architectural improvements?",Intel,2025-10-15 19:18:08,26
Intel,njoyfid,"We have seen how Lunar Lake on MSI Claw 8 AI+ performs on early firmware and drivers, Intel even managed to improve Lunar Lake performance up to 30% after that. I expect 50% performance improvement on Panther Lake is very possible.",Intel,2025-10-15 21:34:53,6
Intel,njpivm1,"It really is a mess, someone should've been fired long ago.",Intel,2025-10-15 23:34:18,6
Intel,nmbtn83,"The only thing they want you to care about is 3/5/7/9.  If the naming is too clear, then they can't steer you to the higher margin/profit variants. At some level it needs to be unclear. Not defending them, but how things are.",Intel,2025-10-31 06:26:27,1
Intel,njrrggy,Intel wasn't a static target since the 10th gen. They've been pushing (at least in terms of performance) since then. It's mostly the efficiency which stepped back. And they messed up their current lineup in terms of performance and pricing but they've improved for sure.  Next gen seems good for them though. Hope they price it well. AMD isn't zen 1/2 either. They're bumping core numbers too and introducing some new cores as well.,Intel,2025-10-16 10:08:28,1
Intel,njolo2l,"I mean, why?  Is AMD going to be coming out with anything more powerful than the HX 370 in that power range?",Intel,2025-10-15 20:28:01,16
Intel,njo9lo1,because 50% more cores need 50% more power for 50% more performance.,Intel,2025-10-15 19:27:37,32
Intel,njopbmv,"There are so many variables here that we have no way of accurately saying that.  It's possible that Xe3 is simply treated more like RDNA in this benchmark, and that the Xe2 140V is unfairly biased towards in that way. If that is the case, the 890M and 8060S may be better metrics to base off, and we do see over 50% gains for a 50% wider GPU than the 890M.  It's also possible this is a power-limited scenario. Like for like on TDP, this would be a solid improvement given the GPU both has to be moved off-tile compared to Lunar Lake, and has twice as many CPU cores to fight for power.  This could be pre-release drivers not getting the true 100% out of the hardware or silicon with non-final clocks still being tuned. The point is there's no way to know for sure.",Intel,2025-10-15 20:46:36,15
Intel,njpsirt,I don’t understand why you people continue to use that MSI claw to demonstrate “intel improved lunar lake performance post launch” when there are millions of lunar lake laptops that never had performance issue of that one handheld.,Intel,2025-10-16 00:28:36,3
Intel,njrpj24,"they're keeping their jobs because this is technically better than the past. back in the day you'd have model numbers exclusive to a retail store, much less specific to an OEM, because businesses wanted to feel like they got a bespoke deal.   These days the SKU naming is mostly for accounting purposes, while the ""real"" naming decisions are made by the OEMs. basically most people are buying the Thinkpad/Yoga/ROG ""brand"" rather than the specific processor model, which only a much smaller crowd bothers to comprehend. It's like how people buy the Steamdeck rather than whatever APU is in there, which is fairly old at this point.",Intel,2025-10-16 09:49:54,-2
Intel,njsegc9,"Energy efficiency is huge in mobile but I was explicit in saying they were no longer a static target in graphics, where they were not making significant or impressive gains in iGPUs for many years, it seemed. Now they are making one of the best iGPUs on the market.",Intel,2025-10-16 12:58:39,1
Intel,njopfv9,"Possibly yes.   Current AMD handheld chips are better than Lunar Lake performance. Assuming normal cadence. Next generation would be similar performance to Panther Lake.     I was hoping more of a leap frog, rather than similar performance. %50 would be a very clear edge.",Intel,2025-10-15 20:47:12,-4
Intel,njoa1f3,Is the TDP fixed to be the same between LL and PTL in this test?  I genuinely don't know.,Intel,2025-10-15 19:29:49,11
Intel,njqexqp,But LNL's TDP is too low compared to H45 cpu,Intel,2025-10-16 02:47:44,4
Intel,njrcz7w,"> because 50% more cores need 50% more power for 50% more performance.  insightful, and easy to forget given feature bragging    :)",Intel,2025-10-16 07:38:36,2
Intel,nmbtgoa,FTFY: because 50% more cores need 50% more power for 35-40% more performance.  Cause it don't scale linearly.,Intel,2025-10-31 06:24:35,1
Intel,njqijcd,"Mostly because people with LNL laptops are less likely to games since gaming are not the point of those laptops -> less testing, whereas the Claw 8 AI+ is a gaming PC handheld, so it is mostly use for gaming purpose and thus have more people testing for its performance.",Intel,2025-10-16 03:12:16,3
Intel,njrgeuz,"As weird as it sounds actually Lunar Lake improvement mainly comes from MSI Claw not laptop, that because majority people who use MSI Claw give feedback the most which is why Intel focusing on the handheld first then laptop.    Intel even use Claw as benchmark for Lunar Lake compared to laptop. You can read from this article :  https://arstechnica.com/gadgets/2025/04/intel-says-its-rolling-out-laptop-gpu-drivers-with-10-to-25-better-performance/   Also Claw got BIOS update way faster than any laptop with the same chip so it helps Intel to mitigate power and boost behavior to maximize the performance. There is so many bug reports on Arc forum, most of them are Claw users, that's why laptop got benefits too.",Intel,2025-10-16 08:14:34,1
Intel,njote4v,AMD isn’t coming out with a better next gen iGPU for mobile (Gorgon Point) since it’s a simple refresh. Same arch with same CU count based on rumors.  The generation after will be competing with Nova Lake,Intel,2025-10-15 21:07:31,19
Intel,njow153,"Amd handheld with Z2E isn't better than Intel Lunar Lake, you can see the comparison on MSI Claw sub or even on youtube. Z2E in most game is 10% slower than Core Ultra 7 258V, it only won in the game where Intel GPU performs bad.    Not to mention at 17w Z2E losing badly to 258V, Intel is on their own league, it's not even competition for Amd.",Intel,2025-10-15 21:21:46,17
Intel,njoudob,Intel is gonna have better integrated graphics than AMD,Intel,2025-10-15 21:12:50,3
Intel,njoar3w,I have no idea.,Intel,2025-10-15 19:33:29,11
Intel,njqtu4m,"We don't know what TDP was run here, and both MTL amd ARL H have been 28W outside of the Ultra 9 SKUs. It's entirely possible this was run at 28W, which is also in reach of Lunar Lake's boost envelope.",Intel,2025-10-16 04:38:47,6
Intel,njqtb7t,The lunar lake laptops are tested by reviewers all the same as the strix point ones. Regardless it’s misleading because it’s not “lunar lake” but rather the performance profile/boost behaviour of that specific MSI CLAW that was changed and people act like it’s lunar lake’s drivers doing “30% magic”.   It’s not.,Intel,2025-10-16 04:34:25,4
Intel,njrhrow,"And is there documented so-called “large” performance improvements on LNL systems that already performed as expected on day one (it tied 890m on high power and was always better at low power, talking about real games not 3dmark)? Or was it only bringing the Claw back to where LNL should always perform?",Intel,2025-10-16 08:28:47,2
Intel,njoxfxz,"True. I don't see how the next igpu from Amd going to use rdna 4, it won't even support fsr 4. Meanwhile Intel going to push their igpu tech even further with XeSS XMX 3 with Xe3 and Xe3P, they will be way ahead of Amd in igpu market especially when Intel Lunar Lake already beating Amd Strix Point and Z2E.   Intel also dominating mobile market. Honestly it's not looking good for Amd.",Intel,2025-10-15 21:29:28,7
Intel,njp7qsx,Yeah people act like strix point is in that segment..... It's not.,Intel,2025-10-15 22:28:04,11
Intel,njp58ob,🫨,Intel,2025-10-15 22:13:13,3
Intel,njs0rkq,Wouldn't be the first time.,Intel,2025-10-16 11:27:26,1
Intel,njqutc3,"Oh, I guess it be like that.",Intel,2025-10-16 04:46:59,0
Intel,nis4cle,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Intel,2025-10-10 14:44:00,15
Intel,nirfu8b,"""we are tending to prefer e cores now when gaming""   That's very surprising",Intel,2025-10-10 12:29:22,23
Intel,nisac9a,If only they release a high level C-card. Battlemage kinda never scratched the mark.,Intel,2025-10-10 15:13:09,6
Intel,niv55kq,"The point of Xe3 being actually battlemage instead of Celestial, this is so horribly confusing. I can't understand what's going on, nor why they would do that.",Intel,2025-10-11 00:24:27,5
Intel,niryzd1,I’m guessing Xe3P will be on Intel 3-PT.,Intel,2025-10-10 14:17:01,4
Intel,nitrjvx,"Tom is a funny guy, love it when he gets camera time",Intel,2025-10-10 19:37:13,5
Intel,nisfksg,Cool hope they will manage to release it soon not in 12 months when everyone will be talking about RTX 6000 and RDNA5/UDNA or leaks about them.,Intel,2025-10-10 15:38:45,2
Intel,niwlua9,"When is the panther lake reveal going to be, CES?",Intel,2025-10-11 07:13:32,1
Intel,nirs4bu,"e cores are surprisingly powerful now, and games are getting more multithreaded. It might be better to spread them out over 8 less power hungry cores than 4 P cores and then spill over to the slower cores. Especially in a laptop environment where efficiency matters and the GPU is usually holding back gaming performance so much that CPU performance is really not that important.  In fact, I bet it's mostly down to not spilling over to e cores from P cores. That's what causes slowdowns and stutters usually attributed to e cores. The engine has to wait a little longer for workloads assigned to e cores than to P cores. The if the main thread is assigned to a P core that's all well and good, but if sub tasks are distributed among the P cores and then something important is assigned to a slower e core it holds up the sub tasks and in turn the main thread. But most importantly, it doesn't do so evenly. Maybe you don't need something assigned to e cores, or the e core task is light and doesn't hold up the other threads. Then things will run at the pace of the P cores, but every now and then you'll have a slowdown.",Intel,2025-10-10 13:41:08,33
Intel,nirh07n,Wonder if there's some energy star requirements.,Intel,2025-10-10 12:36:46,5
Intel,nirq8f2,"Shouldn't be. That's what the math has always said.  As soon as software can scale to ""many"" cores, the tradeoffs that go into single powerful P style cores are a bad deal. Both frequency and sequential optimizations (like multi-level branch prediction) scale poorly.  Gaming benchmarks tend to have a really strong feedback loop that favors last gen hardware design though. So seeing the benefits to E cores for gaming requires 1.) e-cores exist for a while 2.) some games optimize for them.  Long term, an optimal Intel-style processor design will look something like 4 ""legacy"" P cores + dozens of E cores.",Intel,2025-10-10 13:30:47,2
Intel,nitssdt,It's either N3P or 18AP.,Intel,2025-10-10 19:43:43,5
Intel,nirzgoz,That’s why rumors want Intel to throw away P-cores (Israel team) and only keep E-cores (Austin) to make Titan lake a unified core with only E-cores remaining.,Intel,2025-10-10 14:19:27,18
Intel,niuj3v7,>and games are getting more multithreaded.  Can you give me a few AAA examples of modern games which get a noticeable improvement in AVG. FPS or 0.1/1% lows when using more than 8cores/16 thread's ? I'm genuinely curious which games you are talking about.,Intel,2025-10-10 22:04:33,3
Intel,nirymua,"Really should focus on the Main/Sub thread part. Most games will usually only load up 1-3 cores, with the rest of the cores only used for incidental workloads with lower priority and sync requirements (Multithreading is hard yo, Multithreading with latency requirements is mindnumbing).  This makes plenty of games very suitable for the P/E architecture, as long as you have enough P cores for the Main threads, the E cores will be perfectly sufficient.",Intel,2025-10-10 14:15:14,4
Intel,nirya2r,"Energy Star probably won't be around for too much longer, at least in it's current form.  https://www.npr.org/2025/08/13/nx-s1-5432617/energy-star-trump-cost-climate-change",Intel,2025-10-10 14:13:27,2
Intel,nispl6v,That makes me imagine in a different reality if Xbox had remained with Intel and NVIDIA and their next generation would use an Intel E-core based SoC with NVIDIA chiplet.,Intel,2025-10-10 16:27:41,7
Intel,niwprxq,It has already happened. Stephen Robinson - heading the Austin team - is now the lead x86core architect.,Intel,2025-10-11 07:54:44,3
Intel,niwoax2,"Off the top of my head, Starfield, Bannerlord, BeamNG, UE5 games due to how the rendering pipeline works, etc.",Intel,2025-10-11 07:39:15,1
Intel,nisife2,"Most games of the past will only load up a few cores, but that's beginning to change. Cyberpunk 2.0 loads up 16 threads/cores easily, and some others like Battlefield 6 also scale pretty well. If you have less than 16 threads on fast cores like Panther Lake and Arrow Lake, then you can run into issues. Or if your 16 threads are on one die and any spillover has to go across the SoC die like with AMD and probably Nova Lake.",Intel,2025-10-10 15:52:33,3
Intel,niwk1j6,"Seeing how Nvidia working together with Intel to make integrated high end GPU i can see the possibility of Xbox using that chip, maybe in the future.",Intel,2025-10-11 06:55:10,5
Intel,nitfd2e,I'm tired of AMD slop consoles and handhelds,Intel,2025-10-10 18:34:17,-10
Intel,nixk3v6,">Starfield  [Not really](https://youtu.be/BcYixjMMHFk?t=1015), game doesn't benefit from more cores/threads.  >Bannerlord  Sadly, there's no benchmarks of different CPUs that I could find for this specific game, but I do know that this game heavily relies on a good CPU, but without decent data(review), which shows multiple CPUs tested, it's hard to understand the benefits of more cores/threads,  >UE5 games due to how the rendering pipeline works  It's true that UE5 can utilize 8 cores / 16 threads, but more than that? I'm not sure, if possible, provide a review/video which shows that UE5 scale with more cores/threads, so far, it seems that it is limited at 8c/16t - big channels rarely add UE5 games to their CPU benchmarks, but I found Remnant 2,[ and it doesn't show any benefits](https://youtu.be/3n537Z7pJug?t=1056) of more than 8 cores.  I heard that ""games are getting more multithreaded"" like 4-5 years ago, and in most cases, it wasn't true, with more than 8c/16t almost no games scale on CPU-side, and even when they do, in most cases it's a minor improvement over 8c/16t configuration, like 1-3%.",Intel,2025-10-11 12:43:47,2
Intel,nj1kiyw,"UE5 by default does not support multithreading well. actually I don't think async shader is even considered a default feature yet despite being added two years ago. Only the editor compilation step uses all threads, but it doesn't need to react to user input so it would be more surprising if it didn't use all threads.  If you're seeing good thread use in a UE5 game is thanks to the developer breaking up work with their own engine changes.",Intel,2025-10-12 02:44:36,1
Intel,niy0x5h,"Considering AMD’s lag in gaming performance especially in ray tracing, I would be totally onboard with that.",Intel,2025-10-11 14:29:59,2
Intel,nix9wlx,Huh????,Intel,2025-10-11 11:23:59,2
Intel,nixt1zv,"12900K being that high in the Starfield benchmark, like actually within error margins next to 7800X3D despite being an older platform with half the L2 of raptor lake and lower clock speed than even 7700X, shows that the game benefits from more cores and threads very much. Yeah 9950X should outperform it technically I guess but the split L3 between two CCDs probably holds back the advantage of having more cores.  Yeah sadly there's no benchmark for Bannerlord. But supposedly it does disperse its tasks to plenty of threads.   Here's some benchmarks showing core and thread usage in a few games. While the benchmark tool doesn't get into the details of how those software behave obviously, but at least according to the graphs some games like Tarkov really suck at distributing its tasks to several cores but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  [Will This Do? — Intel Core i5-14600KF vs. i5-13600KF vs. R7 7700X vs. i7-14700KF Benchmark - YouTube](https://www.youtube.com/watch?v=HY_weucfLPQ)  [Isn't it a blast now? — Core Ultra 9 285K benchmark. Comparison with R9 9950X, R7 9800X3D, and i9...](https://www.youtube.com/watch?v=q1zAX1VNdf0)  I haven't seen a core thread benchmark for UE5 games yet, but that's just how the engine is supposed to work though.",Intel,2025-10-11 13:42:49,2
Intel,nj4298q,"To be fair, Sony seems to be fed up with AMD lagging behind and are pushing them to do better. Consoles will likely continue to stick with AMD, at least for the next generation. Intel graphics aren't quite ready yet, and Nvidia is going to be way to expensive unless they take an old SoC and repurpose it like Nintendo.",Intel,2025-10-12 15:05:26,3
Intel,niy0ck1,">12900K being that high in the Starfield benchmark  It's because Creation Engine 2 worked better with Intel hardware until AM5 X3D chips, and 12900K great result in this test just shows that it's still a good CPU with 8 Performance cores, not that it's core count matters in any significant way - for example, 7700X delivers identical performance with lower core count/threads.  >lower clock speed than even 7700X  Clock speeds can't be compared between different architectures, different architecture = different efficiency, what matters is IPC.  >shows that the game benefits from more cores and threads very much  [here's a better example where more intel CPUs are present in Starfield test](https://youtu.be/XXLY8kEdR1c?t=29m5s), as you can see, increased core count/threads past 8 performance cores and 16 thread's is meaningless and won't provide any noticeable performance improvements, 6 faster P-cores on Intel 14600K provided better result than slower 8 P-cores on 12900K.   Going from 7950X to to 7700X results in 5 FPS loss, which is like 2-3% less FPS, and it's mostly because of lower clock speed(5.7Ghz Vs 5.4Ghz boost).   >but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  That's the point, I agree that modern games can and will utilize 8c/16t configuration, but so far I don't see a trend of games becoming more ""multi-threaded"" past that point, as I replied to that person, I'm curious what games now become more multi-threaded than few years ago, I find that observation speculative and without evidence - more cores and threads is a great approach for workloads, but games generally don't care about it past a certain point, in this case it's 8c/16t.  It could change with next-gen consoles, if they will use AMD 12 core CCDs and games will be optimized to utilize more cores&threads.  What's important now is good cores or good cores+X3D cache, not core count - even 7600X3D with 6 cores and 12 threads is better in gaming than most Intel/AMD non-X3D CPUs with way more cores.  Edit: typo",Intel,2025-10-11 14:26:44,1
Intel,niylmew,"Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X in baldurs gate 3, a highly multithreaded game, it has better 0.1% lows even, though I'm sure that one is a contained incident since BG3 is very dynamic and hence results are not always repeatable. Same with Starfield.   That's because in video games, core count matters less than how fast the CPU can access or manipulate very dynamic types of data in a random memory address.    X3Ds are not just ""whats important now"". They're going to win in old or new games, and they're winning in games because they have a large buffer of low latency data. Not winning in productivity benchmarks because productivity is about processing matters more than accessing data, and its more easily multithreaded too in many cases.    But having more cores is still more advantageous in software and games that can occupy them.",Intel,2025-10-11 16:21:28,1
Intel,nizip6t,">Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X  I said ""can and will"" not that every game is going to benefit from it.  Point of discussion was to prove that games are using more than 16 threads or they're not - it seems like you overestimated Starfield, UE5 reliance on core count/threads and most games care about 16 threads at most.   >core count matters less  Yes, that's why I replied to that person and asked him to provide me with modern AAA games which are more ""multi-threaded"" now than AAA games a few years ago, I feel like what he ""observes"" is what he wants to believe, and not something that actually happened with optimization in games.  >X3Ds are not just ""whats important now"".  AMD sells 9950X3D, best productivity and gaming CPU, if you really need both(workloads/gaming) it's the optimal way, for an average user, only benefit of E-cores is lower power draw when idle - but they do matter more if you need those workloads, I agree, but we discussed gaming performance and core reliance.",Intel,2025-10-11 19:15:30,1
Intel,nj1tulq,If Panther Lake is going to be part of Xe3 then what does that mean for a possible B770?,Intel,2025-10-12 03:51:16,2
Intel,nizcl4d,"What even is Intel naming at this point, before it was the one thing about their products that was at least *kinda* consistent.",Intel,2025-10-11 18:42:08,3
Intel,nj2hurj,"B770 is probably likely coming out at the end of this year, I don't see any reason why they would not sell it.",Intel,2025-10-12 07:32:20,2
Intel,nih90cf,Seems like Intel chips make way more sense for portable pc consoles than AMD going forward.,Intel,2025-10-08 19:56:24,19
Intel,nihman0,"Transcript for the 'What's Next' section:  >What's next? Well, I I would say uh larger investments in graphics and AI. And so if you imagine in a PC, this workload is not going away, right? The world is becoming more graphical. The world is embracing AI and more rapid than any other technology ever. So I would expect Intel to respond to that, right? It's not like we have to guess what is what is big and what is coming. We know and what's coming is more AI and more graphical experiences.  With this being said, I hope that means that the Nvidia/Intel deal means that the ARC division is not going away any time soon.",Intel,2025-10-08 21:00:02,8
Intel,nigsicb,"Panther Lake with Xe3 without any doubt will be generational uplift. Intel Lunar Lake which is last year chip still able to put Amd newest chip like Z2E to the shame.   Xe3 with 12 Xe cores not to mention with XeSS XMX is going to be terrific combo, seeing how disappointing Amd Z2E i can see Intel going to steal iGPU gaming market from Amd. Next year could ended up with more OEM using Intel chip for handheld.",Intel,2025-10-08 18:33:15,16
Intel,nij93k3,I found it interesting that he specifically mentions improvements in performance per area for Xe3. A big complaint of Xe2 is that the die size is noticeably larger than a similar performance GPU from AMD or Nvidia. Hopefully this means better financials for Intel’s graphics division with this new architecture.,Intel,2025-10-09 02:46:46,4
Intel,nii800b,Says nothing about discrete graphics. Think that's clearly dead at this point.,Intel,2025-10-08 23:00:42,-8
Intel,nihwdsn,"Lmfao now I’m feeling conflicted about my very recent MSI Claw purchase, but I suppose panther lake handhelds will likely launch middle of next year anyway",Intel,2025-10-08 21:53:14,2
Intel,niq882u,"Tom says that variable register allocation and 25% thread count per Xe core will improve utilization which was a problem with predecessors. He says they've been ""addressed"". It looks like at least perf/power wise it's 25-30% at the same process compared to the predecessor, and that may be true for perf/area as well.",Intel,2025-10-10 05:55:28,1
Intel,niljjpz,Sounds like you're clearly wrong.,Intel,2025-10-09 14:02:51,2
Intel,niicq4o,"I know how you feel but honestly Claw 8 AI+ is still amazing handheld, at least your Claw will aged better than mine because i only have A1M Ultra 7.   Claw with Panther Lake will be released in between Q2-Q3 2026 so you don't have to regret anything.",Intel,2025-10-08 23:28:43,7
Intel,niihlwk,"I get the feeling that handhelds with 12 Xe3 cores are going to be really expensive, so maybe you still didn't do too badly.",Intel,2025-10-08 23:58:03,6
Intel,nilq0j4,"Lmao, sure. Any day now...",Intel,2025-10-09 14:36:08,0
Intel,nij8lxc,I don't think so considering how expensive LNL was,Intel,2025-10-09 02:43:39,-1
Intel,niv5lac,"Why is panther lake igpu simultaneously Xe3 and Arc-B series ? This is so confusing.       Also it states being 50% faster, but it also has 50% more compute units.",Intel,2025-10-11 00:27:22,5
Intel,niqq5bp,What should be the desktop gpu equivalent?,Intel,2025-10-10 08:54:26,1
Intel,nkwiahy,So around a 3050 laptop performance?,Intel,2025-10-23 04:15:22,1
Intel,nipaq6n,The problem is how much is the price?,Intel,2025-10-10 01:57:39,0
Intel,nirogih,they should really do better on GPU,Intel,2025-10-10 13:20:46,0
Intel,nip7gnd,"Despite the vast improvement more Xe3 cores than we saw Xe2 cores in Lunar Lake, we're only seeing minor gains in average framerate performance at the same wattage based on the internal benchmark, though there are much more meaty improvements in the 1% lows. I guess its bottlenecked by its RAM bandwidth. Also I'm sure Xe3 will scale better beyond 17W. I wonder how the 4 Xe3 SKUs will fare, that's a good way of knowing how much RAM bandwidth affects things.",Intel,2025-10-10 01:39:13,-8
Intel,njovxfo,"Just because you have 50% more physical units, doesn't mean you'll automatically get 50% more performance. Remember when the Z1E came out and it was supposed to be 50-100% more power than the steam deck?",Intel,2025-10-15 21:21:11,1
Intel,niqqyxy,RTX 3050 Max-Q or 1660 ti Max Q laptop card. Within that range.,Intel,2025-10-10 09:02:58,4
Intel,nlbl2xp,Yes,Intel,2025-10-25 15:13:57,1
Intel,nis9kyv,"For them to do a 'big iGPU' design they would need to do a few things:  1. Use a different socket.  The iGPU will get too big for the existing one.  2. Upgrade the memory bus to 256bit.  That uses some power and silicon.  3. Add cache to the chip for the iGPU to use (to make up for the poor bandwidth they get from LPDDR5, even with a wider bus)  If they don't do this then the chip (which again won't fit in their default socket) will be massively bandwidth constrained to the degree that it's pointless.  Basically, do all the stuff that Strix Halo did.  Issue is Strix Halo didn't sell very well, the idea is a proof of concept and the concept needs faster memory to really work well.  LPDDR6 might help a lot at getting these big chip ideas from a XX60 level to a XX70 level, but they still require a lot of expensive chip work and a new motherboard design custom to the big socket.  I suspect we'll get there eventually, but it'll be a few years before people start calling it quits on dGPUs.",Intel,2025-10-10 15:09:34,9
Intel,niq7bnn,"50% is not minor...  Also they said >50%, or greater than 50%. It looks like 70-80% to me.  Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.",Intel,2025-10-10 05:47:17,12
Intel,nje1ogx,Maybe I'm confused about what they're advertising but weren't they just going head to head with 4060s with the b580? Wouldn't 3050 range be a huge step backwards?,Intel,2025-10-14 03:21:13,1
Intel,niqr96c,"Damn, still behind my old 1060 pc",Intel,2025-10-10 09:05:57,-1
Intel,nitxpqo,And some of the tiles are internal not on TSMC.,Intel,2025-10-10 20:09:28,3
Intel,nix2sd3,"On-package memory doesn't raise the device price from and end user perspective. It's a margin challenge for Intel because OEMs want the margins from the memory, so Intel needs to pass it along at cost.   Technically, on-package memory can even be cheaper because it can let you simplify the rest of the PCB. Iso-speed, that is.",Intel,2025-10-11 10:15:42,2
Intel,nje21sq,Are you saying you expect people to not want discrete gpu's in the future?  What about the direction of gaming would make you think that?,Intel,2025-10-14 03:23:34,1
Intel,nix2vkc,"> Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.  The 4 Xe tile is on Intel 3, so it'll have a significant clock speed deficit vs the 12Xe tile.",Intel,2025-10-11 10:16:39,0
Intel,niq9fth,"No I'm talking about the internal game benchmarks they shared. Yeah the GPU architecture  itself is a lot better on paper, but they probably got bottlenecked by the low bandwidth of 128-bit LPDDR5. The uplift in 1% lows in some titles is impressive though.   [power-efficient-gaming-26.jpg (2133×1200)](https://www.techpowerup.com/review/intel-panther-lake-technical-deep-dive/images/power-efficient-gaming-26.jpg)  And yeah I'd not be surprised if 4 Xe3 is going to be closer to the 8 Xe2 in LNL than people would expect.",Intel,2025-10-10 06:06:48,-4
Intel,njxpw7k,The b580 is a full power desktop card. The one they're advertising here is a tiny little integrated gpu on a laptop processor,Intel,2025-10-17 07:48:12,1
Intel,niqri91,Well it's not out yet so these are just predictions. Better to be conservative with estimates than overpromise.,Intel,2025-10-10 09:08:33,4
Intel,niydl4w,"We already knew they cancelled desktop variants though, Panther lake was going to be around Arrow Lake laptop performance no matter what.",Intel,2025-10-11 15:38:29,-1
Intel,njoe8zc,"If people can get a XX60ti level iGPU in a laptop, which means you don't have to worry about MUX switches (hardware or software), possibly have a cheaper platform (if scaled) since you don't need a separate GPU board, have access to a ton of RAM (many mid-range GPUs are RAM limited), and due to reduced complexity and the ability to better dynamically manage power can get a more effective slim chassis with less throttling... then yeah, it'd be pretty good.  The next big jump is LPDDR6 which looks to potentially hit a 50% bandwidth increase and marginally lower latency.  Combine that with a wide bus and you're looking at enough to power a XX70 series mobile chip.  The framework is immature but it could be competitive in the future.  We'll see!",Intel,2025-10-15 19:51:03,1
Intel,niqbe7s,"Buddy, that's 140V with optimizations. They are telling you what they have done to further optimize their graphics, including drivers and power management. and that they'll further apply this in the future. Did you really look at the slide?",Intel,2025-10-10 06:25:06,10
Intel,niqal4v,"This benchmark has nothing to do with panther lake, it's about the effect of their power management software on lunar lake before and after it is applied.",Intel,2025-10-10 06:17:27,7
Intel,niqaj3v,Isn't that lunar lake before and after?,Intel,2025-10-10 06:16:56,3
Intel,nir70pe,It has around 120GB/s of bandwidth.  Thats pretty close to the 6500XTs bandwidth.,Intel,2025-10-10 11:28:51,2
Intel,nje1eeu,"Dota2 mentioned, I like Intel.",Intel,2025-10-14 03:19:26,1
Intel,nj0ndfp,"this has been leaked a lot, but not confirmed",Intel,2025-10-11 23:11:37,2
Intel,njoo0kf,"Oh laptop gaming sure, but that's pretty low level gaming. Don't most of the people at that level just put up with whatever laptop/prebuilt stuff is available anyway? I really don't think it's going to move the discrete market that much?",Intel,2025-10-15 20:39:56,1
Intel,niqhv6g,I was missing the context. Thanks for the heads up! Yeah that last update for LNL driver was mad good.,Intel,2025-10-10 07:28:09,3
Intel,niqhrao,Oh really? Wow thanks for the heads up. I really missed the context.,Intel,2025-10-10 07:27:02,1
Intel,niqhxw8,Apparently so! I made a mistake.,Intel,2025-10-10 07:28:56,1
Intel,niv48z3,It's 153GB/s.  9.6GT/s x 128 bit width x 8 bit/byte.,Intel,2025-10-11 00:18:22,1
Intel,nh7ov8y,\>HUB  That's shovelware and I'm not clicking that shit.,Intel,2025-10-01 16:46:06,5
Intel,nh35vfm,"They've tested it with just three AMD cpus. Weird pairings. I would pair B580 with any i5 from last couple years. Seriously their testing is getting less and less ""real life"" and more for the purpose of a good clickbait.",Intel,2025-09-30 22:21:40,11
Intel,nh4qct7,"I couldn’t believe the video I was watching, the thumbnail was pure clickbait. They fixed one game, and improved some others a little.  And NEVER an Intel CPU in sight. I like the B580, but it deserves better ""testing"" than this.",Intel,2025-10-01 04:07:28,4
Intel,nh7wmoo,The problem is still here. 9060 xt with 5600 is faster than B580 with 9800x3d and B580 still loses 8% with 5600 instead of 9800x3d. Which means that cpu bottleneck starts much sooner for ARC gpu.,Intel,2025-10-01 17:23:05,1
Intel,nh1pojo,"Is this related to historically Intel compiler not compiling code efficiently for AMD cpus? Does it also make Intel CPUs overhead better? If yes, then they really fixed something. If no, then they wrote code normally for AMD cpus as well as Intel cpus.",Intel,2025-09-30 18:04:03,1
Intel,nh5d94z,"the B580 is a modern gpu, it should be paired with a modern cpu, preferably an Intel one, going with old school Ryzen cpus is a bad path to take.",Intel,2025-10-01 07:33:46,-3
Intel,nh2ket3,"The problem is that this title makes it sound like they fixed the cpu overhead issue in every game, but that's not the case. Some games are still affected. Obviously any fix is better than nothing, but depending on what games you want to play, you will still see an overhead issues. It is nice though that in some of the games the overhead problem has been addressed though.",Intel,2025-09-30 20:32:44,27
Intel,nh2q72d,"Haven't watched the video, but damn.   If rue, HUB has officially become clickbait garbage. Not outright lying, but intentionally conveying misleading half-truths. Starting at least with the AMD fine wine video.",Intel,2025-09-30 21:00:02,17
Intel,nh5salf,"The exact CPU model is hardly relevant. The point was to show CPUs of a certain power level. Not sure how this makes it less ""real life"" as if all three CPUs shown weren't extremely popular for their time. Also not sure what one would achieve when using Intel CPUs specifically.",Intel,2025-10-01 10:08:26,7
Intel,nha4yh9,"These guys absolutely *refuse* to test with an Intel CPU, it's very weird.",Intel,2025-10-02 00:11:44,4
Intel,nh4qb2n,"5700x would have made a lot of sense compared to 5600  12400f,13400f should be included as well",Intel,2025-10-01 04:07:07,2
Intel,nh5omi0,Exactly. Its nice that some games were fixed but between that and calling the issue (entirely) fixed there is a big gap.,Intel,2025-10-01 09:32:35,5
Intel,nh5zjzc,"Indeed and it is something very interesting, because the reason the driver overhead is an issue on 6 core cpu's is because modern games use all 6 cores, so if the driver has a big overhead the CPU has nu free cores available for the driver.  Especially Intel CPU's with more (e-)cores in their budget CPU's should probably suffer a lot less.   I would expect an i5-12400 to suffer the same as a Ryzen 5 5600,  But an i5-13400/14400 has an additional 4-e cores to deal with the overhead. The 13500/14600 even has 8 e cores.      I would very much like to see the comparison   i5-12400, i5-13400, i5-13500, Ryzen 5 5600(X)",Intel,2025-10-01 11:10:33,4
Intel,nh2x17x,Would not have mattered. the cpu bottleneck in the intel driver is single-thread limited and largely due to draw-calls being stored in system memory as a buffer for the frametime render. (at least as far as my debugging could lead me)  (that's also why its less impacted on ddr5 systems no matter what CPU),Intel,2025-09-30 21:34:07,9
Intel,nh2vrkw,"They pushed the 6 core parts too hard as ""good budget parts"" for too long to not keep using them as their reference...  Even when we're seeing games that now have strokes when they don't have 8 cores to play with.",Intel,2025-09-30 21:27:35,3
Intel,nh293fj,"possibly. Honestly weird as hell to not through in intel cpu results with the same gpus so they can see if it's a all cpu overhead issue, or an issue with amd cpus with intel gpu. Like even one game if intel/amd gave about the same performance you could likely rule it out somewhat.  Also weird to say they fixed it if by the sounds of it to say it's fixed if a very limited number of games were updated to work better in also seemingly limited scenarios. That's more like finding a specific problem on a specific game they were able to improve rather than an underlying fix which if implemented should help everywhere.",Intel,2025-09-30 19:38:19,10
Intel,nh2wf2h,"No, B580 also performed poorly on i5-8400 and 10400 (both are 5600X-esq)",Intel,2025-09-30 21:30:55,9
Intel,nh63wlq,"The driver compiler is for GPU, not CPUs.",Intel,2025-10-01 11:42:38,2
Intel,nh63zie,9800X3D is the fastest CPU for gaming on the market. The usage of Intel or AMD CPUs doesn't inherently matter.,Intel,2025-10-01 11:43:11,3
Intel,nh2w6ck,It's been official for awhile. Funny how it takes a positive Intel title for people to realize.,Intel,2025-09-30 21:29:41,14
Intel,nh2wkyc,Do you have peer-review data that disputes the data they presented?  Their video covers the scope of what got fixed.,Intel,2025-09-30 21:31:46,2
Intel,nhcifz7,Welcome to the age of influencers...,Intel,2025-10-02 11:35:19,1
Intel,nlycxb6,"Nvidia 5000 series has overheads issues even with 5800x3d... like 20% performance hit, you can fix it by capping fps or using Intel cpus. AMD CPUs always act weird when reaching 100% utilization.",Intel,2025-10-29 04:12:18,1
Intel,nh63ttf,That's not how CPUs work in games.,Intel,2025-10-01 11:42:06,3
Intel,nh2wpqg,Then they optimized drivers.,Intel,2025-09-30 21:32:27,-1
Intel,nha4uln,Do you have peer-review data that confirms it?,Intel,2025-10-02 00:11:05,0
Intel,nh5t225,The video shows the B580 not having driver overhead issues with the 5600X but we don't know if it did previously on anything other than Spider-Man. His tone suggests the 5600X performance used to be worse but he provides no data other than Spider-Man to suggest it was for other games.,Intel,2025-10-01 10:15:35,1
Intel,nh6os05,The fact that a switch from a 5600 to a 5700 with hardly a frequency gain eliminates the overhead issue suggest that is kind of  how games/drivers work.,Intel,2025-10-01 13:48:12,2
Intel,nh328pe,Which also fixed performance on 5600X,Intel,2025-09-30 22:01:23,7
Intel,nh8rcea,"But that's now how games utilize CPU's.  What you get from games is usually 1 main thread that takes up one big core, then many less intense threads, that may or *may not* utilize the other cores, and to varying degrees. AKA you can have mainthread use 100% of core 0, but cores 1,2,3,4,5 are all 50% used, leaving 50% for other tasks to run freely.",Intel,2025-10-01 19:50:32,2
Intel,nfolcd9,"Eventually you get the drivers where they need to be. People love to whine about Arc drivers but on the iGPU side people never complained. The community is never, ever happy with anything. Focusing their eggs on the Higher performance parts and then a slower LTS cadence keeping the old stuff alive is the right move.",Intel,2025-09-22 23:20:53,21
Intel,nfvr586,"They will update, just not day 1 for certain games. Who's playing with integrated graphics?",Intel,2025-09-24 02:32:50,3
Intel,nfw1d1a,"cant say i blame them for scaling down driver updates for pre-arc graphics hardware, ever since they announced intel was getting into gpus proper ive been wondering how long it would take before this happened.  lets be honest no one was needing monthly igpu updates for 2021 hardware that was so underpowered it couldnt even tie with a GT1030 ^(\[why does everything come back to 14nm\].)",Intel,2025-09-24 03:41:26,3
Intel,nge544o,Running an N97 Mini PC with integrated and noticed the recent driver branching. I'm still impressed that Star Trek Online is playable on it.,Intel,2025-09-26 23:18:44,1
Intel,nfxw4kc,Developers have been complaining about iGPU drivers for years. Matrix multiplications in OpenGL driver are still broken on a lot of integrated chips. It's driving me nuts every time I get a user report. 😭,Intel,2025-09-24 13:04:37,3
Intel,nlw94jc,"me man omg, im on intel iris xe and wanna try out battlefield 6 so bad. Arc Graphic users are having fun with day 1 driver updates.",Intel,2025-10-28 20:57:48,1
Intel,ng1xrn2,Get some ARL CPU’s which have matrix math in hardware.,Intel,2025-09-25 01:38:55,1
Intel,nil3hc6,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Intel,2025-10-09 12:28:52,2
Intel,nimk9vp,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Intel,2025-10-09 17:05:36,2
Intel,njlc3cu,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Intel,2025-10-15 09:13:26,1
Intel,nkp7gzk,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Intel,2025-10-22 00:53:17,1
Intel,nksv7pa,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Intel,2025-10-22 16:17:48,1
Intel,nm887uc,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Intel,2025-10-30 17:38:48,1
Intel,nmj83z4,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Intel,2025-11-01 13:55:37,1
Intel,niosucw,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-10 00:13:23,1
Intel,niowtna,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-10-10 00:37:02,1
Intel,nicof1i,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-08 01:46:19,2
Intel,njq2uvf,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Intel,2025-10-16 01:31:17,1
Intel,nkq87np,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Intel,2025-10-22 04:59:31,1
Intel,nkwr1mw,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Intel,2025-10-23 05:27:23,1
Intel,nkxufey,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Intel,2025-10-23 11:35:54,1
Intel,nmj7ctn,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Intel,2025-11-01 13:51:07,1
Intel,nmsah2r,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Intel,2025-11-02 22:47:21,1
Intel,nktqa25,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Intel,2025-10-22 18:45:49,1
Intel,nmk5tv3,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Intel,2025-11-01 16:57:33,1
Intel,nkvsdop,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Intel,2025-10-23 01:24:29,1
AMD,nmb8bbo,"It's especially bizarre when you realise how many products RDNA2 is present in. Not just discrete GPUs, but current gen consoles, rembrandt APUs in a lot of budget laptops, and Zen 4 and 5 desktop CPUs (though there are probably not many people gaming on these at least). Dropping RDNA1 support is understandable given how few products lines it was used in, but RDNA2 is still extremely relevant on top of not being that old.",hardware,2025-10-31 03:23:00,465
AMD,nmbmak7,"Man the 6000 series was the ONE gen where by god, they competed at the top end and had extra vram all the way down the stack as a way to sell their cards as a longevity thing...   a 3080 vs 6800XT was a tough choice, and if it wasnt for the evga queue I would have likely grabbed either that was for sale for MSRP in the depths of pandemic.  and IIRC it was the most successive gen recently if you looked at steam hw survey  and now its getting axed, while even the 20 series are not...",hardware,2025-10-31 05:14:42,161
AMD,nmb773n,AMD this is not how you retain/gain market share.,hardware,2025-10-31 03:15:11,230
AMD,nmba2tx,Never miss an opportunity to miss an opportunity.,hardware,2025-10-31 03:35:44,275
AMD,nmbb10v,Radeon being really fucking stupid exhibit #1523,hardware,2025-10-31 03:42:40,107
AMD,nmbo5d9,"This hurts. I left my Vega 56 behind prematurely because it didn't have driver support. Now rocking a 6800xt, so this makes two cards in a row for me.  Radeon is supposed to be the budget choice with fine wine driver support.  It's becoming skunk beer instead of fine wine.",hardware,2025-10-31 05:31:52,74
AMD,nmbkdzj,I'snt the 6000 series like their most popular cards on the steam hardware surveys? They wanna lose even harder almao.,hardware,2025-10-31 04:57:33,60
AMD,nmb6r0u,"AMD actively blocked official Zen 3 support on B350/X370 motherboards for over a year (despite working beta BIOSes from Asrock and people successfully cross-flashing B450 BIOSes onto B350 boards), screwing over Zen 1 adopters who bought into AMD in 2017 and to which AMD owes its revival on desktop to. They only relented after Intel released non-K Alder Lake parts. So not too surprising.  *typo",hardware,2025-10-31 03:12:09,135
AMD,nmcgydp,"Puzzling. AMD (unofficially) just got over the hump of their previous cards being inferior from being stuck with FSR3, and now this.   When is RDNA3 moving to the farm, 2027?",hardware,2025-10-31 10:21:03,12
AMD,nmbgrrq,"Ya know the more i think about this the more pissed off I guess.  PC gaming has become increasingly unaffordable in recent years. I'm one of those old school ""60"" buyers. ya know, $200-300 for a GPU, and I plan to use it for around 4-6 years.   Since around 2018, we've been getting squeezed. First it was ray tracing and all that AI upscaling crap. Nvidia decided to make the 2060 $350 and threw anyone under that under the bus. Yeah, the 16 series existed, but let's face it, those cards aged like milk too.   Then COVID happened, the price inflation happened, and I was stuck on a 1060 through the pandemic. It lasted me well, but I knew it was aging, and quite frankly, GPU manufacturers stopped giving a crap about us. THey gave us the 3060 for $330, which was a joke. And this was before the ""inflation"" all the yuppie tech bros on these subs use to justify ridiculous prices.   I managed to hold it together through COVID, and upgraded during the price crash in 2022-2023. AMD was hit first, the RX 6600 type cards could double my 1060's performance for under $300, while Nvidia was STILL charging $340 for the 3060. It was a joke. Nvidia didn't get it, I dodn't wanna pay over $300 for a fricking GPU so I bought a 6650 XT for $230. It was an amazing deal at the time.  When the 7000 series launched around then, and the rest of the series was rolled out into 2023, honestly, it offered a very poor value for the most part. RDNA2 cards were literally sold next to RDNA3 ones for most of RDNA3's lifespan. And quite frankly, there was often little reason to buyt RDNA3, it was more expensive, and offered little in price/performance over RDNA2.  RDNA2 was eventually phased out in the higher price classes, but for a while, the RX 6800 was sold along side the 7700 XT, the 6650 XT was sold next to the 7600, the 6700 XT/6750 XT were in their own niche that AMD never really replaced, and the 6600 remained supreme as the ultimate budget option, competing with the fricking RTX 3050 and thrashing it in value.  heck, I was recommending RX 6600s as recently as this year, and really only stopped when we FINALLY saw the market move with stuff like the RTX 5050 for $250, the RX 9060 XT 8 GB for $270, and the RTX 5060 for $300. Even then, what killed the RX 6600 more than anything was the fact that AMD just FINALLY phased it out of production a few months ago and it was no longer a cost effective recommendation.   And now, AMD pulls the rug from under RDNA2 users? This fast? Really?  I dont care if they axe RDNA1, the writing has been on the wall for RDNA1 for a while, no mesh shaders, RT, DX12 ultimate support, yeah, RDNA1 is done. But RDNA2 is still a viable architecture. Maybe it doesnt support the fancy AI bull#### for FSR4 or whatever. Do you know how much I care? Not at all. Im in a budget class where I dont care if I get the fanciest graphics. I just wanna RUN THE GAMES. And to do that, i need DRIVERS. There's no reason they can just make the games run FSR2/3 and allow older users to use that. Sure, maybe it will be blurrier, I admit FSR is an inferior upscaler, but it works, and again, I'd rather run a game with a little blur going on than not at all.   To not support the series with drivers, when this could render the GPU incompatible with new games is a DISGRACE. I bought my 6650 XT planning to use it until 2027, hoping that my next sub $300 GPU would offer twice the performance and more than 8 GB VRAM. I dont wanna upgrade now, spending $250-300 for 10-50% performance improvements (seriously, the 5050 is barely faster than the 6650 XT in raster) and 8 GB VRAM. I just dont. As such, I guess I'm gonna be stuck using an obsolete and no longer supported card for the mean time. Thanks a lot AMD. And oh, guess what, given that Nvidia is no longer charging like 50% more than AMD for the same class of product, I'm probably not gonna go AMD next time. Not if this is how I'm treated as a consumer.  Ive always been like at least somewhat biased toward Nvidia/intel because it seems like every time I buy AMD I get burned for some reason. SOmetimes its lack of VRAM, poor driver support, support for some BS API, or just poor performance like on older AMD CPUs I've owned. But yeah, they're just kinda giving me flashbacks to the late 2000s/early 2010s again when they didnt support their products while nvidia did. And I'm kinda feeling burned right about now.  EDIT: since the responses im getting seem snarky and trollish so far, you send me that kinda crap, you're getting blocked.",hardware,2025-10-31 04:26:47,95
AMD,nmbhc6y,"It is not uncommon for AMD to have shorter driver support or continued optimization.  for that matter, fine wine was just marketing for, ""we have shitty drivers at launch"".",hardware,2025-10-31 04:31:29,73
AMD,nmbodgw,Wow this is a problem if they continue this trend on the 7000 series.,hardware,2025-10-31 05:34:00,11
AMD,nmbdb63,I guess this confirms project Red Stone will not be coming to RDNA 2 cards (FSR4.0 support),hardware,2025-10-31 03:59:31,16
AMD,nmclxik,What about the RDNA2 APUs like the   * Ryzen 7 7735H with its RDNA2 680M iGPU * Ryzen 7 6800H with its RDNA2 680M iGPU * there are alot more Ryzen APUs in the 6xxx and 7xxxx series with decent RDNA2 iGPUs,hardware,2025-10-31 11:02:16,8
AMD,nmeftdf,"They updated their message about it, ""maintenance mode"" will still get game updates and whatnot, and the USB-c port was a mistake, they've since fixed it...",hardware,2025-10-31 17:03:55,7
AMD,nmbmaar,It’s not the first time. And obliviously not the last time amd do that . Why it become a shocking news ?  For example   Like RX580 . Mega popular and card have less then 4years of active support . Then 3 years of bug fixes   I stil remember amd fans bragging about rdna2  like is better choice then rtx 30 at least in terms of memory .. funny,hardware,2025-10-31 05:14:38,10
AMD,nmblff1,I'll probably be back to Nvidia next year at this rate lol.,hardware,2025-10-31 05:06:52,16
AMD,nmcml22,"Did not know that ""Loyal Radeon Customers"" was a thing.",hardware,2025-10-31 11:07:26,3
AMD,nmdzy81,"I think this is it for me when it comes to my next purchase. I'm very happy with my 6800 and I was planning to stay with AMD, but the fact they cut its lifespan so suddenly is pathetic. Why live with the mediocre feature set at this point.",hardware,2025-10-31 15:46:30,3
AMD,nmbd7vw,this is why I never see Radeon as a serious product.,hardware,2025-10-31 03:58:48,25
AMD,nmcf7uv,Hahahahga.   My 8 year old 2060 is chillin with dlss4 in a media server for super resolution,hardware,2025-10-31 10:05:44,6
AMD,nmeds5z,"Let us retrace our steps to the spirited online debates of 2019, when early RDNA faced off against early RTX.  Early RDNA was pronounced the rational choice for the discerning enthusiast: modestly cheaper, often quicker in traditional rendering, and crucially unencumbered by ""gimmicks"" such as dedicated RT silicon or tensor cores. Ray-tracing was dismissed as an overpriced marketing indulgence, and DLSS was confidently predicted to vanish within the year. The RX 5700XT in particular was held up as an example: less costly than the RTX 2070 Super, marginally faster in raster. And best of all it was destined for superior longevity because the ""silly"" RT/AI gimmicks would surely perish like HairWorks. FPS-per-dollar at the moment of purchase would remain eternal as the only relevant metric of which GPU to buy.  Eternity, it seems, endured roughly until Adrenalin 25.10.2. AMD has now consigned both RDNA1 and RDNA2 to maintenance-only purgatory, with the driver graveyard following soon. Which is baffling considering how many systems and different products use early RDNA hardware. Even the RX 6750GRE finds itself affected, a card launched in late 2023. Imagine the uproar had NVIDIA deprecated the RTX2000+3000 series so quickly.  The RX 5700XT, once lauded for its value, is now remembered mainly for its hardware/software headaches. And when compared to the competition it now also lags in image quality (confined to FSR2/TAA) and framerate (due to increasing presence of RT in games). On the used market it usually trades for almost half the price of an RTX 2070 Super, a card it was supposedly better than in 2019.  Meanwhile the entire RTX 2000 series continues to receive full driver support, the GTX1000 cards that launched in 2016 are only starting to lose full support now. Games grow ever more dependent on RT. DLSS has become the gold standard rendering method and arguably the main reason NVIDIA secured overwhelming market share dominance in the 2020s. And even a 2018 RTX card benefits from the latest revisions and updates for all the newest ""gimmicks.""  AMD's 2025 RDNA4 series with its focus on ""ML accelerators"" and beefy RT units? They're actually pretty good cards (the 9070XT is AMD's best card in years) but they're also a belated concession by AMD that NVIDIA’s RTX blueprint was, in fact, the correct one. And the passage of time now positions early RDNA as a temporary measure, a stopgap until AMD figures out how to get back in the fight.  It remains quietly amusing how utterly wrong the YouTubers and online forums have misread the future about five-six years ago. Architecture/features/support, as it turns out, matter more than raster parity at a 5-10% discount.",hardware,2025-10-31 16:54:04,7
AMD,nmbckem,I'm still happy with my 2060 12gb.,hardware,2025-10-31 03:53:56,16
AMD,nmbdp88,"Radeon division saw the success and good will the cpu/mobo side of amd has gotten for their am4 longevity and support, and went: yeah, let's do the fucking opposite.   This is beyond dumb. Probably switching back to intel/nvidia in the next year or two when i switch gpu",hardware,2025-10-31 04:02:30,16
AMD,nmc736q,"Does driver optimisation really exist? The whole video fails to actually prove that driver updates actually make a huge if any difference they just expect us to take this as a fact. I know I haven't bothered to keep on top of driver updates for every new game releases for years now.   At some point software is actually finished, the desire for constant updates is a weird desire of the community.   Seems people started getting happy again and we need some new thing to be faux upset over?",hardware,2025-10-31 08:46:17,12
AMD,nmbjclj,....so the SteamVR issues with DirectDraw on my 6800XT machine causing horrible lag are never going to be fixed. Sick. That machine literally only does VR and has been running 24.3.1 for months because of that.,hardware,2025-10-31 04:48:31,9
AMD,nmbrsqb,"My friend wants to upgrade his 6600, I was initially recommending the 9060xt 16GB   I've told him to spend the £50 on the 5060Ti 16GB. The saving doesn't make sense when they'd like ~5-8 years for a PC ideally",hardware,2025-10-31 06:07:41,9
AMD,nmcbfzj,"This is why I don't buy AMD cards.  I've been burned before. With a laptop. Had to hack drivers to even work with newer versions of windows after 2 or 3 years. When W10 came out, it was a complete no go because of no drivers from AMD.  Meanwhile, my parents computer, a 4770k with a gt750 is happily chugging along running Windows 11, besides TPM detection hack.",hardware,2025-10-31 09:30:09,8
AMD,nmbbhkm,How did this get past Lisa Su?,hardware,2025-10-31 03:46:01,14
AMD,nmd23wv,AMD never misses a chance to miss a chance,hardware,2025-10-31 12:51:30,3
AMD,nmdu1v6,Oh I guess my 6950xt I bought brand new two years ago is already obsolete 🤡,hardware,2025-10-31 15:18:11,4
AMD,nmcl6mc,"Yep I love AMD cards but this was a bone headed move. Especially because they love to reuse IP in subsequent years like the new base Xbox ally, (ally X uses RDNA 3.5). I think they will have to walk this back or risk losing further customers.  I can see why they have done it. The shift to more complete compute engines means they are dragging along some baggage that cant do all the new things, but this is the bed they've made... Keep it going another two years AMD",hardware,2025-10-31 10:56:16,2
AMD,nmfh8vu,AMD is doing it to itself at this point.,hardware,2025-10-31 20:16:32,2
AMD,nmib274,won't buy amd shit anymore,hardware,2025-11-01 09:26:30,2
AMD,nmcgk53,"Genuine question since I've seen people both defend and criticize this, and I haven't had a chance to watch the video yet. My understanding is that AMD will no longer provide game optimizations for the mentioned cards (5000 and 6000 series), but will continue to provide drivers (""support"") for them. This *feels* fast but is it *unusually* fast? How long do cards usually receive game optimizations? Specifically, how does Nvidia compare?",hardware,2025-10-31 10:17:35,5
AMD,nmc12ro,"my GPU is released 3 years ago man, this sucks.w",hardware,2025-10-31 07:43:43,3
AMD,nmc796g,lucky me for not pulling the trigger and building a 6600 xt budget gaming pc.,hardware,2025-10-31 08:47:57,3
AMD,nmclrh6,"Oh great so the choices are now AMD ""wait we sold you a graphics card lol?"" and Nvidia ""4K60FPS Net* performance! (*30EBITDA frames at 960p)"".  Fucking joy",hardware,2025-10-31 11:00:55,4
AMD,nmcp8cr,Anyone have a summary for those of us who can't watch the video when they are reading this post to figure it out?,hardware,2025-10-31 11:27:46,4
AMD,nmc7dgh,Guess I wont be getting a 9070XT for Christmas 😭 Really wanted to go AMD this time but they don't make it easy,hardware,2025-10-31 08:49:06,2
AMD,nmd3y13,I woke up in my garbage can. Came to reddit to find out why. This is NOT what I expected to have placed me there. ╮(╯▽╰)╭ off to shower the refuse off me!,hardware,2025-10-31 13:01:52,2
AMD,nmc2l58,"Here's my hot take on this and playing devil's advocate.      Everyone is freaking out over nothing.   They quite clearly say 'Bug fixes' and 'maintenance mode' in that response from AMD Germany.      You think nVidia does driver optimization for 2000 and 3000 series cards? fuck no.. Those cards are also in 'maintenance mode'   making sure the game works with the card and bug fix and security patch if needed.      I can see why people are upset, but I also see it as AMD being transparent. Their older cards are in 'maintenance mode' now, which means they are still supported, will still get bug fixes, will still work with your games, but if you want the best experience, buy a newer card.      Nvidia does exactly the same, they just haven't given it a name.      Fight me 😂",hardware,2025-10-31 07:59:14,1
AMD,nmbeqwq,"Outrage is fun, but no.  AMD state they will maintain it for bugs which is all it needs. These card are well understood by developers and game optimization is the ***developer's job*** not AMD's.  AMD might be involved in performance optimizations with new cards is when developers aren't familiar with them and new shaders (often based on old shaders) are found to not work well with very different architectures but that's not happening with cards designed in 2017.  There is no dropping of driver support. These are mature cards, known ISAs, they are already optimized for in current game engines.  If a developer is optimizing for RDNA2 and finds a bug AMD will still fix it. If AMD wants a new feature to run on these cards they will optimize for it. If there is a new DX12 or Vulkan extension AMD will add it. If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  We went through this outage cycle when AMD [split the driver stack for Vega back in 2023](https://www.tomshardware.com/pc-components/gpus/its-curtains-for-polaris-and-vega-as-amd-reduces-driver-support) and this really feels like a repeat.     EDIT: I was, of course, right: [https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games)",hardware,2025-10-31 04:10:38,-5
AMD,nmfjvrm,Hardware unboxed farming ragebait now?,hardware,2025-10-31 20:30:39,2
AMD,nmcdhb0,Loyalty is a one way street.,hardware,2025-10-31 09:49:57,1
AMD,nmd4gpk,"RDNA 1 and RDNA 2, no AI not relevant.  Since RTX 20, yes AI yes relevant.  Nvidia started earlier with AI = more supported GPUs.  AMD started late with AI = less supported GPUs  No AI = wasting resources.  AI is where most of the resources will go, this is where the world is going. It is not AMD or Nvidia, it is where the world is going.  Which is why GTX GPUs like Maxwell and Pascal switching to maintenance mode just like AMD is doing with RDNA 1 and RDNA 2.  It doesn't matter who started earlier(RTX 20 or RDNA 3), the base line from now is: Does your GPU has ML capabilities or not...  You can hate it or love it...",hardware,2025-10-31 13:04:51,1
AMD,nmhfez5,"I think this is shitty. Not sure how it makes sense to them. But it looks like it won't effect linux users at least. I guess if you needed a reason to switch from windows 10 as a 5000/6000 card user, here it is...",hardware,2025-11-01 03:55:42,1
AMD,nmkb4xa,"Rip amd resale value. All the people that meme’d about 4090/5090 prices are gonna look real dumb when they try to sell their outdated rdna1/2 cards lmao, meanwhile 5090 and 4090 users can sell their cards at almost msrp prices.",hardware,2025-11-01 17:24:21,1
AMD,nmkddp5,Rest in Piss AMD. Intel or Nvidia next time,hardware,2025-11-01 17:35:53,1
AMD,nmbzizk,I don’t like these clickbaity thumbnails.,hardware,2025-10-31 07:27:14,1
AMD,nmdpzh9,"I don't really get the fuss.  It's okay to phase out support for older hardware. Yeah, RDNA 2 was still very wide-spead, but still.",hardware,2025-10-31 14:57:56,1
AMD,nmeeyax,Thank you HUB for continuing to look out for consumers!,hardware,2025-10-31 16:59:44,1
AMD,nmgup6g,They already clarified: they do not.  https://videocardz.com/newz/amd-clarifies-rdna1-and-rdna2-will-continue-receiving-game-optimizations-based-on-market-needs,hardware,2025-11-01 01:25:33,1
AMD,nmbd1ss,"This is being blown hugely out of proportion. People keep pretending like they dropped total support. No, they just dropped day one game dev driver support.",hardware,2025-10-31 03:57:33,-15
AMD,nmbpjej,"I just bought a brand new Zotac Zone 3 months ago, and now it'll no longer be receiving new game optimisations already.",hardware,2025-10-31 05:45:15,1
AMD,nmc4bj6,ai money > game money....   help us intel your our only hope!,hardware,2025-10-31 08:17:26,1
AMD,nmcp4ho,Unfortunately AMD product support has always been shit. AM4 was the exception to their history not the rule.,hardware,2025-10-31 11:26:57,1
AMD,nmcwt70,"I game on a MSI Claw 8 AI+, I don’t think this affects me",hardware,2025-10-31 12:19:12,1
AMD,nmcxf2j,Is there a petition? Something like amd give us back the fine wine?,hardware,2025-10-31 12:23:00,1
AMD,nmd4i8n,Can you RMA your GPU for this reason?,hardware,2025-10-31 13:05:05,1
AMD,nmd7sss,I was thinking about getting a 9060XT rig for some 1080p gaming but nah fuck AMD going with Nvidia after all.,hardware,2025-10-31 13:23:26,1
AMD,nmhvmjv,But then they don't and the insufferable have to walk back their fake outrage.   Yawn.,hardware,2025-11-01 06:33:14,1
AMD,nmbsvru,On the other hand Nvidia drivers are still borked on my 1070 so I have to use 566.36.,hardware,2025-10-31 06:18:42,-1
AMD,nmc9k6z,What a pathetic decision my AMD. I might even go back to Nvidia now. They are a little more expensive but their cards are supported much longer.  AMD... F U,hardware,2025-10-31 09:11:07,0
AMD,nmcn02w,"Another windows related problem I'm glad I got rid off. I don't understand why an OS with such high market share gets dumped on so often by every manufacturer.  AMD and Intel GPUs run on open source drivers on Linux. Even all Nvidia RTX (GTX 16xx too) are getting more love on Linux nowadays.  Features and general improvements are still coming for Vega which I ran until recently. The oldest supported AMD GPUs on Linux are from the HD79xx series (real, not rebranded chips) from around 2012-2013 if I remember.   By supported I meant useful for daily use and normal consumers.  Meaning that all GPUs stated in the video are still completely fine, same goes for the USB-C output of the RX79xx.  In my eyes that's all planned obsolescence which here is irrelevant because of open source drivers. The corporations but also hundreds of free developers take part in. Should a corpo bail out then you still have the massive community doing its best to keep your ""old"" hardware out the landfill. Often engineers and devs from said corpos still help in their free time after said corpos ""pulls the plug"".  Thanks to the suits shitting on customers as always.",hardware,2025-10-31 11:10:41,0
AMD,nmcerx2,"Doesn't seem like it's that big of a deal, I have a 6900 and I got it like 4 or 5 years ago, it's probably time to upgrade.",hardware,2025-10-31 10:01:46,-2
AMD,nmec055,"I'm ashamed to have built lots of PCs for friends and family over the last few years, enthusiastically defending the decision to go with RDNA 2. This makes it hard to argue that I can trust them going forward. I really tried, but this will force me back to NVDA.",hardware,2025-10-31 16:45:26,0
AMD,nmbyyuq,Thats what you get for going AMD I guess💩 There is a reason why they only have 8% market share despite selling a lower prices.,hardware,2025-10-31 07:21:16,-4
AMD,nmce61m,is sad hardware unboxed chosed the way of click-bait and disinformation AMD state they still maintaining it rdna1 and 2 but just not get the day one game patchs and again rdna 1 and 2 lack features that the new vulkan and new games use...,hardware,2025-10-31 09:56:16,-3
AMD,nmd9ht5,"Well when they did support their gpus better with constant performance improvements in the past when even the features were pretty similar to Nvidia's (before RTX), they were still losing market share. you can only do so much when nothing you do really matters. plus looking at steam charts, doesn't look like anyone bought these much. so why is anyone even shocked lol.",hardware,2025-10-31 13:32:49,-1
AMD,nmcv2wa,that really sucks. I think they still have life in them,hardware,2025-10-31 12:08:11,0
AMD,nmbk5av,Hardware Unboxed never cared when AMD almost didn't support Ryzen 5000 on X370 but now they're furious. Too funny.,hardware,2025-10-31 04:55:25,-24
AMD,nmbuki4,Well I guess my next GPU would be from Greedvidia,hardware,2025-10-31 06:35:55,-11
AMD,nmb9rby,"HBU pointed out that Asus ROG Xbox Ally, a console that launched ~2 weeks ago, is RDNA2!!",hardware,2025-10-31 03:33:23,346
AMD,nmb8y26,I mean they dropped VEGA non-security support within the same year new APUs with VEGA were released. I guess this is one way to force AMD laptops to buy new chips,hardware,2025-10-31 03:27:30,97
AMD,nmbduty,I only bought my RX 6750 XT in 2023. 😭,hardware,2025-10-31 04:03:42,43
AMD,nmbkgyx,Don't forget Steam Deck.,hardware,2025-10-31 04:58:16,7
AMD,nme279s,"It's not really bizarre when you realize that AMD isn't ""dropping support"" and that this is all misinformation.",hardware,2025-10-31 15:57:14,1
AMD,nmeg1ig,"Yeah, people are blowing this out of proportion, Amd stated maintenance mode will still get game updates and other things, just no new features. And the USB-c thing was a mistake on their part, they've fixed it.  Their announcement was poorly worded, all's okay now.",hardware,2025-10-31 17:05:03,1
AMD,nmei7ni,">and now its getting axed, while even the 20 series are not...  Even the 10 series have been supported up to this month.",hardware,2025-10-31 17:15:44,4
AMD,nmc9o61,"Wasn't a tough choice at all, this is a great example of the difference between the two companies. Not a told you so, but also... It was at no point in contention based on all the history and the market / software positioning at the time. Just look at the posts in this subreddit back then: ""there are no dlss games."" I got chewed out here for saying there will be. The only thing incorrect in that response is how much further ai frameworks and hardware went than ""merely"" the best performing anti aliasing. My 3080 feels no age, and it's now joined by lots of 3090s in a cluster serving production workloads that my customers value highly. Ati has no appropriately valued in terms of effort answer.",hardware,2025-10-31 09:12:17,30
AMD,nmc5bmr,"Without driver side assistance that 6800xt will likely reach parity with a 3070, when the dust clears.  Pretty sure the 6950xt is only slightly faster than a 3080 for the same reasons now.",hardware,2025-10-31 08:28:04,4
AMD,nmc2mjl,"I went from a 6800xt to a 4090, and sure the 4090 is way way more powerful, but it made me realize that the 6800xt was not bad at all. especially if you undervolted that sucker.  It ran at higher clockspeed, ran cooler and you couldnt hear the fan. the performance in 4k gaming was actually decent.",hardware,2025-10-31 07:59:37,3
AMD,nmf3atc,"They nailed that gen and got almost no market share out of it, learned all the wrong lessons as a result.  They always go hard on the gens with new console releases, which i suspect is why they’re pulling resources from RDNA now.",hardware,2025-10-31 19:02:38,1
AMD,nmvj2c2,a 3080 vs a 6800xt was an easy choice.  3080 was much  faster in all the ways that mattered.,hardware,2025-11-03 13:26:35,1
AMD,nmbc4ma,Anyone owning a 7900XTX should plan on selling theirs within the next 2 years before it loses complete value,hardware,2025-10-31 03:50:42,86
AMD,nmcanv3,"These news also come hot on the heels of FSR4 being exclusive to 9000 series, while nVidia cards allows Transformer even on 2000 ~~Ampere~~ Turing series(Non Optical Flow FG is still an open question though). Intel upgraded their Xe with support for Alchemist.  Why would anyone invest in AMD card? Sure nVidia might have higher barrier for entry, but these cards have more features and you can keep them for longer. Not to mention they lose less of their value on the second hand market if you want to sell. And on the other end of the price spectrum we have Intel, sure it's rough right now, but at least they are trying and improving. Although, that being said, you should never buy based on promises.",hardware,2025-10-31 09:22:15,13
AMD,nmb8lvl,This is exactly how there marketing team expects them to retain market share and increase it. Stir the zealots up by getting everyone to trash talk your company. That way the Zealots will go buy the newest product to prove the trash talkers wrong.   I mean what could go wrong with such a solid straight.,hardware,2025-10-31 03:25:06,-12
AMD,nmcnv34,"you're not the market they're after,  you never were.",hardware,2025-10-31 11:17:20,-1
AMD,nmbbqk7,AMD back to doing AMD things.,hardware,2025-10-31 03:47:51,129
AMD,nmcj0nb,"At first I thought it was only a hyperbole that meant to be a joke, but the longer I follow tech news, the truer it is.",hardware,2025-10-31 10:38:47,9
AMD,nmbqc2g,"They've not had a good track record in this for ages. GCN gets dropped 4 years earlier than Maxwell, Vega gets axed prematurely, and now RDNA 1/2 gets the boot, probably because they're refocusing on the ML neural processing stuff with Redstone, and these legacy products were just a stopgap when Nvidia took the RTX path.",hardware,2025-10-31 05:53:02,32
AMD,nmdgc8k,"I hate the Green team to an extent, but realistically they're the only good option on the market right now. After loving ATi for years, my first GPU I absolutely loved which was an HD 5670 and then later on the HD 7970 GHz and the R9 290X. But after all those great competitive years, came the RX 480 and Vega. Really Vega was the straw that broke my back, they completely fumbled the launch of it, it was late to market, used lots of power at the time relative to the competition and slow compared to the 1080 Ti, with only 8GB of VRAM and then on it's way out the door like you said... they dropped support of Vega early and to be honest the Vega and RDNA1 drivers were terrible for months. Honestly, it just turned me off AMD for good. Rather than converting customers and keeping loyal fans they turned me away.",hardware,2025-10-31 14:08:50,4
AMD,nme6ddg,"Dude, same! I went from a Vega 56 to a 6800xt. I was planning on getting a 9070xt soon but I guess I'm going team green now? 🙃",hardware,2025-10-31 16:17:34,1
AMD,nmc1h6e,Vega is still getting driver support.,hardware,2025-10-31 07:47:50,-11
AMD,nme5h3c,"You shouldn't have left your Vega behind. You were sold a lie by influencers, and you're being sold another.",hardware,2025-10-31 16:13:09,0
AMD,nmc7pgo,"The most popular AMD cards. Without them, they are crushed by NVIDIA",hardware,2025-10-31 08:52:26,46
AMD,nme56k8,The Steam hardware surveys are totally manipulated and unreliable.,hardware,2025-10-31 16:11:43,-2
AMD,nmbf2fk,"It wasn't even that nice, msi just dropped the updates for all their boards and more or less told AMD they could drop them as a board partner or shut up",hardware,2025-10-31 04:13:07,113
AMD,nmf6k3z,"Reminder that they were also going to block support for 400-series chipsets.  > Q: What about (X pre-500 Series chipset)?  > A: AMD has no plans to introduce “Zen 3” architecture support for older chipsets. While we wish could enable full support for every processor on every chipset, the flash memory chips that store BIOS settings and support have capacity limitations. Given these limitations, and the unprecedented longevity of the AM4 socket, there will inevitably be a time and place where a transition to free up space is necessary—the AMD 500 Series chipsets are that time.  https://web.archive.org/web/20200528020249/https://community.amd.com/community/gaming/blog/2020/05/07/the-exciting-future-of-amd-socket-am4  Link is to the archive, the original post was either deleted or they changed how the community post urls work.  Funny that it magically ended up being a non-issue for 400-series chipsets, but 300-series were impossible. Then later, 300-series magically were able to work.",hardware,2025-10-31 19:19:42,10
AMD,nmbf5ml,I literally bought a 12600k out of spite because of this only for my X370 Crosshair to get support for the 5000 series.,hardware,2025-10-31 04:13:49,25
AMD,nmbk713,You stole my comment.,hardware,2025-10-31 04:55:50,3
AMD,nmdp5jh,">AMD actively blocked official Zen 3 support on B350/X370 motherboards for over a year  That was due to the size of the flash storage for the UEFI/BIOS. They didn't want to put out an update that broke a bunch of systems but in doing so, made people upset about the lack of an upgrade path.",hardware,2025-10-31 14:53:47,-3
AMD,nmbkpu7,"It was a bit of a clusterfuck though, as the amount of storage space available for the BIOS varied between motherboards (which is *why* they originally didn't want to support the old motherboards).",hardware,2025-10-31 05:00:28,-16
AMD,nmd8hdc,"> actively blocked  > relented  I hate how this gets dramatized and hated it at the time, too. They didn't want to have official support for something that would literally blow up in their faces. Once they were sure it was broadly fine, it was fine.",hardware,2025-10-31 13:27:12,-6
AMD,nmi6iby,Probably,hardware,2025-11-01 08:35:54,1
AMD,nmbxqto,"> To not support the series with drivers, when this could render the GPU incompatible with new games is a DISGRACE. I bought my 6650 XT planning to use it until 2027, hoping that my next sub $300 GPU would offer twice the performance and more than 8 GB VRAM. I dont wanna upgrade now, spending $250-300 for 10-50% performance improvements (seriously, the 5050 is barely faster than the 6650 XT in raster) and 8 GB VRAM. I just dont. As such, I guess I'm gonna be stuck using an obsolete and no longer supported card for the mean time.  To be clear, AMD is still providing drivers -- they're just not providing new game-specific driver-level optimizations.  I'm not sure how much this actually matters for older cards, since engine support for RDNA 2 and earlier should be pretty dialed-in to the point where driver-level optimizations shouldn't be as necessary. Not supporting new Vulkan extensions could potentially be problematic, but I'm also not sure it matters if these extensions are meant to support hardware that RDNA 2 cards (and earlier) don't have.  [Vega and Polaris are also in this ""extended support"" phase](https://www.reddit.com/r/Amd/comments/17qn9e3/amd_begins_polaris_and_vega_gpu_retirement/), and looking at the driver pages for a Vega and Polaris card (I chose the Radeon VII and the Radeon RX 480), their latest driver update was in August 2025, which isn't that long ago. I would be curious to see if owners of these older cards have seen any degraded game experiences beyond just the expected aging.",hardware,2025-10-31 07:08:29,44
AMD,nmcb3i9,"I feel like this is a bit disingenuous. I hate how expensive graphics cards are as much as the next guy and these business practices are definitely fucked up.   But paying 350 € for a 9060 XT 16 GB in 2025 is like paying 270 € in 2015. That's a crazy good deal for this type of performance, no? What even is a comparable card from 2015 for that price?   Inflation is a real thing. I can't think of many other products with this much performance increase over any specific time period that doesn't shoot through the roof in pricing.  Edit: OP is the kinda pathetic user that blocks people just because they offer a nuanced take in a discussion forum. I hope someone reports them because who needs that kinda behavior on this sub? And of course they had to get another snarky reply in before blocking.",hardware,2025-10-31 09:26:38,18
AMD,nmdwi98,"Am also using a 6600 it’s kinda of a bummer but I’m happy so long as it actually still gets some form of support. It’s frustrating because afaik, this is the most popular generation. In my country, ts is still sold at $240 brand new and the next best choice is the intel b550 which is sold at $340 so upgrading is barely even a choice.",hardware,2025-10-31 15:29:57,1
AMD,nmbm1k1,So you're saying you should've bought a 3060.,hardware,2025-10-31 05:12:25,-2
AMD,nmblajp,"You have to wait until 2028 since you're a 2nd hand & sub $300 buyer to get 2x 6650xt perf. Nvidia/amd owe me 4080 perf at $300 because 2060 matched 1080. ""I wont upgrade until that happens, until that happens, you lose ngreedia""",hardware,2025-10-31 05:05:38,-13
AMD,nmenmrb,It's called capitalism.,hardware,2025-10-31 17:42:52,0
AMD,nmbwm9i,afaik fine wine was a term made by up by us not amd.,hardware,2025-10-31 06:56:51,55
AMD,nmbkbib,Reddit: nah it was totally untapped performance.,hardware,2025-10-31 04:56:55,43
AMD,nmkm4ow,"Fine wine definitely was a thing back with the GCN architecture. Part of GPU drivers is bug fixes, but the whole game ready part is Nvidia/AMD optimizing games for the developer. Back with GCN the AMD architecture was a bit more complex and developers used it poorly. Developers did stupid things like tessellating concrete barriers and the surface of the water to hell and back. So over time you would have real gains and the AMD cards would have a better lifespan than their Nvidia counterparts. The RX 580 was phenomenal.  I would call the fine wine back in the day more shitty developers than shitty drivers.  Fine wine ended with RDNA, it was much simpler to optimize and all over the consoles. That means RX 5000 series going forward don't get those same bumps over time and we've seen it.  AMD fans repeating fine wine these days are talking about a golden age of Radeon that has long since ended without ever having experienced it themselves.  The better aging has been on the Nvidia side with things like DLSS squeezing more performance out of cards. Framegen is going to be a godsend for the 40-series and 50-series in 4-5 years.",hardware,2025-11-01 18:20:35,2
AMD,nmbjq5i,"Even if that were true, and the drivers are just shit at launch, you pay for the performance you get at launch, and any additional performance that comes from future driver improvements is free performance you didn't pay for.",hardware,2025-10-31 04:51:47,1
AMD,nmcclco,">  fine wine was just marketing for, ""we have shitty drivers at launch"".  Sure, but for the consumer it meant they were getting a more beefy card than they paid for... eventually.  For some that works out. For others it does not.",hardware,2025-10-31 09:41:34,-1
AMD,nmcptnj,> It is not uncommon for AMD to have shorter driver support or continued optimization.  [AMD only put Polaris on a slow-track after 8 years on the market.](https://www.techpowerup.com/315547/amd-puts-radeon-vega-and-polaris-gpus-on-a-slower-driver-update-track),hardware,2025-10-31 11:32:12,-1
AMD,nmbv1o7,"I'd wager they lose game optimisation before the RTX 40 series does, especially as they lack dedicated hardware for ML upscaling (FSR4 hardware acceleration).",hardware,2025-10-31 06:40:43,13
AMD,nmf7bf4,"If UDNA ends up being as much of a change as gets suggested, then it will absolutely happen.",hardware,2025-10-31 19:23:42,1
AMD,nmdqgvv,It's only ended game specific optimizations. Other updates will still come but they don't promise anything from this point forward. So we have no idea.,hardware,2025-10-31 15:00:21,1
AMD,nmcuko2,Current gen AM5 9000 series Ryzens still have RDNA2 iGPUs lol,hardware,2025-10-31 12:04:50,12
AMD,nmguuve,"Yeah both ""issues"" weren't actually issues, but miscommunication on their part. They already apologized.",hardware,2025-11-01 01:26:39,2
AMD,nmfa206,"You could also not buy new cards and play older games instead. NV comes with its own baggage and I don’t think it’s better than AMD, they are pretty much similarly bad towards their consumers.",hardware,2025-10-31 19:38:16,1
AMD,nmbfkje,*This* is? For how long have you known this was going to happen? And why didn't you tell anyone?,hardware,2025-10-31 04:17:05,4
AMD,nmpr61c,Same with my 2080 ti,hardware,2025-11-02 15:17:19,1
AMD,nmdxsfx,Only with hacks the 20xx series only officially Supports dlss1 and 2,hardware,2025-10-31 15:36:09,-3
AMD,nmibgbo,"2060 super outlived 5700xt, lmao it was selling at the same price",hardware,2025-11-01 09:30:49,2
AMD,nmvlegx,It is unbelievable just how wrong this sorry narrative is. But people lap this garbage up and updoot it into places it doesn't belong.,hardware,2025-11-03 13:40:30,1
AMD,nmbo8xv,Funny how a laptop with a 2060 will have longer GPU support than a laptop with a RX6800S,hardware,2025-10-31 05:32:47,17
AMD,nmdglv7,AM4 look bad now compared to current CPU. Will admit it was good during its time when Intel was weaker. At least the CPU side of things are more sane and with Intel coming up soon excited ag the possibility of longer drivers with team blue and green combined.,hardware,2025-10-31 14:10:13,1
AMD,nmbep10,Why?,hardware,2025-10-31 04:10:14,-3
AMD,nme3j45,"No, you're falling for a blatant lie, because that's the only way anyone wants to talk about AMD anymore.",hardware,2025-10-31 16:03:40,-2
AMD,nmch202,"Seeing what is sometimes output by vendor shader compilers, yes, driver optimization does exist and AMD’s drivers are definitely not finished on that front.",hardware,2025-10-31 10:21:56,6
AMD,nmcwxvu,"> Does driver optimisation really exist?  Depends I guess. Nvidia seem to do a lot of it, not sure about AMD.  Nvidia have always said many developers don't have a clue what they're doing and there are always a lot of optimisations required. Given the complexity of modern APIs there nearly always seems to be giant - potentially game breaking - bugs that need fixed too.",hardware,2025-10-31 12:20:02,3
AMD,nmd6rqs,"While you are correct... The problem is that the driver is not gonna be updated from here onwards, so no new Vulkan extensions, and DX12 features.  Those GPUs could very well shit themselves in a couple of years with any random game.",hardware,2025-10-31 13:17:44,2
AMD,nmcfc5i,"At first glance you'd think so:  https://www.youtube.com/watch?v=aWfMibZ8t00  But there is a catch, HWU did not express themselves clearly, and that was launch review data vs fresh, not fresh vs fresh, and other youtubers stirred a bit of drama, because they couldn't replicate the gains by testing just the drivers. For example:  https://www.youtube.com/watch?v=hf1q1nwoj8k  I suspect that the primary reason for rushing game ready drivers is to narrow the range of drivers being used, to make for much easier QA.  Personally I only update when there is promising feature or bugfix to be added, as in last few years nvidia tended to break a lot. This is where I would see issue with slowed down/maintenance drivers. In AMD case people have expectation of FSR feature update for older GPUs and this might mean them not getting it on older hardware if that happens. That'd be a good reason to be upset.",hardware,2025-10-31 10:06:48,1
AMD,nmbk20n,You realize how old DirectDraw is right? I'd expect whatever game you're running in VR that uses that would need a graphics wrapper to perform correctly in Windows Vista and newer (even with Nvidia graphics cards).,hardware,2025-10-31 04:54:39,6
AMD,nme4xde,"What. DirectDraw is basically deprecated since the Windows 7 era, nobody works on or cares about it anymore.",hardware,2025-10-31 16:10:27,1
AMD,nme5yj2,Sad to see people openly screw their friends like this.,hardware,2025-10-31 16:15:32,1
AMD,nmcntxh,Was it the laptop thing that requires the OEM drivers? Even Nvidia does that.,hardware,2025-10-31 11:17:06,3
AMD,nmbpny0,It probably didn't. Shes basking in the glory of the OpenAI deal right now and could give less of a fuck about gamers.,hardware,2025-10-31 05:46:28,63
AMD,nmbu8t6,"Lisa Su is not a gamers friend, she follows the money and has shareholders interests at heart, not gamers.",hardware,2025-10-31 06:32:34,33
AMD,nmbug03,This is completely on brand to the way AMD graphics division is run under Lisa.,hardware,2025-10-31 06:34:38,31
AMD,nmfbho9,She is the one diverting resources to AI lmao,hardware,2025-10-31 19:45:58,2
AMD,nmcm09t,"Seriously? She *is* part of the problem at AMD, but no one can publicly admit it, my guess is predominantly due to identity politics. While she's done good things (or employees under her have, ie: Jim Keller), which include winning contract bids for consoles, the majority (if not the entirety) of the company's success via Ryzen and EPYC, derive from Zen (re: Keller).  She's like a modern day Steve Ballmer, a la Tim Cook; maintaining the status quo, comfortable and complacent, completely devoid of innovation, vision, or the courage to make bold moves which entail risk, often due to fear or cowardice.  Who do you think hired Jack Huynh, the ruthless businessman running Radeon (incl the guy before him, or the guy before that guy, Raja Koduri)? AMD's marketing is beyond embarrassing; they're simply pathetic. Robert Hallock was the only bright spot and he left in order to literally work for their biggest competitor.  You don't think she signs off on (or oversees) key business decisions like pricing, FSR, RT? What about the bumbling launch of Zen 5? Or how they asked influencers on how to price RDNA 4 and for months, the community basically *begged* them to not fuck up another golden opportunity (don't be or pull an AMD), and somehow they were still planning to launch w/ an MSRP of > $700 (\~$750, IIRC), up until the announcement and had to cancel the presentation hours before *during* CES.  Then, after all that, the BEST they managed to do was copy Nvidia (-$50) by listing a fake MSRP, at least in the US, then lied about availability when claiming inventory issues were solely due to demand despite having warehouses full of cards for months from the delayed launch which was due to waiting for Nvidia pricing + selling ALL RDNA 3 inventory to milk RDNA 4 pricing (and not due to the expiring rebates they weren't going to provide retailers or the fact that AIBs would only sell models > $2-300 above MSRP).    Meanwhile, you didn't think it was fishy that  coincidentally they just so happened to conveniently fail to manufacture Reference cards for the first time since I can even remember, meaning that RDNA 3/2/1, Vega, and Polaris ALL had reference cards, so at min dating back at least > a decade)?",hardware,2025-10-31 11:02:53,4
AMD,nme8lzj,No. It's not going to be any worse than it has been up to this point.,hardware,2025-10-31 16:28:44,1
AMD,nme7nha,"This is about how any GPU support works. Nvidia is no different. They will *publicly claim* otherwise for internet clout, but the outcome is the same. Never mind that Nvidia drivers in general are in a really bad spot right now, and apparently this will continue for the foreseeable future.  Best of all, this doesn't apply on Linux! AMD cards always get *everything* that card can *possibly* support, whether AMD is willing to do it or not, and even when it's almost unreasonable to do so. Meanwhile, Nvidia cards have to deal with whatever the official support is.",hardware,2025-10-31 16:23:58,4
AMD,nme8523,"It's more shitty clickbait misinformation around AMD actually daring to tell the truth about how GPU support works. This video exists *solely* to feed AMD haters some slop, nothing more.",hardware,2025-10-31 16:26:23,2
AMD,nmhqtrn,"If i could go back in time i would've got a 5070ti, i wouldn't have liked it, but it would've been the better pick",hardware,2025-11-01 05:41:02,1
AMD,nmctuz5,"Nvidia do game specific driver optimization at least for the 3000 series, it does make a difference. Also, I got DLSS4 and DLSS override in their software for my 3080. When I had a Vega 56, AMD even broke older features in their drivers.",hardware,2025-10-31 12:00:06,8
AMD,nmcx97i,"> You think nVidia does driver optimization for 2000 and 3000 series cards? fuck no.. Those cards are also in 'maintenance mode'  Sounds like a hot scoop, you'd think someone would have reported this",hardware,2025-10-31 12:21:59,6
AMD,nmchils,another example of why you should never ever be transparent to an average consumer,hardware,2025-10-31 10:25:54,7
AMD,nmd5yx4,>You think nVidia does driver optimization for 2000 and 3000 series cards? fuck no.  Nvidia just stop supporting 1000 series card for game ready drivers and they will only push security stuff... we are talking about GPUs that are 9 years old at this point.,hardware,2025-10-31 13:13:15,3
AMD,nmijx1x,"""You think nVidia does driver optimization for 2000 and 3000 series cards? fuck no.. Those cards are also in 'maintenance mode'""     This is just wrong completely. 20 series are still on a the mainline driver branch as of today. AMDs maintenance drivers are separate like for vega which doesn't get the latest version.",hardware,2025-11-01 10:57:00,1
AMD,nmbon3e,">game optimization is the developer's job not AMD's  In a perfect world yes, but IMO the past decade has shown us how many devs are overwhelmed with properly and correctly utilizing DirectX 12, and in a way that does *not* cause shader compilation stutters. It's what led to awkward situations where DirectX 11 had or still has better performance / less or no stutters (e.g. see Vermintide 2 with shader stutters in DirectX 12 mode, or various Unreal Engine games from the past years)",hardware,2025-10-31 05:36:35,24
AMD,nmbhhje,"""Critical security and bug fixes"" maintenance usually means on softwares that the EOL is approaching and support is kept on a thin lifeline ( not substancial )   That is a common enough catchphrase to raise the redflag, not sure why we have to split hairs here.  Wether the cards are mature or not is out of the question when new games are continuously added, and drivers get updated, it's not like those didn't matter last month ...",hardware,2025-10-31 04:32:45,53
AMD,nmc5wyf,"> If there is a new DX12 or Vulkan extension AMD will add it.  This is precisely the opposite of what *is already happening* with Vulkan extensions w.r.t RDNA2 support.  > If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  Luckily for Steam Deck owners, they use a driver primarily developed and maintained by Valve (or people paid by Valve), not AMD.",hardware,2025-10-31 08:34:15,13
AMD,nmbrxh6,">AMD state they will maintain it for bugs which is all it needs.  That's not what they do. They freeze development by branching the code bases off. This effectively means they no longer do development on the legacy branch.   Even in theory fixing bugs in old unmaintained code is difficult because it stops looking like the cold you're working on on a  regular basis. But in practice they basically never fix bugs outside of the most severe and most basic. Crash fixes to extremely  basic desktop use.  But at a broader general level your statement is incorrect. GPU drivers need constant development because the targets for development constantly change.   A new Vulkan version is imminent (announced by Mesa devs very recently), and RDNA 1 and 2 cards won't get it even though they're fully capable. New DX versions are released  frequently as well, again same issue.  Game development changes and they start using these newer APIs, or they use existing but barely or never used features, triggering issues in drivers due to lack of testing. This happens quite frequently. These GPUs are only a few years old , some 5 years max.   >If a developer is optimizing for RDNA2 and finds a bug AMD will still fix it.   They won't. They don't do any of this in the legacy branch for Vega.  >If AMD wants a new feature to run on these cards they will optimize for it. If there is a new DX12 or Vulkan extension AMD will add it. If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  They've never done this for the legacy Vega branch. It's stuck at exactly the Vulkan and DX API when they froze development, even though the hardware is far more capable and is fully Vulkan 1.4 compatible along with a long list of other extensions AMD never added.   On Linux the development is partially done by the community and Valve at the kernel level,  and at the usermode level (for Vulkan) it's entirely by the Mesa community and Valve, with no AMD involvement.   As you can guess the driver support on Linux is light-years ahead. Everything from newest rdna4 to GCN 1.0 is still fully supported and updated with constant Vulkan extensions and full scale development broadly at all levels of the driver stack.   Yes this applies to all sorts of GCN hardware because they're extremely capable still. You can do shader emulated ray tracing on Vega and RDNA1 with playable performance.on Linux. There are YouTube videos of people showing Indiana Jones off running on this hardware.  If they were good stewards of the legacy branch you might have a point but they very literally branch it off and freeze it and basically never touch it again.",hardware,2025-10-31 06:09:02,35
AMD,nmctf3k,"> If AMD wants a new feature to run on these cards they will optimize for it. If there is a new DX12 or Vulkan extension AMD will add it. If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  The announcement said literally the opposite. Do you think just saying the opposite of reality is gonna make it true?",hardware,2025-10-31 11:57:10,8
AMD,nmbk5tm,"I think the main problem is no new game support like Nvidia/Intel does with their Game Ready/Game On drivers, which means as time goes on the performance of RDNA2 relatively will fall further and further behind Nvidia's or even AMD's own RDNA3. RDNA4 gpus in newer games  >These card are well understood by developers and game optimization is the ***developer's job*** not AMD's.  this should be true in an ideal world but we know a lot of game are unoptimized mess and need launch drivers to not run like shit",hardware,2025-10-31 04:55:33,21
AMD,nmbg13i,"It’s really weird how they manage which series to support, like didn’t Polaris JUST got into the same split driver this year?",hardware,2025-10-31 04:20:43,7
AMD,nmc64s8,"Driver side support is still needed for peak efficiency where multiple archs are targeted.  The cards will still work, just not efficiently.    That's not something anyone should be celebrating when RDNA2 is AMDs only successful generation, arguably better than it's competing Nvidia generation if you don't care about RT.",hardware,2025-10-31 08:36:31,2
AMD,nmvlk3s,"Ragebait is their bread and butter. As these comments clearly show, the internet fell for it yet again.",hardware,2025-11-03 13:41:24,1
AMD,nme624v,"Don't worry, the video content is all clickbait too!",hardware,2025-10-31 16:16:02,0
AMD,nmbi2fo,"> ""RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security fixes and bug corrections. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenaline Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.""  > -AMD to PC Games Hardware (translation)  Seems in proportion.",hardware,2025-10-31 04:37:36,25
AMD,nmbgcnr,">No, they just dropped day one game dev driver support.  Did you just make that up ?   Adressing ""critical"" issues like bug fixing and security isn't optimising games "" later on "".  It's plain and simple an EOL around the corner.",hardware,2025-10-31 04:23:20,15
AMD,nmbeyej,nobody want 1 driver every 2 years like they did with their vega GPUs that were supposed to have lot of VRAM to be future proof lol.,hardware,2025-10-31 04:12:15,8
AMD,nmbev3d,"95 percent of people won't notice the difference. What AMD should do is continue 'supporting' the cards, eg let the latest drivers still be installed, not change a line of code for said drivers, and let placebo do the work.",hardware,2025-10-31 04:11:33,4
AMD,nmbf1tf,Which is extremely ridiculous. Day 1 driver support is not anything hard to do and should be a basic expectation. Now you’re at the same level parity as a 1080ti.,hardware,2025-10-31 04:12:59,4
AMD,nme7vor,"I mean, AM5 is going to have at least 3 gens of CPUs, so this is yet another lie.",hardware,2025-10-31 16:25:06,0
AMD,nmjcoc2,The outrage will stop if AMD backtracks. But it doesn't seem they will.,hardware,2025-11-01 14:22:24,2
AMD,nmbw4cr,Really? What issue are you running in to? I'm currently rocking a 1070Ti with 581.08 in Windows 10 without issue. There was a fairly long period earlier this year with Nvidia drivers causing my system to randomly hard lock that kept me from updating but it seems they resolved that in the last 2 or 3 months.,hardware,2025-10-31 06:51:46,7
AMD,nmc0146,"Nvidia drivers have been absolute garbage since Nvidia started focusing on AI. No one has called them out besides JayzTwoCentz AFAIK, which goes to show how great techtubers are at holding companies accountable.  The fact that Maxwell and Pascal are being dropped with these drivers is sad.",hardware,2025-10-31 07:32:32,2
AMD,nme6h53,"That is incorrect, this is misinformation.",hardware,2025-10-31 16:18:06,1
AMD,nmddz7y,Do you even know that Linux drivers are mostly not from AMD? What kind of logic make you think they abandon a large portion of user but somehow still going forward with much smaller group? This doesn't affect Linux simply because there is nothing to be affected from the beginning,hardware,2025-10-31 13:56:40,1
AMD,nmctq5s,...and this is exactly why they do shit like this. Chumps like you will just come back to daddy for some more abuse (like a battered wife).,hardware,2025-10-31 11:59:12,6
AMD,nmvl6z5,This decision doesn't force you to do anything. Your RDNA 2 card isn't suddenly trash.,hardware,2025-11-03 13:39:17,1
AMD,nme8jp5,"Putting aside that the Steam charts are manipulated, likely for the reasons you describe, you're essentially right. The internet wants AMD gone for some hateful reason, it's that simple. If that happens, GPUs will get way shittier, but apparently that's what people want. So stupid.",hardware,2025-10-31 16:28:24,-1
AMD,nmbmmf9,"We were one of the main driving forces that saw AMD revert their decision, but sure, make stuff up: [https://www.youtube.com/watch?v=NsBRNck\_-wA](https://www.youtube.com/watch?v=NsBRNck_-wA)",hardware,2025-10-31 05:17:43,30
AMD,nmc2k4d,"Aka: The Steam deck but with Asus and Xbox branding for more  edit: [Ryzen Z2 A GPU (CPU is listed in notes)](https://www.techpowerup.com/gpu-specs/ryzen-z2-a-gpu.c4306) and [Steam Deck GPU (CPU in console notes)](https://www.techpowerup.com/gpu-specs/steam-deck-gpu.c3897) If you lot look at this and think its not the same Chip, you're an idiot and believing Microsofts crappy advertising. Same architecture, same GPU name, same clock speeds, and same Theoretical Performance metrics. A 1080p screen, bigger battery, and supposedly ""better"" ergonomics don't make such a system go from 499 (Steam Deck) to 799 (ROG Xbox Ally), This is price gouging",hardware,2025-10-31 07:58:56,74
AMD,nmdcic3,"Worst part about that is that neither MS or Asus will give a shit, because if they did, they’d have skipped launching that product in the first place",hardware,2025-10-31 13:49:03,10
AMD,nmbrhm1,It's RDNA3.5. Steamdeck's Van Gogh is RDNA2 if I remember right.,hardware,2025-10-31 06:04:36,-19
AMD,nmhr3t3,"Not to undermine the shitty behavior from AMD but … rog ally from years back uses z1 extreme which is RDNA 3, and the Xbox ally uses z2 which is rdna 3.5 If he says that, he’s just wrong.",hardware,2025-11-01 05:43:56,0
AMD,nmbb533,"There is  the rebrand of older Ryzen mobile APU already,   [https://www.reddit.com/r/hardware/comments/1ohh7fp/amd\_again\_reshuffles\_mobile\_lineup\_with\_ryzen\_10/](https://www.reddit.com/r/hardware/comments/1ohh7fp/amd_again_reshuffles_mobile_lineup_with_ryzen_10/)  So time to get the same old tech, just with a new name and usually the same price.",hardware,2025-10-31 03:43:30,37
AMD,nmc0h1d,I am half torn about it. I got 6700xt last year with the thought of running Linux and Windows. Should have taken a shot with the B580 in hindsight.,hardware,2025-10-31 07:37:19,8
AMD,nmbknqx,"Did you know Valve using RADV Driver, Linux Driver didn't affect by this change.",hardware,2025-10-31 04:59:56,41
AMD,nmd2z02,Yes but Steam Deck is Linux so no problem!,hardware,2025-10-31 12:56:20,5
AMD,nmh4bh2,"It really does seem like clickbait and ragebait.  At this point, AMD has probably exposed pretty much everything the hardware actually does in the drivers.  Even if they were doing ongoing feature work for the drivers, if a game comes out next year that requires a new feature then the old hardware probably wouldn't run it regardless.",hardware,2025-11-01 02:32:47,1
AMD,nmikqx8,"And 900 series, both Maxwell and Pascal ended support at the same time. Which makes sense since Pascal is just a tweaked and improved iteration on Maxwell.  You could have bought a card in 2014 and gotten driver updates up until last month.",hardware,2025-11-01 11:04:49,2
AMD,nmcelga,"What resolution are you playing at? My 3080 didn't have enough VRAM for the highest textures in Diablo4 when I played at 3440x1440, and lowering it to the next highest texture was a proper downgrade in quality as it looked very blurry in comparison. I ended up upgrading it 2 years ago due to that.",hardware,2025-10-31 10:00:10,17
AMD,nmcacol,"I mean, I brought a 2080 TI BE (evga exlcusive reduced cost variant), and at launch of it and not later, I still maintain that there was very little games for it, BFV and later Control were it, and if you didnt like them then tough  and DLSS1 was shit too, esp BFV with RT on and DLSS on, it wasnt great tech at the time.  and by the time DLSS was widespread, its more the 30 series and 100% in the 40 series. the first time I think I truly loved and saw the future of DLSS was with cyberpunk 2077, where people were complaining of issues, while the 2080ti with DLSS on played perfectly fine at 4k 60 without RT on, but that was the launch of the 30 series and it was still not very widespread. and RT chugged the card hard.   it wasn't what swayed me buying a 2080 ti, it was EVGA's support and warranty. and the fact that i had a 4k60 screen and nothing, NOTHING else can push that properly raw or DLSS.",hardware,2025-10-31 09:19:13,3
AMD,nmcvlpy,You are doing your customers dirty by not using pro-level hardware.,hardware,2025-10-31 12:11:36,1
AMD,nmd73p7,Right now a 10GB 3080 is often performing much worse than a 16GB 6800XT.,hardware,2025-10-31 13:19:35,-3
AMD,nme0vk6,"To be completely fair    there was several years there back in the day where PhysX was the extra feature that was being sold as the reason to get Nvidia, and that wound up amounting to nothing.    So it wasn't entirely unreasonable to expect that DLSS wouldn't amount to anything either",hardware,2025-10-31 15:50:55,-1
AMD,nmez1v6,Bro you're tripping I had to replace 3080ti because it was clearly showing age and couldn't keep up with modern games.,hardware,2025-10-31 18:40:59,0
AMD,nmc9gzj,"its more per game eh, on average you are right, but a lot of game do perfer AMD what with them being the console GPU and all that, and esp if you turn on RT then well...  but that is still plenty powerful right, a modern 5060 will still struggle to out do it in raster performance, and its massive 16 GB vram of the time means it has longer run life than the 8 GB variants too  nvidia fucked with the entire stack in 40 gen to give you one tier down, and that continued even farther in 50 gen so what was a 50 card is now a 60 card and that is why the generational increases are not that big and thus these older cards are still very much powerful and good.  and both AMD and nvidia has taken the piss with pricing on top of giving you a full ass tier down in terms of chip, a 650 dollar card (in theory, we know pandemic etc. etc.) now is not a 80 series card but the 9070XT, which while is better, is not exactly 2 generational leap better that you'd think it would be.",hardware,2025-10-31 09:10:13,7
AMD,nmfmjc8,">Without driver side assistance that 6800xt will likely reach parity with a 3070, when the dust clears.  6800 maybe, but not 6800XT. The 6800XT is substantially faster than the 3070 (in raster). Drivers aren't going to magically compensate for a 25% performance gap.",hardware,2025-10-31 20:44:56,3
AMD,nme13pk,> 6950xt is only slightly faster than a 3080 for the same reasons now    dude I had a OC formula 6900xt for quite a while and that thing was faster than a 3090 at native,hardware,2025-10-31 15:51:59,1
AMD,nmc3hh6,"yep and with nvidia being fuckwads and the 80s are gimped to hell and back, and a 3080 to 5080 jump isnt even the normal 100% increase and if you wanted that you needed a 5090 or 4090.   the same would be said for anyone with a 6800XT or above, which means for many MANY people this is still really a powerful chip that can do a lot of current gen gaming, and the 90 series being mid end means you dont get much at all jumping up, and they are more expensive msrp wise (although pandemic etc etc)",hardware,2025-10-31 08:08:34,4
AMD,nmcc9f0,"The thing is, who are they going to sell it to?  I think the type of people to buy used GPUs of that calibre are also the type of people to have heard the news and who know the price should be significantly lower.",hardware,2025-10-31 09:38:21,34
AMD,nmgd0lh,HUB has shown multiple times that their recommendations and predictions (like with the pricing of the 5000 series) are for the can and they are still allergic to Ray Tracing it seems.,hardware,2025-10-31 23:26:48,3
AMD,nmbxs0d,Sounds like it’s going to be a pretty good time to pick up 7600XTs soon if they’re dropping below 3060 prices.,hardware,2025-10-31 07:08:51,10
AMD,nmcp6sm,"> Anyone owning a 7900XTX should plan on selling theirs within the next 2 years before it loses complete value  ""Wow, thing X is no longer going to be supported. ***I should pass this problem on to some clueless, other rube.***""",hardware,2025-10-31 11:27:25,8
AMD,nmczcne,"I mean, you shouldn't be making purchasing decisions on deprecating assets based on future resale value. You buy them for the value they provide to you at time of purchase.",hardware,2025-10-31 12:35:06,2
AMD,nmf7tx7,As it was when they were ATI before AMD bought them. They have always been like this and seem incapable of change.,hardware,2025-10-31 19:26:23,7
AMD,nmemgy3,"TBF, nvidia's simply reaping the benefits of being afirst adopeter. They reakesed the 20 series at a higher cost with features deemed 'useless' back then, while AMD focusedon raster.",hardware,2025-10-31 17:37:03,3
AMD,nmcg258,> Why would anyone invest in AMD card?  Reddit karma,hardware,2025-10-31 10:13:09,20
AMD,nmdqody,>Why would anyone invest in AMD card?   Better Linux support is the only reason why I'm using an AMD card,hardware,2025-10-31 15:01:23,2
AMD,nmcvaz2,"> Why would anyone invest in AMD card?  A functioning moral compass. Even with this massive L on AMD's record, Nvidia are still markedly more anti-consumer.",hardware,2025-10-31 12:09:40,-16
AMD,nmbaf3x,The marketing team was not making this decision bro,hardware,2025-10-31 03:38:17,85
AMD,nmcg39p,AMD has a marketing team?,hardware,2025-10-31 10:13:25,8
AMD,nmbh5dk,"That only really works if there's new products to buy. I had a 7900XTX and they decided to not make anything worth buying this gen, so I dipped.",hardware,2025-10-31 04:29:56,8
AMD,nmdbxwl,"The question is, what is their market outside of a handful of forum users?",hardware,2025-10-31 13:46:03,3
AMD,nmbmfyy,"The stock went up like $100 bucks in a few weeks, AMD isn’t missing, they simply don’t care anymore about certain customers",hardware,2025-10-31 05:16:03,54
AMD,nmbpzw7,They never stopped.,hardware,2025-10-31 05:49:41,16
AMD,nmf7ml5,ATI doing ATI things going back decades.,hardware,2025-10-31 19:25:18,4
AMD,nme2q7n,"This ""news"" is misinformation. You've been exposed to lies for so long that you don't know what the truth is.",hardware,2025-10-31 15:59:45,-5
AMD,nmccfon,"I think its pretty clear that people are referring to full driver support and not just keeping the lights on quarterly security updates. More than that, they are talking about their Vega in the past, and their 6800xt in the present.",hardware,2025-10-31 09:40:03,12
AMD,nmdu3rq,"Literally, rDNA2 was the last gen amd was competing because dlss was quite early and not very good",hardware,2025-10-31 15:18:26,0
AMD,nmbk7s1,Glad someone else remembers this.,hardware,2025-10-31 04:56:01,32
AMD,nmdpqin,That was an excuse and a lie on AMD's part - see my [other comment](https://www.reddit.com/r/hardware/comments/1okjrn7/comment/nmbl86p/).,hardware,2025-10-31 14:56:41,7
AMD,nmbl86p,"That wasn't really why. High-end X370 boards with 32Mb BIOS chips didn't get support, while low-end B450 boards with 16Mb BIOS chips did. The first 300-series chips with official Zen 3 support [ended up being the lowest-end A320 boards](https://www.tomshardware.com/news/vendors-finally-enable-ryzen-5000-support-on-a320-motherboards), while official X370 support didn't roll out for [almost another two months](https://www.techpowerup.com/290832/asrock-first-out-with-official-support-for-zen-3-cpus-on-x370-motherboards).",hardware,2025-10-31 05:05:03,35
AMD,nmbkzm3,Who's fault it was a clusterfuck? Who advertised compatibility?,hardware,2025-10-31 05:02:54,25
AMD,nmd8k4p,You aren't wrong. I made a point of getting a board with the larger amount of storage (can't remember which but there were two common sizes) specifically because I was concerned about that causing issues with the promised long term platform support. I'm running a 5700X3D on a board I bought alongside a 1600AE. That's a completely neurotic detail to focus on and noone can be blamed for not doing so. I dunno if it's more AMD or the board manufacturers fault that lower storage boards were available but it shouldn't have happened.,hardware,2025-10-31 13:27:37,0
AMD,nmczbup,doom the dark ages would run sub 10fps if you didnt download the game ready drivers that released around the same day on my rx7600. so there''s precedent for games that just NOT working if you dont get day 1 support,hardware,2025-10-31 12:34:58,20
AMD,nme6rha,"Not supporting new Vulkan extensions could matter on Linux, where AMD in general is regarded as the better choice.",hardware,2025-10-31 16:19:31,1
AMD,nmd400e,"Well those older cards struggle to run new games, often for reasons other than lack of day 1 drivers but that lack of drivers ain't helping. Not a good look by amd.",hardware,2025-10-31 13:02:11,0
AMD,nmd3sfh,"Cool. ""Inflation.""  I can't afford that. $300 max or I drop out of the market. Don't try this crap with me.  Edit: demand curves are also real. You raise the price enough and people drop out of the market. You guys seem okay with that. You leave a snarky comment, you get blocked. Not everyone can be a yuppie with a 6 figure job showing off their 5090 on reddit.  Edit2: you guys also ignore the entire bottom of the market has been decimated over the years. If we went by 2017 pascal prices this is what ""inflation"" would look like:  5050 (replacing 1050)- $140  5060 (replacing 1050 ti)- $175  5060 ti 8 gb (replacing 1060 3 gb)- $250  5060 ti 16 gb (replacing 1060 6 gb)- $310  Except we ignore nvidia had all that ""inflation"" when they launched the 20 series and effectively destroyed the low end market. So let's not talk about all of this nonsense as if it's just impersonal economic forces beyond gpu makers' control.",hardware,2025-10-31 13:00:59,-4
AMD,nmekfdz,"Yep current upgrade options are:  5050 for $250- literally a side grade for a higher price than what I bought the 6650 xt for  3 years ago  9060 XT 8 GB for $270- not giving AMD my money again  5060 for $300- 50% increase, same 8 GB VRAM, ewww  9060 XT 16 GB for $350- $50 over budget and why would i give amd my money again?   5060 ti 16 GB for $430- are you fricking kidding me? $430 for a 60 card? Way over budget. No. No way.  And yeah intel ain't an option if you wanna actually ensure widespread library compatibility.",hardware,2025-10-31 17:26:45,0
AMD,nmbmgyh,Not for 50% more money.,hardware,2025-10-31 05:16:19,7
AMD,nmbm61u,"I didnt ask you. im not second hand. Back in the day we used to ACTUALLY see ACTUAL price performance gains where last year's 80 card would be the next's 60 card (for example 980 vs 1060, 580 vs 660). Really, the last time AMD had a product this poorly supported that I bought, you could get a like a 960/970 to replace your old HD 5850. BRAND NEW. Im getting sick and tired of being relegated to a second class citizen for not wanting to spend what used to be GTX 80 money on GPUs and its even worse when these companies dont support what products do exist.",hardware,2025-10-31 05:13:33,13
AMD,nmeqr1q,Buddy you don't wanna start a discussion about capitalism with me. I have tons of criticisms of the system and the gpu market is becoming an outright market failure.,hardware,2025-10-31 17:58:17,1
AMD,nmht4p4,Having more VRAM than Nvidia was part of the fine wine.,hardware,2025-11-01 06:05:40,1
AMD,nmc89o6,"Well to some degree it was to be fair. Because some of their architectures were better suited for games that hadn't been launched yet.   GCN had a terrible CPU bottleneck compared to Maxwell/Pascal in many poorly threaded DX11 games. Which was alleviated by later DX12 games, or in some cases just from games adding DX12 support.   Not like that is only a AMD thing either. Kepler aged poorly as DX11 took over, even on cards that weren't VRAM constrained. Pascal similarly aged poorly vs Turing in many newer DX12 and Vulkan games. The performance didn't change much, just the games used to evaluate performance. Run the original test suite used at Turing launch and you will get the ""not much better for more money"" result again.",hardware,2025-10-31 08:58:03,10
AMD,nmbk728,"And your point? The same happens with Nvidia, driver updates improving performance after product launch.",hardware,2025-10-31 04:55:50,12
AMD,nmdizca,"Yep, it'll be pushed by whatever RDNA5 or UDNA architecture is used in the consoles. Ask those RDNA1 puchasers if they should've bought 20 series instead once Ray Tracing and upscalingbecame prolific in games.",hardware,2025-10-31 14:22:24,6
AMD,nmgs5ka,My previous 3 cards were Nvidia.,hardware,2025-11-01 01:08:01,2
AMD,nmc1p2b,Amd always had shorter driver support than nvidia,hardware,2025-10-31 07:50:05,19
AMD,nmcmguc,"They dropped support for Fury, Vega, and Radeon VII unusually quickly as well, didn't they? Meanwhile, Nvidia ended optimised driver support for Maxwell, an architecture first released in February 2014, like a week ago.",hardware,2025-10-31 11:06:30,13
AMD,nmgr5lu,I thought the “Nvidia tax” was common knowledge? More money for better software,hardware,2025-11-01 01:01:11,2
AMD,nmbjacj,are you serious? Saying anything bad about AMD gets you severely downvoted or shadowbanned online. This is fact.,hardware,2025-10-31 04:47:59,2
AMD,nme3nc9,Nope. Native DLSS 4. Set in the nvidia app or via nvpi. At least know what you're talking about if you're making statements like this,hardware,2025-10-31 16:04:14,5
AMD,nmbqhsc,A laptop with a MX450 is getting more support lmao,hardware,2025-10-31 05:54:35,21
AMD,nmbylyb,"Intel Arc Alchemist, a first-gen architecture, is still getting game-specific driver updates while 6x50 RDNA2 cards released at the same time won’t…",hardware,2025-10-31 07:17:30,10
AMD,nmd699q,"If you have better suggestions, I'm all ears. I spent a whole day trying to troubleshoot it. And then half a day a few weeks ago, thinking there was a better solution by now.   Since, it's what SteamVR uses to this day. As otherwise, it causes horrible framerate issues. At 24.3.1 or older, I get a solid 90FPS in my 90Hz headset without issue. Anything newer than that, it runs great...if I'm completely still. And then I start moving. You can replicate the issue by going into SteamVR settings and manually disabling it.",hardware,2025-10-31 13:14:51,2
AMD,nme6a2y,In what way?   Spending £50 more on the faster GPU that has a better software suite is bad?,hardware,2025-10-31 16:17:08,3
AMD,nmcwxia,"AMD stopped supporting the mobile version of the chip. For a while, it was possible to mod the desktop driver and make it work on Windows 8/8.1. When Windows 10 came out, they also stopped supporting the desktop chip. Due to the new windows driver model, it was not possible to mod any of previous drivers at all.",hardware,2025-10-31 12:19:58,6
AMD,nmcgdrw,"If she cares so much, why doesn't she fix it?",hardware,2025-10-31 10:15:59,3
AMD,nmdhxo7,"Idk ehy isnt this a top comment. Because this is very true whrn I was employed by them. They woukd rather spend more time on their marketing rather than invest in proper teams in software, hardware and QA. They treat their customers like guinea pigs and it seems that their behaviour isnt changed. Best part the parts they had were cheap to make and yet if priced correctly they woukd take the market by a storm. But no they chose to hide everything bin everything down and treat their products like worthless piles of silicon whike sucking up to OpenAI and Co. Also they like to shift the narrative towards board partners not complying and such very scummy behaviour while only interested in miliking the customer. At least release better products than your competitors but no. Intel should destroy AMD with their CPU while Nvidia should destroy AMD as well.",hardware,2025-10-31 14:17:01,1
AMD,nmff515,Thanks!,hardware,2025-10-31 20:05:20,2
AMD,nmdbpiu,"The whole thing started with release notes probably written by an engineer with no understanding of PR.  Meanwhile AMD marketing doesn't have a much better understanding of PR.  Knowledgeable reviewers openly calling this ""end of support"" is the extra kick in the balls.",hardware,2025-10-31 13:44:50,2
AMD,nme6bmt,"Finally, someone gets it. The truth is being used as an excuse to lie.",hardware,2025-10-31 16:17:20,1
AMD,nmccrtd,"Not to mention that with their low marketshare, they sure as shit aren't going to be allocating the minimal effort they put in, into the cards with the least marketshare.",hardware,2025-10-31 09:43:16,7
AMD,nmchbht,Nothing you said makes it AMDs responsiblility to make up for some other business' shitty code.,hardware,2025-10-31 10:24:12,-4
AMD,nmcq5sg,"GPUs are hardware, not software.",hardware,2025-10-31 11:34:42,-5
AMD,nme3pva,It's *crazy* how blatant misinfo like your post gets upvoted constantly. People are just in love with lies.,hardware,2025-10-31 16:04:35,-2
AMD,nmgluts,">This is precisely the opposite of what *is already happening* with Vulkan extensions w.r.t RDNA2 support.  I can refute that very easily. AMD just released their ""[VK\_AMDX\_dense\_geometry\_format](https://registry.khronos.org/vulkan/specs/latest/man/html/VK_AMDX_dense_geometry_format.html)"" Vulkan extension is the latest driver.   If you bother to look it up you'll see RDNA4/RDNA3/RDNA2/and RDNA2/3 based APUs reporting support for it.  [https://vulkan.gpuinfo.org/listdevicescoverage.php?extension=VK\_AMDX\_dense\_geometry\_format&platform=windows](https://vulkan.gpuinfo.org/listdevicescoverage.php?extension=VK_AMDX_dense_geometry_format&platform=windows)",hardware,2025-11-01 00:25:40,-2
AMD,nmdmdmo,"> On Linux the development is partially done by the community and Valve at the kernel level, and at the usermode level (for Vulkan) it's entirely by the Mesa community and Valve, with no AMD involvement.  I think that's a little bit unfair towards AMD. On the kernel side, they put in the work with amdgpu, writing parts of it from scratch *and* getting it being accepted into the mainline kernel - which is a steep barrier to entry (the battles on the LKML were a fascinating read :) ), but which makes long-term support infinitely more viable due to the nature of Linux kernel development.  On the userland side: While RADV of course has been and still is first and foremost Valve-driven, the no AMD involvement hasn't been true anymore for a while. AMD driver devs have actively contributed to RADV for a while now, and recently they had the saliency to make Mesa RADV the officially supported Vulkan driver, going so far as to actually discontinue AMDVLK. And similar to the kernel side, being part of Mesa means a much higher likelihood of long-term support.",hardware,2025-10-31 14:39:47,2
AMD,nmgkg11,I know what the announcement said but I actually *understood* it instead of jumping to outrage based on a poor understanding of how drivers are developed and maintained.   [https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),hardware,2025-11-01 00:16:07,-2
AMD,nmgmf75,">The cards will still work, just not efficiently.   That's 99% up to developers. If they write optimized shaders then they will work efficiently. AMD isn't doing anything to reduce performance.",hardware,2025-11-01 00:29:33,1
AMD,nmbpn7e,It's wild to read since even the almost 11 years old GTX 960 has still been receiving game ready drivers,hardware,2025-10-31 05:46:16,13
AMD,nmbg58u,"I see it with Intel Wifi drivers, the actual driver in device manager never change, but Add/Remove just gets a new number.",hardware,2025-10-31 04:21:39,4
AMD,nmbi6fq,"I did and wasn't alone notice the difference when they released drivers from the latest AAAs on not a longer so beefy cards. It's refreshing.  Your agument is making people like complete idiots or ignorants, or careless from which they won't get any use of counting FPS or writing on reddit anyway.   Your solution is egregious because a thin minority will notice and spread the word regardless, just like a thin minority of people is doing benchmarks ...  However, a huge percent of people is fine playing at 30 FPS on a botchered resolution with stuttering left and right, not sure why you bring up that one.",hardware,2025-10-31 04:38:33,-6
AMD,nmbgir7,"Not even that, you won't get any substancial drivers from now on. OP just made that up.",hardware,2025-10-31 04:24:42,-2
AMD,nmeklaf,I get this system wide system hang every now and then for no reason that I can find. I've read that it's due to multi-monitor perhaps but I'm not 100% sure all I know is that all the new drivers do it after 566.36. I updated a couple of days ago to see if it was fixed and it still did it.   It will do it a handful of times a day but it just gets very annoying when I'm trying to watch a film.,hardware,2025-10-31 17:27:35,1
AMD,nmcxjrz,"Good to hear it is fixed, my issues were BSOD, freeze, display driver constantly crashing and restarting (many people report this as black screen etc, it's just the driver restarting) with the launch driver from the 5060 Ti, 572.something if I remember correctly. I just gave up in the end, I'm not averse to trying to fix things and could understand what was happening quite clearly but nothing seemed to help, going back to a 4000 series card on 566.36 fixed everything.  I assumed it would all be fixed eventually but the card was not usable while this was happening so I just returned it.",hardware,2025-10-31 12:23:51,1
AMD,nmvkz5e,"Despite being ""Aboslute garbage"" the drivers are still better than AMD at its very best.  Also lol why would anyone listen to LazyTwoCentz. He hasnt been right about this stuff in forever. Didnt we already have this discussion where you cited his video where he was wrong about SSD bricking?",hardware,2025-11-03 13:38:01,1
AMD,nmcsnn3,"I have been surprised by that too, the driver experience I had with a 5060Ti was shockingly bad and I wasn't alone, their forum is full of people having a terrible time with their 2025 drivers. Very few youtubers or writers seemed to cover any of it.  I just returned the card and assumed they'd figure it all out eventually, then I would re-buy... now I'll probably wait until 2026.  I'm not sure it has anything to do with ""focus on AI"" or anything but it's definitely not the driver experience I was used to from Nvidia.",hardware,2025-10-31 11:52:02,0
AMD,nmdpgkj,"I know and? They are still involved. I just wanted to point at the difference between proprietary platforms (that nowadays shit on their customers as a service) and open source ones. Them theoretically dropping their ""support"" within MESA has less impact than within the same situation on their Windows proprietary driver.  AMD abandoned their first proprietary driver of two (Amdvlk) in June on Linux for their Enterprise users. Amdgpu-Pro will soon follow. They are focusing on RADV now which is open source.  For example I reported an OpenGL bug 1.5 years ago on Vega and the first to take a look over it was someone working for AMD.  Official support drop on Windows: you're screwed on the first issue.  Official ""support"" drop on Linux (open source): If related GPU firmware is open enough, development might just slow down if at all.",hardware,2025-10-31 14:55:18,2
AMD,nme6n0f,This comment would make way more sense if it was being said about Nvidia. Oh well.,hardware,2025-10-31 16:18:54,2
AMD,nmcvl0e,"What a weird comment, they don't consider it an issue. There isn't any 'abuse' going on.",hardware,2025-10-31 12:11:28,2
AMD,nmvsjom,"I selected the cards believing that they would receive quick and high quality game optimizations into the future, even past the point where the card can easily handle 1440p and even 4K AAA workloads. That's what my folks have been using their cards for. This is dependent on AMD actively doing their best to keep the cards relevant. They clearly aimed to cut costs with this measure and are now backtracking to try and calm negative customer sentiment. I'm not buying it, and it's going to be difficult to restore my trust. Will the 9000 series be fully supported 5 years from now? Who knows?",hardware,2025-11-03 14:19:57,1
AMD,nmc085h,Why is it such a problem they stop optimisation of game code for old hw which should be the job of game developers anyway? This doesn't mean they stop fixing bugs or whatever other maintenance drivers require for OS updates.  All this drama sounds like some people think this means these products become useless.,hardware,2025-10-31 07:34:39,-2
AMD,nmbohl9,"You said you were upset at 400 series not getting ~5000~ 3000 support and not so much 300.   And then went on to vomit out garbage I'd expect Reddit would say like ""you got your money's worth"".   Edit: actually, a little bit beyond that point, you went on to praise AMD for supporting first gen motherboards as well as they did. That isn't just indifference, that's rear end kissing. My bad.",hardware,2025-10-31 05:35:07,-26
AMD,nmfyy8u,"Using a better processor, more ergonomic design, and a higher refresh panel. But sure we’ll go with the same thing as the Steam Deck. it also has somewhat better battery life and if you put steam OS on, it runs even better.  Edit: corrected typo",hardware,2025-10-31 21:55:59,8
AMD,nmwxn3h,https://www.techpowerup.com/gpu-specs/ryzen-z1-extreme-gpu.c4157  This is what’s in the rog ally  https://www.techpowerup.com/gpu-specs/ryzen-z2-extreme-gpu.c4304  This is Xbox rog ally   I dunno you where you found that cobbled together bullshit link.  Get it through your head. Steam deck apu is a different architecture.,hardware,2025-11-03 17:42:25,1
AMD,nmbse3j,The Xbox Ally uses RDNA2 bro wtf u on about,hardware,2025-10-31 06:13:45,55
AMD,nmbbx7n,Are the GPU drivers rebranded too? That 2022 model G14 is no longer supported with the latest gaming drivers due to it using a RX 6800S GPU and 6800HS CPU,hardware,2025-10-31 03:49:13,12
AMD,nmhg0st,"At least with linux, you can rely on open source driver support. So I think only windows users should be effected.",hardware,2025-11-01 04:00:39,1
AMD,nmd3hjs,"You're fine.  I got that card at launch and I'm running it for at least a few more years.  Still a solid card and will be, despite the lack of support.",hardware,2025-10-31 12:59:16,1
AMD,nmhg4j2,Good time to be a linux user lol.,hardware,2025-11-01 04:01:30,1
AMD,nmbod95,You can install Windows on the Steam Deck. I never said this was a good idea!,hardware,2025-10-31 05:33:56,-7
AMD,nmh4ye2,yeah,hardware,2025-11-01 02:37:21,0
AMD,nmcshdm,"Yeah he's full of shit, my 3080 does not have enough VRAM for BF6 and is struggling a lot in RT games.",hardware,2025-10-31 11:50:49,18
AMD,nme5ysa,"Absolutely, 2080Ti wasn't a great buy at the time, way too expensive for uncertain future feature support. Buyers just got lucky with the longevity of it later on, as well as the covid shortage.",hardware,2025-10-31 16:15:34,1
AMD,nmcy8zd,Customers can ask for consumer hardware specifically for renting it at lower prices.,hardware,2025-10-31 12:28:14,7
AMD,nmdgsb8,"they pay for a service, not a part name",hardware,2025-10-31 14:11:09,8
AMD,nmdqay3,So its stuttering at ultra settings instead of 6800xt's 40 fps. 3080 has dlss 4 and that is the end of the comparison.,hardware,2025-10-31 14:59:32,8
AMD,nmvjec6,that is not possible.,hardware,2025-11-03 13:28:35,0
AMD,nmddepr,Also these cards are cheap to manufacture and these conpanies choose to sell it at high margins to us just because conpetition is scarce with AMD pricing Nvidia - 50 while making the data center cards all that powerful ehile leaving us holding the bag.,hardware,2025-10-31 13:53:45,2
AMD,nmkah3t,"Maybe at 1080p, at 1440p and especially 4k the 3090 kills the 6900xt consistently.",hardware,2025-11-01 17:20:54,2
AMD,nmgntau,"My 6900xt is now a part of my VM cluster, it is still an absolute great card. Linux drivers have breathed new life into the things",hardware,2025-11-01 00:39:03,1
AMD,nmc701a,Jump from 3080 to 5080 is quite great tbh. Your fault for not using technologies where most of development cash went.,hardware,2025-10-31 08:45:26,0
AMD,nmdx61k,Aquaman,hardware,2025-10-31 15:33:07,13
AMD,nmcoebl,Most people don't follow any of this stuff,hardware,2025-10-31 11:21:27,8
AMD,nmc8rwa,"Where I am, a 3070 is cheaper than a 7600XT on the used market. Imho, there's 0 reason to go for a 7600XT in such a scenario",hardware,2025-10-31 09:03:09,18
AMD,nmeavy1,"Why is this a problem? If people can't research stuff they spend money on, it's their fault. Caveat emptor.",hardware,2025-10-31 16:39:58,-1
AMD,nmcgv8t,Also better Linux support,hardware,2025-10-31 10:20:17,23
AMD,nmvk1hi,AMD cards havent been good since before AMD bought them.,hardware,2025-11-03 13:32:28,1
AMD,nmd7ihd,"> A functioning moral compass.  Buddy, AMD is a massive corporation who are just as bad as the rest of them. You are not supporting some garage startup who will change the world, AMD is just another soulless company who has shown on many, many, maaaany occasions that they'll choose profits over all else.  I still have no idea how AMD convinced so many nerds that they're the scrappy underdog who will always be on *the gamers'* side, when every other action they take signal otherwise. It's honestly impressive.",hardware,2025-10-31 13:21:50,14
AMD,nmvk39q,Anyone with a functioning moral compass would have given up on the hell that is AMD corporate.,hardware,2025-11-03 13:32:46,1
AMD,nmbycbw,You should not buy every gen anyway,hardware,2025-10-31 07:14:42,15
AMD,nmdfvfv,"It's the market they want they're concerned with, not the market they have.  Growth > Sustainability in the current marketplace.",hardware,2025-10-31 14:06:27,1
AMD,nmbna6w,Unfortunately all they care about now is the openAI deal,hardware,2025-10-31 05:23:48,41
AMD,nmhjuuz,"Ah. That's unfortunate. Could you tell me the truth, please?",hardware,2025-11-01 04:33:45,3
AMD,nmdvgd4,"And from those sources:  > Manufacturers have to drop support for some of the older Ryzen parts in order to usher in support for Zen 3. For example, the Asus and Gigabyte removed support for AMDs 7th Generation A-series and Athlon X4 series (Bristol Ridge) processors.   So it wasn't an excuse. It was a difference of opinion as to what needed to be supported and what didn't.",hardware,2025-10-31 15:24:57,-1
AMD,nmboc2d,"Initially AMD didn't advertise compatibility, it was motherboard manufacturers advertising it and basically bullied AMD in to trying to make the best of a dumb situation.",hardware,2025-10-31 05:33:37,-17
AMD,nmd9a62,"Devs testing a dozen cards is easier than vendors testing every SKU of every arch in every new game and hard coding workarounds. When devs don't do the former, for some reason gamers expect the latter.",hardware,2025-10-31 13:31:39,2
AMD,nme68yg,You can’t afford PC gaming then.,hardware,2025-10-31 16:16:58,2
AMD,nme4gci,"Well then you're just going to have to ""choose"" to drop out of the market based on ignorance. Nobody is going to coddle you here.  Inflation is far too real, stop pretending it isn't. It existed long before tariffs or COVID or whatever else.",hardware,2025-10-31 16:08:11,2
AMD,nmeslu6,I'm not your buddy. Good day.,hardware,2025-10-31 18:07:44,0
AMD,nmbkj6j,"If that were the case (that Nvidia driver performance improves at the same rate as AMD's) there would be no such phrase as ""fine wine"" and op's comment about shitty day one drivers would apply equally to Nvidia.",hardware,2025-10-31 04:58:49,2
AMD,nme5rd7,"> Ask those RDNA1 puchasers if they should've bought 20 series instead once Ray Tracing and upscalingbecame prolific in games.  This isn't going to happen until RDNA1 cards no longer work, if that.",hardware,2025-10-31 16:14:33,-1
AMD,nmj3zqd,"Ok, don’t buy new cards and play older games instead that you haven’t played yet, or wanted to replay them.",hardware,2025-11-01 13:30:09,1
AMD,nmcc6dt,"Tbh I think people are overreacting about this, old cards are mature enough that they rarely ever need game-specific optimizations, the bigger problem is the lack of features which was already a problem even when those cards were on full support, 20 series is still getting some new DLSS features while being older than RDNA2, it's pretty embarrassing.      At the bare minimum FSR 3 should continue being improved or a compatible FSR4 version must be released to RDNA2 and ideally RDNA1 also.",hardware,2025-10-31 09:37:29,-1
AMD,nmc4vzj,"Well that's just not true, AMD was well ahead for a while because of how long GCN lived. But that's beside the point, Nvidia already does this with previous generations, they just don't officially state it.  And even *that* is beside the point, because duration of driver support is far from the only factor that goes into a GPU choice.  edit: I'd be fascinated to know why this is downvoted. What did I say wrong?",hardware,2025-10-31 08:23:28,-9
AMD,nmdaiap,"I bet if you compare day 1 driver performance on a 980ti to the driver just before that in a dozen 2024/2025 games, there's basically no difference (because they didn't actually do anything).",hardware,2025-10-31 13:38:24,-2
AMD,nmhrvva,"More money for the myth of better software. It's just prestige pricing, the tax is for the brand name.",hardware,2025-11-01 05:52:05,-1
AMD,nmc4auj,Thats funny cause I aint got shadowbanned from amd subs but got shadowbanned from nvidia.,hardware,2025-10-31 08:17:14,5
AMD,nmbutrl,"This sort of ebbs and flows with time/trends, and depends on the sub/forum, I'd say pcmasterrace fits (or certainly did for multiple stints over the past 5+ years) your description, and the Techpowerup! forums are significantly pro-Radeon / anti-Nvidia these days. I don't dive as much into the comments in this sub to judge it.",hardware,2025-10-31 06:38:31,5
AMD,nmblpjv,"Nah, I see AMD bad twice as often as I see NV bad here.  I still see AMD drivers comments from pre-pandemic days to this day.  I see it more than NV fire cables, missing vram, price hikes, tier changes, their shady marketing, the geforce program, and they have had bad drives multiple times since AMD's bad drivers including a NV bad driver release just this year.",hardware,2025-10-31 05:09:23,4
AMD,nme3809,"That's a blatant lie, as proven by *every single thread* you people try to say this in, including this one.",hardware,2025-10-31 16:02:10,1
AMD,nmf1m84,"Which game specifically if you don't mind answering? I doubt SteamVR it self uses Direct Draw as Direct Draw was effectively abandoned by Microsoft after Windows XP.  In any case, my suggestion for if it really is something using Direct Draw... is to use the dgVoodoo2 graphics wrapper.",hardware,2025-10-31 18:54:03,2
AMD,nmhmca9,hes amd fan,hardware,2025-11-01 04:56:26,3
AMD,nmvhdxq,"It's not a ""faster"" GPU with a ""better"" software suite, *especially* not for the price. I'd have thought all the nonsense with Nvidia drivers that's been going on for the past year or so would have gotten people to realize this, but apparently not.",hardware,2025-11-03 13:16:28,1
AMD,nmw4n6t,As a contractor or at HQ?,hardware,2025-11-03 15:22:27,1
AMD,nmct0jv,"If AMD wants their customers to actually be able to use their cards like they can with Nvidia, sure. But they *technically* don't have to do that I guess and people can just get fucked.",hardware,2025-10-31 11:54:29,3
AMD,nmf2z4q,"You sound like you had the exact same issue as me, as 566.36 and the symptoms you mentioned sound very familiar.  Only thing I could maybe suggest giving a try (and this is a pretty wild guess as I've no clue what solved it for me and have always assumed Nvidia eventually solved it on their end) is changing the CMD2T RAM setting from 1 to 2, performance impact is negligible.",hardware,2025-10-31 19:00:58,1
AMD,nmf371b,"See my reply here for a wild stab at a solution https://old.reddit.com/r/hardware/comments/1okjrn7/amd_throws_loyal_radeon_customers_into_the_trash/nmf2z4q/  Oh, you returned your card, that's one way to solve it :P",hardware,2025-10-31 19:02:05,1
AMD,nmhu9am,">I have been surprised by that too, the driver experience I had with a 5060Ti was shockingly bad and I wasn't alone, their forum is full of people having a terrible time with their 2025 drivers. Very few youtubers or writers seemed to cover any of it.  They're afraid to make daddy Nvidia mad. Real knights in shining armor they are.",hardware,2025-11-01 06:18:04,0
AMD,nmvvwxy,"> They clearly aimed to cut costs with this measure and are now backtracking to try and calm negative customer sentiment.  They haven't and aren't doing any of these things.  > Will the 9000 series be fully supported 5 years from now?  The same as every single other gen in history: it will, yet it doesn't matter if it is or not.",hardware,2025-11-03 14:37:58,1
AMD,nmd0xi9,> old hw  If you consider products that launched this month old sure. RDNA 2 is in the xbox ROG ally ~~and allyx~~,hardware,2025-10-31 12:44:40,3
AMD,nmbuu0f,"We saw an issue, we brought it to the attention to as many people as we could. There wasn't really a need to go all crazy outrage mode given it still wasn't an irreversible change, nothing had happened at that point, it was just a plan they had for the future. We told them it was a very bad plan, told the community to let them know it was a bad plan and just mere days after the video went live they reversed course.  You are acting in bad faith here so I'm done with the conversation.",hardware,2025-10-31 06:38:35,17
AMD,nmhjl76,I hope u meant ergonomic,hardware,2025-11-01 04:31:19,1
AMD,nmbsj89,Ah I thought they mentioned XBAX.,hardware,2025-10-31 06:15:11,7
AMD,nmi9761,"It hurts a tad because I still play EA WRC (has anticheat) and using a Moza wheel ( is there even Linux support) so still stuck with Windows just for that. The game runs poorly as it is on Windows, could end up better or worse in Linux?",hardware,2025-11-01 09:05:54,1
AMD,nmbqvpd,Back to SteamOs ;),hardware,2025-10-31 05:58:27,22
AMD,nmc8u4z,Why would you do that?,hardware,2025-10-31 09:03:48,12
AMD,nmd32eo,"Literally no one does that, yes you can, but there are no benefits of doing it!",hardware,2025-10-31 12:56:52,3
AMD,nme1l5i,That’s like downgrading to an ASUS Ally,hardware,2025-10-31 15:54:17,-1
AMD,nmdr1ba,"I have a 10 GB 3080 and battlefield runs completely fine, although I am at 1440p",hardware,2025-10-31 15:03:11,11
AMD,nme40yb,DLSS doesn’t matter if your vram is full. It’s more like 70 FPS smooth vs 20 FPS stutter. Less than 12GB is not worth it in 2025.,hardware,2025-10-31 16:06:05,-6
AMD,nmw0uf9,"Not only is it possible, it’s a fact.",hardware,2025-11-03 15:03:23,1
AMD,nmki2ej,no   it didn't    consistently better game FPS and benchmark scores,hardware,2025-11-01 17:59:52,-1
AMD,nmc995o,"3080 was 700USD, now 5080 is 1,000USD.  Try again, more like 5070 Ti is the successor price wise which is about 15% weaker than the 5080.",hardware,2025-10-31 09:08:01,0
AMD,nmc95jv,"FG is not a good enough draw, esp since you can apply it after the fact with third party stuff, or mix FSR FG or XeSS FG with DLSS that it does have.  the raw increase at the EXACT same setting, be it DLSS or FG or raw RT is not anywhere near what it was before, when you would get double the fps at the same setting.",hardware,2025-10-31 09:06:59,0
AMD,nmcol83,"My point is that the people buying a high tier AMD GPU used are not ""most people""",hardware,2025-10-31 11:22:54,26
AMD,nme6e2j,Not just 'better' - it is a lot better.,hardware,2025-10-31 16:17:40,7
AMD,nmcw0ju,That’s actually the same thing,hardware,2025-10-31 12:14:11,6
AMD,nmdeq5l,"Bro I used to work for AMD and the things that they do are not moral. They only care about their end margins. People like you and me who buy their products mean peanuts to them. They want to seem like the underdog but clearly are not. Sadly this was probably planned some time back and probably the same situation might happen to the 9000 series. Not sure why people love AMD, they are just as or even more scummy than Nvidia cheating their loyal followers and rewarding them with worthless piles of silicon.",hardware,2025-10-31 14:00:33,2
AMD,nme1zmo,"> AMD is a massive corporation who are just as bad as the rest of them  Why do people always say this when AMD always makes better stuff, time and time again? Nobody actually knows or cares about this ""garage startup"" garbage. I care about *results*, and that is what AMD delivers.  > when every other action they take signal otherwise  Virtually all of which is misinformation, such as the OP, and that garbage earlier in the thread about Zen 3 motherboard support.",hardware,2025-10-31 15:56:12,-6
AMD,nmdbuk1,There's not even a real worthwhile upgrade from top end rdna2.,hardware,2025-10-31 13:45:34,5
AMD,nme2atz,Once inventory settles in on a new Gen of gpus I sell my card to a friend that needs an upgrade while it still has good value and get the new top card.,hardware,2025-10-31 15:57:42,1
AMD,nmdkze2,Where is the growth in question though,hardware,2025-10-31 14:32:38,3
AMD,nmvklkt,Someone tell AMD negative growth is not growth.,hardware,2025-11-03 13:35:47,2
AMD,nmderqb,"No matter what, AMD will always be the cheaper and worse off-brand version of NVIDIA, whether that be for gaming or AI. They try so hard to be like NVIDIA, when they should really be their own thing and build their own brand rather than copy NVIDIA's homework and only get 80% of the way there. Sadly they never learn. Oh well, back to buying NVIDIA and just hoping NVIDIA praises us peasants with a decent performing product at a respectable price.",hardware,2025-10-31 14:00:47,7
AMD,nmivxg6,"God forbid, AMD customers get the choice of either using their newest and latest CPUs, and the cost of some old shitty CPUs on architectures that nearly killed AMD.  It's not like B550 officially support those anyway.",hardware,2025-11-01 12:37:01,0
AMD,nmbokcj,"Uh no, when AMD announced AM4 they had marketing slides advertising compatibility.",hardware,2025-10-31 05:35:52,19
AMD,nmda3b9,no it wouldnt run on well ANY AMD card without those drivers. i was just specifying which one i had. those drivers were mandatory and when you started doom without them the game would tell you to update your drivers. so the devs knew the update was needed and so did AMD.,hardware,2025-10-31 13:36:06,2
AMD,nmbu4tb,"Lately it has applied more to prior gen Nvidia products than AMD ones, not really about shitty day 1 performance that they fix, just the product getting better as time goes on. It's just hard and takes a long time to undo or reframe a statement that is already ingrained in peoples minds for a particular company applying to another.",hardware,2025-10-31 06:31:27,5
AMD,nmbu8eq,> shitty day one drivers would apply equally to Nvidia   5000 series would like a word.,hardware,2025-10-31 06:32:27,6
AMD,nmdq8xv,"The Nvidia hive mind screams at anyone who tries to bring up actual problems. For me, it took over 6 months after launch for my RTX 4090 to finally work properly for my 3 monitor setup despite reporting the issue to their support team 6 days after the product launch.",hardware,2025-10-31 14:59:15,1
AMD,nmkxgvr,"My first GPU was the 580, I wasn’t able to use it for Blender after like 2 years (end of 2021) because AMD stopped supporting it in their GPU compute software. I was irritated that the GT710 had support but my GPU didn’t.  You’re being downvoted because you’re factually incorrect",hardware,2025-11-01 19:19:58,1
AMD,nmkxpt0,"Maybe you’re the odd one out, most people have known that NVIDIA’s price premium had some justification.",hardware,2025-11-01 19:21:17,2
AMD,nmvhl8u,"I'm not a ""fan"" of anything, I don't waste my time with that garbage. I care about the results, and Nvidia does not deliver. Their last good cards were the 10s, and that may end up being literal at this rate.",hardware,2025-11-03 13:17:41,1
AMD,nmf7lq4,"I've never touched that setting, I'm not even sure I would even have it. I'm on a 5820k on an MSI SLI Plus motherboard and my memory frequency is stick because the 5820k doesn't like a memory overclock. I only have two sticks anyway 8GB each at 2133mhz. No idea why the Nvidia driver would have any effect on system memory anyway?",hardware,2025-10-31 19:25:11,1
AMD,nmvwnd7,"They absolutely would have if they hadn't been tarred and feathered in the public square. That's what the words ""maintenance mode"" mean in this industry. We cannot believe them anymore if they argue that the cards will be supported as long as they are still useful for newly released games.",hardware,2025-11-03 14:41:49,1
AMD,nmfbc87,"ROG X is RDNA 3.5. In case of these new ""mobile"" products I assume they use their own drivers anyway. I also assume AMD references RDNA 2 to the specific GPU cards phased out years ago and not still selling iGPU/APUs.",hardware,2025-10-31 19:45:10,1
AMD,nmbxocn,"\-accusing someone of making stuff up  \-provides a video supposedly showing that they did express issue with 300 not getting new CPU support  \-it doesn't and actually makes him look worse  \-generalizes the video, engages in gaslighting, and accuses me of ""acting in bad faith""  \-fans base/subscribers upvote you and downvote me  Frankly, I wasn't expecting much from a techtuber and his army of drones but this is something I thought only LTT and their drones were capable of. Incredible.",hardware,2025-10-31 07:07:47,-12
AMD,nmhjn94,"Yes, Apple’s voice to text is shockingly horrible",hardware,2025-11-01 04:31:50,1
AMD,nmr9vgh,"A.) The Z2A has a higher TDP and is in a device that no-one should buy as it's specs are about as good as the Steam Deck I can agree with that, however the base model ally is only $50 more than the equivalent OLED sku     B.) The Z2 Extreme that is in the Ally X is a major improvement to the Aerith chip in the Steam Deck and for $350 more than the 1TB OLED Steam Deck you are getting a performance boost, more ram, bigger battery, and better I/O in the form of two USB4 ports including one that meets the spec for Thunderbolt 4.     C.) Just because you don't like the price of something does not mean that it isn't worth it to someone. The only reason that the Steam Deck can be as cheap as it is, is because Valve prints money with their 30% cut on all platform sales, whereas ROG is unable to compete due to not being able to subsidize the costs even with the help of Microsoft. If you do think that the Ally is overpriced there is an easy solution, Don't buy it. If you truly hate this luxury product to the point of attacking anyone who points out that is might have some value to them, then it's time for you to step away from the internet for a bit and do some self reflection.",hardware,2025-11-02 19:45:22,0
AMD,nmiqiel,"I was reading a bit more, and it sounds like they aren't completely ending support. So you might be OK. I've never played that game, so no idea if it will run better. It might, or it could be about the same. You could get an idea of how it will run by seeing if anyone has done benchmarks on it on YouTube. If youre not sure something will work, or need help doing something on linux, grok has been really helpful for me.",hardware,2025-11-01 11:55:38,1
AMD,nmvihg7,to use mods.,hardware,2025-11-03 13:23:05,1
AMD,nmcf2do,To play game not work on steam os,hardware,2025-10-31 10:04:21,-2
AMD,nmgmb1b,"3080 12GB, 3840x1600, no issues with VRAM. It's not fast enough to do frame-gen, which would be nice, but incurs some serious input latency.",hardware,2025-11-01 00:28:44,3
AMD,nmlshoh,"I have no issues with BF6 at 3440x1440 on a 10gb 3080. Textures on ultra too. I cap to 120 fps and if it drops below that it's because I'm CPU bound by my 5800X3D which is surprising to me, but in real heavy moments on large 64 player maps it can drop to ~110.",hardware,2025-11-01 22:07:18,2
AMD,nmez61y,"""Hey if I render 30% less pixles it works fine!""",hardware,2025-10-31 18:41:35,-4
AMD,nme8mdm,DLSS does matter when the game forces horrendous TAA. Making vram arguments is pointless when these cards can not even run those settings at an acceptable framerate. If your target frames are below 100 i suggest you get on with modern standards.,hardware,2025-10-31 16:28:47,5
AMD,nmvjgk9,DLSS reduce VRAM usage..,hardware,2025-11-03 13:28:57,1
AMD,nmkkhm6,Almost all the videos I see on Youtube tell a different story. There are some games where the 6900xt performs better but usually the 3090 is destroying it at 4k https://youtu.be/ji3PJ-UvGl0?si=MyislXbS2JjwCCOe,hardware,2025-11-01 18:12:14,3
AMD,nmcscct,"Right, barely anyone buys high tier AMD gpus. I don't expect most people buying cheaper, older, and used hardware to follow this news more than the average consumer in this market would.",hardware,2025-10-31 11:49:52,-2
AMD,nmdf39i,"I would that ever since the RX480 there wasnt ever since a good time to buy AMD which was feature deficient while costing as much as Nvidia. Not even sure why they exist at this point though. People also seem to have forgotten Zen5% as well just because they are AMD, ridiculous...",hardware,2025-10-31 14:02:27,3
AMD,nme2s4s,How much market share has AMD been able to capture in the server space?  consumer grade stuff is pennies on the dollar compared to enterprise.,hardware,2025-10-31 16:00:01,0
AMD,nmdonrz,AMD already has most of the non-AI supercomputer. Going after AI workloads especially now that they own Xilinx makes a lot of strategic sense even without Nvidia as the market leader.,hardware,2025-10-31 14:51:19,7
AMD,nmesila,"I mean, as written that sounds like a self fulfilling prophecy. We all thought the same about AMD trailing Intel for decades and then we had 14nm+++ and look here we are with Intel in crisis scrambling for new architecture and fab locations.   I wouldn't count Radeon totally out, even if they haven't had successes in many years. But yeah, they've been trailing the achievements of Nvidia through that time.",hardware,2025-10-31 18:07:16,1
AMD,nmfwzp0,People said that about intel,hardware,2025-10-31 21:44:16,1
AMD,nmbwm9h,"I'm probably wrong but I thought they advertised long term support for AM4, which didn't necessarily mean a motherboard would support all CPU's released for the AM4 socket.",hardware,2025-10-31 06:56:51,-5
AMD,nmdpp41,Then blame the developer for not testing on AMD. Hardware vendors shouldn't have to fix bugs in other people's software but they do out of the hope that they sell more hardware.,hardware,2025-10-31 14:56:29,3
AMD,nmdxdak,"> wouldnt run on well ANY AMD card without those drivers  I'm just saying that devs shouldn't be silently relying on one vendor's particular implementation and then blaming the other vendor when it doesn't work exactly the same. They should figure this shit out before it gets to the users. Relying on vendors to make day 1 drivers and users to install them is an anti pattern and a horrible experience. Drivers shouldn't have bugs, but devs shouldn't drive their games into a brick wall out of spite, either. Of course, only like 5 people actually use Radeon, so why would they give a shit",hardware,2025-10-31 15:34:06,2
AMD,nmcl4ld,That I can agree with. NVidia driver debacle for the past year has been horrible. Their razor sharp focus on AI is showing.,hardware,2025-10-31 10:55:49,0
AMD,nmkyvul,I'm factually incorrect because... you think Blender is relevant to a comment about game support?,hardware,2025-11-01 19:27:31,1
AMD,nmw30lp,"> They absolutely would have if they hadn't been tarred and feathered in the public square.  Which happens regardless of AMD's actual words and actions.  > That's what the words ""maintenance mode"" mean in this industry.  *Nobody* knows what the words ""maintenance mode"" mean in this industry. It might as well be another awful gamer buzzword, like ""optimization"".  > We cannot believe them anymore if they argue that the cards will be supported as long as they are still useful for newly released games.  Anyone admitting to being swayed by any part of this manufactured drama is admitting to willingly choosing to believe what they know is misinformation. That happens *all the time* with AMD.",hardware,2025-11-03 15:14:20,1
AMD,nmcesk0,\> fans base/subscribers upvote you and downvote me  Perhaps it's just people that are reading this exchange and think you are wrong. If you believe in delusions then no one can change your mind.,hardware,2025-10-31 10:01:55,7
AMD,nmuqepk,"Imagine being this confidently wrong. You literally have not looked at the specs for the CPU and GPU of the Z2A, its literally the fucking same.  Also THE PRICE LITERALLY ISN'T WORTH IT! The amount of defending Microsoft here for an inherently worse product is insane.",hardware,2025-11-03 09:36:17,1
AMD,nmd36nx,"There are not that many which do not run! A handful mostly multiplayer, i literally have almost 100% of my extensive Steam library running!",hardware,2025-10-31 12:57:32,2
AMD,nmf2cdm,4k monitors are still not that prevalent and having higher resolutions on smaller monitors makes no sense. 4k on 24inch is not necessary for example. So it’s not like there is a need for higher resolutions like there is a need for improvement of graphics.,hardware,2025-10-31 18:57:44,7
AMD,nmvj8p5,"""hey, if i use some obscure terrible resolution i can complaing GPUs wont run it as lower resolutions would.""",hardware,2025-11-03 13:27:39,1
AMD,nmebqdy,"But they can run these games, if the VRAM is available. You are just lying.",hardware,2025-10-31 16:44:06,-2
AMD,nmw0qf7,"True, but not enough to save a 10GB card.",hardware,2025-11-03 15:02:48,1
AMD,nml08n6,"I literally had both cards, at a much later date then that video    Granted, my version of the 6900xt was much faster than reference 6950xt even, so it shouldn't be surprising",hardware,2025-11-01 19:34:45,0
AMD,nmddzrd,Barely anyone know AMD exist. AMD has been nonexistent from the steam hardware survey and almost all professionals prefer to use Nvidia. These caeds shiwn be thrown into the trash as they belong very disappointed with AMD's behaviour.,hardware,2025-10-31 13:56:45,4
AMD,nmdlo0q,"Zen5% was quickly buried because of the strong X3D for enthusiast desktop and Turin for datacentre. Strix Point is just okay, but they have a steady measurable presence in all things x86 cpu. Not for gpu",hardware,2025-10-31 14:36:09,1
AMD,nme0ybe,"""Zen 5%"" never existed, it was bullshit influencer garbage like everything else.",hardware,2025-10-31 15:51:16,0
AMD,nme3tp4,Radeon Instinct in the data centre gpu space? Not much of anything at all vs overall market growth.,hardware,2025-10-31 16:05:06,3
AMD,nmby32z,"What was the point of making a big deal about AM4 socket's longevity if it didn't have support for new CPUs on older motherboards? It would be basically the same as Intel.  Like, the only thing is that it maybe makes developing new CPUs cheaper for AMD since they don't have to create a new socket/platform.",hardware,2025-10-31 07:12:02,8
AMD,nmdinqb,"It's hard to know if he's talking about NVIDIA or AMD, because the RX 5700 series also was plagued by horrible drivers. I remember the black screen issues, multi-monitor setups just straight up not working and horrible gaming performance for months with RDNA1. It seems anything labelled 5000 is just cursed with bad drivers.",hardware,2025-10-31 14:20:45,2
AMD,nmev5j5,Taking them longer than it took AMD to get 5700 fixed,hardware,2025-10-31 18:20:53,0
AMD,nmkzhmn,"No, but you specifically brought up GCN as an example of AMD’s long software support. Is it unreasonable to see that AMD has generally targeted a 4-5 year software support lifetime and then expect their GPUs to continue that trend?",hardware,2025-11-01 19:30:45,1
AMD,nmcfck3,Imagine watching the video he linked and thinking I'm delusional.    It's Reddit so it's possible I guess.,hardware,2025-10-31 10:06:54,-2
AMD,nmd3x4c,And your point is ? I should not play game ?,hardware,2025-10-31 13:01:43,5
AMD,nmgmmql,"Speak for yourself, my 32 inch 4k monitor paired with my 5090 slaps",hardware,2025-11-01 00:30:59,-2
AMD,nmw5tpk,4k is not some obscure resolution brother it's not 2012 anymore wake up.,hardware,2025-11-03 15:28:14,1
AMD,nmedcdb,"""At an acceptable framerate"". Good your game is running stable at 40, with ghosty taa or muddy fsr.",hardware,2025-10-31 16:51:56,4
AMD,nmdp3p9,Thats the thing measurable presence doesnt mean their produxt are even remotely good. I feel the X3D are a hack and would probably benefit all Von Neumann architectures. Enterprise probably cared about Turin due to steep sales AMD was peoviding to cloud provider to take their chips after the multi year long deals with Intel expired. While on GPU side no noteworthy changes have been happening. At this point it just feels that AMD serves as a reminder to Intel and once Intel gets back on its knees it may be back to status quo of Intel for CPU and Nvidia for GPUs like in the early 2010s.,hardware,2025-10-31 14:53:32,2
AMD,nme1cz4,Techpowerup article on Zen5%. Just stating facts based on gaming.  https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/,hardware,2025-10-31 15:53:13,3
AMD,nmc1flo,"Hmm, I suppose you're right and I didn't think this through.",hardware,2025-10-31 07:47:23,5
AMD,nmfv9ap,"pretty sure the 5700 was a hardware issue. i dont think it was ever actually fixed, they just did their best to hide it.",hardware,2025-10-31 21:34:07,2
AMD,nmev047,"I was referring to NVIDIA; I upgraded my RX 5700 to 5070. NVIDIA problems have been worse and going on longer than it took for AMD to fix, but both 5k series drivers have been cheeks",hardware,2025-10-31 18:20:07,1
AMD,nml37h5,It's really seeming like you don't understand the comment chain. Why exactly are you asking *me* that question?,hardware,2025-11-01 19:50:47,1
AMD,nmdn7q7,"You can, but you wont get day 1 patches, but given that most recent AAA titles overwhelm the Steam Deck by now a moot point again. Drivers still will be available in Windows but not day 1 drivers! And that the SD is not really affected by it due to its linux nature is the entire point, you never got day 1 patches for it anyway!  But if you install Windows on a Steam Deck just to play about 3 games which do not work in Linux, you have to ask yourself the question wouldn´t you be better off with another console which probably will get you more performance. You usually buy the SD for the console like experience and for the inputs but other consoles have surpassed it in performance by now, and also come with Windows but with a less user friendly experience out of the box!  I have yet to hear about a person runnung windows on that thing permanently!",hardware,2025-10-31 14:44:02,2
AMD,nmgn15s,"4k is only good for 27inch and upper fyi. And even for 27inch, 1440p is perfectly enough.",hardware,2025-11-01 00:33:48,3
AMD,nmhzw83,"Even worse then tbh, means it's baked in to being trash.",hardware,2025-11-01 07:21:19,5
AMD,nmlbmcy,> For how long have you known this was going to happen? And why didn't you tell anyone?,hardware,2025-11-01 20:35:24,1
AMD,nmdv3to,"When SD was released, there were very few options in market, either $1000 Chinese windows handheld, or $400 SD with decent performance.",hardware,2025-10-31 15:23:17,1
AMD,nmdh8x0,To think this was supposed to be used as a single universal wire for high end VR headsets…,hardware,2025-10-31 14:13:30,125
AMD,nme4thc,Usb-c pd is something I wish was more standard on GPUs.,hardware,2025-10-31 16:09:56,23
AMD,nmdhbe1,"Great to hear this was a typo. It would have been a ridiculous thing to remove for any reason, especially under the inane guise of “streamlining” the driver as some speculated.",hardware,2025-10-31 14:13:51,10
AMD,nmdau7t,Waiting for the outrage farmers to apologize...,hardware,2025-10-31 13:40:10,0
AMD,nmdtbf1,"Unfortunately VirtualLink was going to lead to a signaling clusterfuck, and Meta abandoned PCVR immediately anyway.",hardware,2025-10-31 15:14:35,25
AMD,nmeafa2,There’s so much spare PCIE bandwidth that some graphics cards even come with M.2 slots.  May as well turn these GPUs into expansion cards while we’re at it.,hardware,2025-10-31 16:37:42,26
AMD,nml56z9,Honestly was super useful for me virtualizing on a consumer board when all the other lanes were shared. Just used the USB on the GPU like any other port.,hardware,2025-11-01 20:01:31,2
AMD,nmdioiz,AMD specifically said ***this*** driver removes the feature. It's no surprise people using USB-C power delivery would **not** update immediately and thus not many would test it in 24 hours.,hardware,2025-10-31 14:20:52,95
AMD,nmdhb9h,"TPU did test it, they updated the article.",hardware,2025-10-31 14:13:50,26
AMD,nmi4x75,Yeah but how else could we make shitty clickbait videos about it?,hardware,2025-11-01 08:17:51,-3
AMD,nmg7rmx,"""users that require this feature are adviced to roll back to 25.3.1""  nice ""typo""",hardware,2025-10-31 22:52:37,16
AMD,nmdczgu,"Very little of the outrage was towards this    This did look like a bug, there was no benefit to AMD to disable the USB-C.",hardware,2025-10-31 13:51:33,34
AMD,nmlx06v,Maybe it should be AMD that apologized for writing misleading patch notes? This is what AMD themselves wrote in the patch notes:  >USB-C power charging has been disabled for Radeon RX 7900 series graphics products. Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.,hardware,2025-11-01 22:33:37,8
AMD,nmij4h8,"The whole driver release seems to be a clusterfuck. So, no, just because the maintainers for the driver don't know what they're actually doing, it doesn't look better.",hardware,2025-11-01 10:49:38,3
AMD,nmddztx,Did they ever refund the defective vapor chamber lot #'s?,hardware,2025-10-31 13:56:46,5
AMD,nmvn890,most of outrage was for the branching of drivers (legacy support) which wasnt takenack.,hardware,2025-11-03 13:50:58,1
AMD,nme03xb,What do you mean signal clusterfuck ? Laptop has been doing this for years,hardware,2025-10-31 15:47:16,27
AMD,nme5zw4,Meta?  Valve screwed everyone by not supporting on Index.  Wasted everyone's time and money.,hardware,2025-10-31 16:15:43,14
AMD,nmdu6te,Meta is only *one* player here and an unreliable one at that.,hardware,2025-10-31 15:18:50,14
AMD,nmerrhz,More like cards that only come with x8 lanes having a NVMe slot to use the full x16 slot that they are already using,hardware,2025-10-31 18:03:23,11
AMD,nmgd2jv,"I'm actually confused how they got that advice.  Because this article states that AMD reached out to them to let them know that this was misinformation, but the article from yesterday stated that ""AMD"" advises rolling back the driver.  So is AMD sending TPU mixed messages, or did TPU not actually talk to AMD regarding their original article?",hardware,2025-10-31 23:27:08,5
AMD,nme83g7,Reducing driver bloat?,hardware,2025-10-31 16:26:10,-1
AMD,nmdk9iq,They offered to repair/replace the cards IIRC.,hardware,2025-10-31 14:28:56,1
AMD,nmenb70,"Virtual Link runs 3.0 signals over pins that every other standard uses for 2.0, and is not reversible. Even if you could reasonably get that link negotiated and noise free over 5m lengths (a challenge that Valve publicly abandoned), you'd have a consumer cluster fuck of confusion as their standard USB C cables failed to work properly and looked identical.  Quotes from wiki:  > Unlike most alt-modes this remapped A7, A6, B6, B7 to carry a USB 3.0 signal, instead of the usual passive USB 2.0 signal. This means that one would not be able to extend the cable using a standard USB-C 3.0 cable, which has these pins mapped only for unshielded USB 2.0 signals. Also this required the VirtualLink port to also detect the correct orientation of the USB-C plug to ensure that the USB 3.0 TX and RX lanes are correctly connected.  > In VirtualLink mode, there were six high-speed lanes active in the USB-C connector and cable: four lanes transmit four DisplayPort HBR 3 video streams from the PC to the headset while two lanes implement a bidirectional USB 3.1 Gen 2 channel between the PC and the headset. Unlike the classic DisplayPort USB-C alternate mode, VirtualLink has no USB 2.0 channels active, instead providing a higher speed USB 3.1 Gen 2 (SuperSpeed+) over the same A6, A7, B7, B6 pins. VirtualLink also required the PC to provide 15 to 27 watts of power.  > To achieve six high-speed lanes over USB-C, VirtualLink required special cables that conformed to version 1.3 of the USB-C standard and used shielded differential pairs for both USB 2.0 pairs.",hardware,2025-10-31 17:41:16,29
AMD,nme6m41,Maybe for longer cables? usually you don't get 5m long cabales for higher bit rate USB C standards (like 40gps and the new 80 one). Those that get to longer length usually are limited to 3m and are hella expensive due to active components in cables. If optical transceiver will get cheap enough there might be a case for those in the future.,hardware,2025-10-31 16:18:46,6
AMD,nmenow5,"How many Indexes were sold? Now ask how many Rifts, Vives, and more importantly Quests were sold.   Valve couldn't get virtuallink working reliably enough for a consumer product. They're not responsible for its failure.   > The adapter cable was originally meant to provide added convenience for Valve Index users, making it so they could rely on a single USB-C connection to the headset rather than requiring separate physical connections for video, USB, and power.  > However, for multiple technical reasons we no longer believe that the product would deliver that added convenience. Foremost on that list is reliability. Our current testing indicates the VR connection may fail to establish in a reliable manner. Additionally, Virtual Link technology has not been widely adopted by manufacturers, laptops in particular (where a single connection could be the most beneficial), translating to very few PCs having viable ports for the connection.",hardware,2025-10-31 17:43:10,13
AMD,nmeflc1,Valve put out an exclusive title and provides hardware support to this day. Unless you’re suggesting that they became a publisher like Meta did I don’t know what else they were supposed to do,hardware,2025-10-31 17:02:49,6
AMD,nmlzfwt,"No point making more big VR games when Half Life Alyx (the one AAA VR exclusive) gets outsold by Indie games with a 1 man dev team. And btw I love VR, I'm just telling it like it is.   Sony did the same with the Horizon VR game, once they saw the sales of that and PS VR2 they stopped development on all their upcoming VR games and killed all their studios that only made VR games.",hardware,2025-11-01 22:48:02,1
AMD,nmdxxmz,It's the only player that matters for hardware standards. I say that as an avid PCVR player that uses non-Meta hardware to this day.,hardware,2025-10-31 15:36:51,7
AMD,nmi9qmg,"> but the article from yesterday stated that ""AMD"" advises rolling back the driver.  ""USB-C power charging has been disabled for Radeon RX 7900 series graphics products. Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.""  They put this in the changelog, so no reason to ask AMD for the same thing if they put the answer in the changelog already .. or so everyone thought .. once the posts went up, they reached out ""nono, it's all a mistake"" .. so we published that .. then I was like ""wtf mistake, these people can't be for real"" .. so I fired up a 7900 XTX and tested it for myself, and oh surprise, it has always worked, without driver, with launch driver, with newest driver, so we updated our stories.  AMA",hardware,2025-11-01 09:11:55,16
AMD,nmgbk4r,I had 6900 XT with a bad vapor chamber. 30 degree gap to the hotspot and always almost instantly hit the thermal limit. RMA'd and they gave me 7900 XTX which was great at first (assume it was a refurb/repair) but now after a year also suffers from about 25 degree gap when under stress from a big game. Max temps on the hotspot still hits 105 unless I undervolt and enable Chill,hardware,2025-10-31 23:17:21,3
AMD,nmf7zom,"Common dude, VirtualLink was all about PCVR in 2018/2019.  Rift came out in 2016, so irrelevant.  Vive 2016, irrelevant.  Quest targeted standalone VR, it was not for PCVR, it didn't even have DP.  Index was clearly supposed to support VirtualLink, then they drop it and later say they'll have some addon but it was dead at that point.  Go plug PSVR2 into a Turing GPU, that is essentially VirutalLink working just fine.  No technical reason Valve couldn't have gotten it to work.  They chose not to.",hardware,2025-10-31 19:27:14,-9
AMD,nmf64jx,What exclusive title are you talking about? Alyx is still very much playable without any Valve hardware.,hardware,2025-10-31 19:17:27,3
AMD,nmei3pv,"Everyone was going to support VirutalLink (NV/AMD/Valve/...), NV shipped Turing with VirtualLink and then like 6 months later Valve ships Index without VirtualLink.  That was the end of VirtualLink, no one was going to waste more time on it.",hardware,2025-10-31 17:15:12,8
AMD,nmlzpmt,Make more games which btw they were making but then never announced said games many which are believed to be cancelled post half life alyxs release.,hardware,2025-11-01 22:49:38,1
AMD,nmdyzjq,"Oh, please.",hardware,2025-10-31 15:41:53,1
AMD,nmgckum,Did you try to RMA that one too?  Or does it fall under the original warranty?,hardware,2025-10-31 23:23:59,0
AMD,nmfdrzc,"> The adapter cable was originally meant to provide added convenience for Valve Index users, making it so they could rely on a single USB-C connection to the headset rather than requiring separate physical connections for video, USB, and power.  > However, for multiple technical reasons we no longer believe that the product would deliver that added convenience. Foremost on that list is reliability. Our current testing indicates the VR connection may fail to establish in a reliable manner. Additionally, Virtual Link technology has not been widely adopted by manufacturers, laptops in particular (where a single connection could be the most beneficial), translating to very few PCs having viable ports for the connection.  PSVR2 also does not use VirtualLink. It needs a port capable of DP Alt Mode and 12v USB PD. VirtualLink ports happen to be compatible. It's an important distinction. It doesn't repurpose the USB 2.0 data pins for 3.0 speeds, which is part of the VirtualLink spec and was the issue.   https://steamcommunity.com/app/992490/discussions/17/3879344463500344882/",hardware,2025-10-31 19:58:04,12
AMD,nmf7u38,"Flagship was probably a better word, but I meant PCVR exclusive",hardware,2025-10-31 19:26:24,8
AMD,nmlzyd4,Can't make games exclusive when they run on x86 Windows PCs but it is Steam exclusive as in you can't buy it anywhere else.,hardware,2025-11-01 22:51:02,1
AMD,nmfe15f,"Everyone? Only one VR headset ever announced an intent to support it, and not a single laptop ever implemented it.",hardware,2025-10-31 19:59:24,7
AMD,nmfnsph,"VR barely exists, and you're claiming ""everyone was going to support VirutalLink"", which was a proprietary non-standard abuse of a standard?  No.  No one was going to meaningfully support that.  You're lucky there was even a single platform where it worked.",hardware,2025-10-31 20:51:43,7
AMD,nmei14i,"I mean it’s an exaggeration for sure, but not that huge of one. More VR headsets in use today are made by meta than by every other brand combined, and something like 70% of all new headsets sold are theirs.   They own that market pretty thoroughly, unfortunately.",hardware,2025-10-31 17:14:51,7
AMD,nmemtvl,"What's the market share and revenue share for Meta's headsets? What do the next most popular competing headsets look like? Are the successful competitors PCVR headsets with a cable, or are they wireless standalone headsets?",hardware,2025-10-31 17:38:51,7
AMD,nmg1iaa,"It was standard that was supported by every VRPC company, as well as Meta.  That's hwo standards are made.",hardware,2025-10-31 22:11:58,-3
AMD,nmgj0iz,"Name a ""VRPC company, "" as you put it, that used VirtualLink.",hardware,2025-11-01 00:06:25,1
AMD,nmxo8jr,"NVIDIA, AMD, Oculus, Valve, and Microsoft were part of consortium.  What hardware company was not?",hardware,2025-11-03 19:51:03,0
AMD,nmxpm63,"Name a ""VRPC company, "" as you put it, that used VirtualLink.",hardware,2025-11-03 19:57:47,0
AMD,nm820d2,"EDIT: AMD has clarified, [and TPU has tested](https://www.techpowerup.com/342468/usb-c-power-delivery-is-fine-on-rx-7900-xtx-we-tested-it-amd-confirms), that AMD made a mistake in the release notes. Hooray.  [AMD corrects error, confirms Radeon RX 7900 USB-C port still delivers power - VideoCardz.com](https://videocardz.com/newz/amd-corrects-error-confirms-radeon-rx-7900-usb-c-port-still-delivers-power)  >~~AMD hasn't explained why it made this change, but since no RX 9000 cards include a USB-C port,~~ **~~it may have been removed to streamline the already large driver package.~~**  ~~Is type-C power so much more complicated than type-C display that it bloats the driver package? Anyways, I've never heard someone who bought a $200 - $1000 GPU~~ **~~EVER~~** ~~complain, ""Damn, I wish AMD removed hardware features to make my once-a-month downloads save a few hundred MB!""~~  ~~Seems like~~ *~~a very~~* ~~flimsy excuse by AMD, if true. Videocardz is speculating, but it seems like AMD's workaround is to use an ancient (in terms of games) driver, so it's seemingly working just fine in~~ **~~some~~** ~~drivers.~~  ~~//~~  ~~The real problem: some bidirectional USB-C to DisplayPort cables~~ [**~~require~~** ~~power output on the USB-C port~~](https://www.reddit.com/r/UsbCHardware/comments/gh6w4e/comment/k4ugi44/)~~:~~  >~~REVERSIBLE CABLE: This 6.6ft/2m DisplayPort to USB-C bidirectional cable connects your USB-C Laptop to a DisplayPort Monitor or DisplayPort device to a USB Type-C display; Note:~~ **~~USB-C port (computer or monitor) must support Power Delivery (minimum 5V)~~**  ~~Here, USB-C GPU to DisplayPort monitor would mean~~ *~~only~~* ~~the GPU can provide power and now these cables may no longer work properly.~~",hardware,2025-10-30 17:08:51,91
AMD,nm852lq,"RDNA1, and especially RDNA**2**, are *extremely* popular today, especially as RDNA3 wasn't as competitive. All 6000 APUs, all Z1 / Z2 handhelds, all Steam Decks, all 6000 dGPUs, all 5000 dGPUs, many 7000 APUs, etc.  These are likely ***tens of millions*** of devices using RDNA1 and RDNA2. Is AMD hurting for cash that badly?",hardware,2025-10-30 17:23:33,86
AMD,nm8qm2o,It's crazy to think that some refresh models like 6750xt/6950xt released in 2022 will only get 3 years of game driver optimization. Some of them may not be even out of warranty,hardware,2025-10-30 19:05:27,71
AMD,nm7rzya,">RDNA2/RDNA1 GPUs to sub-branch in latest driver  Does this mean that AMD will soon cut the driver support for these GPUs? This looks kind of worrying TBH, I would understand if they only did it with RDNA 1 considering most of those GPUs don't even support DX12 Ultimate therefore they are already considered obsolete with most upcoming modern games onwards, but with RDNA 2? they aren't even that old only 5 years old, and still is being used to power the current gen consoles which will be only replaced on 2027. It's way too soon for this GPU architecture to be dropped.",hardware,2025-10-30 16:20:47,61
AMD,nm8k28w,"Removing hardware features that customers paid for through driver update. Planned obsolescence, boooooo! 👎",hardware,2025-10-30 18:34:44,30
AMD,nm8vspu,"Last flagship RDNA2 card to launch was the RX 6950XT, which I own*. It launched in May of 2022. RX 6750XT also launched at the same time. RX 6750 GRE launched in October of 2023. RDNA 2 APUs are still being used in current handheld devices. Gaming devices that launched in 2025 potentially won't get any driver level performance fixes for games that released the same year. The Xbox Ally uses RDNA2.   Any way you slice it, five year old architecture getting relegated to 'bugs and security only' status in drivers is an absurdly short time frame. Vega was bad enough, but this is somehow even worse.  I can't see myself buying or recommending another AMD GPU/APU in the near future if this is the level of driver and optimization support they're content to provide even relatively recent devices.  *(Functionally an RX 6900XT with a factory overclock, which I bought in 2023 at a steep discount). Edit: (Got the Ally and Ally X confused, corrected)",hardware,2025-10-30 19:30:25,29
AMD,nm9v3g7,"Yup, final nail in the coffin, not buying AMD GPUs again in the foreseeable future.  There is a reason Nvidia is at the top.",hardware,2025-10-30 22:30:34,27
AMD,nm9ycy6,But the internet has assured me that AMD is my friend?,hardware,2025-10-30 22:49:18,26
AMD,nm9viae,Does this mean my USB-C portable monitor will stop working on my 6800?,hardware,2025-10-30 22:32:54,8
AMD,nm7yx1i,"This is what people need to remember when they think AMD cares of long term GPU driver support. They already screwed over customers who bought new APUs with VEGA chips and within the same year, discontinue non-security updates to their drivers.",hardware,2025-10-30 16:53:52,48
AMD,nm9ddin,The YouTubers pushed RDNA2 over RDNA3 to their audience. You know because used products cheaper then new products or something.   Crazy how bad this timeline is against their credibility. Not saying they would have predicted this but that follow up video where HUB stands by their 5700 XT recommendation over RTX 2060 Super just got another slap in the face.,hardware,2025-10-30 20:55:46,20
AMD,nm9r2wr,A reminder that Nvidia still supports USB-C in their drivers,hardware,2025-10-30 22:07:21,10
AMD,nmaht9h,"Yep. Learned my lesson, next card is an Nvidia now. Just bought 9070XT earlier this year.",hardware,2025-10-31 00:41:03,6
AMD,nm82ruh,TIL there are desktop gpus that have usbc,hardware,2025-10-30 17:12:32,16
AMD,nmc806b,**A user on the AMD sub** [**confirmed**](https://www.reddit.com/r/Amd/comments/1ojxjuq/comment/nm8xzdp/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) **VirtualLink & PowerDelivery still work.**,hardware,2025-10-31 08:55:24,2
AMD,nmcc64x,It is wild to have a simple (and USB PD is very simple relatively speaking) feature removed from your card not long after you bought it.  If we had sane consumer protections this would not be possible/would be an easy class action.,hardware,2025-10-31 09:37:25,2
AMD,nm8d56i,"We’re in a race to the bottom to see who can provide the shittest treatment to their customers, aren’t we?  Between this and Intel’s announcement of moving their driver support for Xe-LP to a Legacy Branch, it sure is fun to be in the GPU market.",hardware,2025-10-30 18:02:12,5
AMD,nmbm7wv,Why is there a Type C port on the back of a GPU?,hardware,2025-10-31 05:14:02,2
AMD,nm8mei7,"People can drama as much as they want to, but GPUs will be working as usual. The vast majority of users won't ever notice the slightest difference.",hardware,2025-10-30 18:45:40,-13
AMD,nm7p8cj,"Hello BarKnight! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-30 16:07:39,0
AMD,nm8fl3o,"Oh they're not thinking about the consumers or users downloading the package. They're considering their own archival storage, downloads, hosting, validating, compiling etc. We do this kind of pruning in other sectors of software development all the time. Taking just one virtual machine out of production permanently usually yields thousands in savings per year. Same mentality but different verticality",hardware,2025-10-30 18:13:42,53
AMD,nm8luaa,It's a niche use case but I think that also forces anyone using PSVR2 to buy a virtuallink adapter or stay on outdated drivers.,hardware,2025-10-30 18:43:03,12
AMD,nm8j2w2,"They are also actively being sold in substantial quantities. I understand that AMD's discrete GPU market share is small, but they sell a ton of APUs. Such extremely short support period where they cut software resources from supporting a literally actively sold product that people will likely use for 5+ years from now is really concerning.",hardware,2025-10-30 18:30:05,47
AMD,nm8ty23,Amd drops driver support like a hot potato once they move on to a new arch.  Just because they stayed on gcn for ten years people started thinking amd supports their products.,hardware,2025-10-30 19:21:28,36
AMD,nmakwsk,"This is what confuses me. Are they actually effectively cutting off game optimization for millions of hand held owners? That can't be possible, right? Those things are still on physical and e-commerce ""shelves"" around the world. They can't possibly be sunsetting products that are still actively being sold. Doing that to rdna1 would make slightly more sense, but even that is lame given that it launched in 2019 alongside the Turing refresh, if I remember right.  It's easier to justify the Nvidia tax in your mind if you count on long term support. I used an rx 480 4g from 2016 to 2022 (intended to upgrade when rdna2 launched, but laughed at the pandemic pricing). I've had an rx 6700xt in my system since early 2023, I think. If that is being cut off even from game optimizations in new drivers, that will dissuade me from buying a new Radeon gpu. I keep things for a few years at a minimum, sub 5 years of active support is extremely disappointing.  I wonder what the fallout from this will be.",hardware,2025-10-31 00:59:08,8
AMD,nm8xat7,This article is a bit misinformative. AMD isn't dropping support for RDNA1 and 2 outright.,hardware,2025-10-30 19:37:45,-5
AMD,nmve18t,"RDNA1 and RDNA2 wasnt competetive at launch, let alone now. Its a legacy architecture on life support.",hardware,2025-11-03 12:55:33,0
AMD,nm9ufyh,"Like mine.  That's not mentioning drivers are still broken for many people, myself included.  Truly AMD moment.  At least Intel has an excuse of being a new player.",hardware,2025-10-30 22:26:53,31
AMD,nmaznwi,Does game driver optimization really have any substantial impact on performance? My uninformed guess would be maybe it’s 2-3% better but the drivers can’t do magic and games can’t be that wildly different. Ending support so early still sucks but I wonder how much impact it actually has on games?,hardware,2025-10-31 02:27:12,-1
AMD,nm8ag7r,"VC just posted an update to include a response AMD made to PCGH, seemingly confirming the move to ""maintenance mode."":  [https://videocardz.com/newz/amd-confirms-focus-shifts-to-rdna3-and-rdna4-rx-6000-and-rx-5000-lose-day-1-game-optimizations](https://videocardz.com/newz/amd-confirms-focus-shifts-to-rdna3-and-rdna4-rx-6000-and-rx-5000-lose-day-1-game-optimizations)  (translated)  >RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security fixes and bug corrections. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.",hardware,2025-10-30 17:49:34,30
AMD,nm7v929,">It's way too soon for this GPU architecture to be dropped   This isn't new to AMD    They dropped vega in 2023. So 6 years after the architecture launch, but <2 years after launching APUs with Vega iGPUs    RDNA 2 products are _still releasing_ in APUs, wonder what happens there (Xbox Ally)    Say what you like about Nvidia, but they support their products far longer.  Hard to justify AMD GPUs when they do this twice",hardware,2025-10-30 16:36:22,94
AMD,nm7z2oj,They will call it “extended support” and not discontinue it,hardware,2025-10-30 16:54:36,12
AMD,nm8lrg9,"That's actually kinda sucks. I've bought AMD GPU's for last decade as they were best price / performance offers in my area, but now my 6800 is basically discontinued. Guess I have to pay big bucks now just to get a card that can hold on for at least 5 years on the driver side.",hardware,2025-10-30 18:42:40,9
AMD,nm7tglz,I’m only speculating but I would guess the sub branch means they will get driver support still but not necessarily all the new features of the main drivers?,hardware,2025-10-30 16:27:47,4
AMD,nm7tbvj,false news,hardware,2025-10-30 16:27:09,-11
AMD,nmae4s1,"Whatever that reason, this isn't it because nvidia's been doing the exact same thing since they got into the GPU market.  At some point you stop optimizing for older architectures and focus on the one ones. GPU's 2 generations old are generally about where that line is. And that's all that's happening here.  The only thing different now is that AMD was public about it, and others trying to turn it into a news story with misleading titles and ranting posts on reddit.",hardware,2025-10-31 00:19:11,-5
AMD,nmafu6j,"And here the internet is trying to convince you that AMD is somehow doing something evil, when they are in fact only being open about something that has been standard practice in the industry for as long as there has been a gaming GPU industry.",hardware,2025-10-31 00:29:23,-13
AMD,nm9dutz,"Kidding me all I've seen is post after post of AMD astro turfing and links to GN or HUB to validate it.   ""NvIdIa Is RuInInG gAmInG!""",hardware,2025-10-30 20:58:09,18
AMD,nm803lt,"Last AMD GPU I bought ( HD5850 ) I got driver support for the 7 years I kept it before upgrading.  RX480 still had driver support as of 25.5.1 ( so may 2025 ). That's 9 years of support even tho ""official"" support has been dropped in 2021.  That's what they mean by ""sub-branch"", you think it's discontinued, but it really isn't yet.",hardware,2025-10-30 16:59:29,1
AMD,nm83n07,">They already screwed over customers who bought new APUs with VEGA chips and within the same year, discontinue non-security updates to their drivers.  Vega products still get driver releases with game support, fixes and improvements. Here are just a few examples (open and read the Release Notes) :  Mendocino drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html)  Rembrandt drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html)",hardware,2025-10-30 17:16:40,-6
AMD,nm8x44h,This misinformation is what people need to remember? How about no?,hardware,2025-10-30 19:36:50,-8
AMD,nm9v8et,"9000 series will get the same treatment, only reason AMD is doing this is because people buy.",hardware,2025-10-30 22:31:20,11
AMD,nma02c3,I remember 2070S vs 5700XT was argued at the time that AMD offered more value. The AMD card cannot even launch new global illumination games and has no competitive upscaler compared to DLSS4.,hardware,2025-10-30 22:58:52,13
AMD,nm9vmtq,Reminder Nvidia still supports 10 year old GPUs.,hardware,2025-10-30 22:33:37,16
AMD,nmm1nj5,Intel might be a more viable option than AMD currently is by the time you need to upgrade but yea Nvidia is a safe bet.,hardware,2025-11-01 23:00:49,1
AMD,nm8cdtn,also see Nvidia Turing 2xxx series,hardware,2025-10-30 17:58:37,17
AMD,nmbzwzw,"> We’re in a race to the bottom to see who can provide the shittest treatment to their customers, aren’t we?  This is literally the same level of service AMD always provided.",hardware,2025-10-31 07:31:18,3
AMD,nm972ro,And this is called apologism. These companies are relying on people like this to erode expectations.,hardware,2025-10-30 20:25:14,9
AMD,nm9vir1,"Yeah you see ""working as usual"" for AMD is still broken.  Driver hangups, high hotspot temperatures, random boot issues, etc.  All I experienced in one year of owning my 6750XT that I bought brand new.  It's perplexing because I also have 5700x CPU and that thing fucking rocks, I wouldn't recommend AMD GPU to anyone yet their CPUs are incredible that I only have praise for them.  Runs cold, sips power, incredible performance and stability.",hardware,2025-10-30 22:32:59,8
AMD,nm8qhe8,"for me its all very strange because my laptop had an amd apu and gpu, now the drivers dont work for the apu. really frustrating",hardware,2025-10-30 19:04:50,2
AMD,nm8fycn,"To disable hardware features you ***sold?*** Seems scummy, less pruning.  You sell **hardware**, you support it. Next time, AMD ought to only ship what it **will** support, not disable hardware it sold.",hardware,2025-10-30 18:15:27,51
AMD,nm9mdi4,If removing one VM saves thousands per year then somewhere along the line you lost the plot.,hardware,2025-10-30 21:41:36,17
AMD,nm8p8xn,"They just re released a bunch of zen3+rdna2 laptop cpus too, same thing with new name, just like they did with z2a previously which is basically steamdeck oled cpu",hardware,2025-10-30 18:58:55,20
AMD,nmbvvpa,"the fact the very AMD desktop chiplet IO die has RDNA2 iGPU in it.   They are basically drop FULL support for the newest product they are still selling lol wtf.  if that is their intent, may be they should have updated Zen 5 IO die to RDNA 3.5.",hardware,2025-10-31 06:49:18,6
AMD,nmdarb6,This right here!  Hell AMD dropped Mantle support moment MSFT announced DX12. Imagine if AMD kept supporting Mantle as DX12 got out of announcement stage to actual market presence.  But naaaaah why support a feature we promoted endlessly!,hardware,2025-10-31 13:39:45,7
AMD,nm9o1is,"The article is precisely clear, IMO. Sub-branch, maintenance mode, no specific new game updates. That's all basically the same thing to me.",hardware,2025-10-30 21:50:20,22
AMD,nmbznlm,Legacy support is not new or unique. Neither is it anywhere close to full driver support.  That's actually the status gtx 750ti and gtx 900 series were put on just some weeks ago,hardware,2025-10-31 07:28:35,3
AMD,nma4nwk,This is the dark side of AMD that people don't tell on the comments in social media,hardware,2025-10-30 23:24:35,18
AMD,nmd2k6m,"Pretty sure there was a video showing how the legacy drivers for the older GCN cards didn’t work at all with Halo Infinite, but with custom Nimez drivers were able to run the game pretty well.",hardware,2025-10-31 12:54:01,3
AMD,nm81yyg,"To underline this, Nvidia is preparing to stop game-ready support for Maxwell released in 2014 and Pascal release in 2016 now. Compare that to RDNA released in 2019 and RDNA2 released in 2020, this is quite bad by AMD.",hardware,2025-10-30 17:08:40,63
AMD,nm81yo7,"Vega products still get driver releases with game support, fixes and improvements. Here are just a few examples:  Mendocino drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html)  Rembrandt drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html)",hardware,2025-10-30 17:08:38,15
AMD,nm9xcbo,> They dropped vega  Nobody cared about Vega. Especially not Raja Koduri.,hardware,2025-10-30 22:43:24,-2
AMD,nm9ky54,You won't get day 1 drivers for newly released games but they are not removing the support for GPUs. It will continue to get updates and fixes. Vega still receives updates and fixes.  Also there is always Linux.,hardware,2025-10-30 21:34:18,2
AMD,nm7tpck,They will get support but way less frequently.,hardware,2025-10-30 16:28:56,23
AMD,nm7xv98,"yeah, it's only going to get security updates.",hardware,2025-10-30 16:48:53,9
AMD,nmc56zl,"Nvidia is only now stopping support for Pascal and Maxwell, GPUs that are around a decade old and 4-5 gens behind......  If I bought a GPU that was still being made a year or two ago, that was released three years ago, I expect more support than this.  No clue why you have to defend multi billion dollar companies, they won't reward you for sucking their dick, if anything they will throat fuck us even more.",hardware,2025-10-31 08:26:41,10
AMD,nmvf9c6,It was never standard practice. In comparison Nvidia supports its architectures twice as long.,hardware,2025-11-03 13:03:16,3
AMD,nm9zd6a,"Fact of the matter is, if you buy a new in production nvidia card, you can expect ten years of full support witout any ifs and buts.  In the past you could at least argue AMD had to drop unpopular generations with low sales volume like Vega and wouldn't do the same for popular ones. If they actually put a very successful gen like RX 6000 into maintenance mode after just five years, they're really hurting their image.",hardware,2025-10-30 22:54:59,12
AMD,nm80d2u,">RX480 still had driver support as of 25.5.1 ( so may 2025 )   It still gets security updates, not the same level of support later GPUs get",hardware,2025-10-30 17:00:45,28
AMD,nmaes50,"i'm pretty sure this sub-branch is a separate thing from the maintenance drivers for those even older cards.  All AMD is doing here is no longer releasing day 1 game drivers with 5000 and 6000 series support. All the normal driver updates will still inlude the 5000, 6000 series.   Those day 1 game driver didn't include optimisations for older GPU's anyway, even if those cards were supported. so really, nothing substantial has chanced, except that AMD's upfront about it.",hardware,2025-10-31 00:23:02,0
AMD,nm8929v,They’re “retiring” as of last year:  https://overclock3d.net/news/gpu-displays/one-foot-in-the-grave-amd-starts-retiring-polaris-and-vega-by-reducing-driver-support/,hardware,2025-10-30 17:42:55,15
AMD,nmvfl85,"well people stopped buying, as AMD market share keeps declining for a decade straight.",hardware,2025-11-03 13:05:21,1
AMD,nmd2ah0,"Unless there is a huge shift in design philosophy between RDNA5/UDNA, I'd hope they'd drag RDNA4's design forward.  RDNA1/2 are just too primitive when you factor in where the market has been going for since RTX/DLSS entered the scene.  For years I was surprised Youtubers, let alone the audience, didn't call out AMD for basically not moving the needle for features/functions NV was shoehorning everywhere they could leading them to become wanted features.  RDNA4 finally shows up, and the Youtubers in soyjak fashion all pointed and shouted ""AMD finally did it"" as if they should be praised for showing up to the race NV and Intel have been having for the last 2+ years, or if you get honest due to Intel's supply - NV has been solo running for 7+ years.  *EDIT: added a word for clarity",hardware,2025-10-31 12:52:31,1
AMD,nmd3nr0,"People comparing 2070S vs 5700XT were already shifting the goalposts.  ""Raster is king"" I believe was coined by GN which basically turned his audience against NV's features.  The 2060S barely lost to the equal priced (after price cut because AMD wanted to put it against the 2070 initially, but the supers blind sided them. Factor in NV even raised the price on the 2060S because AMD tried to release the 5070 XT at $450 [again targeting the 2070] - imagine what a $350 2060S would have done for RTX adoption and directly AMD's margins, AMD should be grateful NV is greedy.)  The RTX 20 series got a bad shake from the Youtubers and yes some of that blame is NV's fault. But by the time DLSS 2.0 rolled around, they should have reflected but rather then investigate these new features and realize their potential they stuck to metrics (I blame laziness - ""you expect us to bench what we do and upscalers too!"") they felt comfortable with and used that to basically mislead their audience through omission.  EDIT: typo and clarity",hardware,2025-10-31 13:00:15,4
AMD,nmaf0ud,There is literally just one game that wont launch on the 5000 series.,hardware,2025-10-31 00:24:29,-1
AMD,nmaffb2,"Actually Nvidia's dropping support for a 10, 9 and 7 year old GPU's next month.  And AMD is not dropping support of the 5000 and 6000 series GPU's. All the normal driver updates will still support those GPU's.   It's only the day 1 game specific drivers that wont be supported on the 5 and 6000 series... which never contain any optimisations for older GPU's anyway.",hardware,2025-10-31 00:26:55,-7
AMD,nm8do0x,"Technically, laptops that have the NVIDIA GPU hooked up to USB-C also count as USB-C GPUs too.     I found this out the hard way when using NVCleanstall.",hardware,2025-10-30 18:04:39,14
AMD,nma21oe,"You should RMA your GPU. That's definitely no driver issues, your particular unit seems defective.",hardware,2025-10-30 23:09:58,-4
AMD,nma5exl,"I worked at one place using full-price EC2 instances, we complained about how expensive that was and the execs shrugged. 😂",hardware,2025-10-30 23:28:48,6
AMD,nmcl8xw,"Right. A big point of VM is to *save* money in a half dozen ways, even if it costs a little more to hire people who can manage tons of them.",hardware,2025-10-31 10:56:47,3
AMD,nme7bos,And thus you get to a point where you trim the service levels on the containers and vm-services on your cloud platform and the amount of the containers and machines,hardware,2025-10-31 16:22:19,1
AMD,nmdpv3z,Means they dropped support for Zen 5 shocking...,hardware,2025-10-31 14:57:19,1
AMD,nmad3dl,"Their drivers already well optimized. there isn't much to gain with game specific optimisations. hell, even on current gen GPU's they barely do anything most of the time.  The issue here is that AMD stated outright what the whole industry has been doing for decades, so now suddenly it's a story.",hardware,2025-10-31 00:12:58,-17
AMD,nma5kdi,"AMD causing RDNA 2 and 3 cards to BSOD with shitty drivers twice in span of 3 months didn't get mentioned once by *certain* YouTubers, but proportionally smaller issues on Nvidia side, including 3000 series, were all over said channels.  Because Nvidia bad.",hardware,2025-10-30 23:29:39,26
AMD,nmvej8t,"Oh they do tell, then get attacked by a mob of ""fine wine"" AMD fanatics.",hardware,2025-11-03 12:58:42,1
AMD,nm9nrso,"Yeah, that's like NVIDIA stopping updates for the RTX 3000 series, also released in 2020.",hardware,2025-10-30 21:48:54,22
AMD,nm9ik1l,Fuck me. Now I have to worry about this when my 2070S dies. Was planning on going AMD :(,hardware,2025-10-30 21:21:59,5
AMD,nm8l5vt,They stopped giving some vega products updates like the whole 5000 series. Here's 5625U with Vega 7 released in 2022 which stopped getting support  https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-5000-series/amd-ryzen-5-5625u.html,hardware,2025-10-30 18:39:53,18
AMD,nm83oeg,Cuz people are so focused on value all the time and only pay attention to immediate value. People don't think about the fact that these are businesses so if a company is taking more money for their product then there's a chance that they also have better services to support the product. It doesn't always mean this but it definitely makes sense. If you have a company that offers really great deals and beats the prices of the competitors all the times then they're probably going to have less than warranty or less than customer support because of the funding.,hardware,2025-10-30 17:16:51,2
AMD,nmbqrss,"Not the point. Its a positive feedback loop   AMD don't support their products due to a low userbase, encourages people to avoid their products until they're popular, cycle repeats",hardware,2025-10-31 05:57:22,8
AMD,nm82lzo,"Vega products still get fixes and improvements, not just security updates.   [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-5-1-POLARIS-VEGA.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-5-1-POLARIS-VEGA.html)",hardware,2025-10-30 17:11:45,3
AMD,nmadl0o,"No, the only thing they are not getting anymore is day 1 game specific driver releases.   Everything else they are still getting full support.  You don't need game specific drivers for a architecture that already has pretty well optimized drivers. Not to mention that this has been standard practice for ages already, by both vendors, just never outright stated.",hardware,2025-10-31 00:15:52,-1
AMD,nmd2v9u,"AMD has kind of historically dropped features/functions that didn't instantly catch on or others (community/devs) didn't flesh out for them.  If you actually look at AMD's handling of the GPU division after dissolving ATI, this isn't surprising. At all. If anything, because RDNA4 is more on par with current Intel/NV offerings - it makes more sense.  The real black eye is that AMD continues to drip feed RDNA4 to users so it's not like users can openly embrace RDNA4 which would soften the sting.  But this is AMD dGPU, which is why I facepalm seeing how Youtubers treat them with kids gloves.",hardware,2025-10-31 12:55:45,5
AMD,nm8ia6q,"Indeed, but the drivers still work and your games still work so *shrugs*.",hardware,2025-10-30 18:26:22,-10
AMD,nm9l8o8,AMD also has a solid open source stack on Linux. So you're really never not supported.,hardware,2025-10-30 21:35:47,-10
AMD,nm8b9c6,"Read the entire statement, not just the headline. I just gave you a bunch of examples where Vega products still get driver releases with game support, fixes and improvements",hardware,2025-10-30 17:53:23,-5
AMD,nmvfy7k,It's most stable these days when it comes to GPUs for gaming.,hardware,2025-11-03 13:07:37,1
AMD,nmakqfg,"AMD is dropping support for RDNA and RDNA2, it will just be security updates and bug fixes. The latest driver update yesterday didn’t include the new Vulkan extensions for those architectures.  Also earliest Nvidia card not supported is 8 years old, not 7.",hardware,2025-10-31 00:58:04,17
AMD,nma3ur9,"According to XFX GPU is fine and all issues I have don't fall under warranty  Issue *is* with drivers, okay hotspot temps are bit weird but they are still easily in limit (85-90c) and it's probably just thermal paste since normal temp is perfect and GPU is quite without any throttling.  Driver hang ups happen when alt tabbing and AMD already caused that to result in BSOD twice with drivers.  Issue isn't hardware here, it's software.",hardware,2025-10-30 23:20:04,8
AMD,nmaf9ao,"This exhibits a fundamental misconception of why game-specific drivers even exist. Drivers are optimised--***games*** are not. Each new game can make its own nonsensical, non-standard design about how to implement a feature.   Most of the time, nobody cares: old GPU sucks at a new game? Yeah, that tracks. *Obviously*. AMD isn't new to making GPUs nor GPU drivers. However, there are still times when game-specific drivers help.  In the end, AMD is **not** abandoning game-specific drivers: it still offers them for RDNA3 & RDNA4. If AMD *truly* wanted to ""expose the industry"" lmao, they'd end all game-specific drivers for all GPUs and all generations.",hardware,2025-10-31 00:25:54,13
AMD,nmeufyp,>there isn't much to gain with game specific optimisations  Doom the dark ages runs sub 20fps if you play it without the driver that launched that week. So yes there is stuff to gain with game ready drivers,hardware,2025-10-31 18:17:12,1
AMD,nmbw75w,"doesnt matter now, I am full nvidia Fans now thanks to their long term support.   Radeon would need to fix this and sell their GPU at a huge performance/price to get me to switch. (just like they did in CPU zen1 era)",hardware,2025-10-31 06:52:33,0
AMD,nmaco1l,"Because that was not a wide spread issue, just one you experienced.",hardware,2025-10-31 00:10:29,-6
AMD,nmm1ehv,At least there's Intel.,hardware,2025-11-01 22:59:23,1
AMD,nm8nqa2,"Congratulations, your link proves exactly what I was saying. There are almost 20 driver updates since 2023 in the link you provided. I don't have a clue what you are talking about.  [https://imgur.com/a/0E2g0KS](https://imgur.com/a/0E2g0KS)",hardware,2025-10-30 18:51:50,-11
AMD,nm888mk,Did you mean to reply to someone else?,hardware,2025-10-30 17:38:54,6
AMD,nmavgng,"my bad, i saw it on videocardz.com",hardware,2025-10-31 02:01:46,1
AMD,nmbqtpv,The 0.01% of the username that use Linux will be thrilled,hardware,2025-10-31 05:57:53,5
AMD,nmvvubm,"if by stable you mean decreasing by 2% every year then yes, stably decreasing.",hardware,2025-11-03 14:37:34,0
AMD,nmb5kb4,*SIGH*   AMD is NOT dropping support!   The ONLY thing the 5000 and 6000 cards are being dropped from is special day 1 game specific driver releases. They will still be supported in the normal driver updated.   This fucking lie is very tenacious.,hardware,2025-10-31 03:04:09,-2
AMD,nmb6ffn,What the fuck are you reply to? Certainly not my comment.    Seriously what are you even on about? Nothing you said makes any sense in regards to my comment.   Exposure the industry? Wtf? Stopping with day one drivers? Where the hell are you getting this?,hardware,2025-10-31 03:09:56,-10
AMD,nmc5ovz,"Yeah same, kinda, this decision is the final nail in the coffin for me at the time being.  Yeah Nvidia will ass fuck me as well but at least they are more gentle.",hardware,2025-10-31 08:31:55,2
AMD,nmver6x,After getting burned many times by AMD i too am going to stick with Nvidia now unless AMD offers a significant price/performance benefit. Im also going to consider anyone claiming AMD has better drives as insane as this has been true for over a decade of my experience.,hardware,2025-11-03 13:00:06,1
AMD,nmc4ztu,Except it was considering AMD issued a quick mid month fix and acknowledged the issue,hardware,2025-10-31 08:24:35,5
AMD,nmb6j3l,"My case was a 5700XT, people always, ALWAYS, will tell on AMD that it has better cost per frame more bruteforce, that you should use X adrenaline version, undervolt it and use Y version of windows. Guess what, it shited on me when I was playing rainbow6, press F.",hardware,2025-10-31 03:10:38,-1
AMD,nm8pbg1,"These aren't the ""Game Ready"" drivers or whatever amd calls it. It's just fixes instead of added support for newer games. Compare the release notes of both drivers you'll see what I'm talking about",hardware,2025-10-30 18:59:15,20
AMD,nm88vp4,Yeah the person that you were replying to.,hardware,2025-10-30 17:42:02,1
AMD,nmc0t92,Linux is so much better than Windows. More people should use it.,hardware,2025-10-31 07:40:56,0
AMD,nmw07eo,It's been stable for some time when you look at steam hardware survey,hardware,2025-11-03 15:00:05,1
AMD,nmb79zq,"Go see the driver released yesterday. They didn’t release the new Vulkan extensions for RDNA2, only for RDNA3 and RDNA4.  It’s essentially in maintenance mode, those cards will only get security updates and bug fixes.",hardware,2025-10-31 03:15:44,12
AMD,nmd3m3c,"By those standards, NVIDIA also isn’t dropping support for Pascal and Maxwell, because they’re also providing critical and security patches through to 2028, and the only thing they’re dropping are the Game Ready updates.",hardware,2025-10-31 12:59:59,3
AMD,nm8zs37,1. I see you have been proven wrong and decided to move the goal post. Not cool bro. You said Vega products don't receive updates. 2. Game ready drivers for Vega APU? Are you serious? What AAA or AA games were you expecting to play on a Vega APU? What kind of request is this?,hardware,2025-10-30 19:49:50,-15
AMD,nm9xjco,The idea that you need specific drivers for specific games is absolutely insane and backwards.,hardware,2025-10-30 22:44:31,-3
AMD,nmc0vp1,"Unless you want to play an EAC game   Which is where the bulk of game time is spent, big omission there",hardware,2025-10-31 07:41:39,4
AMD,nm9fkuu,"1. The discussion was about supporting products for new releases the entire time. If something comes out and has issues or runs like ass on hardware that’s less than 5 years old because of a lack of software support from the company then that’s a problem. This *has* happened with AMD before, they put product lines into a skeleton maintenance mode well before they should.  2. Lmao. Rofl, even.  Edit: I should also specify that these product lines are still actively being released and sold new. They *should* get shit for that kind of support.",hardware,2025-10-30 21:06:49,12
AMD,nmc104r,Steam Deck is pretty popular without EAC. So I wouldn't say bulk of the games.,hardware,2025-10-31 07:42:57,-2
AMD,nmc1c8o,>Steam Deck is pretty popular without EAC   5 million units is good for a PC handheld. Its a drop on the ocean of the PC market,hardware,2025-10-31 07:46:25,7
AMD,nmc1v9i,dGPUs don't sell much more.,hardware,2025-10-31 07:51:49,-1
AMD,nlnwkh8,This is getting tiring. Just how many Mendocino has AMD made and still have in stock.,hardware,2025-10-27 15:29:14,180
AMD,nlnvpuo,Oooof those Zen 2 chips only have 4mb of L3 cache.,hardware,2025-10-27 15:24:59,84
AMD,nlo1dgo,"It's so on brand for AMD to come up with a new naming scheme and already be halfway through the names. When Zen 6 comes out it will be Ryzen 4xx.   Why even rename the old generations, you are only making it more confusing for general public. It's not like all the text/video reviews, articles, and promotional material are all going to change to reflect the name change.",hardware,2025-10-27 15:53:01,35
AMD,nls8wbf,"Two product launches were missed in that article, [but were covered by other sources](https://www.techpowerup.com/342290/amd-introduces-new-ryzen-branding-ryzen-10-zen-2-and-ryzen-100-zen-3-processors):   > Here are the ""new"" Mendocino CPUs based on Zen 2: > - Athlon Silver 10: 2C/2T > - Athlon Gold 20: 2C/4T  It's nearly 2026 and AMD is relaunching dual core CPUs with an architecture from 2019. And these dual core Zen 2 CPUs already [lose to the quad core Intel N100](https://www.cpubenchmark.net/compare/5316vs5157/AMD-Athlon-Gold-7220U-vs-Intel-N100) which has been on the market since 2023.  This is the most miserable product launch from AMD in a long time.",hardware,2025-10-28 06:10:51,12
AMD,nlnylg0,"Honestly, if this means that they keep the cadence:  Zen2 10/30/40   Zen3 100/110/120   Zen4 200/220/230   Zen5 300/350/370   **Zen6 400/450/470**  ...then I can live with it.  Putting the CPU generation on the third letter like 8745HS = Zen4 was rubbish.  Priovidedthey use XYZ model number where:  X is the CPU gen   Y is the APU tier   Z is reserved for bullshit refresh  I can live with it...",hardware,2025-10-27 15:39:24,53
AMD,nls8ei5,"Well uh. 100, 200, and 300 are different series. So 30 is different from 10 right?  > AMD Ryzen 3 30 > > Series: Ryzen 10 Series  I give up.",hardware,2025-10-28 06:06:36,9
AMD,nlrfsld,"to make this more clear:     Ryzen 7 170 = Ryzen 7 7735HS     Ryzen 7 160 = Ryzen 7 7735U     Ryzen 5 150 = Ryzen 5 7535HS     Ryzen 5 130 = Ryzen 5 7535U     Ryzen 3 110 = Ryzen 3 7335U     Ryzen 5 40 = Ryzen 5 7520U     Ryzen 3 30 = Ryzen 3 7320U     Athlon Gold 20 = Athlon Gold 7220U     Athlon Silver 10 = Athlon Silver 7120U     Specifications are 100% the same. So, in this case, AMD ""just"" changed the names of it's 2023's mobile portfolio. Which, as well, was already a rebranding of older CPUs.     Source: [3DCenter.org](https://www.3dcenter.org/news/news-des-2526-oktober-2025)",hardware,2025-10-28 02:39:00,8
AMD,nloi4y7,this is going to need a new decoder disc!,hardware,2025-10-27 17:15:39,16
AMD,nlo5m3g,"How do you ***know*** all these confusing names are clearly an attempt to submarine old CPUs into sold-as-new systems? Because EPYC has avoided all this bullshit for just under a decade.  EPYC 7xx**1** \- Zen1  EPYC 7xx**2** \- Zen2  EPYC 7xx**3** \- Zen3  EPYC 4xx**4** \- Zen4 small dies  EYPC 8xx**4** \- Zen4c  EPYC 9xx**4** \- Zen4  EYPC 4xx**5** \- Zen5 small dies  EPYC 9xx**5** \- Zen5  AMD uses another letter (F, P, X) for additional differentation, *if necessary*. AMD knows this is  readable + fucking simple + can fit ***1000s*** of SKU permutations (2 digits + 3 letters).",hardware,2025-10-27 16:14:04,31
AMD,nlps832,"I just want to know when RDNA4 embedded boards will be available and what they'll be called. Is that too much to ask?  I guess maybe the ryzen 100 is what I'm waiting for?  Edit: nevermind, 680m is RDNA 2...I have no clue",hardware,2025-10-27 21:07:22,6
AMD,nlnwokf,You gotta admit Intel's naming scheme is way more consistent and sensible than AMD's.,hardware,2025-10-27 15:29:49,42
AMD,nlo2o40,Wow. That's a sign of a lot of stock piled up.,hardware,2025-10-27 15:59:20,4
AMD,nls5hzv,It takes a degree to understand the naming scheme... They are clearly doing this so to confuse the consumers,hardware,2025-10-28 05:41:39,3
AMD,nlnydgw,"So sick of branding reboots. Why can't they just choose a naming scheme that will last and stick to it forever (and stop skipping generations willy nilly!). All they're doing is either trying to make a disappointing product look better than it is, or they're tainting otherwise good products with cheesy names (""AI MAX"" is just cringe). I guess they're probably fooling someone, since they keep doing it.",hardware,2025-10-27 15:38:18,10
AMD,nlpgobw,"Confusing CPU names, GPU names, TDPs, cooling, ram speeds, etc etc... Buying a laptop is such a shitshow",hardware,2025-10-27 20:08:56,6
AMD,nlw7jp1,Why not just give them all names in Klingon?,hardware,2025-10-28 20:50:08,3
AMD,nlo23xs,It sucks making your namescheme super confusing so consumers think [some number] is the latest stuff.,hardware,2025-10-27 15:56:35,6
AMD,nlnyghh,Do they have that much leftover silicon...?,hardware,2025-10-27 15:38:43,5
AMD,nlo3fi6,"Honestly this naming scheme is not bad by itself; makes it easy to which generation of chip you are getting, and bigger number within generation = better is fine.  Now AMD just needs to keep using this naming scheme, at least for a few years (ideally at least until Zen 11 = 900 series chips, then if they want another naming scheme I would give them a pass).",hardware,2025-10-27 16:03:09,6
AMD,nlt1lm9,That's sad... I remember playing anno 1800 on the cheapest and lowest laptop tier ryzen 5300u 300$ hp 245 g8 in 2021/2022 .,hardware,2025-10-28 10:47:56,2
AMD,nlup5xh,I had no idea they were still making Zen 2 and 3+ chips,hardware,2025-10-28 16:26:36,2
AMD,nlo5trm,Intel and AMD decided that it's better to compete in more confusing naming than performance.,hardware,2025-10-27 16:15:10,4
AMD,nlnvr7f,Remember when Intel called AMD’s laptop cpu naming scheme “snake oil”?,hardware,2025-10-27 15:25:11,7
AMD,nlo41ao,This smells like E-waste that is going straight to the developing countries.  Why not just recycle it the proper way.,hardware,2025-10-27 16:06:11,4
AMD,nlsnahn,AMD is slowly turning into an Intel...,hardware,2025-10-28 08:29:55,2
AMD,nloga2x,"Glad that the naming is being cleaned up, kind of useless to see AMD 7000 badges on laptops and not know if it's using a processor two or a full three generations out of date. By the time Zen 6 launches most AMD laptops in the market will be three or four generations out of date...",hardware,2025-10-27 17:06:39,1
AMD,nlozr8h,Im more than fine if they find their way to 4050 laptops that needs to be best bang for the buck as posdible. Remember 4050 to 4070 laptop sales make up MORE THAN QUARTER OF ALL OEM SALES. So market share is market share. Just dont let them go on a fight with the new intel 7s.,hardware,2025-10-27 18:42:53,2
AMD,nlnucgb,"Hello Shadow647! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-27 15:18:11,1
AMD,nlt4xs8,"Another loss for the customers, getting old shit under a new name. SAD!",hardware,2025-10-28 11:15:03,1
AMD,nlnyqb8,"Why didn’t they plan for Zen 2 ahead of time so that Zen 5 could be 400, Zen 4 could be 300, Zen 3 could be 200 and Zen 2 could be 100?",hardware,2025-10-27 15:40:05,1
AMD,nls6mkm,What a shame with AMD rebranding scheme,hardware,2025-10-28 05:51:30,1
AMD,nlo0dcd,"They might still be making them. TSMC n7 is now a very mature node, and it wouldn't be surprising to me if the low end market is now easier to satisfy with older products rather than cut down newer ones.",hardware,2025-10-27 15:48:06,76
AMD,nlo0pc7,They're probably still producing them because 6nm is cheap and can compete against Intel 7 of which apparently there's a shortage of capacity.  Windows 11 upgrade cycle I guess.,hardware,2025-10-27 15:49:44,29
AMD,nlo09ug,"probably still have a lot, and they are the cheaper to manufacture, so AMD can use that to have against cheap Celerons from Intel",hardware,2025-10-27 15:47:38,23
AMD,nlof7nt,"Oh, they're even reusing code names  [https://www.techpowerup.com/review/amd-3800-plus-venice/](https://www.techpowerup.com/review/amd-3800-plus-venice/) <- CPU from 2005 was venice   [https://www.techpowerup.com/340405/amd-prepares-epyc-venice-platform-to-break-the-1-000-w-power-barrier](https://www.techpowerup.com/340405/amd-prepares-epyc-venice-platform-to-break-the-1-000-w-power-barrier) <- 20 years later the upcoming CPU is venice",hardware,2025-10-27 17:01:28,11
AMD,nlp6hmj,"I woldn't be surprised if it's still in production, it's cheap on a mature node and competes with Intel's counterparts performance-wise",hardware,2025-10-27 19:17:03,2
AMD,nlqbyf3,"Would it be less tiring if they called them Athlons? The world needs cheap chips, not everyone needs the latest and greatest.",hardware,2025-10-27 22:55:21,2
AMD,nloecsb,"These are general purpose CPUs for any OS, but 4MB L3 cache is pretty terrible to sell on a Windows PC in 2025 especially.",hardware,2025-10-27 16:57:18,43
AMD,nlomjzp,"And it's completely fine for the market segment it's going to   4c/8t of Skylake level performance is completely fine for the average non enthusiast computer user who is not playing games. Who edits a word document, uses TurboTax, and watches Netflix.   Fine for low end gaming too.",hardware,2025-10-27 17:37:08,19
AMD,nlo8anf,They're cheap low power chips that go into Chromebooks. I'm not sure why you expect much there.,hardware,2025-10-27 16:27:29,10
AMD,nlo7wwl,literally,hardware,2025-10-27 16:25:36,1
AMD,nlocwto,Kaby lake also had only 4MB.,hardware,2025-10-27 16:50:20,-6
AMD,nluof24,"Knowing AMD they will leapfrog the expected Ryzen 4xx name and move to something new like 5xx or 'Ryzen 25 AI' because ""Fuck it, why not?"". I'm sick of thier BS naming. In fact I'm angry with Intel as well, but Intel had the best naming scheme ever in the 14900K or 14900HX etc and they ruined it by creating ""Core Ultra"" and restarting. I guess they didn't want to be embarrassed having the 15900K (285K) be slower than the 14900K.",hardware,2025-10-28 16:23:00,10
AMD,nlp17pw,"I imagine it's because the 300 series was a naming scheme change, but they still want to produce and sell older CPU's.  So rather than have the 7020 series (Zen2+), 7035 series (Zen3+), 8000 series (Zen 4 refresh), and 300 series (Zen 5) on the market, it'd instead be the 10, 100, 200, and 300 series respectively.",hardware,2025-10-27 18:50:21,9
AMD,nlsgy5l,Apparently not if you look at comments,hardware,2025-10-28 07:23:34,4
AMD,nlo6tf8,">Honestly, if this means that they keep the cadence:  Thinking ***this*** is the final major rebrand shows a lack of faith in the marketing department.",hardware,2025-10-27 16:20:06,70
AMD,nlo4s2j,"I agree.  If they can just keep a fairly simple naming convention like you outlined for a decade then I'd be exceptionally happy, the constant and overly convoluted naming schemes they have been using since zen2/3 is irritating.  They had a good thing going with zen and zen+ but as soon as they started with the 4000 on laptops decoder ring nonsense it started getting ridiculous again.",hardware,2025-10-27 16:09:54,15
AMD,nlunjd0,"It would've been better if AMD actually had the correct number to signify what generation their architecture was. For instance, HX 370 should've been HX 570 instead. That way this new rebrand would've made sense. Then Zen 2 could've been 240. Zen 3 could've been 320 etc. But nah they had to start at 3xx for Zen5 to ""one up"" Intel's 200 Core Ultra naming. Typical AMD bullshit.",hardware,2025-10-28 16:18:47,3
AMD,nlo9gs6,"I don't mind the specific naming, but they clearly can't stick to one naming so it's just bad when it'll switch in a year again",hardware,2025-10-27 16:33:17,7
AMD,nloa4ms,"Yeah, the frequent debrands are annoying, but at least this one is good.   The previous naming convention in your example was just trash.   Zen 6 mobile and NVL will funnily enough both launch around the same time, both using 400 as their naming scheme.",hardware,2025-10-27 16:36:35,2
AMD,nloaqrz,"I thinK Zen 6 will be 500 series since Gorgon Point, a Zen 5 Mobile refresh will be 400 series. I mean AMD skips a number every time they do a new numbered architecture for Desktop.",hardware,2025-10-27 16:39:40,1
AMD,nlp8m66,"> Putting the CPU generation on the third letter like 8745HS = Zen4 was rubbish.   It was... fine? I mean, the 8 is pretty much the year. It's the same as 25745HS   2 or 3 alfanumeric to represent the newest product and performance level in that year.   Sure a 22930XXX can beat a 25240U, but then we'd also need to add a Cinemark score to the product name to be fair with everyone.",hardware,2025-10-27 19:27:56,1
AMD,nlqfl5p,This was a needed and welcome change because of decoder discs.,hardware,2025-10-27 23:15:21,-1
AMD,nlp2d52,epyc isn't aimed at the gullible consumer market. i consider myself tech savvy and even i can't keep up with amd's naming conventions. 99% users buying a laptop or whatever have no idea what generation of amd/intel they are getting.,hardware,2025-10-27 18:56:10,24
AMD,nlqbp0y,"they cant fool datacenter guys with bullshit naming, but they can definitely fool consumers with it.",hardware,2025-10-27 22:53:56,11
AMD,nlpm33z,No one buying EPYC is actually confused though as no one is buying from model name alone.,hardware,2025-10-27 20:36:03,9
AMD,nlp6ug2,It was rather unpopular when they tried doing the same thing to the mobile market.,hardware,2025-10-27 19:18:51,0
AMD,nlomzwm,"nah, the 288v vs 285h etc is really stupid. wouldnt it have made more sense to be like: 255hx, 255h, 255v, 285hx, 285h, 285v etc? (yes, i am aware these are different families of chip.)  and if it ends in 6 like 256v 286v it is 16gb not 32gb. its just really silly to me. i dont think the ram should be even paired with the marketing name, i just want to know about the CPU from the name, not what its paired with. thats what specsheets are for.",hardware,2025-10-27 17:39:18,14
AMD,nlnz8x5,They are the same though? There are some Raptor Lake and Meteor Lake SKUs hidden in the latest Core 2 series naming,hardware,2025-10-27 15:42:37,24
AMD,nlo0roe,"Right, because with ""Ultra"" they delivered what we all associate with that word...",hardware,2025-10-27 15:50:03,4
AMD,nlv02sz,"It was... Core Ultra is a total mess though, I really need to sit down and try to figure out what they're doing there.",hardware,2025-10-28 17:18:39,1
AMD,nlow68g,Which socket of intel has even remotely the same number of chips as am4?  All other sockets are far more coherently named by amd then the ultra x760 pro max bullshit intel is doing now,hardware,2025-10-27 18:24:34,-1
AMD,nlqewlc,"I'm quite happy for them to ditch the previous scheme which objectively sucked, but yeah, it's not that fucking complex to just pick something and stick with it. Don't see why it is so hard for any of them, and what the hell is going on in the Monitor world.",hardware,2025-10-27 23:11:38,1
AMD,nlo1j0b,The OEMs want it this way.,hardware,2025-10-27 15:53:46,-3
AMD,nlqf6mt,"This is undoing such a confusion, the chip generation is back at the front of the number.",hardware,2025-10-27 23:13:09,-2
AMD,nlo0cad,"Could be that they’re still producing silicon, since these would be on older nodes.",hardware,2025-10-27 15:47:58,14
AMD,nlone9b,The article answers this. They bought fab capacity years ago and are obligated to use it,hardware,2025-10-27 17:41:12,2
AMD,nlp0omb,"That slide is the bullshittiest bullshit to ever grace the earth. Their naming scheme sucks fine. Bu they still tell you if a chip is older zen name from the branding. Some less cache cpu bins of intel doesnt even tell you that.  Do you know the difference between a 13600, 13500 and 13400? İ5's ? Or the mobile H branding? Yea its that bad on both ryzen mobile and Intel chips.[This video sums it up prettttty good](https://youtu.be/05RGUtSk3P0?si=LhCoBdIMiZSkkxmU)",hardware,2025-10-27 18:47:38,3
AMD,nlqdt91,"Sure, they are cheap chips for people with a budget. What's your point? Should cheap technology not exist? Do developing countries not deserve computers?",hardware,2025-10-27 23:05:37,13
AMD,nlonvz4,"Do you really think the average non enthusiast needs 8c of zen 5 to run TurboTax or watch Netflix?   No, 4c of zen 2 is still perfectly adequate for that.",hardware,2025-10-27 17:43:34,8
AMD,nlo4vdh,"Because AMD messed things up by having repurposed and ""plus"" SKUs taking their own thousand in their line up.  Zen 1 began with 1000. Zen 5 should be 5000.",hardware,2025-10-27 16:10:22,13
AMD,nloyk9u,Their small core design probably means they have fewer defects for cut down products anyway.,hardware,2025-10-27 18:36:47,18
AMD,nlsgdkr,"Can their 6nm skus actually compete against Intel 7 though?   Whether it be through node or design choice, RPL-H seems like it can completely smack around AMD's most advanced 7nm class parts in ST perf, though idk how binning ends up looking throughout the stack for ST Fmax.   Battery life may be comparable or better for AMD, I didn't check, but this and ST perf seem like the two more important metrics that laptop OEMs seem to check off on, that and then nT perf and iGPU perf.",hardware,2025-10-28 07:17:43,3
AMD,nloc91r,"Also the performance per clock bump between Zen2 and Zen3 is almost linear with the extra die area Zen3 takes up. that they're on the same node and thus the price/performance remains relatively static is probably a non-trivial reason for them finding places to deploy Zen2 designs still, especially in cost sensitive devices (like the Steamdeck)",hardware,2025-10-27 16:47:08,16
AMD,nloxckc,"Mendocino was an Intel codename from 25 years ago (for a P6-based Celeron), so these companies not only reuse their own codenames, but steal their competitors' old codenames.  That's kind of the point of codenames though; they're just for internal use, not trademarked like the Real Marketing Names.  (AMD did stop using other people's trademarks as codenames, though, after GM got mad about Corvette and Camaro.  Using Corvette was fine as it was a type of ship, but using Camaro as the name for the worse variant of Corvette was too on the nose.)",hardware,2025-10-27 18:30:32,21
AMD,nlp1mvw,Why is that specifically bad on 2025 Windows?,hardware,2025-10-27 18:52:30,5
AMD,nlplnxi,The real hardware requirements of Windows haven't changed since Windows 7.,hardware,2025-10-27 20:33:54,1
AMD,nlozuol,"Genuinely for MOST things people do with computers a 6700K equivalent performance level is entirely viable, it'll browse the internet, handle some light image editing, run office apps.  pair it with 16GB of ram, a halfway decent SSD and video hardware (integrated or discrete) that has hardware accelerated decode for all the common video formats and you've got something that's most definitely hanging in there as ""usefully fast""",hardware,2025-10-27 18:43:22,26
AMD,nlscbop,"You mean, it's fine when AMD does it, lol...",hardware,2025-10-28 06:40:53,3
AMD,nlzgiz8,"I have an i7 7700 and I play everyday games like Age of Empires IV, CS2 and stuff. Maybe it feels limited in Cities Skylines 2 and 2024-2025 AAA but that's it.",hardware,2025-10-29 10:34:20,1
AMD,nloezut,"Which launched 7-8 years ago, though. Companies only rebrand CPUs still being sold-as-new.",hardware,2025-10-27 17:00:24,11
AMD,nloj4xb,AMD... Advanced Marketing Disaster.,hardware,2025-10-27 17:20:28,28
AMD,nloentp,"They’re going to fuck it up about 20 more times in my lifetime, if current trends continue",hardware,2025-10-27 16:58:48,16
AMD,nlozy6c,"""No Way To Prevent This"" Says Only Department Where This Regularly Happens",hardware,2025-10-27 18:43:52,16
AMD,nlojp62,Crap... All the more reason for either CPU maker to skip to 500 just to appear more modern...,hardware,2025-10-27 17:23:12,2
AMD,nlqjp2r,companies are starting (in some cases) to not even tell the consumer what generation it is from. They just put how many cores it has.,hardware,2025-10-27 23:38:11,7
AMD,nlrv396,"Dropping the ""i"" for Core Ultra is also really dumb imo.",hardware,2025-10-28 04:18:34,7
AMD,nlo05kk,"Not really miffed about Core 2 series being just Raptor Lake, it’s not really hidden as that entire lineup is just old stuff and not really wedged with any newer products.  The Ultra 200U series on the other hand, that’s pretty annoying, even if it’s technically not just Meteor Lake but Meteor Lake with a node shrink, it’s pretty misleading",hardware,2025-10-27 15:47:03,7
AMD,nloycbq,"Honestly at this point, I think this is what the OEM asked.   The fact that almost all mobile devices SKUs sucks (except Apple), and are full of rebrands regardless of where the SOC came from, is definitely not a coincidence.  Intel/AMD/Qualcomm/Mediatek all do the same thing.",hardware,2025-10-27 18:35:39,4
AMD,nlnzzes,Yeah but they don’t have the “ultra” designation. ultra = fancy igpu and the NPU. Core = not that,hardware,2025-10-27 15:46:13,1
AMD,nlpzu25,Ultra is superior naming to endless rebranding of old architectures,hardware,2025-10-27 21:47:35,1
AMD,nlpzn79,What does that have to do with naming scheme,hardware,2025-10-27 21:46:34,6
AMD,nlo89fh,"OEMs at some point will demand AMD remove spec sheets from AMD's websites: ""Apple doesn't write clock speeds or caches. AMD ought to do the same, tbh. Just keep that internal to us. This is just too public.""",hardware,2025-10-27 16:27:19,8
AMD,nlo9wfg,Until they are unable to sell more expensive laptops with newer chips and cry at AMD for a way to differentiate them again...,hardware,2025-10-27 16:35:28,5
AMD,nlt0u31,the OEMs have too much power.,hardware,2025-10-28 10:41:29,1
AMD,nlr764i,Sure. And bringing a new set of numbers and refreshes to the mix.,hardware,2025-10-28 01:49:43,3
AMD,nlozuv5,Cheaper nodes now?,hardware,2025-10-27 18:43:23,2
AMD,nlqcswu,"Unlikely it's out of obligation. If they can keep selling it as new, why wouldn't they? Cheap for them to make.",hardware,2025-10-27 23:00:01,6
AMD,nlpq60o,"Yeah but Intel was right about AMD’s laptop chip names. Sure Intel still uses Alder Lake for budget 15th gen desktop chips, but AMD’s laptop naming scheme still sucks",hardware,2025-10-27 20:56:47,10
AMD,nlzea75,it's a waste of resources with many much more capable older chips being sent to landfill,hardware,2025-10-29 10:14:51,0
AMD,nlsbsow,"AMD probably has to take perfectly good eight core chips to fill demand for their six core models. Segmenting the newest generations below that probably makes no economic sense for them, because odds of having three actually defective or underperforming cores are too low",hardware,2025-10-28 06:36:06,6
AMD,nlq9eiw,"I've got a great idea, let's name them after earth-movers!",hardware,2025-10-27 22:40:58,6
AMD,nlqbrfe,"Because windows memory access / storage management in terms of the CPU fetching the required data to run whatever software isn't perfect (not gonna get into MacOS / Linux here). Software is also generally getting more demanding to run over time.  More L3 cache means the cpu can grab more data from the rest of the system in a ""single operation"" (it's obviously not one operation but I'm simplifying here). This means it has to spend less time / clock cycles getting data and thus has more resources freed up to throw at your games / rendering software.",hardware,2025-10-27 22:54:17,12
AMD,nlqxwpf,"Because MS Office 97 programs typically use about 20MB per instance, thus won't fit fully into cache.",hardware,2025-10-28 00:57:28,8
AMD,nlpkiik,Cause 2025 windows is bad 😎,hardware,2025-10-27 20:28:07,12
AMD,nlq06kv,It's like 8GB VRAM,hardware,2025-10-27 21:49:26,0
AMD,nlro0fk,"No, they haven't written any new ones, but starting up a clean install of Windows 11 eats up about 5 or 6GB of RAM, just for the OS. Anything you want to run on top of it will need more, so it'd be honest to say ""hey, 16GB of RAM is the minimum"" (though that'd hurt cheap-hardware OEM offerings). According to Microsoft, with 4GB you're golden.  They also never listed SSDs (plain old SATA3 SSDs, not ""fancy"" NVMe) as a requirement, but anybody that has tried to run (or been forced to) Windows 10 onwards on an HDD knows the pain and suffering that brings. Again, another requirement that everybody know of, buy isn't written down.  For the CPU they list 2 cores @ 1 GHz, which is honestly ridiculous. I mean, you've got to have a TPM 2.0, but with 1 GHz and 2 cores you're fine.  The only part where they seem to have updated requirements is the Copilot+ part, which honestly, most people don't really care about.   https://www.microsoft.com/en-us/windows/windows-11-specifications",hardware,2025-10-28 03:29:56,0
AMD,nlp39l4,"Yes exactly!   I have a 6700 hooked up to a TV and it's plenty fast for typical computer things and I'm still playing games on it too   The people in this sub think you need a crazy powerful computer to do basic things, it's absurd  The steam deck cpu is about equal to a 6700 yet it's still playing new games. It's GPU is the struggle point",hardware,2025-10-27 19:00:41,14
AMD,nm186xb,It does help to have lots of single thread perf for the occasional React abomination that takes hundreds of milliseconds to respond to a mouse click.,hardware,2025-10-29 16:32:25,1
AMD,nlqtzmy,"I literally have an i7-6700 system with a GTX1650 that's going to a friend's wife's kid to play Minecraft and Roblox or whatever. It's mediocre in every sense of the word. Another friend is getting an i5-8400 system with...I don't know, I'll have to figure out what I have kicking around after I move house. And yeah...it's just fine. Not amazing or anything. But reasonable enough.",hardware,2025-10-28 00:36:26,1
AMD,nlu4jju,???    I am a big of n100-like CPUs as well.,hardware,2025-10-28 14:46:16,0
AMD,nlob09f,"From the fucking article:  >These are Zen-2 based processors originally designed for entry-level systems such as Chromebooks.  Perhaps read before you ""well, ackshully...""",hardware,2025-10-27 16:41:00,7
AMD,nlt0kep,I always prefered Annual Marketing Disaster.,hardware,2025-10-28 10:39:10,7
AMD,nlr24g3,It's not like Intel or Nvidia are much better.,hardware,2025-10-28 01:20:45,5
AMD,nlo26q0,Pretty big gap between the LNL NPU (or iGPU) and MTL/ARL though.,hardware,2025-10-27 15:56:57,8
AMD,nlp292i,"No way that will happen to all of them, but there have already been OEM (or customer) specific SKUs of CPUs that are not officially documented on the CPU manufacturers websites. The examples I'm thinking of are server CPUs but I could see it happening with some special laptop CPU sometime.",hardware,2025-10-27 18:55:36,5
AMD,nm1ciec,"Older chips that are in chassis with degraded batteries, bad hinges, sticky keyboards 1366x768 screens, etc.  Yes, many can be made usable with dedicated attention from an IT professional to clean them up, replace batteries and install Linux. But at the resource cost of that is the price of a used computer on eBay, plus a knowledgeable friend to do the last part because otherwise, you can't trust there's no preloaded malware (i'm batting one of four on that).  It doesn't scale.",hardware,2025-10-29 16:52:35,1
AMD,nm44sah,"Eliminate these and all that will happen is people will be forced to pay more to equally fill that land.  I have a 2500K system still up and running. What are you doing to reduce landfill, apart from preaching that the poor don't deserve computing?",hardware,2025-10-30 01:12:32,0
AMD,nlt67lu,Can wait to unlock my AMD CPU once again.,hardware,2025-10-28 11:24:56,5
AMD,nlsuer7,They use geographic names because those can't be trademarked and no one can sue.,hardware,2025-10-28 09:43:25,3
AMD,nlr0z0v,> Software is also generally getting ~~more demanding to run~~ *less optimized* over time.  Apollo missions ran on a Casio wristwatch,hardware,2025-10-28 01:14:18,9
AMD,nlsvnbe,"There's nothing windows-specific here. I can run the same software on Linux and windows, like a browser, and it should perform the same in terms of cpu perf. The only difference in this space is maybe transparent hugepages, I don't think Windows has those, but it would impact the TLB more so than general cache hit ratios. The physical address to cache set mapping function is an implementation detail within the CPU and the same across OSes.",hardware,2025-10-28 09:55:10,2
AMD,nlqrj8i,"More like 2gb vram honestly, even intel give 9mb l3 cache on their i3s",hardware,2025-10-28 00:22:49,7
AMD,nlp5o6e,"Part of the reason the Steamdeck hangs in SO well with PS4/Xbox One generation games is that its cpu utterly annihilates the SOC those used.  It's (slightly) behind the base PS4 on gpu grunt, 1.6teraflops to 1.8 iirc, although driving a lower resolution.  buuuuut comparing benchmarks (I did this a while again and it'd probably take me a while to source the numbers properly again) of a quadcore Zen2 at 3.6Ghz to the desktop version of the 8 core Jaguar at 2.1Ghz in the PS4 Pro.. a single core of the Zen2 is around 60% of the performance of THE ENTIRE SoC in the PS4Pro, and it's got 4 of them  Visual stuff you can scale up/down in quality and resolution, but cpu stuff tends to be more inflexible and so the 'deck hangs on in there",hardware,2025-10-27 19:12:57,9
AMD,nm1919r,"Intel kept iN-gen0-bin0-[KFUT] for 13 product cycles!  To be fair, they have fallen off lately.",hardware,2025-10-29 16:36:25,1
AMD,nm3bcc0,you are talking like these cheap new laptops aren't also the same crap but even cheaper. prices of usable nice ryzen thinkpads dropped a lot,hardware,2025-10-29 22:29:56,2
AMD,nlus85w,"Indeed. However, what I was getting at was a callback joke to AMD's disastrous Bulldozer, Piledriver, Steamroller, and Excavator generations.",hardware,2025-10-28 16:41:27,6
AMD,nlssryk,"I don't really like this argument, because I think people are overestimating what the Apollo Guidance Computers did.  Those computers were just going through basic trigonometry, calculus, and algebra. Most of the work the AGC did could be (and in the case of Apollo 13, was) done by the astronauts using pen and paper.   Something as simple as upscaling an image from 512x512 to 1024x1024 using simple Bilinear interpolation requires way more processing power than flying to the moon.  Was it extremely impressive for the time? Absolutely. Are a lot of modern programs unoptimized? Yeah, definitely. But what we're doing on modern computers is several orders of magnitude more complex than what the AGC had to do, and is in no way comparable.",hardware,2025-10-28 09:27:09,10
AMD,nlt074x,Apollo missions were mostly analog.,hardware,2025-10-28 10:35:56,4
AMD,nlriwb1,Very fair point.,hardware,2025-10-28 02:57:07,0
AMD,nlua111,>The physical address to cache set mapping function is an implementation detail within the CPU and the same across OSes.  I did not know that but it makes perfect sense. Appreciate the feedback :),hardware,2025-10-28 15:13:30,2
AMD,nlpja89,It's kind of amazing we're at the end of the PS5/Xbox series life cycle and yet devs are still targeting PS4 in so many new games.,hardware,2025-10-27 20:21:57,5
AMD,nluu9kq,"Reminds me of that missile memory leak where they were just like ""add more memory who gives a shit"". Imagine if agc had way more capability and then the 13 crew all died because they could no longer do it on pen and paper in time.",hardware,2025-10-28 16:51:13,2
AMD,nlpo8wp,"Black Ops 7 is coming out in a little over two weeks and it'll run on a launch PS4, which is wild frankly. thing is 12 years old.",hardware,2025-10-27 20:47:00,9
AMD,nlr1jvb,The PS5/Xbox Series still being the same price or even more expensive than when they launched isn't helping matters.,hardware,2025-10-28 01:17:35,5
AMD,nlrsevp,Microsoft especially looks ridiculous raising the price of the series,hardware,2025-10-28 03:59:09,5
AMD,nlowlwy,"Excellent and trouble-free performance with open source drivers, as we've come to expect from AMD.  Other vendors remain non-options when it comes to an open source stack.",hardware,2025-10-27 18:26:46,8
AMD,nmtvo49,"AMD here truly living up with their ""Advanced Marketing Disaster"" reputation...",hardware,2025-11-03 04:38:40,191
AMD,nmtu7s4,"This is bad timing. After it has been proven that 6000 series cards can run FSR 4, cutting off customers from future updates and the newest software comes across as pure greed.  This is a serious violation of trust. Why should we buy Radeon ever again?  They need to make this right and the sooner the better.",hardware,2025-11-03 04:27:38,225
AMD,nmu9yps,"You'd think that AMD would have learned from Ryzen how they need to win minds to win marketshare, but they keep screwing up their GPU department time and time again. Nvidia minus $50 doesn't cut it, even if their raster performance per dollar is a bit better sometimes. Officially backporting RDNA 4 to the 6000 series could have been a good win for AMD, instead they shoot themselves in the foot like this.",hardware,2025-11-03 06:45:28,33
AMD,nmur735,Please don't let go about the Vulkan instructions! That will have huge application and usage ramifications in the future!,hardware,2025-11-03 09:44:33,11
AMD,nmttv42,cpu amd vs intel  gpu amd vs amd + nvidia,hardware,2025-11-03 04:25:00,41
AMD,nmudq1g,"Far as I know ""maintenance mode"" is used to specifically preclude ""new feature updates"". Since AMD is trying to claim both simultaneously apply, I think AMD's GPU or marketing division has its head so far up its own rear end that they've achieved ouroboros status.",hardware,2025-11-03 07:22:23,24
AMD,nmwm47c,They again reiterate that RDNA 2 will receive less support than RNDA 3. The original post was correct and now they are just trying to gaslight people into thinking it's fine.,hardware,2025-11-03 16:47:49,5
AMD,nmtukga,But reddit told me that AMD cleared up everything with their response and it was all one big misunderstanding!,hardware,2025-11-03 04:30:18,94
AMD,nmvbrog,But this isn't a marketing problem. Nor a messaging one. AMD just doesn't want to support their products longer than 3 years it seems. Sucks to be you I guess.     The marketing department just couldn't cushion the blow of that realization. On a side note be grateful for Intel's  blanket software stack support of x86 or their CPUs would also be dropped just as quickly.,hardware,2025-11-03 12:40:59,10
AMD,nmv1ssn,"As a consumer i'm also confused, so i'll buy that green thing instead.",hardware,2025-11-03 11:25:49,15
AMD,nmtxy4y,Hopefully my wallet is not confused. It glows green...,hardware,2025-11-03 04:56:31,18
AMD,nmuj5ur,"This doesn't sound as a 'revert' per se, which is quite terrible, because it tells me FSR4 will not be released on RDNA2 whatsoever, although it could technically run it, but on the other hand, how many times do people need to realize AMD simply isn't up to par with nVIDIA when it comes to driver support and forward architecture thinking?  Turing (2018) has dedicated RT/Tensor cores specifically designed for RT/DLSS, whereas RDNA3 (2022 - **4 YEARS LATER**) doesn't, and with that, I think I said it all.  In fact, I'm already sure RDNA 3 and even 4 will have the same faith, as UDNA/RDNA 5 will be a totally different architecture - which, by the way, will be literally a copy of Turing from a core perspective, which was released, once again, in 2018 -- 'Radiance Cores' are the ray-tracing cores, and 'Neural Arrays' are the tensor cores - obviously, much more powerful than Turing's, but you get the point - AMD is about 9 years behind nVIDIA in terms of architecture philosophy at this point.  So do not act surprised that 5700 XT didn't have more value compared to 2070S at 100$ more as some of people thought, there's a reason ""nVIDIA -50$"" doesn't work anymore, those 50$ will bite you in your ass next time and people have woken up.",hardware,2025-11-03 08:18:41,17
AMD,nmuewe2,"Steve is my anger translator. Thanks, Steve.   -  Side note: I was deciding between a 9060xt and a 9070 to replace my 2080ti before this news. Now I'm deciding between a 5070 and waiting for a B770. I don't* trust AMD/RTG not to screw me over.",hardware,2025-11-03 07:34:36,9
AMD,nmueezm,"People shouldn't care about these game optimizations that nobody (without internal access) can really quantify.  What we can quantify is Vulkan/D3D12 feature support. Features that do not require new hardware. And there, Vega and older have no received anything since they were moved to the ""maintenance"" branch. And the recent driver release only adds `VK_KHR_shader_untyped_pointers` for RDNA3+, despite this being a quality of life extension that everyone can support.  This means either graphics programmers have to plan around the RNDA2 drivers for the next 3 years and not require anything new, or they only support RDNA3+. Both are bad, and this is only caused by AMD wanting to save money in the wrong place.",hardware,2025-11-03 07:29:30,10
AMD,nmtzkia,Welp to all those owners of a ASUS ROG G14 2022 where they’re stuck with an RDNA 2 GPU and CPU,hardware,2025-11-03 05:09:55,7
AMD,nmvefhi,How big of a deal is it that these cards don’t have day-1 support for new games? Will they still get support within a week or so? How well will they run new games without support?,hardware,2025-11-03 12:58:03,3
AMD,nmve0a0,Further proof that AMD has been the Hyundai/Kia of graphics cards since the ATi acquisition.,hardware,2025-11-03 12:55:23,2
AMD,nmuymj3,It's always fun to find out things we've had forever are actually magical and we just didn't know better.  Thanks AMD for pointing out how awesome end of driver support really is.,hardware,2025-11-03 10:57:14,1
AMD,nmwy8gf,"Finally got to watch the video - where has this GN been the last decade or so? I get you possibly not liking NV, but this dude advertised AMD products along side KY Jelly.  The last time I saw him actually speak about AMD dGPU with this level of honesty was when he was pissed AMD did a price drop minutes before release. Which upset him (my opinion) not because of what AMD did but because he had to redo the video.  He opened the review with a good 5-10 minute history lesson on how AMD's dGPU division was poorly run and then slapped NV for being greedy while giving AMD a pass.  He clearly knows the history of AMD dGPU, he went over Vega! and Radeon 7! But yet still sang praise for AMD while slamming NV in previous videos. I'm not looking for him to praise NV, but if he's saying NV is ""ruining gaming"" - what does he think AMD has been doing? Saving it?  Also more vulgarity than I'm use to from him. The check double bounced or he finally snapped as his credibility (at least for me) is basically gone the way he astro turfed AMD.  Is the OG Steve back? I doubt it. The line finally arose where he had to stand on his ""consumer"" advocacy as AMD is not giving him any wiggle room to defend them.",hardware,2025-11-03 17:45:10,-1
AMD,nmvvo1u,"This ""double space after period == lawyer"" thing is such bullshit.  Double-spacers are people who learned to type in an institutional setting 30+ years ago, from 1990s Mavis Beacon, or from a parent who learned that way.  Single spacers are people who found themselves at a keyboard and figgered it out.  I wouldn't be surprised if a lot of lawyers double-space because demographics, but I double-space because Mavis Beacon and because my mom did data entry work in the 80s.",hardware,2025-11-03 14:36:39,-2
AMD,nmu73sp,"Imagine you have 2 car companies in the world:  One of them provides cars that are more expensive, charge extra for the normal amount of seats and their cars will likely melt at some point in time.  And you'd still have to heavily consider buying it, because you somehow have less confidence and trust it the only rival company.  (Last time I posted this I got downvoted, maybe this time people will understand me.)",hardware,2025-11-03 06:18:00,-28
AMD,nmtvodi,Oh come on .. not the first time.. and obliviously not the last . It’s not due to fsr4 discovery .. that’s old amd policy about generation support .. typical graphics card generation is supported by amd - 3-4 years of full support . Then 3 years of maintenance . And product become legacy   Legendary rx480- 4 years of support . 3 years of maintenance   Legendary RX580 3 years of support . 3 years of maintenance   Vega - 3 years of support . 3 years of maintenance   How old rdna2 ? 3 years ? Guess what comes next ? Why it become a shocking news?   You can predict when rdna3 will become obsolete . Or when Udna become history .. convenient.. for amd,hardware,2025-11-03 04:38:43,-32
AMD,nmw07i7,We hate AMD today? Dramatubers being dramatubers,hardware,2025-11-03 15:00:06,-9
AMD,nmvnn18,Yet another example of why all the blind trust in Gamers Nexus is such a bad idea.,hardware,2025-11-03 13:53:16,-7
AMD,nmtulh6,>UB jumping with joy  You people think you're funny.,hardware,2025-11-03 04:30:32,-26
AMD,nmuw613,YouTubers trying to row back their recommendations I see.,hardware,2025-11-03 10:34:06,-16
AMD,nmvy8m3,"Nvidia got to tech jesus, for shame.",hardware,2025-11-03 14:50:04,-11
AMD,nmugj0s,Never underestimate the SNAFU and FUBAR creation capabilities of Marketing and HR departments.,hardware,2025-11-03 07:51:28,29
AMD,nmuoy70,"**- Cells.**  \- CONFUSION. I bought RX 6600.  **- Interlinked.**  **-** CONFUSION. I bought RX 6650 XT.  **- Within cells interlinked.**  \- TWO PCs, DOUBLE CONFUSION. AMD drops support.  **- Dreadfully distinct.**  \- INFINITE CONFUSION. Games still run.  **- Dark.**  **-** CONFUSION. Consoles and handhelds still use RDNA2.  **- Interlinked.**  **-** PURE. UNSTOPPABLE. ETERNAL CONFUSION.  \- **You've been confused.**  \- I've been confused.  **- Why don't you say that three times?**  \- I've been confused. I've been confused. I've been confused.",hardware,2025-11-03 09:20:26,36
AMD,nmz5035,You stole that from the youtube comment.,hardware,2025-11-04 00:23:34,1
AMD,nmtw6xz,"It's truly mind boggling. They are the 2nd player in a very lucrative market right now, but they just keep cheaping out in the worst ways. I mean, why? Do they need more developers to continue support and they don't think it's worth it?   Also why were they so slow with rocm support as well?    I love Lisa for turning around their CPUs, but the GPU division has been tumultuous... Which is crazy considering GPUs are probably the highest value commodity today. They need to figure this out man.",hardware,2025-11-03 04:42:40,103
AMD,nmue2au,"> This is a serious violation of trust. Why should we buy Radeon ever again?  This is probably going to be a downvoted hot take, but AMD and Nvidia really don't care about enthusiast support.  We are such a small subset of their revenue compared to AI, enterprise, prebuilts, and game consoles. We barely move the needle, nor do our recommendations.   Obviously they would prefer to have happy enthusiasts so you're going to see marketing push to smooth things over but make no mistake, it won't alter the business decisions they make about long-term support, it will only change the messaging.  They don't care about us, and the only power we have is to not care about them as a brand, only evaluating purchases product-to-product (which, to be clear, is not a strong position for the consumer to be in... this is what a monopoly or duopoly does to consumer power).  Intel seems to be the only one who actually \*needs\* positive sentiment to grow in market share right now. But we've seen them pull the same tricks when they have a stronger market position in other product segments so the same advice applies.",hardware,2025-11-03 07:25:52,31
AMD,nmtxxof,"AMD would need to offer a major discount on their GPUs compared to Nvidia at this point for me to even consider buying. I'm currently using a 4070 Ti Super and will probably look at upgrading this next gen in \~2027 so I'm very much a potential future customer.  Normally I'd be open to considering UDNA for my upgrade, especially if they match Nvidia features at a cheaper price, but now that's gonna be an uphill battle for them convincing myself and many others to even give them the time of day.   Even though we know very little about next gen GPUs atm I can say with like 90% certainty my next upgrade is probably RTX 60 and not UDNA. Say what you will about Nvidia but they continue to maintain very solid support for their gaming GPUs across many generations.",hardware,2025-11-03 04:56:25,33
AMD,nmultoj,"No. The leaked version use INT8 instruction. So they are not running the same thing, also worse performance.",hardware,2025-11-03 08:46:59,6
AMD,nmu4ako,"> Why should we buy Radeon ever again?   Because the alternative isn't any better. Who cares how long the GPU is supported if it doesn't have enough VRAM to play new games after a few years like my poor 3070? They had the hardware set up to do it, all they needed to do is pay a couple of dollars more for the same memory chips they put in the 3060, but no, they choose to give it 8GB knowing it would be obsolete in a few short years. Planned hardware obsolescence or support ending too soon, either way gamers are getting screwed in today's GPU market.",hardware,2025-11-03 05:51:42,6
AMD,nmummcm,> Why should we buy Radeon ever again?  Because NVidia is still a lot worse?,hardware,2025-11-03 08:55:17,-6
AMD,nmtx3uy,It's not timing.  It's how you get ppl to buy new cards.,hardware,2025-11-03 04:49:50,-6
AMD,nmu8juz,"AMD must position themselves with the most positive consumer sentiment they can manage to counteract the frankly rabid and overwhelming sentiment Nvidia has managed to generate for themselves. AMD must lean into the value proposition, because Nvidia can both outproduce them and eat essentially arbitrary amounts of loss-leader pricing cuts in the gaming market if they really need to in order to aggressively maintain market share.  Next-gen performance uplifts are in many ways essentially given, and they will only matter to whoever can afford to buy it. The kind of person who can afford to buy a new AMD card can just as easily buy a competitively-priced Nvidia card, and can be easily swayed by sentiment when performance per dollar is comparable.  They're cutting the trust in their value proposition in order to save a little bit of money so they can redirect it to next-gen products whose overall sentiment is ""this still isn't as good as their competitor"".",hardware,2025-11-03 06:31:41,-3
AMD,nmu0pwo,who was buying Radeon again ?,hardware,2025-11-03 05:19:40,-9
AMD,nmvd26w,The drivers always sucked ass anyways. But then again the alternative is not great either. I’m buying a used 30 series GPU,hardware,2025-11-03 12:49:21,-1
AMD,nmtz73e,"> Why should we buy Radeon ever again?  The answer to this question actually doesn't change with this. You would presumably buy Radeon cards, if one cares for the morals and ethics of the situation, because they're the lesser evil.",hardware,2025-11-03 05:06:50,-34
AMD,nmvnqnl,"The ""violation of trust"" is entirely invented. This is just the Firefox thing all over again, except worse somehow.",hardware,2025-11-03 13:53:50,-3
AMD,nmubk65,"To be fair it ran an inefficient and inferior version that may not have provided performance gains in practice (especially in regard to the outputted image quality). It didn't really run it properly. I can see why it was scrapped, they probably considered using it but canceled it because it didn't meet their standards.  Either way sunsetting probably their most used architecture which is still perfectly capable of running modern games is beyond dumb.",hardware,2025-11-03 07:00:57,-5
AMD,nmuga7f,"AMD do not even ""know"" their place in CPU either. It is like they make no effort push aggressively to eat the remaining \*still large % intel market share after sooo many years, AMD CPU seems to perfectly happy NOT to have more market share than Intel In CPU market lol.  If Nvidia suddenly fallen to below 50% market share in dGPU, you can bet they will fight hard to get back to market dominance. Thats Jensen style we talking.",hardware,2025-11-03 07:48:56,-6
AMD,nmunxhu,You mean the same Ryzen where they run the most bullshit and consumer misleading naming schemes that put even intel to shame as they rebadge generations old mobile CPUs to look like newer ones?  Oh well I guess its the consumers fault for not carrying around the [naming wheel](https://www.reddit.com/r/Amd/comments/1ekq4tr/amd_naming_wheel_why/) when going shopping.,hardware,2025-11-03 09:09:16,13
AMD,nmuuhwr,"Tbf, where I live the 9070 xt is 600 and the 5070 ti is 800+ still. That does cut it imo.",hardware,2025-11-03 10:17:41,6
AMD,nmvqei4,"You'd think that the internet would realize just how wrong their entire narrative is. Instead they just repeat the same tired old lies without thinking about any of it, and pretend to act surprised when lies win over truth.",hardware,2025-11-03 14:08:26,-2
AMD,nmuq6r9,Intel arc market share so bad it doesnt exist as compeittion :*(,hardware,2025-11-03 09:33:52,9
AMD,nmvqjc7,"No, they're just *very* tired of people demanding FSR4 support on cards that literally cannot handle it.",hardware,2025-11-03 14:09:10,-3
AMD,nmtv6ea,You're confused,hardware,2025-11-03 04:34:55,76
AMD,nmtvy9l,"To be fair though a bunch of them was corrected by the top comments, well except for the r/Radeon subreddit which is currently fullforce by gaslighting their own community by telling them that *""It is totally normal and expected!"" ""Nvidia does the same as well!"" ""Whole situation is overblown!""*  etc etc.",hardware,2025-11-03 04:40:49,59
AMD,nmtw5nw,"Reddit says a lot of stupid things.  Technically new arch code could potentially break functionality for older cards  and removing old code could make developing new arch code easier and faster but even so, do you actually expect them to keep both branches up to date? I'm pretty sure I remember someone complaining that exactly happening before with Vega. And they aren't even that old.  Short term they'll probably keep both relatively up-to-date to save face but long term when no one is looking they'll start to make the ""maintenance"" branch less of a priority.",hardware,2025-11-03 04:42:24,20
AMD,nmvpd4q,They did and it was. Steve is a liar.,hardware,2025-11-03 14:02:44,-5
AMD,nmvr35e,"Lying works, yes. The worst thing about humans is how much they love to send and receive lies.",hardware,2025-11-03 14:12:10,-4
AMD,nmv1l6k,Nvidia couldn't fix the burning cables in 4 years.,hardware,2025-11-03 11:23:59,6
AMD,nmujvyg,"I was looking yesterday and choosing between a 5070ti and a 9070xt. The 9070xt is about 50 bucks cheaper, but that's just not worth it for me. 50 bucks to have better Ray tracing and support seems like a no-brainer",hardware,2025-11-03 08:26:16,13
AMD,nmvqu01,"Why? The choice here is so simple: the 9060 XT, or the 9070 if you think the extra money is worth it. This is all misinformation, nobody is ""screwing you over"" here except Nvidia.  edit: God this hobby is so fucked.",hardware,2025-11-03 14:10:47,-3
AMD,nmvrcdv,"It's not a big deal, likely, and not having ""day 1 support"" isn't usually very interesting.  But also *absolutely none of this* applies to Linux, *only* to Windows. That matters a lot more than people think, especially now.",hardware,2025-11-03 14:13:32,-2
AMD,nmtu84e,"Hey BlueGoliath, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-03 04:27:42,4
AMD,nmuy9ot,Why are you pretending it won't be received exactly the same way? The HUB thread is posted on there and no one is defending them. That desperate for karma?,hardware,2025-11-03 10:53:55,3
AMD,nmunepv,You can if you want!,hardware,2025-11-03 09:03:36,2
AMD,nn065vs,Poor fucking ATI. Went from ahead of everyone with the Xbox 360 gpu(one of the first programmable gpus) to acquired by AMD and mismanaged into the ground.,hardware,2025-11-04 04:06:08,1
AMD,nmyhu3k,"""double space after period == lawyer""  He didnt say this though",hardware,2025-11-03 22:15:59,2
AMD,nmua4fv,"You probably got downvoted because the analogy doesn't work, at all.",hardware,2025-11-03 06:47:00,32
AMD,nmucren,"Problem is Nvidia GPUs are generally more reliable and versatile than AMD despite shit like Fermi house fires, wood screws memes and the shitty 12v connector.   We didn't know how good we had it in the Radeon HD 4000 days, now AMD only shows up for its participation trophy.",hardware,2025-11-03 07:12:50,11
AMD,nmtyv5l,The difference here is that RDNA 2 GPUs are *still* in production 3 years later,hardware,2025-11-03 05:04:05,41
AMD,nmtxh6s,"RX 480 became legacy in 2023, that's 7 years of support  Vega launched in 2017, legacy in 2023, that's 6 years of support.  28nm GCN products became legacy in 2021, so for something like the HD7970, that was 9 years of support, for a Fury X, the last 28nm flagship, it was 6 years.  RDNA 2 launched in 2020, so it's approaching 5 years, RDNA 1 is over 6, it launched in 2019  So in terms of predicting when AMD will make cards legacy, somewhere between 6 and 9 years.",hardware,2025-11-03 04:52:47,13
AMD,nmtzukb,"Where are you getting this information? I haven't been able to find a support page from AMD clearly outlining which products are supported, in maintenance, or in legacy.  Polaris and Vega are still getting maintenance updates, as recently as August: https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-8-1-POLARIS-VEGA.html  They even get game fixes, still, as recently as May: https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-5-1-POLARIS-VEGA.html  They were only put on a separate driver branch in late 2023, six years after Vega launched in 2017 and seven years after Polaris launched in 2016. Doesn't seem like they're in legacy status just yet.",hardware,2025-11-03 05:12:16,8
AMD,nmyycbv,How so?,hardware,2025-11-03 23:45:39,1
AMD,nn03ifh,That's how most top 1% wannabe commentors are on reddit.,hardware,2025-11-04 03:48:32,3
AMD,nmuewr3,> Also why were they so slow with rocm support as well  Still waiting for GFX1150 support (6.4.4 barely supports it).  https://github.com/ROCm/ROCm/issues/5108,hardware,2025-11-03 07:34:42,9
AMD,nmu7psu,"Gaming GPUs are a tiny drop in the bucket of their current and future GPU revenue, they're not  ""the highest value commodity today.""",hardware,2025-11-03 06:23:48,49
AMD,nmugbbr,"> It's truly mind boggling. They are the 2nd player in a very lucrative market right now, but they just keep cheaping out in the worst ways. I mean, why? Do they need more developers to continue support and they don't think it's worth it? >  >   AMD right now is a SOC for the gaming market. They stopped giving a shit after getting big contracts from Sony and Microsoft.",hardware,2025-11-03 07:49:15,6
AMD,nmujv1a,"AMD always been like this for GPU departments... i am not sure why people still giving them money just because of the on-paper performance, nvidia however worse in price-performance, their drivers values always been way way way ahead of AMD.",hardware,2025-11-03 08:26:00,-7
AMD,nmwmlfb,A lot of enthusiasts also head up their company's IT department. Someone who has a Nvidia GPU in their own PC is likely to recommend the company buys Nvidia cards,hardware,2025-11-03 16:50:09,2
AMD,nmvnsz1,"Enthusiasts bring the marketing and PR. AI and enterprise bring the cash. That's why big public presentations show off the latest gaming cards, and the latest enterprise stuff is all much smaller or direct presentations.",hardware,2025-11-03 13:54:11,2
AMD,nmtz0k2,"I don't really think that it even matters at this stage.  When AMD tried to price their products cheaply with plenty amounts of VRAM, people still bought Nvidia. When AMD had the faster GPU, people still bought Nvidia. Radeon is honestly a lost cause.",hardware,2025-11-03 05:05:21,11
AMD,nmws9qw,"6800xt - FSR3 quality 45fps    FSR4 quality 42fps   wow, very worse performance   keep in mind that fsr4 performance looks better than fsr 3 quality",hardware,2025-11-03 17:16:56,8
AMD,nmubuqx,I bought amd this time since it was cheaper and nvidia was being way too greedy. Also I didn't expect them to ditch a perfectly capable architecture in just 3 years. That's bad even by their standards.,hardware,2025-11-03 07:03:51,12
AMD,nmusk5l,"You are right, but at the same time this is two different things  GPUs with 8GB are indeed hardware obsolescence, but you upfront know what you buy - you can wait a bit to see benchmarks, or decide on previous cases.  Cutting GPU support short is after the fact - you already brought you GPU, you atleast expect some form of longterm support. I'm not familiar, but I guess Neither AMD or Nvidia disclose how many years of support you will get (like Phone manufacturers which says 3 years of updates, 5 years of security).  The whole can of worms is that AMD is vague intentionally by saying ""by market demand"" and at the same time freshly releases devices with the architecture they intend to go Maintain Mode (except its only dGPUs)  I can absolutely see once UDNA drops, AMD to leave RDNA3 as well as it doesn't support AI.",hardware,2025-11-03 09:58:24,11
AMD,nmwesvz,"I think too much emphasis is being placed on the launch date of RDNA2 desktop cards. Even if we strictly go by those, I'd argue 5 years is a bit short.   But the bigger issue is that RDNA2 products are still being sold brand new. Today. The Asus Xbox *just launched* a couple weeks ago.",hardware,2025-11-03 16:11:58,2
AMD,nmtysai,"By destroying your brand reputation even further, perhaps. I'm pretty sure Nvidia will happily welcome those people.",hardware,2025-11-03 05:03:27,32
AMD,nmu167r,"You couldn't even buy AMD cards in my city (non-US) lol. It's constantly out of stock and when I come to the physical store, the employees there seems gobsmacked that the retailer they work at even carry AMD cards. Probably because nobody has asked them for one.",hardware,2025-11-03 05:23:37,13
AMD,nmu1ily,I'm a budget hobbyist. I usually don't buy the latest games and my 6800xt is the most powerful card my 5600x CPU can handle before creating a bottleneck.    I'm not going to upgrade my GPU until I have replaced most of the parts in my rig.  But I will remember that Radeon isn't going to give me long term support and will consider my options as someone who has been burned twice in a row by Radeon.,hardware,2025-11-03 05:26:40,12
AMD,nmvow9b,"AMD aren't an ""evil"" to begin with, and it's getting tiring to see people go on and on about ""AMD aren't your friends"" like it means anything.",hardware,2025-11-03 14:00:10,-3
AMD,nmuu0k6,"If you look at the high margins market share of servers and Amd is limited by number of wafers. Normal consumer and especially low margin ultra high volume is just not something they are about or care for compared to Intel who own the fabs that make the chips and need to keep those fabs churning out stuff, especially old fabs and then you get cheap high volume consumer chips.",hardware,2025-11-03 10:12:54,3
AMD,nmvp9yh,"You have no idea what the market actually looks like. It's not ever as simple as ""pushing aggressively"" based on what some awful tech websites/influencers are telling you.",hardware,2025-11-03 14:02:15,0
AMD,nmww99y,"It wasn't always like that; pretty much from the Ryzen 1000 to 5000 series, they really pushed hard to undercut Intel while offering more cores and eventually more IPC, as well as committing to AM4 so people had upgrade paths without new motherboards, which really won people over. The more recent naming shenanigans and price hikes are the result of them having captured such huge marketshare in the DIY space.  This is why having an overwhelmingly dominant company is bad, and competition is necessary for consumers to get good deals.",hardware,2025-11-03 17:35:54,6
AMD,nmw7z06,It’s confirmed working though at a performance and quality uplift compared to FSR3. I’d understand if the performance was worse than native or quality was horrendous but those are both false. The tech works they just haven’t flipped the switch to make it official that’s the problem.,hardware,2025-11-03 15:38:41,11
AMD,nmy64bz,"I'm not the one that said RDNA2 was now in ""maintenance mode"" and yet it would still be receiving ""new features"" through updates in the same sentence. AMD is trying to have its cake and eat it too, and given AMD's GPU side has a long history of saying shit it shouldn't say and making anti-consumer decisions where it can to pinch pennies I'm well past the point of giving them any benefit of the doubt or letting things slide. AMD corporate chose to keep these people employed, therefore they consider this standard of behavior acceptable.",hardware,2025-11-03 21:17:02,1
AMD,nmu0frb,Even the AMD subreddit wasn't buying it. People who tried to fluff this statement were pretty heavily smacked down for it.,hardware,2025-11-03 05:17:13,33
AMD,nmvpgdj,"> ""It is totally normal and expected!"" ""Nvidia does the same as well!"" ""Whole situation is overblown!""  Crazy that the truth is being called ""gaslighting"".",hardware,2025-11-03 14:03:14,-4
AMD,nmwdix5,"Steve isn't asking you to trust him, he literally showed the statements from AMD.",hardware,2025-11-03 16:05:37,9
AMD,nmvzq7i,Those knees must be really sore..,hardware,2025-11-03 14:57:39,7
AMD,nmwnyyh,Does it affect cards like 5060 ti? I've seen it uses the usual 8 pin connector. Ive been using 8 pin connector on my RX 6600 XT without issues...,hardware,2025-11-03 16:56:36,2
AMD,nmvu8wd,and yet the cables havent burned down a single computer or residence.,hardware,2025-11-03 14:29:00,-6
AMD,nmwdvzb,$50 wasn't worth it even before this blunder. The 5070ti is just a better product. If they could actually get the 9070xt to MSRP it would be an easy choice.,hardware,2025-11-03 16:07:26,3
AMD,nmvwpny,"Even ignoring all that, the 5070ti will eventually resell for a good bit more than $50 than the 9070xt anyway.",hardware,2025-11-03 14:42:09,4
AMD,nmwxalj,"My usual line is that, for the *inflated prices Nvidia asks for their GPUs, Jensen would have to personally come over to my house to install it, then eat my sweaty chocolate starfish...   -  But Jensen hasn't terminated or drastically reduced driver support on mainstream GPUs like this... Ever? So, no, AMD hasn't earned my business, they've earned my telling them to fuck off.",hardware,2025-11-03 17:40:48,6
AMD,nmw3ns7,>thread is about AMD screwing over RDNA 2 owners  >brings up Nvidia  Never change.,hardware,2025-11-03 15:17:34,9
AMD,nmvz2kj,Thank you. I just switched my gaming machine from Windows to Garuda earlier this year.,hardware,2025-11-03 14:54:19,1
AMD,nmz2j71,"The literal exact words were, [""Typically I only see double spaces after periods today if something was written by a lawyer.""](https://youtu.be/dkPPejQXFNo?t=532). You're right, that's an iota or two less certainty than ""=="" signifies. I am undone!",hardware,2025-11-04 00:09:18,1
AMD,nmufe5y,Mostly because r/fuckcars imo.,hardware,2025-11-03 07:39:41,-20
AMD,nmvq4ec,"Putting aside that fires and bad connectors automatically disqualify you from ever being ""generally more reliable and versatile"", *it would still be untrue without literal fires*.",hardware,2025-11-03 14:06:54,-3
AMD,nmtzyo0,Imagine pain of rx580 owners ..only 3 years of support . Officially their production ended in 2019 . But due to mining crisis production was restored in 2020 .   And that’s AFTER card hited maintenance status   They stopped resurrected manufacturing only in 2022 .   In 2023 - rx480/580 hited EOL,hardware,2025-11-03 05:13:14,-18
AMD,nmuek8u,"The RX580 (2017) was in production to 2021, some small amounts into 2022. Where is the outrage then?",hardware,2025-11-03 07:31:01,-10
AMD,nmtxzsk,Legacy  in terms of amd - when active support and maintenance mode passed . So legacy for rx480 hited after 4 years of main support . And 3 years of maintenance .. with final version in 2023   Maintenance- they decide base version where they will fix bugs . But no features or extra support   Legacy means - no update even for maintenance version . So even if bug will be discovered after hitting legacy - it will not be fixed . Sometimes they does not bother with bug fixing even for maintenance version releases .. I still remember final release of notebook HD5000 family driver for windows 10 .. DXVA not worked there .. was it fixed ? Nope,hardware,2025-11-03 04:56:54,-9
AMD,nmu0wke,Look on DRIVER VERSION . Not package version or release date. Inside those update packs you mentioned - 2023 driver .. which is last version before those cards hited EOL . Those fixes intended for additional software . Not the driver itself,hardware,2025-11-03 05:21:17,1
AMD,nmubru4,AMD has been neglecting Radeon even it was one of their breadwinner in the bulldozer days. That and consoles are another drop in the bucket yet AMD seems to bend over for Sony and Microsoft's design direction.,hardware,2025-11-03 07:03:04,26
AMD,nmuha8u,I guess AMD is fighting to make Radeon dGPU down to 4% market share (from 6%).  expect 4% market share in consumer dGPU market in 3-4years time guys....they really want to kill Radeon themselves.,hardware,2025-11-03 07:59:12,13
AMD,nmvnzs4,"My God, even in 2025 people still want to pretend that Nvidia drivers are better. Did you just miss the last year of nonsense?",hardware,2025-11-03 13:55:14,6
AMD,nmy9b0p,"yeah but, if the workload needs cuda then...  on the enterprise front, there is so much more than that, and I dont think even if someone loves AMD they can try for a swap unless they have an in house dev team that can make use of AMD's features and is willing to spend the dev time to take advantage of that over a lot of the more pre-made stuff for nvidia.  like a swap to firepro cards would need the application to support it, and to have similar enough experience, and at least equal pricing for it to be worth it to an enterprise.",hardware,2025-11-03 21:32:48,2
AMD,nmzqnd4,"I have Nvidia at home, but I prefer AMD discrete cards at work when needed simply because the drivers are not as buggy for non gaming use and goes well with AMD CPU platforms.",hardware,2025-11-04 02:30:01,1
AMD,nmu34bh,They were behind in features despite having more vram. People value features a lot more than AMD thought,hardware,2025-11-03 05:41:04,55
AMD,nmu5tux,"AMD/ATI had plenty of market share (even ahead of Nvidia at points) until they fell behind on features, both hardware and software.  They floated at a fairly healthy amount until 900 invalidated basically their whole lineup and then they essentially didn't compete with the (rather amazing) 1000 series release either. At that point they'd already plummeted down in market share. Say what you want of RTX 2000 but it was pushing new tech at a point when AMD was still releasing RX 500 variants, and even with the costs the RTX 2060 was a better price/performance than what AMD had at the time. If you wanted performance (and even budget at the time) Nvidia was literally the only option, and this is coming from someone that owned both a RX 480 and a Vega 64.  After that they needed to dig out of a hole and what they offered was basically just ""more VRAM"" at a complete lack of any of the features or tech Nvidia was pumping out.",hardware,2025-11-03 06:06:07,36
AMD,nmuginu,"Thats is incorrect, AMD know the trick back in Zen 1 Zen 2 era, they offer VERY massive performance/price over Intel. Thats how they able to get the market shift to AMD ryzen.",hardware,2025-11-03 07:51:21,6
AMD,nmvmknz,Almost as if 99% of people don't care as much about VRAM as Reddit wants you to think?,hardware,2025-11-03 13:47:18,3
AMD,nmuh49o,"NVIDIA is just better! I remember getting AMD „advantage“ laptop thinking it would be better then getting AMD GPU in my desktop just to realise that they could not even get damn adrenaline software to work! Missing software, feature, not updating, recording option comes and goes…. They gave up on GPUs! On the other hand Nvidia simply works! Basically same situation like amd and intel now. I just prefer amd cpus and how they work.",hardware,2025-11-03 07:57:30,-7
AMD,nn0e455,"Sorry I mean latency with optiscaler, used the word upscaler by mistake",hardware,2025-11-04 05:04:03,1
AMD,nmwy0j8,"What about latency? How much latency you get? You can see it with optiscaler, like i have around 0.4 at quality with a 9070. This also counts as performance, it's possible that low-end cards like a 6600 have high latency, and therefore AMD has trouble releasing on the 6000 series. Anyway tell me how much latency do you have.",hardware,2025-11-03 17:44:08,-1
AMD,nmuktdo,"I bought Nvidia exclusively for over a decade. The 3070 will be the last GPU of theirs I will buy, that is the worst I have ever been burned by a piece of hardware. So it's between AMD and Intel for me from now on, and Intel just haven't made a high enough performance product for me to consider them. Hopefully they up their game or China start dropping some good GPUs.",hardware,2025-11-03 08:36:12,1
AMD,nmuus7o,"RDNA3 does support AI. It just doesn't have dedicated hardware like tensor cores, the shader units support WMMA, which can be used to run most AI models.",hardware,2025-11-03 10:20:26,1
AMD,nmv576i,"Because they *deserve* to play on Ultra, nevermind what Ultra actually means.  They want to render 4k textures on their 1080 monitor God damn it.",hardware,2025-11-03 11:53:38,8
AMD,nmvo4i5,"Considering that this is all manufactured, it's clear that no AMD action will help or hurt this situation, and they will just have to ride it in the direction it's heading.",hardware,2025-11-03 13:55:57,-1
AMD,nmvp05l,"You haven't been ""burned twice in a row"" by Radeon. You are being lied to.",hardware,2025-11-03 14:00:45,-3
AMD,nmyficr,I wouldn't extend that to Zen 3. Charging $300 for an entry-level CPU in the 5600X (a 50% price increase) w/o offering an 5100/5300/5500/5600 variant for almost two years was damn near criminal.   Not to mention the fact that they tried to limit support of features like ReBAR/SAM through a paywall in the form of B550/X570 mobos.,hardware,2025-11-03 22:03:41,1
AMD,nmu355g,"AMD sub has lots of people that rag on AMD, AMD customers know firsthand how ghetto it is sometimes to run AMD hardware",hardware,2025-11-03 05:41:16,24
AMD,nmwf74k,"The same statements that nobody understands at all? Never mind how parasocial people get with these damned YouTubers? The comments are just the same stupid memes by people who already chose a side well before this video even existed.  Like here is *the description he wrote*:  > AMD threatened to limit GPU driver support for products it is still ACTIVELY SELLING, then made errors in its driver notes, then fixed them, ""clarified"" its statement, then it clarified the clarifying statement, then it said you're all just confused.  This alone is horrible blatant lies! He tried to pull this shit with the 8 GB ""controversy"" too.  There are so many people *publicly stating* that they are choosing to make decisions based on *blatant lies*. All this tells me is that people don't actually care about the truth. And at that point, *nothing* can be done, and all hope is lost.  This is just the Proton thing and the Firefox thing all over again.  I'm just so tired of the lying, I'm tired of everything being run by shitty YouTuber hot takes. I'm so tired of being told that I'm buying into lies by the people who are *actually* buying into these lies.  Like it's just so *infuriating* that everyone is saying something like *this*...  > Why are gigantic corporations so allergic to admitting mistakes?  ...when *they* are the ones making the mistakes they need to admit to. They *never* will, because ""the customer is always right"" even when they're *harmfully* wrong.",hardware,2025-11-03 16:13:55,-2
AMD,nmwkbpg,That dude is all over this thread trying to run defense for AMD it would be funny if it wasn't so pathetic,hardware,2025-11-03 16:39:13,11
AMD,nmxgy08,"No it only affects the high wattage GPUs, like the 4090, 5090. There may have been instances of 5080 having the issue as well but I think those are rare.",hardware,2025-11-03 19:15:03,3
AMD,nmvxvzc,lol,hardware,2025-11-03 14:48:16,-3
AMD,nmz4f15,"Buddy is claiming Nvidia has ceased support for Ampere.   Well, at least Ampere (and bloody Turing) are able to run the newest DLSS4 transformer model...",hardware,2025-11-04 00:20:09,1
AMD,nmw9eqn,Have you been reading literally any of the thread? Any of it? Never mind the person I responded to is literally trying to buy Nvidia???,hardware,2025-11-03 15:45:40,-7
AMD,nmw1s2h,"Yeah see *none* of this will affect Linux to begin with, because AMD actually cares about Linux support and the Linux devs will actually support AMD cards for much longer than on Windows. What *will* affect Linux is if all this misinfo garbage gets Radeon killed outright... unless AMD goes full APU, in which case nothing bad happens and/or the situation improves considerably.",hardware,2025-11-03 15:08:08,1
AMD,nmzl1sx,Its weird you are trying to litigate steves lived experience.,hardware,2025-11-04 01:57:34,2
AMD,nmtycxc,"Polaris and Vega are still in maintenance then, they got a driver in may 2025 with bug fixes  But the point is, you got your timings wrong on the support length.",hardware,2025-11-03 04:59:54,11
AMD,nmud7nc,"Let me put in that way. Datacenter GPUs > workstation GPUs > APUs for consoles > mainstream PC market  Datacenter and workstation GPUs offer best margins. APUs for consoles offer huge volume and simplicity in a sense that you are going to deliver and support only two APU designs per generation (6-8 years) per customer. PC market offers neither high volume (their marketshare is miniscule), simplicity or highest margins, while also requiring a lot of support (FSR, drivers, many OSes, many OS versions, etc).",hardware,2025-11-03 07:17:19,26
AMD,nmuh0fw,The deal with Sony for PS6 is considered to be at least $30 billion and a lot of that money comes upfront. That's enough for R&D for a gen or 2 of GPUs.,hardware,2025-11-03 07:56:24,8
AMD,nmvnt2r,They haven't done anything of the sort. How can people keep saying this?,hardware,2025-11-03 13:54:12,-2
AMD,nmx3sps,"You realize a lot of people, including myself, haven't had a single issue with nvidia drivers all year, yes? ""every single report about nvidia cards""?",hardware,2025-11-03 18:11:21,5
AMD,nmvvu48,"It are better, the support is faster, and it have more features. The fact that there were problems with the Blackwell launch doesn't imply that they were bad overall, the fix was quick, and the overall result is still positive.",hardware,2025-11-03 14:37:32,2
AMD,nmu49r1,Yeah I'm convinced during RDNA1 and 2 AMD honestly thought things like ML based upscaling and RT were gimmicks that they didn't need to take seriously. Boy were they wrong.,hardware,2025-11-03 05:51:29,39
AMD,nmu4e97,"For stuff like RTX, I 100% agree.  But Pre-RTX, that was also the same case. Historically through Radeon's over decade of sales, the same shit always happen. I'll be surprised if AMD doesn't gut the gaming division in 5-10 years lol.",hardware,2025-11-03 05:52:40,3
AMD,nmvo7ap,"Nobody actually values ""features"", they value all the shitty influencers screaming to avoid AMD like it's a plague.",hardware,2025-11-03 13:56:23,-1
AMD,nmv4ahp,Which is very dumb but shows the brilliance of Nvidia markering. Ray traying is pure meme and dlss completly unnecessary. Only Nvidia feature has is cuda.,hardware,2025-11-03 11:46:30,-12
AMD,nmu6zl4,And kind of why it's a lost cause.  How Radeon behaves for their discrete GPUs feels like they just kind of gave up and that it only exists because custom solutions like the PS5/Xbox/Steam Deck are profitable which also translates to Discrete GPUs.,hardware,2025-11-03 06:16:55,14
AMD,nmv88wy,I mean Intel stagnating was also a huge part of that. NVIDIA isn't showing any signs of having similar problems that Intel had.,hardware,2025-11-03 12:16:26,8
AMD,nmvn715,Yup. People don't care about it. People also don't care about AMD as well.,hardware,2025-11-03 13:50:47,6
AMD,nmx94d1,"I got a 6650 xt. So...I would've been burned hard if amd abandoned the 6000 series. Just buy what's best for your price range. 8 gb vram on the 3070 was borderline criminal though. Even 8 gb on the 5060 is bad these days.   Assuming Amd properly supports their 6000 series I'll be open to buying amd again but I am looking at possibly switching to nvidia next time over this fiasco. It really depends on price. I thought amd was past their ""cut drivers for 3 year old cards"" phase (they did this A LOT back in the day but since then they seemed to improve a lot in that regard). If they haven't really changed though, yeah they're gonna have to relearn why people buy nvidia to the point that they have their evil monopoly in the first place.",hardware,2025-11-03 18:37:05,1
AMD,nmumwpm,"You'd think, but then you have r/hardware users praising AMD for not having driver issues anymore (usually referencing 1-2 gen old hardware with most the issues smoothed out) pretending r/AMDHelp doesn't exist with a shit ton of users trying to figure out the correct driver versions to use for specific games through the power of divination or constantly warning people not to update drivers.  I may be slightly salty after I bought a 7900 XT and the consensus seemed to be that driver issues are a thing of the past (apparently I missed the caveat that it was the case on the 5000 series and maybe 6000), till like 7/10 of the DX12 games I tried were crashing on the day I got the GPU and I went diving through the help forums looking for good drivers.  Oh well seems like NVIDIA aint doing so good these days either.",hardware,2025-11-03 08:58:18,9
AMD,nmy0r9p,">There are so many people *publicly stating* that they are choosing to make decisions based on *blatant lies*. All this tells me is that people don't actually care about the truth. And at that point, *nothing* can be done, and all hope is lost.  What part of that is lies and what is the truth? This video is the first I've heard about this, but I've always thought Gamers Nexus seemed trustworthy.",hardware,2025-11-03 20:51:09,3
AMD,nmy662d,"Seriously. Somehow AMD seems to attract the biggest fucking bootlickers, despite the fact that a decent amount of their underdog status is literally due to their own incompetence.  Pathetic.",hardware,2025-11-03 21:17:16,2
AMD,nmxn0d4,"Good, because I'm not able to afford those monsters anyways. I look for more modest cards like the 5060 ti... less power, and less bulky for my needs.",hardware,2025-11-03 19:45:01,2
AMD,nmwf2ct,Well...Nvidia is still supporting Ampere which is contemporaneous to RDNA2.  Take that as you will.,hardware,2025-11-03 16:13:16,9
AMD,nmwag8i,Could FSR upscaling training be affected for Linux users? I’m not sure if they have to do any RDNA2-specific training for that.,hardware,2025-11-03 15:50:42,1
AMD,nmu4rsu,Amd on downloading site for those drivers CLEARLY said  Driver version 2023 . That’s the last driver version when those cards hited EOL (legacy ) in 2023.  What they updated there ? Control panel ? Overlays ? Help ? Additional architectural agnostic things?  - does not matter.,hardware,2025-11-03 05:56:11,1
AMD,nmvpyur,"Its still making moneys but a bit less and with a bit more effort,   personally i don\`t get it but that\`s their business.  Also personally if i don\`t trust company in home usage i wont recommend it for professional usage more times than yes.   Assuming home usage is not based only on better pricing but its actual trust thing.",hardware,2025-11-03 14:06:03,2
AMD,nmywkux,Not that I don't believe it but out of curiosity do you have a source for that number. Do we have a number for the Nintendo Switch 2 and Nvidia deal as well?,hardware,2025-11-03 23:36:02,1
AMD,nmvw808,"None of this has been true of Nvidia for *years* now. There has never been an actual *fix* for anything. The ""overall result is still positive"" because people are literally accepting worse and pretending that it's better.",hardware,2025-11-03 14:39:36,0
AMD,nmupgom,"> AMD honestly thought things like ML based upscaling and RT were gimmicks that they didn't need to take seriously.  and that's actually a charitable way to look at it.      My opinion is harsher : It wasn't part of what they needed to make good on the Sony/Microsoft contracts, and they weren't going to make it on their own dime.      Now they're building the next round of PS/Xbox hardware on Sony/MS's money, so they build the features.      Because AMD only makes major architectural evolutions when Sony and Microsoft pay for it, that's the ""dark side"" of ""fine wine"".",hardware,2025-11-03 09:26:03,18
AMD,nmvob4q,I like how there's a thread about nobody actually using RT even in 2025 and you're still saying this. It's crazy how *blatant* this misinformation is.,hardware,2025-11-03 13:56:59,-7
AMD,nmupr5e,> ML based upscaling and RT were gimmicks that they didn't need to take seriously.  Half right,hardware,2025-11-03 09:29:12,-14
AMD,nmu517j,"They will be heavily present in gaming, but who knows about discrete GPUs.  Playstations and Steam Decks aren't going anywhere.",hardware,2025-11-03 05:58:36,13
AMD,nmwdniu,"I use DLSS all the time. 90%+ of the time, I see no visual difference, but I get better FPS.",hardware,2025-11-03 16:06:15,3
AMD,nmw0me1,"I suspect you're right.  I also suspect that AMD will have a really solid GPU lineup built on the architecture of the ps6, much the same way that rdna1/2 was so good. Mainly just because someone else footed the bill for r&d. Just enough to keep it semi-relevant.",hardware,2025-11-03 15:02:13,2
AMD,nmvoq5j,Did you just ignore all the drama over the last *four* gens of Nvidia GPUs?,hardware,2025-11-03 13:59:16,4
AMD,nmvg79e,"The stagnation didn't start until later, intel was still making decent performance uplifts during zen 1 / 2.",hardware,2025-11-03 13:09:10,0
AMD,nmvavdr,"Tbf 7/10 seems extremely high and would point towards a faulty card, I had an rdna2 card and it was a mostly smooth experience in terms of gaming (it had annoyances in terms of sleep mode not working properly and some YouTube playback problems)",hardware,2025-11-03 12:34:57,1
AMD,nn05az8,Let us never forget the AMD ceo who chose to not merge with nvidia because Jensen wanted to be the ceo of the combined company. Today Jensen is the ceo of the world’s most valuable company. If I had AMD stock from back then I’d wanna find hector Ruiz and shit on his lawn.,hardware,2025-11-04 04:00:27,1
AMD,nmwgkm1,"But they're not! Everyone keeps saying this, but the reality is that ""support"" depends entirely on *actions*, not official statements (or lack thereof).",hardware,2025-11-03 16:20:45,-2
AMD,nmuibvj,"AMD Software: Adrenalin Edition 25.5.1 for Polaris & Vega Release Notes https://share.google/IYyM0FiazQ0dNlpDj  It fixed a crashing issue in black ops 6, a title from after they entered maintenance mode",hardware,2025-11-03 08:09:57,6
AMD,nmvvp7h,"A lot less with a lot more effort. There's no volume, low margins, dozens of GPUs every gen and you release a new one every 2 years.  >Also personally if i don`t trust company in home usage  People that make decisions to buy thousands of GPUs for datacenter don't care about home use or how AMD treats gamers.",hardware,2025-11-03 14:36:50,2
AMD,nmvyleg,And did you use an Nvidia GPU or are you just talking nonsense?,hardware,2025-11-03 14:51:53,4
AMD,nmw6rsy,">I like how there's a thread about nobody actually using RT even in 2025  The thread doesn't say that. If people in the comments are saying *they* don't use RT, then that doesn't prove anything because the point being made is that Reddit subs like this one don't reflect wider consumer sentiment.",hardware,2025-11-03 15:32:50,8
AMD,nmurm1u,"Not even half right. GN and others actively worked against promoting these features.   I'll be the tinfoil hatter and state AMD took GN and their counterparts opinions as those of the market and now are ""confused"" just like I'm sure YouTubers were that their market pressures didn't do jack shit to push AMD dGPU into the lead.   No one should be confused. AMD dGPU has been running the same since they dissolved ATI. Ie, like Temu Nvidia - just the greed none of the tech.",hardware,2025-11-03 09:48:50,13
AMD,nmw1mu7,"They will likely limp the discrete GPUs along, putting just enough into it to remain ""relevant."" I imagine it's easier to make a solid product for the PlayStation/steam deck type systems when you aren't starting from something woefully outdated and have at least somewhat attempted to stay relevant on paper.",hardware,2025-11-03 15:07:24,2
AMD,nmzevhz,I use fsr and dlss 0% of the time.,hardware,2025-11-04 01:21:24,1
AMD,nmvu8dm,The drama where they increased market share and performance every gen?,hardware,2025-11-03 14:28:55,2
AMD,nmw88ps,"They were though. They were rehashing the same lineup of 2 core, 4 thread i3s, 4 core, 4 thread i5s, and 4 core, 8 thread i7s for *years* before zen came out. Couple percent uplift (up to around 12% or so from what I remember) every gen, nothing huge. Sandy bridge and ivy bridge could mostly hang with an i7 7700 if you got a good cooler and overclocked. Immediately after zen released, Intel went from the stance of ""why would you need more than 4 cores"" to the stance of ""now you can have cores, and single thread thread performance.""  Just for context, Sandy bridge and ivy bridge (gen 2 and 3 core) are largely the same ipc, just ivy bridge is a die shrink so its more power efficient. Comparing them to kaby lake (that last of the 4 core i7s) kaby lake is approximately 25-35% faster, depending on the specific models and benchmark, etc. This includes both ipc and frequency increases. I was running a 3770k overclocked at the time, and remember thinking that Skylake was the first gen even worth considering an upgrade for, then zen came out and changed the game.  We've really been spoiled the last 5-6 years or so. AMD went from lagging single thread performance by around 10%, but having vastly superior multi thread performance, to catching up on single thread with zen 2, then having a second 20%+ performance uplift in zen 3 over zen 2, and another increase of >10% with zen 4. Back in the pre zen days, I remember thinking that cpus in general had nearly hit their theoretical limit, since we hadn't seen a significant generational uplift since Sandy bridge over nehalem.  I guess it wasn't all bad though. You could get a gpu only a tier down from the top card for $300-400. I remember when the original titan came out for $1k and it was outrageous because the 780 was $500. Back when a _70 card was only $300.",hardware,2025-11-03 15:40:00,2
AMD,nmw34x2,"I was considering that possibiltiy but nope, it was drivers and got better with time. Though some games like WoW with DX12 renderer had issues for over a year till I went over to Linux for good.",hardware,2025-11-03 15:14:56,3
AMD,nmwd7tn,"I agree, or maybe they played all the titles that had issues.    I own a 7900xt and have only had 1 issue: BSOD after waking from sleep. I know it's a graphics issue, but couldn't hunt it down.    Doesn't matter anymore because I've switched to debian and everything works perfectly now.",hardware,2025-11-03 16:04:04,1
AMD,nmxdkc2,professional users do care about ROCm being complete garbage though,hardware,2025-11-03 18:58:31,4
AMD,nmvw9s3,"it still brings millions per year, but their choice for sure      in decade or 2 probably everyone will be gamers more or less though :P",hardware,2025-11-03 14:39:51,0
AMD,nmvw0so,Yeah dude AMD is doing so much better in professional and datacenter GPUs,hardware,2025-11-03 14:38:32,-1
AMD,nmw25mq,"What does this even mean? I'm not buying a 5090 for your sake, sorry. I don't need to actually use a 5090 to understand how atrocious *every single report* about Nvidia cards has been for *years*.  edit: I am literally not wasting my money on Nvidia garbage for your garbage fake proofcalls, come on now.",hardware,2025-11-03 15:10:01,-4
AMD,nmwawwa,"Yes it does! Read the thread!  Nobody knows what the ""wider consumer sentiment"" is! Nobody! It's not relevant at all if you don't even know how to quantify it!",hardware,2025-11-03 15:52:54,-5
AMD,nmuspkw,"I mean it's a bit rich to argue greed when you're talking about nvidia's competitor, but yes I remember the ATI days.  Subjective, but personally I definitely don't give a toss about RT / fake frames even if the unwashed masses clearly like 'em",hardware,2025-11-03 09:59:54,-14
AMD,nmvvfy6,"No, that's not at all what the last four gens of Nvidia GPUs have been.  The 2000s were a complete disaster compared to the 1000s (especially at the time, only *now* do the 2000s look even remotely good), the 3000s were an extremely tiny bump while also landing smack-dab in the heart of COVID so they were *hilariously* expensive, and the 4000s and 5000s haven't been much better at all while also having incredibly ridiculous + *dangerous* hardware/software issues.  Marketshare gains, especially in the face of the above, have been such an obvious and hilarious example of manipulation... and *nobody* seems to care. Nvidia will likely *never* see justice for what they've done.",hardware,2025-11-03 14:35:28,-2
AMD,nmw4708,"Oof, glad i skipped that gen then",hardware,2025-11-03 15:20:13,1
AMD,nmxh103,"It quite likely had an element of bad luck, I'd imagine with a worse than usual driver revisions and specific games that had issues, but it did piss me off back then. Off the top of my head I remember like  - Cyberpunk 2077 (with most of the modern features on) - Witcher 3 (next gen update, same as above) - Diablo 4 (at launch, got better later, the crashing I mean, the game itself didn't) - Baldurs Gate 3 (at launch, got better later) - Classic WoW (Wotlk, with DX12 and in specific bosses like Algalon, I'd guess because of some specific shaders having issues?) - Mainline WoW (Dragonflight, with DX12, happening quite randomly)  All quite random driver timeouts, and the only thing I ever found out to be common between my issues was DX12, and I never had any issues on DX11 games. Just happens that I play older games more and the few newer/dx12 games I tried playing had issues (and they often had other people reporting issues too, can find them even now by googling old r/AMDHelp threads).  Funnily enough after completely moving to Linux in later 2023 I only had issues in 2 games out of everything I play  - Classic WoW again with ""dx12"" through its conversion layer - Ghost of Tsushima (with a specific kernel version that introduced some bug that caused timeouts)",hardware,2025-11-03 19:15:28,1
AMD,nmvzmbs,"Amd that's why they are tapping into console market, handheld market and even phone market (via collaboration with Samsung)",hardware,2025-11-03 14:57:06,1
AMD,nmw2rrt,"Yeah, they do. Datacenter brings in revenue an order of magnitude higher than desktop Radeon GPUs",hardware,2025-11-03 15:13:06,3
AMD,nmwz13z,"In other words, nonsense. Just because you've read a few reports from people who had problems doesn't mean that overall driver support is bad. You'll rarely read someone submitting a report just to say that everything is fine. I've had Nvidia GPUs for 20 years, and I've never had any driver problems.",hardware,2025-11-03 17:48:52,6
AMD,nmw3agy,"lol so you dont own an nvidia gpu, ""trust me bro"".",hardware,2025-11-03 15:15:43,5
AMD,nmwd94n,The main way we can quantify it is through sales. and the gap between AMD and Nvidia marketshare has considerably **widened** over the last 3 generations.   So you can either believe 90%+ of the market are buying just solely on brand name with no further research.  Or that the features advantage that Nvidia has enjoyed actually does matter to enough people.  And hardware enthusiasts sub opinions are no more reflective of the wider market than car enthusiast subs are for that market. Enthusiast forums only reflect enthusiast sentiment.,hardware,2025-11-03 16:04:16,8
AMD,nmv4xm0,It doesn't matter if YOU care about RTX or upscaling.  DEVELOPERS are all in on that technology which means you can either play indie games or get a card that utilizes those technologies.  I don't know what's so hard to understand about that.,hardware,2025-11-03 11:51:38,12
AMD,nmxyjil,Cyberpunk might not have been AMDs fault. I had an RTX 3070 and had 5 crashes within the first few hours. That game was just a mess at launch.,hardware,2025-11-03 20:40:36,1
AMD,nmw162g,"i get it i just don\`t understand why companies axe less profitable branches  its like with blizzard i get that one mount in wow bring more revenue than whole SC2 with couple days of work of prolly 1 person   but that\`s today, and SC2 still was been giving hefty profits   and its not interfering with that guy from mounts doing his job :P  But im not a accountant so i can not get that :)  PS. if im counting how much i can get for mount price others will do to soon enough.  taking it to a topic, today there is cosmic size bauble on AI market but its not working that well in everything they try to push it in by force and in the end baubles have tendency to burst on random things more or less violently (depending on size)",hardware,2025-11-03 15:05:03,1
AMD,nmwdnxi,"> So you can either believe 90%+ of the market are buying just solely on brand name with no further research.  Considering that this is *literally how everything in history has ever worked*, it's almost certainly this, despite your attempt to portray it as ""strange"".  Never mind that we don't even have sales data/marketshare to begin with. We have heavily manipulated Steam surveys at best.",hardware,2025-11-03 16:06:19,-2
AMD,nmwkpmu,"> It doesn't matter if YOU care about RTX or upscaling.  May I introduce you to the world ""subjective""?  > play indie games or get a card that utilizes those technologies.  Weird as I've had no issues. Also if you wanna get smarmy ""there's no replacement for displacement"".  Enjoy your thousand hz TV.",hardware,2025-11-03 16:41:07,-2
AMD,nmvohdn,I mean... developers literally aren't. This is misinformation.,hardware,2025-11-03 13:57:56,-2
AMD,nmxz1ty,"Well it was gpu driver crashes rather than game crashes. So it's kind of up to the GPU/driver side to handle misbehavior cleanly and having the usermode side fail rather than the entire driver, even if the game is misbehaving. At least thats my understanding of it. (also it was 2023 so plenty of time post launch)",hardware,2025-11-03 20:43:01,1
AMD,nmz6xbo,"IP holding companies gonna IP hold. Adeia has sued Disney, Nvidia, Videotron, Shaw Communications and others.",hardware,2025-11-04 00:34:49,26
AMD,nmx2854,"> Adeia said in the complaint that its predecessor Tessera pioneered hybrid bonding and advanced process nodes, which are both technologies related to semiconductor manufacturing. The lawsuit said AMD processors that include ""3D V-Cache"" technology, including its AI chips, were made using Adeia's patented methods.  Shouldn’t they be suing TSMC for violating the process patent and not AMD? Like if Adeia patented the use of 3D v-cache for CPU performance it is one thing, but how TSMC manufactured the processors AND might not know due to it being a trade secret",hardware,2025-11-03 18:03:48,36
AMD,nmwxpxp,The only two points I can find are about 3D-V cache and advanced process node. Not sure what to think about it though.,hardware,2025-11-03 17:42:47,7
AMD,nmyrjts,"*If hybrid-bonding is patented by them, as they claim, how come they* ***don't*** *sue Intel then?* Or Sony?  Intel's *Foveros* technique is literal nothing but hybrid-bonding for a die-to-die interconnect.  Apart from the fact that this shop's whole legal whereabouts are quite troubled, to say the least – Seems the company underwent a bunch of sell-outs and renames (TiVo, Tessera, Xperi, Adeia et al) and really comes off as a lousy patent-troll here (especially after AMD already used such claimed stolen technology of theirs for 3D&nbsp;V-Cache for literal years now), something seems quite fishy here …  *AMD isn't remotely the only one in the market using hybrid-bonding* – Samsung offers it too (InFO), Amkor Technology as well, Intel as mentioned, Sony uses hybrid-bonding for their CMOS-imagery sensors basically since ages (almost a decade since 2016 or so) … So something is really off here.  Since even if we take their claimed words for granted (that AMD actually uses their actually patented technology), AMD *never* in a million years would deploy a market-unique halo-product like their 3D&nbsp;V-Cache equipped SKUs since years (to be prominently deployed in the tens to hundreds of millions), **without** having the necessary technological IP or licenses to do so in the first place.  AMD isn't Apple (or Intel for that matter), only very unlikely would stick to such a behavior and generally avoids law-suits at all costs and would \*never\* enter such a risky road to begin with …  ---- Call me crazy, paranoid or Andy already … but this looks very, *very* much like a quite targeted, incredibly dirty approach and harassing fire (hopefully resulting in a complete sales-ban by the one instigating it), to prevent AMD running away uncontested with their market-leading 3D&nbsp;V-Cache CPUs, when no *other x86-competitor* in the market has a realistic chance to catch those nigh unbeatable AMD 3D-Cache SKUs anytime soon, especially if those are deployed in the realms of servers (to further eat away other's market-share).",hardware,2025-11-03 23:08:42,6
AMD,nmyxdal,"Adeia and AMD are both US companies, while TSMC is based in Taiwan. Suing another US company for infringement is less complex than suing a foreign company in a US patent court.",hardware,2025-11-03 23:40:20,2
AMD,nmzeiz9,So they’re a patent troll,hardware,2025-11-04 01:19:20,22
AMD,nmxmcrt,"I think there are a lot of things at play here. One aspect is that even though TSMC provides manufacturing and packaging infrastructure, AMD will still hold plenty of IP for how the caches are built and integrated specifically. So there are many components both AMD and TSMC bring to the table to make the 3D Sandwich work. Adeia might believe that AMD has the IP of the part of the sandwich, for which Adeia wants money. Like Intel couldn't just use 3DVcache like AMD because AMD also has a gazillion patents and IP protections over that.   The other aspect is a tactical one. There could be lots and lots of legal considerations for both fighting chance and expected returns. Maybe they think that TSMC got their back covered twice and thrice and that Adeia only has a chance with AMD, or maybe they can argue for a higher license fee/payment from AMD because they can claim the chips are worth more. I don't know, but I think there are many considerations depending on what Adeia would like to attack specifically.",hardware,2025-11-03 19:41:47,14
AMD,nmzqovs,"A patent holder can sue any step in the chain, from AMD to the consumer who buys the CPU. They pick the one they think they have the best chance of getting money out of.",hardware,2025-11-04 02:30:17,1
AMD,nmywcpd,"The whole thing is fishy as hell by the looks of it alone! Not because, since it targets AMD, but especially that it targets AMD for their 3D&nbsp;V-Cache equipped SKUs (AMD holds a market-leading position with since years now), which stinks against other rivalling x86-competitors (who can't get their sh!t together since years).  **Hybrid-bonding is widely used in the industry.** And according to Grok (I know…), Adeia is actually the rightful patent-holder of some hybrid-bonding IP (Don't know about the validity of it nor can I verify it right now!) and *each and every applicant of such technology* in the current market is a \*rightful\* licensee of Adeia's patented IP.  * Sony for their *Exmor RS* CMOS imagery-sensors (IMX series) since 2016  * Intel with their *Foveros* (–Direct) and partly EMIB  * Amkor Technology (various in-process applications for process-technology customers)  * Samsung (largely for their *InFor*, HBM et al)  * Yangtze Memory (YMTC) with *Xtacking* for their 3D-NAND   * XMC for their *UniIC* (DRAM stacked on SunLune JASMINER-X4 Bitcoin miner ASIC)  * as well as other various licensees  *  *and explicitly AMD with their 3D-Cache for sure as well*  So it really begs the question why of all things AMD (especially as the current position as the x86-CPU market-leader in performance; through their 3D-Cache) would actually use it actually \*unlicensed\* since years.",hardware,2025-11-03 23:34:48,-2
AMD,nmzqu4u,"Or they sued one company to start with, and if it goes well they go after the others.",hardware,2025-11-04 02:31:09,2
AMD,nmz7ono,"While this sounds all to logically at first, yet this is actually pure nonsense — TSMC's fabs in Arizona operate under a U.S. national- and are legally run by a 100% US-domestic subsidiary-company (which is fully owned by TSMC itself, of course), yet which is 100% subject to U.S. laws and regulations.  The above company-structuring was actually necessary and is **mandatory** to have become *a Chips Act awardee* to begin with, as *only US-national companies* (or their U.S. national/domestic subsidiaries respectively) can be possibly subject to any grants, subsidies and whatever monetary pay-outs from the Chips Act in the first place (or *CHIPS & Science Act* for that matter).  > Suing another US company for infringement is less complex than suing a foreign company in a US patent court.  No. That's actually really NOT how anything legal works … You sue the local/national subsidiary and thus *by extension* the actual parent-company.  In practice, it would make virtually no greater difference to sue a foreign company through its subsidiary (other than maybe to respect the national conventions; like paper size Legal/Letter in the U.S.; DIN A4 in the EU and Japan etc), legally it makes exactly zero difference to do so, as you're suing the subsidiary only as a proxy for the parent.  You sue Intel's local subsidiary in Munich as a German; You sue ARM Ltd.'s US-subsidiary in the U.S.; The European Commission sues Apple's Ireland-subsidiary in the EU et al … as the given \*national\* *representation of* Intel, ARM, Apple, Xyz respectively and thus by DEFAULT (ex relatione) the overarching parent-company.  In such cases, the given local/national subsidiary, only acts as a legal proxy for the respective (or not so) parent, yet you're legally directly dealing with the actual parent-company of said subsidiary.  Seriously, did you actually ever read any lawsuits in the past, where a company was sued for Xyz?",hardware,2025-11-04 00:39:17,-2
AMD,nmz1tci,"> *The other aspect is a tactical one*. There could be lots and lots of legal considerations for both fighting chance and expected returns.  Exactly. The fact alone that AMD is just one (1!) of many, many applicants and industry-clients which use hybrid-bonding among the likes of Intel, Samsung, Amkor, Sony, SK Hynix, Kioxia/WD et al … and even minor industry-players (one could argue, would be a \*way\* easier ""target"" to successfully bring to fold legally and press money from), like Chinese *Yangtze Memory* (YMTC; 3D-NAND) or also Chinese XMC (UniIC-coined DRAM for Bitcoin-miner ASICs), really shows, that targeting AMD with their 3D-Cache, would be a *way* harder legal target to actually bring to fold and press money from, as doing so by targeting other rather minor players instead.  So by the looks of it, it can't be really *that* much about money actually (nor the legal difficulty to overcome and bring to fold afterwards), than it seems to be, well … **strategically**, as you put it so nicely.  Also, AMD can't be really called a minor here anymore, as they can afford their legal department fighting for years now as well — It would be quite a long-lasting and costy legal battle for all involved with (given the facts right now) uncertain outcome. Yet it still would majorly weakening AMD financially and from a market-competitive standpoint for years to come …  ---- So the only real question it begs here, is; WHO exactly would profit the most from it, if AMD suddenly can't sell their majorly potent 3D&nbsp;V-Cache equipped SKUs anymore (AMD holds a market-leading position with), given the hypothetical case here, there would be a sudden sales-ban issued thus AMD would have to cease selling those SKUs (cf. Apple-Qualcomm lawsuits back then; Qualcomm got a sales-ban for iPhones)?  *I think we can mostly agree on the fact, that there would be mainly only 1 single lone beneficiary*.",hardware,2025-11-04 00:05:10,3
AMD,nmzxdbx,"> I think we can mostly agree on the fact, that there would be mainly only 1 single lone beneficiary.  That's an absolutely hilarious jump to conclusions from someone that evidently has no idea about the whole thing. But why am I surprised this is an actual take in r/hardware nowadays.  If you want to patent troll your way to a few billion, you go with a target that is the ""correct"" size and can settle out of court with pocket change.  And then you still need a target inside that sort of budget range where you maximize your success chance. It's likely AMD was picked because the patent Adeia hold have the highest chance of winning in a court battle vs some of the IP AMD has.  But sure, it's definitely Intel doing this, the company that has a million other things to do, with *much* higher ROI.",hardware,2025-11-04 03:09:42,1
AMD,nmwhq5w,"RDSEED failures are incorrectly being flagged as correct, leading to potentially predictable encryption keys being generated by the random number generator. AGESA firmware fix coming soon.",hardware,2025-11-03 16:26:29,66
AMD,nmwhmfl,"Hello BrightCandle! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-03 16:25:59,3
AMD,nmzyk6f,"I use AMD for performance, not security.",hardware,2025-11-04 03:16:59,-4
AMD,nmwunr2,"[https://www.felixcloutier.com/x86/rdseed](https://www.felixcloutier.com/x86/rdseed)  16bit and 32bit instructions affected, 64bit doesn't have the same problem.  Normally exposed in C/C++ as these three intrinsics... so switch to use the latter if you use either of the other 2 (and/or wait for microcode fixes)      RDSEED int _rdseed16_step( unsigned short * );     RDSEED int _rdseed32_step( unsigned int *   );     RDSEED int _rdseed64_step( unsigned __int64 *);",hardware,2025-11-03 17:28:16,36
AMD,nmyzpxz,People shouldn't be using these directly in almost all cases. They should use the OS's RNG which has diverse sources of entropy (including RDSEED) and is thus more robust.,hardware,2025-11-03 23:53:18,12
AMD,nmx1bqz,I wonder how often the affected ones are used. It seems pretty rare to want a number that's cryptographically secure but within brute-forceable range.,hardware,2025-11-03 17:59:30,20
AMD,nmx2zpq,"A surprising amount of crypto implementations store their keys in 32bit word arrays internally, so it's probably non-zero.",hardware,2025-11-03 18:07:29,23
AMD,nmx1lob,Very much my thought,hardware,2025-11-03 18:00:49,5
AMD,nmyekun,"Not really representative but the 64-bit version seems to be slightly more popular. Also the 32-bit version seems to be mostly miss-used, one repository invokes rdseed32 twice just to combine the two results into a 64-bit word at the end...  https://grep.app/search?q=_rdseed32_step",hardware,2025-11-03 21:58:52,4
AMD,nmytbfp,Is calling rdseed32 twice and combing the answer unsound? Aside from it just being not as efficient as simply calling rdseed64 once. Does that make the resulting number less random?,hardware,2025-11-03 23:18:13,1
AMD,nmyzbsn,"Security wise it's probably fine, just unnecessarily complicated and inefficient since AFAIR each rdseed/rdrand costs in the order of a thousand cycles.",hardware,2025-11-03 23:51:05,4
AMD,nmzwbll,"Yeah, it is indeed inefficient.  If I were to guess, it's done that way so that the same code gives you a 64-bit rdseed in both 32-bit and 64-bit modes (you can't use 64-bit operands in the 32-bit mode, so you have to use two calls to rdseed to compensate), though it would indeed be more efficient to write separate per-platform code paths.",hardware,2025-11-04 03:03:22,3
AMD,nmuq73p,"No, AMD did not reverse course to my understanding. The new blog statement as far I can parse is just same thing as previously, this time worded by someone vaguely competent.  Disappointing from HUB.  Game support sounds great, until you realize it can mean whatever AMD wants it to mean. Game runs after launch? Game support! Basically same thing as market needs, reduced support but this time with nicer phrasing.  So no more RDNA2 game optimization, unlikely inclusion of INT8 FSR4 in the driver for RDNA2 since the statement excludes new features, FSR4 version that runs even on Windows with driver version 23.9.1 via Optiscaler. Compared to the GTX 1000 series nominally having game optimizations up until this October, and RTX 2000 series having received DLSS4? The end result is inferior support by a company that has a weaker product and market position.    This is less impactful to me than some others, as I am moving over to Linux as much as possible as the Windows 10 ESU period winds down, but 2020 products that are as popular as RDNA2 do not belong in a maintenance/legacy branch in 2025 let alone 2025 products.  I can only assume that AMD is slowly exiting consumer GPU space at this point? Otherwise they need to do some heavy handed trimming in their Radeon division leadership.  EDIT: As others have pointed out, AMD has poor track record for updating Vulkan extensions/instructions in the maintenance fork. This could absolutely cause issues with games and various other software for RDNA1 and 2 in the future.",hardware,2025-11-03 09:33:59,54
AMD,nmu4fra,Will people actually praise AMD for the u-turn?  Now I am more sure they will silently turn off features instead of publicly,hardware,2025-11-03 05:53:04,61
AMD,nmuwfbi,There was no reversal,hardware,2025-11-03 10:36:37,16
AMD,nmur4n9,Please don't let go about the Vulkan instructions! That will have huge application and usage ramifications in the future!,hardware,2025-11-03 09:43:50,15
AMD,nmugb7s,"AMD's marketing, colorised https://www.youtube.com/watch?v=2WZLJpMOxS4",hardware,2025-11-03 07:49:13,9
AMD,nmub1hb,At least they are scared of public opinions lol. Keep an eye out for anything they might do to drop old cards.,hardware,2025-11-03 06:55:52,11
AMD,nmueqcx,"AMD did NOT reverse their blunder. AMD unboxed is at it again, spinning the corporate spin from AMD. Bravo.  They did not say directly they are not in maintenance mode. They are keeping the driver branches seperate. Its such a crock of shit lmao",hardware,2025-11-03 07:32:49,50
AMD,nmuj9za,"This doesn't sound as a 'revert' per se, which is quite terrible, because it tells me FSR4 will not be released on RDNA2 whatsoever, although it could technically run it, but on the other hand, how many times do people need to realize AMD simply isn't up to par with nVIDIA when it comes to driver support and forward architecture thinking?  Turing (2018) has dedicated RT/Tensor cores specifically designed for RT/DLSS, whereas RDNA3 (2022 - **4 YEARS LATER**) doesn't, and with that, I think I said it all.  In fact, I'm already sure RDNA 3 and even 4 will have the same faith, as UDNA/RDNA 5 will be a totally different architecture - which, by the way, will be literally a copy of Turing from a core perspective, which was released, once again, in 2018 -- 'Radiance Cores' are the ray-tracing cores, and 'Neural Arrays' are the tensor cores - obviously, much more powerful than Turing's, but you get the point - AMD is about 9 years behind nVIDIA in terms of architecture philosophy at this point.  So do not act surprised that 5700 XT didn't have more value compared to 2070S at 100$ more as some of people thought, there's a reason ""nVIDIA -50$"" doesn't work anymore, those 50$ will bite you in your ass next time and people have woken up.",hardware,2025-11-03 08:19:53,17
AMD,nmwc3et,"Steve is a little shortsighted on this one. By a lot.    It makes zero sense to announce multiple driver paths, and AMD has never done it unless it means end of life support.   So it's still maintenance mode, and on AMDs best selling generation, and products you can still buy new.",hardware,2025-11-03 15:58:34,4
AMD,nmuehm4,"People shouldn't care about these game optimizations that nobody (without internal access) can really quantify.  What we can quantify is Vulkan/D3D12 feature support. Features that do not require new hardware. And there, Vega and older have no received anything since they were moved to the ""maintenance"" branch. And the recent driver release only adds `VK_KHR_shader_untyped_pointers` for RDNA3+, despite this being a quality of life extension that all hardware can support.  This means either graphics programmers have to plan around the RNDA2 drivers for the next 3 years and not require anything new, or they only support RDNA3+. Both are bad, and this is only caused by AMD wanting to save money in the wrong place.",hardware,2025-11-03 07:30:15,5
AMD,nmv7v77,Funny how these youtubers surf on the useless (temporary) shistorms on Internet,hardware,2025-11-03 12:13:42,2
AMD,nmy6p0y,"I'm a little surprised by the negative feedback here. We haven't just taken AMD at their word, and we did express real concerns in the video regarding how AMD might end up handling this. I would have thought though with what we've got them to confirm at this point we would have enough to keep them in check, which is what we plan on doing.  On that note AMD has confirmed to us in writing that game optimizations for RDNA1-2 will be rolled out at the same time as RDNA3-4.  ""Yes, game optimizations and support for all RDNA Series 1 through 4 will roll out at the same time in both driver packages – including but not limited to CoD, Crimson Desert, and Resident Evil."" <- AMD's words.  So again we can now test upcoming games and hold AMD to this.",hardware,2025-11-03 21:19:51,0
AMD,nmucraj,"I don't think amd ever intended to not support these gpus. They have a strong track record of providing decent support for old gpus.   Hell vega 64 still works on bf6 day 1.  I think they just suck at pr and communicate, which is tried and true amd.",hardware,2025-11-03 07:12:48,-10
AMD,nmvm5du,How can something return that never left?,hardware,2025-11-03 13:44:51,-1
AMD,nmvrl7s,"They didn't ""reverse"" anything, they called shitty YouTubers out for their lies. Naturally, the callout is interpreted as ""confusion"" and a ""reverse"" by said shitty YouTubers.",hardware,2025-11-03 14:14:51,-5
AMD,nmu1e9m,"HUB turned ""these older GPUs are going into maintenance mode"" into ""OMG THIS IS CRAZY AMD IS DROPPING SUPPORT FOR A WIDELY USED GPU AND YOU SHOULD BE ANGRY!!""  Where was the blunder really?",hardware,2025-11-03 05:25:37,-33
AMD,nmvedxq,"Radeon runs itself like a poor company just trying to get by with the minimum viable product while pricing themselves as if they are almost as good as Nvidia.  I don't think this is them exiting gaming GPU, this is just the way they are.",hardware,2025-11-03 12:57:47,17
AMD,nmwp2tn,">No, AMD did not reverse course to my understanding. The new blog statement as far I can parse is just same thing as previously, this time worded by someone vaguely competent. Disappointing from HUB.  The difference between this video and GN's on the same PR release is night and day. HUB didn't even talk about how AMD basically implied RDNA2 cards will no longer get new feature updates which is a massive blow to card owners.",hardware,2025-11-03 17:01:48,8
AMD,nmugkwz,It is honestly discouraging how many people embraced AMD's cheap PR ploy even though nothing substantive has changed.  I also blame Tomshardware and the ones copying them for just claiming AMD reversed course when they didn't,hardware,2025-11-03 07:52:00,57
AMD,nmukpt1,"What is there to praise?? That they managed to somewhat save their ass over what is at best, a disastrous pr incident over nothing?",hardware,2025-11-03 08:35:08,18
AMD,nmutop3,"Is there something to praise? They haven't done jack shit, no new DX or Vulkan support, nor FSR.",hardware,2025-11-03 10:09:38,10
AMD,nmxatts,"Some will, some won't.  I remember the shit show that was Zen 3 support, the back track to allow for it on 400 series boards, and then their refusal to offer BIOS support on the 300 series boards until Alder Lake hit (which was mental considering the 400 and 300 series chipsets are exactly the same).",hardware,2025-11-03 18:45:22,2
AMD,nmvdh9a,It's not Nvidia so of course they will.,hardware,2025-11-03 12:52:00,2
AMD,nmyryqg,"There was no reason to tell us they could have just stopped updating the drivers and no one would ever have known.  I'd just release a new version ever quarter where the only difference was the version number, would be fun watching people say it gave then a 5% uplift in FPS when it actually did nothing.",hardware,2025-11-03 23:10:55,1
AMD,nmuhu2f,"Separate branches is fine, and quite likely makes sense from a software development perspective if the newer architectures have diverged a lot from the earlier ones.  What I'm still not convinced about is that they'll apply the same amount of effort optimising new games on the old architectures that they will on the new ones. And notice that they didn't say they'll optimise **all** the new games, just that they'll ""optimise new games"". I wouldn't be surprised if RDNA 1/2 get less attention than 3/4 in future.",hardware,2025-11-03 08:04:52,24
AMD,nmvfhjx,>AMD unboxed  What?,hardware,2025-11-03 13:04:43,-2
AMD,nmvn4d0,"Does mesa support that extension on vega?  That would be your smoking gun of AMD's dis concern.  25.10.2 - October 29th, 2025 - release claims to have that extension and lists rdna1 and rdna2 in the compatibility of the whole release. (claims of end of support were greatly exaggerated)  GTX 1080 doesn't seem to have that extension.  So I'm not really seeing AMD as worse than Nvidia here.",hardware,2025-11-03 13:50:22,2
AMD,nmz6xwv,"Unfortunately when an idea has taken root in the hivemind, it's nearly impossible to change.",hardware,2025-11-04 00:34:54,0
AMD,nn02tw8,"perhaps your video title need a little tweak lol.  redditor read title only.   You should have worded to something like ""AMD *""reverse""* their blunder, will RDNA1/2 getting their game support back?""",hardware,2025-11-04 03:44:07,-1
AMD,nmzuksg,"People are angry just to be angry. Someone could announce simultaneously curing cancer, ending hunger, and achieving world peace, and someone would be angry about it over some trivial bullshit.  With respect to AMD's record of supporting older cards, the community doesn't _have_ to wait to hold them to their word. Vega and Polaris are in ""maintenance mode"", so one can always just look at how they're holding up in modern games (particularly compared to contemporary Nvidia cards) to see whether AMD's stance on legacy port is actually impacting customers of those cards in any meaningful way.  Watching YouTube channels that specialize in older/lower cost hardware (e.g., RandomGamingInHD), I haven't seen any major issues with older AMD GPUs, but someone with a more rigorous test suite might be able to uncover issues/edge cases.",hardware,2025-11-04 02:53:01,-2
AMD,nmuef8u,"Yeah. Not doing ""game-ready"" profiles really isn't the same as ""we won't guarantee that the render APIs used by games work any more"", but ehhhhhhh nuances die on the Internet.",hardware,2025-11-03 07:29:35,9
AMD,nmu7y9z,Old gpus like what rog ally is shipping with.,hardware,2025-11-03 06:25:59,19
AMD,nmu40co,One has to be a profesional HUB hater to interpret this situation like this lmao.  Their pressure and and video no doubt contributed to this reversal.  Aint a good look when one of the bigest hardware channels starts not reccomending your GPUs.   Either way the AMD statement is still not good enoigh and just a blanket PR message,hardware,2025-11-03 05:49:07,15
AMD,nmvsbee,"It's insane that anyone believes the last few gens of Radeon alone are ""minimum viable product"". Nobody understands what that term means at all.",hardware,2025-11-03 14:18:44,-5
AMD,nmzvzga,"Sounds like typical hub tbh. Yes they criticize AMD but not as much as DF, GN or LTT does.",hardware,2025-11-04 03:01:20,4
AMD,nmujqrc,This exactly. They are using corporate platitudes. And it was super effective. I’ll believe it when I see game optimizations for RDNA 2 at the same time as later ones.,hardware,2025-11-03 08:24:45,30
AMD,nmvrrow,"In reality, it's actually discouraging that anyone ever bought into the garbage ""dropping support"" narrative.",hardware,2025-11-03 14:15:49,2
AMD,nmwbleo,"It is discouraging how many people embraced FUD and disowned Radeon even though nothing substantive has changed.   When RDNA2 keeps working fine for new games for years and gets a feature update or two anyway, no one will apologize for being reactionary or wrong.",hardware,2025-11-03 15:56:09,-1
AMD,nmuu22o,"It doesn't make any sense at all.  Just look at Nvidia's driver, or Mesa (linux), which is a massive ""driver package"" that supports almost every thing released in the last 20 years.  In fact, Mesa / RADV has implemented RT support on Vega +, and you can play full blown RT games on it at playable framerates.   This is just AMD trying to fuck us over. Remember the debacle with the 16MB bioses and Zen3? Yeah",hardware,2025-11-03 10:13:19,13
AMD,nmujga7,"Obviously they don't, that's the disadvantage when you always change your architecture significantly because you didn't design it with 'forward thinking' in mind in the first place - e.g. Blackwell (2024) is still based on Turing (2018) as a core philosophy (Shaders/RT/Tensor Cores), now look at RDNA, not even RDNA4 does have the same principle, but wait, RDNA5 finally will (Neural Arrays -> Tensor Cores/Radiance Cores -> RT Cores), but guess what will happen with RDNA 3/4 next, then. Obviously, they will be axed off just like RDNA1/RDNA2, because they also aren't 'forward thinking' architectures and AMD finally fully goes the nVIDIA way with it in like 2027.",hardware,2025-11-03 08:21:42,7
AMD,nmvgobc,"Ok, let's apply the same to Nvidia.  Are they spending the same time optimizing all new games for 2000 as 5000?  This is something we cannot know.  The real problem is AMD never should have said anything in the first place.  And this is why so many companies are not transparent at all.  Tech tubers need to stop ragebaiting over minor comments in a change log, and spend more time re benching RDNA1.  Benchmarks are real, claims of ""optimizing"" are not.",hardware,2025-11-03 13:12:05,0
AMD,nmz6nm9,Edgelord stuff from people who have never got over HUB not liking RT 7 years ago.,hardware,2025-11-04 00:33:14,-1
AMD,nmvo181,"RADV support `VK_KHR_shader_untyped_pointers` on all gpus - inluding rdna2, vega and even GCN1 from 2012. It has zero impact on anything hardware specific and is completely handled in common code that even shared with hardware from other gpu vendors.",hardware,2025-11-03 13:55:27,5
AMD,nmut3j0,"The internet just likes getting angry, it turns into a circlejerk.",hardware,2025-11-03 10:03:45,1
AMD,nmy3n67,"I think the term is ""mature"".",hardware,2025-11-03 21:04:55,1
AMD,nmy3e2b,There was no reversal. Nothing changed.,hardware,2025-11-03 21:03:42,1
AMD,nmwgofd,Its just an Nvidia employee probably. LOL,hardware,2025-11-03 16:21:16,-7
AMD,nmvckxu,That's the problem right there.  How does one see game optimizations?,hardware,2025-11-03 12:46:15,1
AMD,nmuue2y,"FYI I'm a software/firmware developer and I've worked on teams where we've split our source code between models, for exactly this reason. It does make some sense, although there are also downsides.",hardware,2025-11-03 10:16:37,14
AMD,nmv37u8,"AMD is splitting up the driver branches because RDNA 1 and 2 were not AI focused and lack dedicated hardware for newer, more AI focused/efficient data sets. With the move toward AI and features that require AI/similar data sets, it makes perfect sense to split the driver branches, separating RDNA 1/2 from the rest. RDNA 3 will be next because it's limited in the data sets that it supports so by the time UDNA/RDNA 5 hits shelves or around the same lifespan that RDNA 4 has at this time, RDNA 3 very well could get some sort of separation as well.  AMD was just very clearly way behind the ball on the AI shift/boom. The same can be said about Nvidia and where their driver branches separate. GPUs with Tensor Cores are on one branch while GPUs without them are on another.  Also, the thing about 16MB BIOS was very true.  It's why updating the BIOS on older boards was one way, dropping support for older CPUs after doing so.  Only so much room on the BIOS and AM4 had an extremely long life span so they could not support all architectures at once.  X570 boards straight up didn't support Zen 1 at all when they released...only Zen+ and newer. Same with updating 300 series boards, dropped support for Summit Ridge all together after the update.",hardware,2025-11-03 11:37:48,2
AMD,nmvhfl9,Releasing everything they have in one big dump does not mean they are actively improving the parts from 20 years ago.  But damn they've got you fooled by it.  Good move marketing team.,hardware,2025-11-03 13:16:45,-3
AMD,nn01u7k,when the suppose 6900XT are become much slower than 3090; thats were you'd see game optimization is being ignored.,hardware,2025-11-04 03:37:47,1
AMD,nmvrwb6,"They don't. Nobody knows what ""optimization"" actually is. Nobody ever knows if it's the developer's fault, the GPU's fault, or the user's fault.",hardware,2025-11-03 14:16:30,0
AMD,nmvjwgo,"Thank god that there are people working on a vulkan driver for Kepler (nvidia 600), also people adapting the current driver to GCN 1.0 GPUs.  Also, Vulkan 1.4 with the latest extensions to pretty much every AMD GPU from the last 10 years.  The same goes for Intel, their Windows drivers are terrible, but the Linux drivers work great and with more performance.  Like, my UHD 630 doesn't even work with half my games on Windows, and on Linux, I have the problem that everything runs on the iGPU instead of the dedicated one.   But yeah, Linux / FOSS marketing or something",hardware,2025-11-03 13:31:38,1
AMD,nl5deuj,"> These Radeon AI PRO R9700 GPU features a 4 nm ""Navi 48"" GPU, equipped with 64 RDNA 4 compute units, providing 4,096 stream processors and 128 AI accelerators for enhanced matrix operations across various data formats. A key improvement over the RX 9070 XT gaming GPU is the increased memory capacity, now doubled to 32 GB. The memory subsystem includes 20 Gbps GDDR6 across a 256-bit wide interface, delivering 640 GB/s of memory bandwidth, supported by a 64 MB 3rd Gen Infinity Cache. The AMD Radeon AI PRO R9700 achieves up to 191 TeraFLOPS FP16 dense performance and up to 1531 TOPS INT4 sparse performance, all within a power footprint of up to 300 W.   It's basically a 9070xt with 32GB of memory.",hardware,2025-10-24 15:10:35,31
AMD,nl63ysv,gddr6 instead of gddr7,hardware,2025-10-24 17:18:41,2
AMD,nl5ink5,Hooray for 9070 XT availability! ^/s,hardware,2025-10-24 15:35:45,12
AMD,nl60813,"[Good thing 9070xt 32gb was denied by Frank Azor ](https://www.reddit.com/r/hardware/comments/1ip5huu/amd_denies_rumors_of_radeon_rx_9070_xt_with_32gb/)  >No, the 9070 XT card is not coming in 32 GB.   After all it's r9700, not 9070xt",hardware,2025-10-24 17:00:34,9
AMD,nl6budy,640GBs is good enough for home use.,hardware,2025-10-24 17:56:46,16
AMD,nl75q4a,It's marketed as a pro GPU though?  It's still fast enough for many purposes but this isn't a consumer product.,hardware,2025-10-24 20:27:33,-12
AMD,nl7dhgx,"My brother in Christ. Just because you're not using a $10000 Ada 6000 with 960 GBps bandwidth, doesn't mean the rest are shit. Marketing has fried your brain  9700 pro - 640GBps  RTX Ada 4000 - 380GBps  RTX Ada 4500 - 432GBps  RTX Ada 5000 - 576GBps  https://www.techpowerup.com/gpu-specs/rtx-4000-ada-generation.c4171  https://www.techpowerup.com/gpu-specs/rtx-4500-ada-generation.c4172  https://www.techpowerup.com/gpu-specs/rtx-5000-ada-generation.c4152",hardware,2025-10-24 21:07:41,38
AMD,nl7eds4,"It competes with things like Nvidia DGX Spark, which has 273 GBs (and a lot less cores). It's not a bad product, the worst thing is that AMD's ecosystem is not as large as Nvidia's.",hardware,2025-10-24 21:12:28,7
AMD,nlb9mk1,"Ada is last generation, or maybe you're purposely comparing GDDR6 cards?  For Blackwell:  RTX 4000 Blackwell - 672 GBps  RTX 4500 Blackwell - 784 GBps  RTX 5000 Blackwell - 1340 GBps  https://www.techpowerup.com/gpu-specs/rtx-pro-4000-blackwell.c4279  https://www.techpowerup.com/gpu-specs/rtx-pro-4500-blackwell.c4278  https://www.techpowerup.com/gpu-specs/rtx-pro-5000-blackwell.c4276",hardware,2025-10-25 14:13:59,6
AMD,nle36ss,"it competes with RTX 5090, 4090 and 3090 too.",hardware,2025-10-25 23:15:23,2
AMD,nl7fz7n,DGX Spark is a little development system with an ARM CPU... this is a workstation card.   I assume its main competition will be stuff like RTX PRO 4000 but at a much keener price.,hardware,2025-10-24 21:21:09,1
AMD,nlbe6xs,"Because they're all GDDR6.  They don't magically stop working just because gddr7 exists.  Thirdly, RTX 5000 48gb Blackwell is minimum $6300 at local supplier VS $1850 for the 9700 PRO.   RTX 4500 32gb Blackwell is $3700 vs $1850 for the 9700 pro.  I have no idea if 140 extra GBps on top of 640 is worth 2x the price.   I would take a GDDR6 at half the price, even if it's Nvidia, over 2x the price gddr7 if the difference is only 140GBps.",hardware,2025-10-25 14:38:23,12
AMD,nl7jifl,"I know, but three of these are $3,900 and DGX Spark is $4,300. With a used CPU/mobo somewhat similar price range. Certainly not bad for a more powerful/flexible system.   RTX Pro 4000 is only 24GB, so you'd have to get 4 of them to match 3 of these AMDs. But again, CUDA can be a rather strict requirement for many people.   At this point I wouldn't say hardware for prosumer local AI is a solved problem at any reasonable price.",hardware,2025-10-24 21:40:41,9
AMD,nlc44vr,OK. I just wanted to be sure it wasn't a mistake that you listed last gen cards.,hardware,2025-10-25 16:53:13,2
AMD,nmsm7pa,"Nothing has changed, this is just a re clarification of their original statement they made because of the backlash. RDNA1 & 2 are still on some form of a legacy branch, and still looks like they not receive the same level of support as RDNA3 & 4 going forward.",hardware,2025-11-02 23:51:49,166
AMD,nmsrmde,This has everything to do with Redstone.  I think this might actually be confirmation that RDNA3 will be getting some version of Redstone but RDNA2 won’t.,hardware,2025-11-03 00:22:32,43
AMD,nmt0fq9,"Yes, we know. And maintenance mode isn't going suddenly become a good thing because people need to believe in AMD again. RDNA2 is simply not old enough to be relegated to a security branch driver.",hardware,2025-11-03 01:15:28,44
AMD,nmtdszy,"AMD is still releasing products brand new with RDNA2. They just dropped some mobile chips in the last month, and the iGPU is RDNA2 based.  Putting those products into maintenance or legacy branch for their drivers is horrible.   Sure, AMD says those products will receive game updates, but what will that support really look like, if those products are in the legacy branch?  And whatever they decide to do in the future will now be done quietly and announcements won't need to be made, as those products are now in the legacy branches.",hardware,2025-11-03 02:36:32,32
AMD,nmtb8om,"Boy, has that Fine Wine^TM gone sour...",hardware,2025-11-03 02:20:58,25
AMD,nmtbder,Now this is what I call moving the goal post,hardware,2025-11-03 02:21:46,11
AMD,nmsstfl,You can never prove AMD didn't give you a game optimization they could have if they cared more.,hardware,2025-11-03 00:29:30,4
AMD,nmt29ww,"The real read here is AMD are essentially winding down their efforts in graphics, in favour of chasing AI compute. Drop the investment into drivers and drop the dedicated graphics uArch for once again repackaging ~~GCN~~CDNA with a name change.",hardware,2025-11-03 01:26:47,10
AMD,nmtmpue,"How do you even know this to be true? How can anyone even validate this? AMD could release drivers where they just make sure the game doesn't crash, and that's that. If it' runs better on an RX 7600 than an RX 6650xt (which are usually close) anyone can just point to the different architectures and say ""That's why RDNA3 is better!"".   Like the RX 7600 is already for the last 2 years significantly better than the RX 6650xt in a maybe 2%-5% of games. One of them, a Call of Duty game, had RDNA3 absolutely destroy Nvidia in that one title, and had large gains over RDNA2, even if on average there was almost no difference if you looked at like a100 titles.   I feel like AMD can really just go ahead with the plan they had like a month ago, and most people won't now, because the difference that game optimizations make is maybe only a couple % some of the time.",hardware,2025-11-03 03:35:05,6
AMD,nmtumho,"At this point, I don't care about what they say. Let's just see what they do.",hardware,2025-11-03 04:30:44,3
AMD,nmt0kj5,"I mean - this is a more concrete statement walking back the dumpster fire of their initial press release. Glad that RDNA1/2 are still going to get the expected level of support. Whether or not it was a walk back or a miscommunication, it’s just another example of AMD completely screwing up when it comes to marketing.",hardware,2025-11-03 01:16:17,5
AMD,nmspb8f,My RX 6900 XT warranty expired less than a year ago. It would be very sad to see AMD abandoning its hardware so quickly.,hardware,2025-11-03 00:09:10,4
AMD,nmto352,"I remember when AMD washed their hands of the original TeraScale. And, to be fair, it made a certain kind of sense since TeraScale was an ATI creation and not a particularly good one (compared to Nvidia’s Tesla).  Even TeraScale 2 and 3 didn’t last long and were quickly abandoned around 2014 in favor of GCN, with the latest drivers still in beta.  Now it seems like they’re doing the same with the early iterations of RDNA, and again, it makes sense since RDNA1 in particular was rough around the edges and is practically obsolete when it comes to UE5 titles.",hardware,2025-11-03 03:44:25,1
AMD,nmsm3yh,"This is scary for AMD. The engineering culture there seems to favor apathetic long term support.  They walked it back *this* time because of the backlash, but clearly, the original plan was to cut and run. They've got to change this if they're looking to stay in the half trillion club.",hardware,2025-11-02 23:51:14,-10
AMD,nmsi5bg,"Seems to me like they walked it back after all the backlash. What do you guys think, was it this way from the beginning?",hardware,2025-11-02 23:29:25,-20
AMD,nmsi13u,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-02 23:28:47,0
AMD,nmslhxz,"No surprise there, only people who believed the original stories were the rage baiters.",hardware,2025-11-02 23:47:51,-18
AMD,nmtrhut,"Reading the statement, it possible they may have reversed course on this, but instead of making a strong, clear statement to counter the shitstorm, they wrap it in this wishy-washy ""clarification"" that easily gives the impression nothing has changed. Plus the usual corporate hogwash (""this bad thing you're upset about is actually a good thing"") waters it down further.   Even if they do in fact continue the support, the reputational damage is done. All the effort and none of the reward. It's almost unbelievable how badly they've handled this, but then again it's AMD.",hardware,2025-11-03 04:07:47,5
AMD,nmyoskl,"Yep. I'm very much in the, ""lets wait and see"" frame of mind at the moment. I DGAF about statements...  They have branched the drivers, now they have to back up this claim about RDNA 1/2 recieving game updates... if we see a big game released and not getting any optimisations within a few weeks then we know we were right to be angry about AMD stopping updates for parts they still sell...",hardware,2025-11-03 22:53:45,1
AMD,nmvefeu,"Exactly. It's misleading. This isn't clearing up confusion. It's saying that AMD hasn't walked anything back from their original announcement and has no plans to.  They also left out the ""market needs"" wording from their previous statement ""clarifying"" the tech notes. >  ""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch.""  We've already seen what happens to cards in maintenance mode and it's not becoming for these cards. E.g., a *gaming* GPU that was released for $1,000 3.5 years ago.",hardware,2025-11-03 12:58:02,0
AMD,nmsreme,"Which is fine, and normal.",hardware,2025-11-03 00:21:15,-52
AMD,nmswvqz,"> and still looks like they not receive the same level of support as RDNA3 & 4 going forward  Ofc they won't, it's because a redstone update is coming up for rdna3 and 4 soon and the features ain't gonna be supported by rdna1 and 2 on the hardware level. You'll see when it gets announced. It's funny that you guys are expecting fsr4 to be supported on rdna2",hardware,2025-11-03 00:53:45,-26
AMD,nmu3zu0,"I'm surprised that your post was even upvoted, and who are you to say otherwise and disprove AMD, stop spreading misinformation. This is conspiracy.",hardware,2025-11-03 05:48:59,-13
AMD,nmuamye,"it always has been but evidently people talk at random  https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html  ""Available exclusively on the AMD Radeon™ RX 9000 series graphics cards, AMD FSR 4 uses an AI-accelerated upscaling algorithm to deliver image quality improvements over AMD FSR 3.1.""",hardware,2025-11-03 06:51:59,0
AMD,nmtzrsz,">Sure, AMD says those products will receive game updates, but what will that support really look like, if those products are in the legacy branch?  We have products in the maintenance branch right now, we know exactly how AMDs ""support"" for them looked and looks like. It's certainly a lot closer to the original statement than the new, PR-weasel word-statement. Maintenance mode has always meant for AMD:  Zero new features, zero attempts to fix long standing bugs, and ""game optimization"" means: we might fix the odd, game breaking, crash bug, if the game is really, really popular (think Minecraft or Call of Duty levels).     Vega and Polaris have long standing blank screen issues with VRR and the driver release note recommendation to resolve this is to just turn it off.  Nothing has changed with this statement but wording.",hardware,2025-11-03 05:11:37,21
AMD,nmvnx4q,Man there is just no reason a product I bought brand new in store not even two years ago should be in the maintenance branch,hardware,2025-11-03 13:54:49,3
AMD,nmttxk4,It means you aren’t going to get FSR4 and other new features from restore most likely. Which at this point no one should have really expected,hardware,2025-11-03 04:25:31,1
AMD,nmv1ve0,"The problem with AMD is that their architectures are just the bare minimum in terms of featureset at the release date of their respective architectures  RDNA1 had no HW accelerated Raytracing, no machine learning accelerators, no DX12 Ultimate.  Turing (RTX 20 series) had these crucial features already back in 2018.  RDNA2 had HW RT and DX12U but it lacked machine learning acceleration. It's RT acceleration was far weaker than Turing.  RDNA3 improved the situation by adding ML accelerators, but they were still quite weak. Improved RT but still a bit weaker than Turing.  Only now with RDNA4 AMD is catching up. The RT acceleration overvall is now better than Turing, although it still doesn't accelerate BVH traversal in hardware, which RDNA5 will bring to the table, catching up to Nvidia's 2018 architecture. Machine learning accelerators are now useful.  RDNA5 will be a major milestone for AMD.",hardware,2025-11-03 11:26:25,2
AMD,nmtxxf4,They're literally moving MI-series to an iterative fork of RDNA,hardware,2025-11-03 04:56:22,7
AMD,nmu5gd1,"Literally the company is exposing itself and you think the opposite, this is conspiracy theory and honestly behaviors like yours should be banned since they create false alarmism.",hardware,2025-11-03 06:02:35,1
AMD,nmsr4ps,"Nothing is being abandoned, jesus.   Nothing AMD are doing here is any different from Nvidia.",hardware,2025-11-03 00:19:39,-2
AMD,nmu8353,"Nothing is being abandoned, do you believe random guys in a sub where you root for a brand(green) or do you believe the company itself? Many people here seem to forget that fsr4 always has been 9000 series exclusive... if it arrives on the older ones it has always been something extra and unexpected",hardware,2025-11-03 06:27:17,1
AMD,nmu6ndv,The 360 shook Nvidia to the point that they spent the following year gaslighting us into thinking unified shaders were overrated so you have to give Terrascale more credit.   Literally MFW when the 8800GTX released 😳,hardware,2025-11-03 06:13:47,0
AMD,nmsnvjl,its most likely what they still plan... its just easier to release a statement tell people what they want to here then stop doing it anyway. Which if I remember correctly in there last few release notes there already doing.   They already stopped offering optimization for day 1 games for RDNA1 & 2 just when they said that out loud everyone freaked. They will just go back to quietly putting it in the release notes.,hardware,2025-11-03 00:00:56,6
AMD,nmsqmu1,"Nothing was walked back.  They just underestimated how reactionary PC gamers are and so needed to spell things out more.  All they ever said was about Day 1 game-specific optimizations.  That's it.  It was never a big deal, and not any different to how Nvidia do things.",hardware,2025-11-03 00:16:47,3
AMD,nmsx0ip,You say this but amd is also known for fine wine development,hardware,2025-11-03 00:54:33,-2
AMD,nmspldv,"It’s not walking back, but rephrasing. They do not mention Day 1 support or whether projects Redstone will ever be ported to RDNA2",hardware,2025-11-03 00:10:44,22
AMD,nmsjbyt,I don't think anything changed.  They just communicated things really badly before.,hardware,2025-11-02 23:35:59,33
AMD,nmsqqz2,The drama came from of AMD's poor communication on what putting them into a separate sub-branch actually meant. This is on them as much as anyone else. A company this large has huge marketing/PR teams whose entire purpose is clear communication with the public... and they've been found lacking on too many occasions.,hardware,2025-11-03 00:17:27,1
AMD,nmtmyfy,"It was actually like 20 years ago. Back then sometimes you could not play new games on 3-4 year old GPUs, because they didn't support the features they were coming up with every generation or two. These days people have gotten used to 8 years of driver, and game support.",hardware,2025-11-03 03:36:44,17
AMD,nmtyab1,Nvidia still releases *new features* for their 2000 series while AMD is basically retiring 6000 series which are one gen newer.,hardware,2025-11-03 04:59:17,6
AMD,nmsul3j,Yea not expecting this is the norm is weird for som ewho didnt get the memo. Its the equavalent of wanting framegen of rtx 40 series to gtx 10 series because its not officially EOL yet.,hardware,2025-11-03 00:39:57,-37
AMD,nmt8x8s,how is it funny when the leaked version was already proven to be working and looking just fine on old cards? what's actually funny and really pathetic is to suck corporate d1ck online,hardware,2025-11-03 02:07:11,39
AMD,nmvvc9c,Wait for the <next generation> is the Radeon fan battlecry lol. The thing is they are not even that much cheaper with their second class software and features. They want to lose.,hardware,2025-11-03 14:34:56,2
AMD,nmsvb38,"No. The latest Nvidia Game Ready Driver is optimized back to Maxwell (GTX 980, etc). RDNA 2 is now on a similar branch to Nvidia Kepler (GTX 680, GTX 780) but those cards released 12 to 13 years ago, not one to five years ago like RDNA 2.",hardware,2025-11-03 00:44:19,31
AMD,nmtdkg7,"nvidia brought dlss 4 to rtx 2000 series. amd doesn't plan to support rdna2 with fsr4 even though the leaked version looks and works just fine. so yes, rdna1/2 is getting abandoned, forever",hardware,2025-11-03 02:35:04,20
AMD,nmsuvmz,I misspoke. I understand it's not going to happen. I'd be surprised if it did.,hardware,2025-11-03 00:41:43,1
AMD,nmstixp,"So many techtubers who should know better have been saying exactly what he said.  All of them are primed to farm outrage for clicks.  Even ""AMD Unboxed"".",hardware,2025-11-03 00:33:37,-10
AMD,nmsur8z,Lumping RDNA 1 and RDNA 2 together is AMDs biggest mistake. RDNA 2 was a huge leap in terms of features and is absolutely a solid architecture to last the remaining console generation.   The support cycle shouldn't simply be based on the number of years. Axing RDNA 1 is fine even though Turing is still supported because it's fallen really behind in features. That can happen when your architecture wasn't as future proof as the competitor.  But RDNA 2 suffers no such thing and there shouldn't be any talk of it getting left behind.,hardware,2025-11-03 00:40:58,5
AMD,nmsqzm5,They've already said that feature support can still exist for older GPU's.   The ONLY thing they've talked about stopping is game-specific DAY 1 optimizations.  That's it.  People freaked out over basically nothing.,hardware,2025-11-03 00:18:50,-10
AMD,nmskdia,">They just communicated things really badly before.     They tend to do that a lot, unfortunately.",hardware,2025-11-02 23:41:46,18
AMD,nmsl8ah,">RDNA 1 and RDNA 2 graphics cards will **continue to receive driver updates for critical security fixes and bug corrections**. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards **(RDNA 1 and RDNA 2)** **into maintenance mode**. Future driver updates with **specific game optimizations will focus on RDNA 3 and RDNA 4** GPUs.  [this was their direct response to inquiry from a media outlet.](https://www.pcgameshardware.de/AMD-Radeon-Grafikkarte-255597/News/RDNA-1-und-2-Finaler-Radeon-Treiber-fuer-Battlefield-6-und-Bloodlines-2-1485427/) don't think incompetence or miscommunication can explain a total 180 like this.",hardware,2025-11-02 23:46:22,17
AMD,nmsl1k6,"Basically, the same stuff then, just ""properly"" worded this time? Feels to me tey aren't stepping back, just gaslighting people then...",hardware,2025-11-02 23:45:22,9
AMD,nmsscff,"Oh please, how hard is it to simple just wait and clarify this information first, especially when these ‘rumours’ centred on hardware this is still current, before people started making their rage bait videos and posts? 🙄  I looked at the information and could clearly see this had to be a mistake.",hardware,2025-11-03 00:26:45,-12
AMD,nmtrioh,"Yeah, you'd end up behind a directx version pretty quickly, and it was even worse before that.",hardware,2025-11-03 04:07:57,4
AMD,nmu5qv7,The fact that we have a leaked version of FSR4 Redstone that's  looking and running surprisingly well on RDNA2 and AMD has decided to never release it should tell you everything you need to know.,hardware,2025-11-03 06:05:20,6
AMD,nmu6cmm,"FSR4 was literally 9000 exclusive, RDNA1/2 miss AI cores  https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html  ""Available exclusively on the AMD Radeon™ RX 9000 series graphics cards, AMD FSR 4 uses an AI-accelerated upscaling algorithm to deliver image quality improvements over AMD FSR 3.1.""  if it comes for older cards it was just a bonus",hardware,2025-11-03 06:11:01,-2
AMD,nmsxsuc,"It absolutely isn't the same.   RTX 40 series came out in 2022, when AMD had just prior released refreshed models with RDNA2 silicon, i.e. the RX 6X50 XT series.   One of those is getting first class support, while one is already considered legacy. I'll give you better: RTX 30 also is first class.   People who bought AMD products rightfully expect good driver support, especially for graphics cards which are not even 4 years old.   Meanwhile, Nvidia is surpassing 5 years of ongoing Game Ready Driver support for its products. Please tell me what ""memo"" didn't I get?   Nobody is asking for backported features. What are you going on about?",hardware,2025-11-03 00:59:17,45
AMD,nmuyd7z,"The leaked INT8 model is not really FSR4, it's some development version, and for all we know it was abandoned sometime last year. We don't know if RDNA3/3.5 version of FSR4 will be this INT8 model or something totally different (like some lighter model running on FP16). And on top of that: No, INT8 model was not just fine on RDNA2, it's performance is terrible and it has some major graphical glitches like light accumulation in Cyberpunk.",hardware,2025-11-03 10:54:49,1
AMD,nmukfln,Source: my ass,hardware,2025-11-03 08:32:06,3
AMD,nmu4j8q,Didn't you happen to notice that the RTX 2000 series has AI cores? While RDNA1/2 doesn't? FSR4 was a 9000 series exclusive initially,hardware,2025-11-03 05:53:59,-1
AMD,nmsx5lo,They lumped them based off fp8 support,hardware,2025-11-03 00:55:24,0
AMD,nmsu8np,"People aren’t complaining over nothing. Gamers want the day 1 patch for the biggest game to improve performance for their GPU. AMD is saying they will make security fixes, but when they do day 1 patches for the next DooM, Battlefield, CoD or FromSoft game, it will be for only RDNA 3 & 4 GPUs. As the owner of a 6800 XT, I am fine with the support my card got but would always appreciate more. But on the mobile side there are RDNA 2 GPUs still being sold new today, and that is disappointing.",hardware,2025-11-03 00:37:53,8
AMD,nmud18y,When did I ever talk about FSR4?,hardware,2025-11-03 07:15:33,3
AMD,nmtylpq,Even much older 2000 series got the new DLSS model.,hardware,2025-11-03 05:01:56,7
AMD,nmt0qmb,"For note, Nvidia also literally ""just"" dropped first-class support for Maxwell, Pascal, and Volta last month (the October 14 Game Ready Driver will be their last).  EDIT: Apparently getting downvoted for [stating a fact](https://videocardz.com/newz/nvidia-game-ready-driver-support-for-maxwell-pascal-and-volta-gpus-till-october-2025). They're shifting to quarterly security updates until October 2028 with them, there will be no more GRDs.  EDIT 2: For clarification since some people here apparently have no contextual awareness - I was backing up the previous poster. My point is that Nvidia's history of ongoing driver support is much longer than the ""over 5 years of the RTX 3000 series"" mentioned by them. It was over 11.5 years for Maxwell, and around 8 years for Volta.  I didn't even factor AMD into the statement, but doing so would paint them in a very poor light in this case, as they have moved cards they are ""currently still selling"" into a slower driver release branch.",hardware,2025-11-03 01:17:19,2
AMD,nmsz408,>Good driver support  This term is really flimsy. Some draw the line at long security and stability updates and hotfixes to breaking problems. Some dont. But I really dont bellieve both red and green team software testing and tweaking old gpus when they run fine on stable api stack. Like polaris rx580 on BF6? That dont have support for a while. But it runs normally because these companies dont actually have to?  As a rtx 30 User I can say this. Game Ready Drivers are buggy as hell and caused me several times to revert to get back lost performance downlocking my laptop.  https://www.reddit.com/r/pcmasterrace/comments/1namedi/the_new_nvidia_581xx_drivers_are_causing_massive/  Heres a thread for the curious.,hardware,2025-11-03 01:07:19,-16
AMD,nmt105l,"> One of those is getting first class support, while one is already considered legacy. I'll give you better: RTX 30 also is first class  5 of the last 6 gameready drivers are focused on dlss4 support. In practice most optimizations are done on a feature support basis. The only reason ampere feels like it's getting priority support is because it's more ml capable and shares most of the latest feature set. That ain't the same for rdna1 and 2, even 3 is barely ml capable and is barely making it  Rather than blaming the driver you should be blaming the architectures for not bein more ml ready, but that's something ya already know when buying rdna older than 4. There's barely any significant optimizations to be had for a mature architecture like rdna2, you ain't gettin magic drivers that are gonna boost perf by 30%. All you're getting are drivers that fix critical problems like game launch failures, people just don't like to hear the truth because it makes them feel like ""second class"" when that has always been the case in practice  The only difference is that nvidia ain't coming out to tell you about it. Amd did a stupid by making an announcement out of it. There would be no anger if they announced that 3 and 4 are moving onto a fsr4 capable experimental branch instead. As usual, amd bad at pr.",hardware,2025-11-03 01:18:56,-16
AMD,nmub2dh,Didn’t you notice that there is leaked version of FSR4 that does work on RDNA2 cards Mr AMD’s Damage Control Department?,hardware,2025-11-03 06:56:06,2
AMD,nmtb8wu,"Which is bullshit, FP8 is all that matters, seriously? Not the fact that RDNA2 added DX12 Ultimate, or INT8 support which means RDNA2 can run the FSR4 INT8 verison and does so viably enough? Nvidia is still supporting GTX 16 series as of now and those don't have tensor cores! They're Turing with all that ripped out and yet Nvidia is keeping them for now.",hardware,2025-11-03 02:21:00,3
AMD,nmudgju,"If you were talking about new features, you were most likely talking about DLSS, right? So the AMD equivalent is FSR... except that here we run into some hardware limitations.",hardware,2025-11-03 07:19:46,0
AMD,nmt9g9e,"you got downvoted bc at first glance it sounds like you're justifying amd, when these nvidia cards are actually old as fuck",hardware,2025-11-03 02:10:22,15
AMD,nmt1207,I had a GTX 980 Ti around the time Metal Gear Solid V: The Phantom Pain was released. Fun times.   It's been... over 10 years since. Fuck.,hardware,2025-11-03 01:19:15,10
AMD,nmtap9x,You know even RTX 20 series can use DLSS4 support right? This isn't the point you think you're making when a RTX 2070 can flip on DLSS4 upscaling and even ray reconstruction. Thus DLSS4 updates to game does benefit older Nvidia GPUs  Meanwhile RDNA2 will be left without FSR4 INT8 despite the leaked one proving the architecture can do it.,hardware,2025-11-03 02:17:43,13
AMD,nmuc65z,"Which isn't exactly the same thing, though. That doesn't mean anything, since it uses different instructions to function and has worse performance. Do you know how many things are made in this field and never released because they don't meet certain standards? It seems like people just got into PCs or gaming yesterday...",hardware,2025-11-03 07:06:57,-1
AMD,nmtu9u1,"fsr4 int8 has worse performance than fsr3.1  There is a reason they didn't release it, the product isn't good enough.   as for  ""Nvidia is still supporting GTX 16 series as of now and those don't have tensor cores!""  What support have they provided that wasn't something that 2000 series also had?",hardware,2025-11-03 04:28:04,1
AMD,nmuwj5a,"The point isn't FSR vs DLSS or why FSR isn't available on 6000 and 7000 series.  The point is that after 7 years not only Nvidia is still fully supporting 2000 cards, they are even releasing new features for them.  Meanwhile AMD is considering their 4 and 5 year old cards ""legacy"".",hardware,2025-11-03 10:37:38,3
AMD,nmuy10z,"I don't think so, I don't agree. A lot of misinformation has been created about this. And in fact, now AMD confirms that it's not like that. Furthermore, these cards are supported just as much as their Nvidia counterparts, don't believe the opposite. The cards that have reached driver maturity no longer need constant updates; shifting the focus to RDNA3 and RDNA4 does not mean abandoning the cards. Believing something like that means acting in bad faith. I also want to point out that even with Nvidia, where cards did not have sufficient hardware capabilities, their features didn't arrive. Where did frame generation for the 3000 series end up, in fact? What's happening now is hysteria and a lot of YouTubers taking advantage of it, a sad situation. ​That said, there's no doubt that AMD used the wrong words initially",hardware,2025-11-03 10:51:42,1
AMD,nm8tdqk,I get putting RDNA1 support on the back burner but it seems crazy to do the same for RDNA2.,hardware,2025-10-30 19:18:45,245
AMD,nm8xnml,"I didn't think RDNA3 was all that different from RDNA2. The dual issue compute thing hardly seems like it gets any use.   Given AMD's move to UDNA in the future, I'm kind of wondering how long support for even RDNA3 and RDNA4 will last now.",hardware,2025-10-30 19:39:28,78
AMD,nm8x7fb,Cards like the 6800xt are still non-RT monsters. In this economy this is a great way to piss everyone off.,hardware,2025-10-30 19:37:17,161
AMD,nm9qdqz,"Hmmm, what happens to the support for the iGPU, the integrated RDNA2 GPU that is in (almost) all Zen 4 and Zen 5 CPU models?  Edit: Since this has now gotten to be a real topic, rightfully so, here's a list of all the other non-Desktop / Zen 4 + Zen 5, Ryzen CPUs that also have RDNA-2 iGPUs:   Ryzen 7 170, 160 - previously named Ryzen 7 7735HS, 7735U   Ryzen 7 150, 130 - previsouly named Ryzen 5 7535HS, 7435U   Ryzen 3 110 - previsouly named Ryzen 3 7335U   These all are Rembrandt-R Series with Zen 3+ CPUs and RDNA 2 GPUs.",hardware,2025-10-30 22:03:19,21
AMD,nmag8cm,"RDNA1 I understand, horribly outdated architecture. But RDNA2? It is the same architecture used in the consoles...",hardware,2025-10-31 00:31:43,14
AMD,nm9zlby,Wine was so fine that they decided to just take it away,hardware,2025-10-30 22:56:14,33
AMD,nmb0hds,AMD sure loves burning consumer goodwill. I wonder if they think the time and money saved from doing this is worth the hit to their driver support reputation,hardware,2025-10-31 02:32:07,19
AMD,nm9i995,They are hurting the good will they gained from the 9070xt launch. Burning RDNA 1 and 2 owners like this is going to motivate them to buy team green when they upgrade. Speaking as someone who has a 5700xt since 2019,hardware,2025-10-30 21:20:26,30
AMD,nm9v2el,This has to be a joke.  Imagine the 3080 not getting day 1 support. Also I'm pretty sure RDNA2 is the best selling AMD generation ever.,hardware,2025-10-30 22:30:24,30
AMD,nmb14d2,Even though RX 6600 is one of the very few RDNA cards that make the Steam survey list...?,hardware,2025-10-31 02:35:58,7
AMD,nm8qyrg,The RTX 20 series became fantastic purchases over time.  One of the greatest architectures ever made.,hardware,2025-10-30 19:07:10,87
AMD,nm9j501,"So I got a bonus at the right time and was able to secure a rx 6900xt.  At what point do I look at the support for the card and say that it's time to update? It's been a perfect card to this point. If it helps, I'm running Linux exclusively.",hardware,2025-10-30 21:24:59,12
AMD,nmaczui,"At this rate, 25.11.1 will drop support for RDNA4 and AMD will only support unreleased cards in their drivers. Get it together AMD, you don't have the cashflow excuse any more.",hardware,2025-10-31 00:12:23,17
AMD,nm8uoxx,"Imagine if Nvidia did this for RTX 30 and RTX 20, reddit would throw a tantrum. Guarantee you'll barely hear a peep about this though.",hardware,2025-10-30 19:25:06,83
AMD,nmbvg5w,"RDNA1 is borderline understandable, but RDNA2 is fully DX12 Ultimate compliant, this is a massive kick in the teeth to anyone with an RX6000 series card.",hardware,2025-10-31 06:44:50,11
AMD,nma8i2w,"A buddy of mine owns a tech shop and the distributors he works with still have new old stock of 6800XTs and 6600XTs. Granted, the market is slower here but still.   Im kinda bummed because i run one of those 6800XTs. Brand new red devil card bought for 350 euros a year ago.",hardware,2025-10-30 23:46:12,6
AMD,nm9sphs,Here am I currently just diciding between 9070XT and 5070Ti for my upgrade. I really don't like the 12VHPWR.,hardware,2025-10-30 22:16:50,9
AMD,nm9ulb7,Weaker software and support is why I am not going back to AMD for now. With NVidia you just get more valued added. My last AMD card was in fact an Ati HD 5850 in 2010. I would love to be back on the red team but I haven't got excited for a Radeon card ever since.,hardware,2025-10-30 22:27:43,11
AMD,nmazude,AMD stays solidifying my choice as to why I only buy Nvidia GPUs,hardware,2025-10-31 02:28:16,7
AMD,nmb27rl,But amd is our friend,hardware,2025-10-31 02:42:45,8
AMD,nm8tgq9,"Bye Bye AMD, never buying from you again. This is pure scam.",hardware,2025-10-30 19:19:09,24
AMD,nm8owtd,"Hello pi314156! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-30 18:57:21,2
AMD,nmbs4it,"Muy first graphic card as a teenager was an ATI Radeon 9600 pro, when I buy graphic cards i dont even look at Nvidia. This shitty move is making me question my loyalty to the brand. They launch a fucking great card in the form of the 9070XT and now they do shit like that. Congrats to Nvidia you dont have competition anymore",hardware,2025-10-31 06:11:01,2
AMD,nmcpsxf,"As I noted they're moving the driver to the legacy branch. This has been confirmed by AMD.  If they do the same thing they did with all past legacy releases, which there's no reason to think not since they used the exact same language and response as last time with, then simply put all driver development is over.  The messaging is extremely clear. When they say the exact same thing as last time and use all identical terminology there's no reason to believe it's something completely different.   This is entirely different from Nvidia. Nvidia continued to extensively support Maxwell and Pascal the entire time. All Nvidia cards got Vulkan 1.4 on day 1 of it's release, and they continued adding more and more extensions this entire year for all of their cards.  See my other posts I made on the topic for more details.",hardware,2025-10-31 11:32:03,2
AMD,nmb12yn,Bought my 6800 like two years ago. I do not like that but I will definitely deny to upgrade in a two years cycle. I will move abroad in a couple of months and cannot take my battlestation with me but even if I would not move these cards a fucking expensive nowadays. There is no way that I change my GPU every two years.,hardware,2025-10-31 02:35:43,4
AMD,nmapbtf,linux drivers are fine. they dont get day1 game optimizations but mainstream kernel support should last for at least another 2 decades.,hardware,2025-10-31 01:25:03,2
AMD,nm8tmgj,Isn't ps5 rdna1 and series x rdna 2? Why drop the gpus that are based on these archs when consoles chips are your highest volume sales.,hardware,2025-10-30 19:19:55,6
AMD,nm8v8x3,What does this mean with the upcoming FSR 4 lite Int4 version that AMD is developing though? Will the official release of it still support RDNA 2 despite it being moved to legacy branch of drivers?,hardware,2025-10-30 19:27:46,5
AMD,nmbm1k0,Hire some software and embedded devs I swear to god. I may be extra pissed because Vitis AI (Xilinx) is a hot mess that feels like it was put together by 5 people.,hardware,2025-10-31 05:12:25,1
AMD,nmc2t67,"i am not too worried. i regularaly used old radeon cards, usually all games still work just fine     still not the best move, rdna 2 is stilli integrated in plenty of products. but i did not read the article, maybe they at least take care of this",hardware,2025-10-31 08:01:33,1
AMD,nmc4j2g,I guess I should be thankful having DLSS4 on my 2080Ti.,hardware,2025-10-31 08:19:38,1
AMD,nmcqs22,I bought RX 6700 this year March... Was about same price as 8 Gb 7600...,hardware,2025-10-31 11:39:13,1
AMD,nmb7jh5,"So...does this mean we won't be getting drivers at all (like they did with the HD 5000 series in 2015) or that we'll get drivers, games just won't be optimized for those gpus? Put another way, if a game requires a certain new driver, am I ####ed?",hardware,2025-10-31 03:17:33,1
AMD,nmbbtn7,As someone who has not cared about updating to day 1 drivers before buying a game for ever really...   How much of an issue is this nowadays? I haven't bought many new cutting edge games so anyone who buys a lot of them can chime in?,hardware,2025-10-31 03:48:29,1
AMD,nmcgy5z,this sub is flooded with idiots who can't even read,hardware,2025-10-31 10:21:00,1
AMD,nm9bz8d,Imagine buying an AMD GPU in 2025.,hardware,2025-10-30 20:48:59,-9
AMD,nmc2b04,#Where is the down vite fest?,hardware,2025-10-31 07:56:17,0
AMD,nm8v743,I can only guess is that RDNA 1/2 lacks AI accelerators and in a bid to catch up to nvidia in terms of software tech they are focusing on the 7000/9000 series.,hardware,2025-10-30 19:27:32,-16
AMD,nm9upp6,"It's not their fault that they want to focus since they're finally at a position to surpass Nvidia in some regards. AMD has served Fine Wine for years, give them a pass here guys.",hardware,2025-10-30 22:28:24,-25
AMD,nm93y8r,"> crazy to do the same for RDNA2.  Ye, especially since they are still floating around in the channel.   I don't know when AMD stopped manufacturing them, but they sure as hell had stockpiles of them that kept showing up until like 2023/24. It sure as hell is not that long enough ago that it warrants dropping day 1 support.",hardware,2025-10-30 20:10:09,118
AMD,nmbq890,"They always do this,  they did the same thing with Vega. AMDs GPU support has always been short specially when compared to Nvidia. Get ready for RDNA3 to get dropped sooner than you'd hope as well.",hardware,2025-10-31 05:51:58,7
AMD,nmaeq17,"RDNA3 was a big architectural redesign, regardless of it being very disappointing.    And this just seems like AMD is moving to match Nvidia's driver support policy.  Basically, only the latest two generation architectures get Day 1 performance driver support.",hardware,2025-10-31 00:22:41,-26
AMD,nm92lt5,RDNA3 has “AI Matrix Cores” that RDNA2 doesn’t. RDNA3 also was “Architectured to exceed 3.0Ghz”,hardware,2025-10-30 20:03:34,-20
AMD,nm9oyhz,"For people with a 6800/6900/6950 XT, AMD's current best offering only provides a 30-40% uplift. Many people don't want to upgrade until they can get at least 50% or even 100% better performance. If people want that right now, they have no choice but to buy Nvidia, especially with the Super series right around the corner.",hardware,2025-10-30 21:55:20,56
AMD,nm9s5nj,how important are day 1 game optimizations though? we're talking couple %s of performance improvement iirc.,hardware,2025-10-30 22:13:37,10
AMD,nm9h5fj,"Yep, I was buying AMD for a decade because they are the best bang for your buck in my country, but with that attitude to the customers - thanks, my next GPU will be Nvidia.",hardware,2025-10-30 21:14:45,9
AMD,nm9y7v1,"I owned 6800XT and it was pretty good in RT too, though of course not as good as 3090 which I also had at the same time. The lack of DLSS equivalent was a bigger issue.  The path-tracing updates that nvidia pushed out to some games ran abysmally on it. But it was doing okayish with previous high level RT in games like Dying Light 2 and Cyberpunk.    https://old.reddit.com/r/hardware/comments/1n31z8b/doom_path_tracing_and_bechmarks/nbn4xyj/",hardware,2025-10-30 22:48:29,0
AMD,nmaaqhc,"Honestly it doesn't matter, most people won't notice and the cards are still going to run fine for a long time, even without driver updates.",hardware,2025-10-30 23:59:07,0
AMD,nm9vudz,Do iGPUs even get day 1 optimizations for games in the first place?,hardware,2025-10-30 22:34:48,19
AMD,nm9swje,That's RDNA3 and RDNA3.5  RDNA2 iGPU are in Steam Deck / Zen3+,hardware,2025-10-30 22:18:01,5
AMD,nmcqmg6,That'll still work fine for Excel and YouTube. No games optimization won't matter,hardware,2025-10-31 11:38:05,2
AMD,nmo6hxt,"The Z2 A (Xbox Ally) also uses RDNA2... And that particular handheld launched last month.  I guess the Legion Go S also uses an APU with RDNA2 (Z2 go), but as that runs SteamOS, it isn't as much of a problem.",hardware,2025-11-02 07:59:32,2
AMD,nmcelkk,Consoles are a whole different thing.  They have their own platform-specific low level API's.  AMD is not building 'Day 1 game optimization' drivers for consoles. lol,hardware,2025-10-31 10:00:11,3
AMD,nmb6rn9,">good will they gained from 9070xt launch   Damn, so older card users have already forgotten about fsr4 they didn't get?",hardware,2025-10-31 03:12:16,13
AMD,nmbkl68,"Meanwhile I've been straight up frustrated at their BS ""MSRP"" lies and horrible prices for most of this year.",hardware,2025-10-31 04:59:18,7
AMD,nmas8y6,Yep 6950xt was a 1100 card and released less then 3 years ago dropping day 1 support already is crazy,hardware,2025-10-31 01:42:33,12
AMD,nm8uz9x,Yeah it's pretty crazy if you compare RTX 20 to RDNA1 seeing how much better Nvidia's architecture aged. It set the stage for where graphics tech would be heading in the next decade and AMD has been playing catchup the entire time.,hardware,2025-10-30 19:26:28,63
AMD,nm8xgtl,"RTX 20 series was so hated back then to the point Reddit even with obvious driver issues of 5700 XT that I get a lot of people who are gaslighting me by saying, ""*I never had issue with mine!*"" ""*Only few people are having issues!*"" similar to what you see under the comments of a unoptimized game today with ""*Mine runs well with no stutters!*""  they were looking past it and was still recommending the 5700 XT over the 2070 Super back then. I know because i was in the market of getting both of those GPUs back then but decided to wait for RTX 3070 instead.  And I am so glad I did, if I didn't and listened to Reddit back then I would have gotten myself the 5700 XT because it was cheaper than the 2070 Super and offered nearly the same raster performance, DLSS wasn't part of the consideration yet for me because back then it was really shitty, I wasn't even aware of DX12 Ultimate API, so i didn't really care if 5700 XT didn't support, which I thought was another Nvidia Gimmick.  Turned out all those ""Gimmicks"" that Reddit and popular Tech Youtubers told me were real enough and is now an important part of video game optimizations moving forward ever since 2023, as clearly shown by games like Alan Wake 2.",hardware,2025-10-30 19:38:33,60
AMD,nmcfuso,Nvidia hasn't been optimizing drivers in the latest games for Turing for a long while now.,hardware,2025-10-31 10:11:21,-4
AMD,nm9jzh5,The Vulkan drivers for Linux are developed independently with lots of the code written by outside parties instead of AMD. Wouldn't expect this to have an impact there.,hardware,2025-10-30 21:29:18,23
AMD,nm9phfu,"Linux AMD drivers are open source and maintained by the community, it regularly gets fixes from contributors, Valve, etc.  For some perspective, open source AMD drivers have Vulkan 1.3 support for AMD cards from 2013.  Your hardware will likely naturally die from old age before Linux drivers stop supporting the 6000 series.",hardware,2025-10-30 21:58:15,38
AMD,nmaq4yi,linux support for vga drivers lasts decades. i think the latest kernel still has code to support ati gpus from like 2005.  https://docs.fedoraproject.org/en-US/fedora/latest/release-notes/hardware_overview/#hardware_overview-graphics-legacy_gpus,hardware,2025-10-31 01:29:55,5
AMD,nmcfs5s,You will likely never notice anything at all.    Games will continue to run just as they would.  You just might miss out on certain Day 1 optimizations for the newest games that help boost performance a little bit(and often only in certain situations).    You will continue to get access to general driver support.  Though I'm not super informed about Linux vs Windows necessarily.  Just speaking generally.,hardware,2025-10-31 10:10:43,3
AMD,nmcfjs6,"Y'all are (basically all, universally) confusing 'driver support' altogether with \*specific\* Day 1 game optimizations.  You'll still have 'driver support' in as far as having access to the latest drivers and other things that might be in them.",hardware,2025-10-31 10:08:39,-2
AMD,nm8v7ma,People already did that when they dropped 10 YEARS OLD Pascal support,hardware,2025-10-30 19:27:36,86
AMD,nm8wvnx,"You're literally responding to an article, on reddit, criticizing them",hardware,2025-10-30 19:35:43,40
AMD,nm8yz5n,That's because reddit has nvidia gpus. How many here have amd gpus and how many of those are 5000 series owners?   That's not exactly a lot of people making noise vs popular nvidias older generations. Might be even why amd did this.,hardware,2025-10-30 19:45:55,17
AMD,nm8uxym,Their fan base might as well be a cult. They got whole YouTube channels dedicated to this kind of nonsense.,hardware,2025-10-30 19:26:17,20
AMD,nmcgfob,"This has been common practice from Nvidia for ages.  lol   Nvidia is not putting effort into optimizing drivers specifically for older GPU's.  They pretty much always focus on their latest two generations.    Doesn't mean there wont still be occasional like 'feature support' things that still get updated, but we're talking specific Day 1 game optimizations.  Which usually contribute a very small boost to performance for certain games and not a whole lot else.",hardware,2025-10-31 10:16:27,2
AMD,nmab4vm,">Imagine if Nvidia did this for RTX 30 and RTX 20  They already have. just unannounced.   Edit: it seems people are fundamentally misunderstanding where going on here. AMD is NOT dropping support for 5 and 6000 series, they are just no longer including them in day 1 game driver releases. That's all.   Nvidia might still include older cards in those, but they didn't invest any resources into optimizing then for the game either, so it's pretty pointless.   Literally nothing has changed, except AMD made it clearer that you can skip day 1 drivers when you have a older card.",hardware,2025-10-31 00:01:27,-16
AMD,nmg4olh,"And?  Nvidia didn't stop supporting Maxwell till just now 2025, that's 11 years of support regardless of DX12U compatibility.",hardware,2025-10-31 22:32:26,1
AMD,nm9zdd9,It's not really an issue with 5070ti tbh. Just be sure it's fully in and forget about it.,hardware,2025-10-30 22:55:01,14
AMD,nma0r03,Resale value is really bad too for AMD GPUs.,hardware,2025-10-30 23:02:42,11
AMD,nmcgsgd,This has been Nvidia's standard practice forever. lol  Nvidia is not building their Day 1 optimizations for anything but their latest two architectures.  Just cuz you still get access to the newest drivers doesn't mean you're benefiting from them with these optimizations.  My 1070 still has access to the latest drivers.  You really think Nvidia is putting effort into optimizing the newest games for Pascal GPU's? lol,hardware,2025-10-31 10:19:37,0
AMD,nmbr1yb,"Yeah, as someone who bought a 3090 back in late 2020 and still happily using it, it is insane to think that the AMD equivalent from back then (6900XT) is moving to legacy driver support.",hardware,2025-10-31 06:00:13,14
AMD,nm908ut,"Yep, artificial obsolete at its finest.",hardware,2025-10-30 19:52:06,41
AMD,nm8ywui,"If you know how to read, RDNA1 & 2 are still supported and will get future drivers.  Nvidia has really gaslighted people into believing that they need bi-weekly ""game-ready-drivers"".    You don't. The majority of the time they make no differences, usually they fix some bugs, and have been validated on the latest hyped AAA game release.   The included performance optimizations are often very insignificant, if they exist at all.",hardware,2025-10-30 19:45:35,2
AMD,nm8ukiw,"chill, lmao. overreacting drama queen",hardware,2025-10-30 19:24:30,-6
AMD,nmcg622,Folks holy shit.  You are not losing access to the newest drivers.  You will still have that for years to come.   It's only the (usually fairly small) boosts for Day 1 game specific optimizations that will be limited to newer architectures.,hardware,2025-10-31 10:14:06,0
AMD,nm8wyv0,Consoles have their own driver and software stack. So they are not effected.  What should be worrying is the steamdeck and the rdna2 based APUs,hardware,2025-10-30 19:36:09,44
AMD,nm8u1b5,"Both are rdna2.   And console games are always optimized for the hardware so this doesn't impact them. And Steamdeck is Linux which is probably exempt from this. But the z2a chip in the low end Xbox ally x is rdna2 which is a brand new product, so they're still selling rdna2.",hardware,2025-10-30 19:21:54,22
AMD,nm8vig5,AMD never stated they are working on int4 FSR 4,hardware,2025-10-30 19:29:02,30
AMD,nmbd1wy,"No, AMD are freezing all driver development for RDNA 1 and 2. Literally nothing will be changed any more, so expect no new features, and new games to rarely to occasionally not work/crash.  It's been confirmed they're moving these GPUs to the legacy branch, and this is a literal branching of the development tree off, making any backporting of code much more difficult.l, and I highly suspect there will be none.  Legacy branch GPUs only get security exploit fixes, and the very rare crash fixes for extremely basic use cases.   For example the legacy Vega branch aside from security fixes got a few basic fixes to browser video decoding crashes, a Microsoft teams crash fix, I think a Photoshop crash fix and the one game they did a fix for was minecraft, which was a crash fix. That's it.  Alternatively AMD GPUs are still more or less fully supported all the way back to GCN 1.0 on Linux. It's better than nothing.",hardware,2025-10-31 03:57:34,4
AMD,nmcizww,"Feature support like this can still be added.  They are talking purely about Day 1 game optimization stuff that comes with *specific game releases*.  Things that usually dont make a big difference anyways.  Everybody is fearmongering over this like crazy, not understanding a pretty basic and normal situation.",hardware,2025-10-31 10:38:37,1
AMD,nmcheb4,This is purely about Day 1 game optimizations.  It's basically all there in the article.,hardware,2025-10-31 10:24:52,3
AMD,nmcjt4m,"These optimizations usually only boost performance by a small amount, and with 'it depends' sorts of conditions based on settings and whatnot.    It's really not a big deal, but pretty much nobody here is understanding the situation.  They think AMD is cutting off general driver support, or that Nvidia has been doing anything different for the past 15+ years. lol",hardware,2025-10-31 10:45:12,5
AMD,nmadlxw,Much easier than trying to imagine being an Nvidia customer in 2025. I'm not sure how you sleep at night knowing what your dollars go towards. Objectively more anti-consumer.,hardware,2025-10-31 00:16:02,-5
AMD,nm8ve12,That has nothing to do with not providing game ready driver,hardware,2025-10-30 19:28:27,28
AMD,nma05nu,How are they fine wine when their older gen cards didn't get any new cool features like Nvidia and they just fell behind when things like RT and HDR became bigger for the gaming scene?  A 2080 can still use the RTX hdr and use the latest dlss versions while RDNA 1 is just dying.,hardware,2025-10-30 22:59:24,19
AMD,nma6o1x,Surpass Nvidia in what?,hardware,2025-10-30 23:35:52,13
AMD,nm9k13x,"> Ye, especially since they are still floating around in the channel. >  >  >  > I don't know when AMD stopped manufacturing them, but they sure as hell had stockpiles of them that kept showing up until like 2023/24. It sure as hell is not that long enough ago that it warrants dropping day 1 support.  You could buy new RDNA2 GPUs like the RX 6600 as of a few months ago.",hardware,2025-10-30 21:29:32,42
AMD,nm987je,Bought my 6750xt in july of 2023...   This is dumb,hardware,2025-10-30 20:30:43,57
AMD,nmbv3q9,Laptop RDNA2 iGPUs are sold to this day,hardware,2025-10-31 06:41:18,6
AMD,nmbqce5,AMD killed Vega support around the same time they launched new Vegas SoCs.,hardware,2025-10-31 05:53:07,7
AMD,nmdieim,Yup who knows maybe redstone is actually launching for UDNA while RDNA3/4 support is dropped to save their margins /s  At this point anything is possible.,hardware,2025-10-31 14:19:27,3
AMD,nmaq5te,Nvidia is providing Day 1 game drivers for the RTX 20 series through to the current RTX 50 series though. They even maintained support for Maxwell and Pascal until very recently.,hardware,2025-10-31 01:30:03,32
AMD,nmbqhwq,Bro.. Nvidia supported day one drivers Maxwell for 11 years.,hardware,2025-10-31 05:54:38,4
AMD,nm9b8n1,Rdna3 doesn't have matrix cores.  They have WMMA instruction sets which in the CU.  They don't have dedicated AI cores.,hardware,2025-10-30 20:45:24,28
AMD,nmalfc3,I have a 1080ti and still will not upgrade until I see improvements to ray tracing computation become better value for dollar and performance.,hardware,2025-10-31 01:02:02,6
AMD,nmaaiuh,I'm about to finally upgrade from my trusty and only just abandoned 1080 Ti to 9070 XT. 400%+ raster Gonna be glorious.,hardware,2025-10-30 23:57:52,5
AMD,nm9wojs,And actually good drivers  I know which brand made my GPU BSOD with two drivers in span of 3 months and it sure as fuck isn't Nvidia,hardware,2025-10-30 22:39:35,4
AMD,nmai0z2,Some games do not let you run the game without new drivers with them having a driver check at startup,hardware,2025-10-31 00:42:20,19
AMD,nmbvc1e,You are also talking about support of DX features as and when they come if hardware permits. Mesh shading was hardware supported in 2018 but needed drivers for when it actually came in a game,hardware,2025-10-31 06:43:40,3
AMD,nm9oryh,"Your next gpu should be whichever one makes the most sense for you, whether it’s amd, nvidia, or intel.  Nvidia is worse than AMD imo. This doesn’t even come close to the stuff Nvidia pulls with AIBs and other 3rd parties. Don’t forget all the downgrades of cards and calling it the exact same thing so consumers are none the wiser.",hardware,2025-10-30 21:54:19,2
AMD,nmaeh24,"Nvidia literally do the exact same thing.  They drop driver support for anything more than two architectures old.  EDIT: When I say 'driver support', I mean in the context of these Day 1 game optimizations that this topic is talking about.  Those are pretty much only ever built for their two latest generations.  Nobody is talking about stripping away driver support completely.",hardware,2025-10-31 00:21:13,-12
AMD,nmcfbgk,"They aren't getting rid of driver updates, only that within newer drivers, efforts surrounding Day 1 game optimizations will be made only for their latest architectures.   It's exactly how Nvidia does it.",hardware,2025-10-31 10:06:38,1
AMD,nma133c,"I think only the Ryzen AI 300 and mobile processors and for example the Ryzen 8000G have better iGPUs, but every other Zen 4 and Zen 5 processor has 2x RDNA2-based GPU cores...  [Here's a guy gaming on the RDNA 2-based iGPU of his 7800X3D](https://www.youtube.com/watch?v=yymkevYwVqQ).",hardware,2025-10-30 23:04:34,13
AMD,nmcmcvb,Consoles are not magic these days. Not all but you can carry a lot of optimizations over between the platforms.,hardware,2025-10-31 11:05:39,2
AMD,nmcjcrt,That is also true! I don’t think about it since my card is so old. Add that to the list,hardware,2025-10-31 10:41:31,1
AMD,nmcjgl3,Yeah that’s true but I feel like that happens often after launch and now you can find them at msrp at least I can at my local micro center,hardware,2025-10-31 10:42:22,1
AMD,nm8zt9a,"I did buy a 5700XT at the time. Worse GPU experience I've had. To have the AMD sub almost unanimously tell me it was my fault, I'm doing something wrong. Nothing wrong with the drivers...lol damn cult over there.",hardware,2025-10-30 19:50:00,37
AMD,nm8xw07,Your 3070 will get day 1 driver support. Probably for the next 3/4 years.  Rdna2 cards like the 6700xt won't from now,hardware,2025-10-30 19:40:36,20
AMD,nmaz3ld,"You can partially thank YouTube channels who absolutely did not want to yield that the forward looking features were actually good.  Honestly, it has been a black mark on a lot of GPU reviews for over half a decade.",hardware,2025-10-31 02:23:46,7
AMD,nmcuh09,"To be fair, they were mostly right with the 'RT' part, the DLSS on the other hand was and still is indeed a game changer.  I also hate when people said '5700 XT was better value than 2070S'. No, it just wasn't!",hardware,2025-10-31 12:04:10,2
AMD,nmaobyv,i think it was the 2080 that performed about the same as a 1080ti but costed more than used or sales pricing of a 1080ti. pascal had huge oversupply issues and they didnt really clear out all those gpus till the 2nd crypto boom after the 30 series launched.,hardware,2025-10-31 01:19:05,2
AMD,nmcgk5s,"For one, this is an unsubstantiated claim. For another Turing just got DLSS4.",hardware,2025-10-31 10:17:35,9
AMD,nm9k5ey,"Ah, well that might be in my favor then. Not too excited to save up for a new card yet :D",hardware,2025-10-30 21:30:09,5
AMD,nmdu2w1,"I mean, if we're talking about community supported drivers,  AMD's community have a good track record. I remember the hd 7870 was deemed as the poor man's GTX 1050 during the pandemic scarcity.    But it really shouldn't be a community effort for AMD's arguably most important architecture.",hardware,2025-10-31 15:18:19,2
AMD,nmci2f4,Distinction without a difference.,hardware,2025-10-31 10:30:44,4
AMD,nmart0f,Well to be fair there is probably a lot more people still using pascal then there is rdna 2,hardware,2025-10-31 01:39:53,12
AMD,nmbtxlj,"And vega, barely 6 yrs in (released in 2017, driver support dropped in 2023) and amd said fuck it.",hardware,2025-10-31 06:29:25,3
AMD,nm9t4o4,"This isn't abandoning them like that, just lowering their priority.",hardware,2025-10-30 22:19:22,0
AMD,nmcglht,Nvidia dropped supporting Pascal with specific game optimizations by Ampere already.   Y'all are basically all confusing general driver support with specific Day 1 game optimizations.   It's crazy that pretty much nobody in this comment section seems to understand the difference.,hardware,2025-10-31 10:17:54,0
AMD,nm9y6ho,(AMD unboxed),hardware,2025-10-30 22:48:16,-7
AMD,nmgf5mi,The 2070 got day 1 optimization for bl4 because it was the target of min specs and so did the 3080.,hardware,2025-10-31 23:40:44,1
AMD,nmaopd5,yeah 30 and 20 series are on tier2 or tier3 support now. almost all the new features omit these 2 generations of cards since theyre 2+ generations old. at least AMD is upfront when they drop hardware down to tier2 support.,hardware,2025-10-31 01:21:17,-8
AMD,nmgfdro,Damn bro amd got you fighting in the trenches.,hardware,2025-10-31 23:42:13,-1
AMD,nmablb8,Except both AMD and nvidia have been silently doing this exact same thing since they started making GPU's.,hardware,2025-10-31 00:04:07,-8
AMD,nmbvq9p,They have the same driver status as gtx 750ti. Whichh will also get some updates every now and then. Calling this gaslighting is hilarious to read,hardware,2025-10-31 06:47:44,11
AMD,nm90u0b,I've had very bad experiences before with driver support from AMD in this regard. 2 years later after I bought a laptop they switched to legacy support... That just sucks mate.  I will believe what you say tho... Would hurt really bad to have a card that is unsupported as soon as I bought it...,hardware,2025-10-30 19:54:58,20
AMD,nm9bdif,They aren't really overreacting though....a GPU that's just hitting 4 years old and is still being utilized in current gen consoles shouldn't be shafted this soon. Considering the 6000 series was already scarce during the GPU mining craze.,hardware,2025-10-30 20:46:03,53
AMD,nm8wnuq,Dude... NVidia lasta with driver support much longer. Where the drama in that? Besides stating the next purchase because of lack of support is not making drama.  Cash grabbers get what they deserve.,hardware,2025-10-30 19:34:39,22
AMD,nm9xs4x,"Not really, AMD drivers are the biggest pile of dog shit I ever had to deal with",hardware,2025-10-30 22:45:56,6
AMD,nm9g36o,"Steam Deck, on Linux at least, is also a completely different driver stack. Developed/maintained by Valve I believe.",hardware,2025-10-30 21:09:23,18
AMD,nmc0tsz,"PS5 is RDNA1.5, it’s not full RDNA2. It lacks things like VRS support",hardware,2025-10-31 07:41:05,3
AMD,nm8w165,"So, they will never release it then? Then what is the point of all the work they were clearly doing with the leaked version of FSR 4?",hardware,2025-10-30 19:31:35,-2
AMD,nmcjcf4,"All they're saying is that older generation GPU's wont get specific Day 1 game optimizations, which only apply to the handful of specific games that get it.  That's it.  It's the same thing that Nvidia does.  All other general driver support will continue.  And it doesn't mean that feature support cant still be added for older GPU's if and when applicable.  You're having a laugh if you think anybody has been going around making driver optimizations for Polaris GPU's in the latest games for Day 1. lol",hardware,2025-10-31 10:41:26,2
AMD,nmd7rbu,"That has been my sense as well. For example just recently I updated my drivers for no other reason than it had been over a year. So from that I know my drivers are dated last fall and they dealt with two new games released moths later (stalker 2, poe 2) just fine.   Stalker 2 performance was perfectly in line with professional benchmarkers. It is heavy, but actually smooth compared to many complaining about stutters. In poe 2 I had no problems with the old drivers unlike some of my friends and a lot of the forum folks.  I'm assuming gaming optimizations mostly apply to multiple games with similar engines etc instead of just one game but I could be wrong on that.    That said I really hope they bring official fsr4 to rdna 2. Those cards are basically mostly for 1080 or 1440p and fsr 2/3 fails below 4k so they really could use it even if it isn't super well performing.",hardware,2025-10-31 13:23:12,1
AMD,nm8vwyi,It does if they just want to focus on RDNA 3/4 thus they dont really have to test RDNA 1/2.,hardware,2025-10-30 19:31:00,1
AMD,nmbmt2w,Consumer extortion i guess?,hardware,2025-10-31 05:19:24,5
AMD,nm9l31s,Even ***today***: $80 for a RX 6500 XT.  [PowerColor Fighter Gaming Graphics Card](https://computers.woot.com/offers/powercolor-fighter-gaming-graphics-card-13),hardware,2025-10-30 21:34:59,31
AMD,nmaol79,You can buy amd laptops and handhelds with _currently produced_ APUs with rdna2.,hardware,2025-10-31 01:20:36,15
AMD,nm9h3rx,You bought amd expecting good driver support? Understandable mistake to make.,hardware,2025-10-30 21:14:31,-30
AMD,nme1z27,Also handheld consoles. The Asus ROG Xbox Ally literally launched 2 weeks ago with RDNA2.,hardware,2025-10-31 15:56:08,1
AMD,nmaw3rj,"He said the same thing in another comment, I dont think he owns an nvidia card",hardware,2025-10-31 02:05:42,4
AMD,nmcclra,"Oh dear folks - having access to the latest drivers is not the same thing as getting actual Day 1 game optimizations. lol  I'm still getting access to the latest Geforce drivers for my 1070!  You really think Nvidia is building Day 1 optimizations for my Pascal GPU for the latest games? lol  Dear lord, I thought this was all common knowledge.  This is literally no different than what Nvidia does.  EDIT: Oh my god, reading through this whole comment section is madness.  Y'all are pretty much universally not understanding the situation here and getting outraged over AMD doing something Nvidia literally does, too.  These takes are going to age so badly once it's properly explained to y'all by somebody you'll listen to.",hardware,2025-10-31 09:41:41,-4
AMD,nmccv7b,This is just wildly untrue.  The context here is specific Day 1 game optimizations.  Nvidia has only ever put any effort into this for their latest two architectures.  Just cuz you have access to the latest drivers does not mean you're getting any kind of game optimizations for them if you're using an older GPU.,hardware,2025-10-31 09:44:11,-2
AMD,nmcweng,"IMHO, the biggest practical downside of sticking with 1080Ti isn't lack of raytracing, but lack of modern upscaling. It's still a reasonably capable card, but lack of latest DLSS/FSR means it's effectively knocked a tier lower when compared to cards that do support it. Especially if you are playing at resolutions higher than 1080p.",hardware,2025-10-31 12:16:41,4
AMD,nmcj1vg,You have the greatest card ever made. Glad to see its still holding on for you. The People's champ card.,hardware,2025-10-31 10:39:03,3
AMD,nmbj9j5,"You can rest assured that your 9070 XT will be abandoned a lot faster, though.",hardware,2025-10-31 04:47:48,16
AMD,nmapx20,"if you update your drivers to a whql certified version and your computer becomes unstable, its usually because of a bad overclock or unstable xmp/docp profiles.",hardware,2025-10-31 01:28:35,15
AMD,nmabp9k,We have both AMD and Nvidia at home and no issue whatsoever.,hardware,2025-10-31 00:04:46,9
AMD,nmauy5t,Did you already forget the fiasco drivers for the 5000 release  at first ?,hardware,2025-10-31 01:58:40,8
AMD,nmcd2ls,FOLKS - this isn't about folks on older cards not getting access to new drivers at all.  Only that specific efforts for game optimizations will be focused on newer architectures.  This is exactly how Nvidia has done things for a long time.,hardware,2025-10-31 09:46:05,3
AMD,nmaoe06,"wow, that's some peak incompetence from the devs. you'd think the game worked without extra hacks added to run it. well, it probably will if the check is bypassed though..  unless, of course, you are nvidia and you don't follow the spec to begin with, so you might have to do that.",hardware,2025-10-31 01:19:25,-5
AMD,nmbu32i,Nvidia is a LOT better for long term support. Most customers don't care what they do with other companies if it doesn't affect them.,hardware,2025-10-31 06:30:57,12
AMD,nma2k8h,"I do agree with that, but the only reason I got 6800 at a time was a lack of VRAM on 3070 and 3080. 3080ti with 12gb was enough for my needs (VR), but was out of my price range. I do understand the shit Nvidia does, but from a customer perspective there ain't much difference if they ""downgrade"" a card with their shit naming scheme, if there is no competition.  I got fucked by AMD drivers once when buying a laptop with AMD GPU that got discontinued one year after release, but I let it pass. Don't think I'm going for it again.",hardware,2025-10-30 23:12:52,4
AMD,nmcde4h,"Plus the forced obsolescence with low VRAM GPU, they are all evil",hardware,2025-10-31 09:49:06,1
AMD,nmagv4s,"What? No. Stop spreading lies. The 750ti was getting game ready drivers for 9 years. We only just got the GTX 1000/Pascal to stop having drivers recently this year and that took 9 years too. RDNA2 released after RTX 30 Series/Ampere and they’re already putting it into maintenance mode just shy of 5 years while Ampere is still going to get game ready drivers day 1, even RTX 20 Series is still getting day 1 game ready drivers and those cards are 7 years old",hardware,2025-10-31 00:35:28,17
AMD,nm9fo1e,"I swapped my spare 1080Ti for a friend's 5700XT because of the issues they were having with it.  I used it myself on Linux, which had no issues, but I don't exactly go around telling my friends to install Linux to fix all their problems (just most of them)",hardware,2025-10-30 21:07:16,16
AMD,nm9x3j7,"I have 6750XT, when I complained about new drivers (twice in span of 3 months) causing BSOD when alt tabbing, I was called mad.  Guess what, AMD admitted to it being an issue *and redditors still instead I was the mad one*",hardware,2025-10-30 22:41:59,13
AMD,nmcv36v,"That subreddit and bunch of others can't tell the difference between a GPU driver and adrenalin software, a driver panel. Add on top techtubers acting like Crypto bros in pushing AMD and dismissing any issues. Radeon subreddit right now full on gaslithing mode regarding these news.",hardware,2025-10-31 12:08:14,2
AMD,nm8ywsy,"Oh, I don't have a 3070 anymore, I sold it back on late 2022 and got myself a 4070 Ti at early 2023 for nearly the same price as what I sold my 3070 back then.  But yeah, If i still have that card today, the only thing I probably would regret from it is the limited 8GB Vram capacity it had, but that is easily solvable by turning down texture settings and using DLSS.  Lack of DLSS Ultimate API support? as well as proper upscaler nowadays such as DLSS and now apparently driver optimization support? That is a lot harder to get by and optimize / tinker for to get respectable image quality and performance overall.",hardware,2025-10-30 19:45:35,0
AMD,nmdzdxg,"AMD recently deprecated their own driver, and will be supporting the open source RADV drivers on Linux. So it's technically official now too",hardware,2025-10-31 15:43:47,1
AMD,nmbcnzi,I've never seen a comment age like milk so quickly.,hardware,2025-10-31 03:54:41,4
AMD,nmatqxs,You mean like DLSS 4 upscaling? Oh wait,hardware,2025-10-31 01:51:26,14
AMD,nmggf5z,"Sorry for caring about the truth and trying to inform people with good information. smh  I've literally never owned an AMD GPU.  Been Nvidia forever, only broken by an ATI card back in the early 2000's.  Not out to defend any corporation for the sake of it at all.  I despite that sort of thing.",hardware,2025-10-31 23:49:01,1
AMD,nmbupry,"The RTX 2000 series is still getting updates even though it came out a lot earlier, even the 1000 was until very recently.   What's this ""both sides"" bullshit when it's clearly wrong?",hardware,2025-10-31 06:37:24,18
AMD,nmbxlco,Care to share example? As that is not true?,hardware,2025-10-31 07:06:56,1
AMD,nm9n5al,"Yup. AMD literally launched more RDNA2 cards **three years ago** in May 2022. Not announced RDNA3, not ended production, but *LAUNCHED* new cards.  [AMD Launches Radeon RX 6950 XT, RX 6750 XT, and RX 6650 XT, New Game Bundle | TechPowerUp](https://www.techpowerup.com/294707/amd-launches-radeon-rx-6950-xt-rx-6750-xt-and-rx-6650-xt-new-game-bundle) \- **MAY 2022**  *If* you bought them at lunch, it means in 3.5 years, these GPUs are now completely out of new game driver updates.  Fuck that lmao.",hardware,2025-10-30 21:45:35,44
AMD,nma2ltx,This isn't about not fixing driver bugs but not adding optimisation for suboptimal game code. Something which should be the job of the game producers anyway.,hardware,2025-10-30 23:13:07,1
AMD,nmchjht,This isn't about general driver support.  That's going to continue.   This is about Day 1 game optimizations.  And Nvidia's practice is literally the EXACT same as what AMD are doing here.    This comment section is insane.  Only like three people seem to have actually understood the situation properly.,hardware,2025-10-31 10:26:07,1
AMD,nm94irb,Games will still work on your machine. Fucking hell. They're not doing an Apple move,hardware,2025-10-30 20:12:54,-11
AMD,nm9j12o,"The SteamDeck uses the open source Mesa driver. Linux will still get driver updates because they are independent from AMD. In fact GPUs all the way back to Terradcale still get driver updates on Linux. And Polaris, Vega and RDNA1 even have software RT support.   It's amazing what can be achieved when profits are no longer prioritised above all else.....",hardware,2025-10-30 21:24:25,17
AMD,nmcisem,"Call it what you want, their point is still the same.  It's not relevant.  And you certainly dont need special optimization help from AMD/Nvidia to enable something like VRS.",hardware,2025-10-31 10:36:55,1
AMD,nm8x8sn,No idea I don't work for amd. In the interview done by ancient gameplay they stated that amd is always trying stuff. They could have put it on pause for Redstone and may come back after.,hardware,2025-10-30 19:37:28,18
AMD,nm96q15,"All of these companies have a ton of projects going at once that are never going to see the light of day because they are competing against other internal projects, just don’t work, market conditions, etc.",hardware,2025-10-30 20:23:31,12
AMD,nmac074,It’s likely the INT8 branch was just the original one before they decided to move to FP8. AMD might release an FP16 version for RDNA3 instead. RDNA3 already runs the FP8 version just as well as the INT8 one in Linux and it looks better.,hardware,2025-10-31 00:06:35,2
AMD,nmasz0n,Well they should be testing rdna 2 Gpus they literally released in 2022 and already dropped day one support?,hardware,2025-10-31 01:46:48,2
AMD,nmck2t8,This isn't about 'testing' anything.  It's about going the little extra mile to add some extra driver tweaks and optimizations for very specific games.    Games will still run on older GPU's like normal.,hardware,2025-10-31 10:47:25,1
AMD,nmb4nvb,"And newegg sells new geforce 1050 cards. If you're willing to expand that to third party sellers on newegg there's a new geforce fx 5200 from *2003*. Probably just sitting in some dusty warehouse somewhere hoping someone who doesn't know what they're doing and/or some *need* for an identical replacement pops up.  I'm not sure ""Still being sold"" really matters without knowing the numbers.",hardware,2025-10-31 02:58:15,-4
AMD,nmcwcp8,Umm actually they just released an Rdna2 handheld.,hardware,2025-10-31 12:16:20,3
AMD,nm9zetc,"My RX 580 got day 1 driver support for a longer period of time than my 6950Xt will.  So yes, I did expect proper driver support and no that is not an ""understandable mistake"". This is a shit move by AMD.",hardware,2025-10-30 22:55:14,30
AMD,nmaq5ra,Mate the 6750xt came out in 2022,hardware,2025-10-31 01:30:03,3
AMD,nmccpx4,"Last time I've owned anything \*other\* than an Nvidia card, it was an ATI card. lol   I know what I'm talking about.  Having access to the latest drivers is NOT the same thing as actually getting these specific game optimizations.  Nvidia has always only actively supported this for their latest two generations.  This has been going on forever now.",hardware,2025-10-31 09:42:45,-3
AMD,nmcog8y,You can probably get access to the latest drivers on GTX 780 ti as well. It's just that the latest drivers are for that card are from like 2020.,hardware,2025-10-31 11:21:50,3
AMD,nmgd5kr,it pains me incredibly,hardware,2025-10-31 23:27:41,1
AMD,nmc5esn,"This wasn't because of that, this BSOD issue was something AMD themselves admitted, XMP or no XMP it was happening, and I never overclock my GPU when it gets the job done performance wise.",hardware,2025-10-31 08:29:00,4
AMD,nmafm39,Wow 1 persons anecdotal evidence. My 5090 hasn't burnt to a crisp yet. Means that 12vhpwr is flawless right ?,hardware,2025-10-31 00:28:03,-7
AMD,nmbdaw7,It was so bad that the 5000 series got so cheap,hardware,2025-10-31 03:59:28,6
AMD,nmc5kso,"Oh no, one generation that launched with less than ideal drivers that got blown out of proportion.  People got more mad at RTX 5000 series drivers than they ever got for far more broken AMD drivers that never get fully fixed.  At least Nvidia has a track record of stable drivers and actually fixing them if something does go wrong once in a blue moon.  Even Intel who is new to the game has put more effort in their drivers.",hardware,2025-10-31 08:30:43,1
AMD,nmc04wj,"I too, would like the driver to support all features from the get go and not have bugs. But that doesn’t work like that, therefore driver denylists are a must",hardware,2025-10-31 07:33:39,1
AMD,nmax295,Don't try to chastise them man. He deserves to have his wrong opinion too.,hardware,2025-10-31 02:11:27,-1
AMD,nmcdaw9,"This is not a lie.    Y'all are making the hilarious mistake of thinking having access to 'Game Ready drivers' is the same thing as actually benefiting from them.  I'm using a GTX1070 and I still get ACCESS to the latest drivers.  But within those drivers, actual Day 1 game optimizations are only built for their latest architectures.  This is how they've done things for a very long time.    We're not talking stripping away access to drivers altogether.  It was specifically pointed out that they're talking about Day 1 game optimizations.",hardware,2025-10-31 09:48:15,2
AMD,nmanx6q,game ready drivers dont really contain optimizations or features for older architectures. just because its supported in the same driver package as the latest cards doesnt mean theyre keeping it in tier1 support like the newest cards.  at least amd is pretty upfront about it. nvidia's been beating around the bush with it for decades now.,hardware,2025-10-31 01:16:38,-9
AMD,nmcch67,Same problem here on 6700 xt. How often does it happen to you? In mine it happened like 5 times? in the last month. I think it's related to the latest driver.,hardware,2025-10-31 09:40:28,2
AMD,nmb7nio,5 and 6000 series cards aren't being dropped from new features (if they can support them)    That's not what this was about at all.,hardware,2025-10-31 03:18:19,-2
AMD,nmaxjec,yep framegen is being artificially limited because ampere and turing support is being dialed back. no reason why FSR3 framegen works on these GPUs but nvidia's own solution doesnt.,hardware,2025-10-31 02:14:19,-10
AMD,nmay8x1,">AMD literally launched more RDNA2 cards three years ago in May 2022.  Try October, 2023.  https://www.techpowerup.com/gpu-specs/radeon-rx-6750-gre-12-gb.c4183  24 months.",hardware,2025-10-31 02:18:36,21
AMD,nm9u6z3,"Exactly! I forgot about the ""50"" variations, thank you for pointing those out. Its incredibly frustrating, I especially feel for anyone that bought one within the last 1 to 2 years. I think we can all agree that it's a shitty move on AMDs part. If I recall correctly, isn't the 6000 series still showing a fairly high usage on Steams Hardware Survey?",hardware,2025-10-30 22:25:28,8
AMD,nmchslg,"It has nothing to do with 'suboptimal game code', oh my god. lol   Developers have NO ACCESS to the driver code from AMD and Nvidia.  And for big releases, Nvidia and AMD will often put in a little extra effort on their end to help tweak things here and there for a little extra performance(and often only in specific situations regarding settings or areas of a game).  It usually doesn't amount to all that much.  And it definitely isn't about trying to correct bad developer code, which drivers cannot do.",hardware,2025-10-31 10:28:21,1
AMD,nma3dvq,"Didn't mention anything about driver bugs, this is about these cards not receiving future game support.",hardware,2025-10-30 23:17:28,-1
AMD,nm94xo6,What about encoding support and enhancements for it? Bug fixes? It's not only about games...,hardware,2025-10-30 20:14:54,8
AMD,nmcinzt,"This isn't about profits.  This is about prioritizations.  They can do better work for Day 1 optimizations for the latest games if they limit how many generations of GPU's they are trying to specifically tweak driver code for.   It's exactly why this has been standard practice from Nvidia since forever.    Also, since basically none of y'all are getting this - there will still be new drivers and general driver support like always.",hardware,2025-10-31 10:35:53,2
AMD,nmcivps,You don’t. That still doesn’t change the fact that PS5 is NOT RDNA2.,hardware,2025-10-31 10:37:40,1
AMD,nmapuce,"He said that redstone was their main priority but rdna 3 owners should stay tuned, its also confirmed that the same fsr4 version is coming to ps5",hardware,2025-10-31 01:28:08,1
AMD,nmar8b3,"From what ive heard its the other way around, the original one being FP8 and they had to rewrite the entire thing in INT8 for PS5 and RDNA3.  if the can make it work i would prefer higher performance with INT8 than higher quality with FP8.  In my opinion after spending months with both versions and DLSS4, if you apply some contrast adaptive sharpening to INT8 FSR4, all 3 are almost indistinguishable unless ur nitpicking",hardware,2025-10-31 01:36:27,2
AMD,nmbec57,"To conflate the 1050 released ***nine*** years ago and the RX 6500 XT released **four** years ago seems silly in terms of sales + quantity.  If you have sales data per SKU per quarter, please share. Who wouldn't want that data?",hardware,2025-10-31 04:07:26,11
AMD,nmd23ox,"> currently produced APUs  Yes, that Z2 Go and Z2 A (which is a rebadged steam deck apu) both fit this description.",hardware,2025-10-31 12:51:28,2
AMD,nmagmrb,"I owned two HD6950s back when Crossfire was still a thing.  Those were supported for less than 5 years, so you could've very easily bought the card a few years in and gotten less than 48 months of active support.   Unfortunately nothing new for AMD.",hardware,2025-10-31 00:34:04,10
AMD,nmcvrk9,"No, it's an understandable mistake. GCN2 and Vega were basically never supported by AMD properly and had very very early driver support end dates.   Polaris (RX 480/580/etc) and the original GCN (hd 7950/70/etc) series were bucking a trend of AMD not supporting gpus. These were the outliers for AMD if you look back historically.   It's understandable you took one of the very few successful AMD products and assumed that's how they operate, but that's just an understandable mistake.  Radeon HD 6000, 5000, 4000 were largely poorly supported in their days. It's been a consistent trend for AMD with only two outliers.",hardware,2025-10-31 12:12:38,3
AMD,nmc1a5d,AMD has managed day 1 driver support with something? I guess believable with the 580 since it was a rehash of 480.,hardware,2025-10-31 07:45:50,0
AMD,nmbtbwn,"Yes, and you expected driver support to inly last 3 years from release? Also there were other rdna2 versions released until 2023 at least.",hardware,2025-10-31 06:23:14,-3
AMD,nml4gw2,Anyone can go to the nvidia drivers page and find multiple recent drivers with notes on game optimization.,hardware,2025-11-01 19:57:37,1
AMD,nmdisbr,"Yeah may have had the same issue with a friend's card before, told him to just cut losses and sell the card. Not sure why people are quickly jumping to AMD's defence about such issues.",hardware,2025-10-31 14:21:24,3
AMD,nmat1wn,"You realize the same argument can be made about the dude complaining about issues with AMD drivers, right?",hardware,2025-10-31 01:47:17,8
AMD,nmdlkmv,"well never had to update the driver for a game myself, at least in 20+ years. before that i had to downgrade my nvidia driver though, because only certain old versions were performant.  guess i just don't end up playing games from incompetent devs somehow..? tbh the whole ""game ready"" driver sounds like a marketing spin from nvidia or something. as opposed to ""our driver never worked, so we have to quick fix it now"" -driver.  or is this solely about the modern DSLL or whatever gimmicks, that would need support game-by-game?",hardware,2025-10-31 14:35:41,1
AMD,nmbsquh,And care to provide a source for this claim? Even if it doesnt come with optimizations some games would not let you launch without specific driver versions,hardware,2025-10-31 06:17:20,2
AMD,nmcdjy7,"It's depressing that you're getting downvoted for this, just like me.   Folks here dont know what they're talking about.   I'm still getting access to 'Game Ready drivers' for my damn GTX1070. lol  Imagine how ridiculous I'd have to be think that meant Nvidia was genuinely working to optimize games around my 9 year old GPU. lol",hardware,2025-10-31 09:50:38,1
AMD,nmcequt,I had the issue with may and march drivers,hardware,2025-10-31 10:01:30,2
AMD,nmbuigm,"Yea, why won't Nvidia just make the needed hardware magically appear on older cards!",hardware,2025-10-31 06:35:20,4
AMD,nmc0nt3,Try two weeks ago. Base ROG Xbox Ally has RDNA2 based GPU.,hardware,2025-10-31 07:39:19,8
AMD,nmbehl5,Oh unbelievable. I forgot about this card completely. A good find.  Absolutely insane. 2 years of game-specific updates is ***nothing*** for a $300+ GPU.,hardware,2025-10-31 04:08:38,4
AMD,nmabs6e,"No, it's about them not being included in day 1 game specific driver releases.  Which aren't needed in 99% of cases anyway, and only bring marginal performance increases.  It's a storm in a teacup.",hardware,2025-10-31 00:05:15,0
AMD,nm9tes2,"Yes and yes. They really should rename it to ""standard support"" or something.",hardware,2025-10-30 22:21:00,6
AMD,nm97o3s,This is specifically for *day one* drivers. Chill out and learn how to read the article.,hardware,2025-10-30 20:28:06,9
AMD,nmci1cb,Read the article man.  Bug fixes and general driver support are continuing.  This is purely about Day 1 game optimizations.  Which usually dont do a ton anyways.,hardware,2025-10-31 10:30:28,1
AMD,nm9xx72,">bux fixes  Tbh those never existed, I can't think of a single issue I had that was fixed on my 6750XT that was down to drivers",hardware,2025-10-30 22:46:46,-1
AMD,nmfaeis,"I want a better upscaler not a worse one, even if it's not a huge difference. The FP version is already as fast or faster on RDNA3 in Linux and much faster on RDNA4, so the only reason to keep developing the INT8 one is PS5 Pro and RDNA2 (which is pretty borderline for performance as an official product).    AMD should release a super light version for handhelds but I don't know if it will have that much in common with the leaked INT8 code or not.",hardware,2025-10-31 19:40:07,1
AMD,nmbeti7,"You're misunderstand what I'm saying, I'm not trying to conflate two cards. Just that ""Still being sold"" isn't really a useful statement about age, as you can find new stock of pretty much any card from the last 20 years if you look for them.  And honestly I think this entire thing is odd, ~~and the translated statement quoted in the top post seems to contradict other statements made from other AMD sources (namely the rep on the discord - I saw a screenshot of their comment I'm trying to find now to reference that said it was only *new feature* support that would be lacking from rdna1&2 - which likely makes sense as in the latest release that kicked off all this shenanigans the ""new features"" were primarily AI-specific ops that don't have hardware support on those devices).~~  EDIT: I still can't find the source, though I'm pretty sure I didn't imagine it, and the only AMD discord I could find doesn't even seem to have the user I remember from the screenshot (""AMD_Vik"" I think?). Though at the end of the day, a screenshot is just a screenshot, not evidence of anything :P  However on that (official) discord there was:  > ""We’d like to inform you that the release notes for AMD Software Adrenalin Edition 25.10 2 posted included misinformation that has since been corrected. There is no change to USB-C functionality on the RX 7900 series GPUs in the 25.10.2 driver. There was an incorrect line in the originally posted release notes that has been removed, and the release notes have been updated.""  Which kinda makes one of yesterday's big drama posts void? And kinda makes me wonder who the fuck AMD have *writing* these ""release notes""? Especially if this current ""dropping support for RDNA1&2"" also turns out to be less than feared.",hardware,2025-10-31 04:11:12,3
AMD,nmcw9ll,Hey in his defense the 1050 released nine years ago probably outsells any AMD GPU.   Lolz.,hardware,2025-10-31 12:15:47,1
AMD,nmd4p8n,"Yeah I know, my point is the Xbox handheld literally released like a week before this announcement lmao.  It's not just devices getting sold, it's literally new hardware lmao",hardware,2025-10-31 13:06:11,1
AMD,nmaolbt,Amd were basically scumbags till after HD 7000 series lol,hardware,2025-10-31 01:20:37,0
AMD,nmbud7x,Umm yes I expect driver support to last more then 3 years. You just proved my point even more the gre came out in oct 23 with the same rdna 2.,hardware,2025-10-31 06:33:50,3
AMD,nmmxqvp,Go do it and prove me wrong then. lol,hardware,2025-11-02 02:06:58,0
AMD,nmdmzia,"No, D3D12 feature sets and/Vulkan extensions are introduced by newer driver versions, along with new Shader Models and shader compiler optimizations. This has nothing with “incompetent developers”, it’s nature of graphics programming.  And bugs within certain driver versions happen constantly, this is software just like anything else on the market, bugs are bound to happen.",hardware,2025-10-31 14:42:53,2
AMD,nmcdfji,"Driver notes for specific game day optimizations usually go over this.  I really thought this was like totally common knowledge by PC gamers paying attention to this stuff.  It's one of the reasons that if all else is roughly equal, buying a newer gen GPU over an older gen GPU is usually worth it.  Means about two more years of focused optimizations(not that this usually makes a huge difference anyways, but it's welcome).  Because Nvidia absolutely drops specific 'Day 1 game optimizations' support for older GPU's.    And again, nobody is talking about getting rid of \*access\* to newer drivers.  Only that specific Day 1 optimizations built within are optimized for newer architectures.  Which is exactly how Nvidia has worked for a long time.  Nobody is 'lying' here, y'all are just not understanding the situation.",hardware,2025-10-31 09:49:28,1
AMD,nmddw20,"because you dont need the extra hardware in blackwell and lovelace for framegen? just like how REBAR support could work on turing and volta GPUs with a firmware update but nvidia refused to do it because those cards were on tier2/3 support at the time, or how nvidia refused to add vesa adaptive sync support to maxwell when the hardware had displayport 1.2 support.  its a pattern of tier2/3 support for cards 2 generations or older. idk why people are convinced theyre still working hard on them. just because theyre in the same driver package doesnt mean anything.",hardware,2025-10-31 13:56:12,0
AMD,nmchy1x,"This comment section has gone insane misunderstanding this situation.   It's wild.  You're like one of three other people who seems to understand this.    Nvidia doesn't support these Day 1 optimizations for older architectures, either.  It's the exact same practice from them since forever and yet so many comments are like ""This is why I buy Nvidia!"". lol Utter depressing hilarity.",hardware,2025-10-31 10:29:40,-1
AMD,nm9fpij,Does it make a difference? Didn't I pay for support? I'm second class citizen? This is why companies treat us like dirt: we accept sub-par customer service.,hardware,2025-10-30 21:07:28,14
AMD,nmcw3n7,Legitimately there's only two products that AMD supported long term:  HD 7000 series (and the rebranded R9 200 series which was just HD 7000 all over again)  RX 400/500 series   That's it. Entire history of the company.,hardware,2025-10-31 12:14:44,7
AMD,nmc136v,"Yes, replied to wrong post ^^",hardware,2025-10-31 07:43:50,2
AMD,nmdnjvt,"no i mean this is very simple. if you use an old card for a new game, it will work. if it doesn't, the game dev is incompetent as they are throwing free revenue away.  it is primarily in the interest of the game devs for their game to work (well), and their job to make it happen. the only actual case where the consumer can expect an issue is if the card is just too weak (or extremely old so the dev won't bother).",hardware,2025-10-31 14:45:43,-1
AMD,nmfid5h,"Why are you here making up conspiracy theories about something that's well known and easy to check? Yes, you do need the hardware to run framegen, otherwise you lose more FPS than you'd gain.",hardware,2025-10-31 20:22:33,2
AMD,nm9nfqw,"Yes, you did. It's an absolutely shitty decision to do it ***this*** soon.  As a reminder, just 3.5 years ago, AMD launched *new* RDNA2 GPUs: [AMD Launches Radeon RX 6950 XT, RX 6750 XT, and RX 6650 XT, New Game Bundle | TechPowerUp](https://www.techpowerup.com/294707/amd-launches-radeon-rx-6950-xt-rx-6750-xt-and-rx-6650-xt-new-game-bundle)  One of them cost $1000 and in 3.5 years, it's fucked for new game updates.",hardware,2025-10-30 21:47:08,11
AMD,nmciavu,You are STILL getting general driver support.    Y'all dont understand that going through every new major game and trying to tweak and optimize complicated driver code just to eek out some small percentage of extra performance is a ton of work.  And now multiply that effort for basically every generation of GPU architecture that you want to do this for.   This is why it's been standard practice from Nvidia forever to ALSO only focus their Day 1 game optimizations on their latest two generations.,hardware,2025-10-31 10:32:46,1
AMD,nm9j54r,"> Didn’t I pay for support?  Actually, no, you didn’t. You paid for a product with the assumption of support, which has been *marginally* reduced. And buying AMD shouldn’t come with any support assumptions given their poor track record.  Without an announcement you wouldn’t even have noticed, game-ready drivers are a nothing burger. AMD support has been an issue but *this* isn’t worth getting that upset over.  Edit: some of y’all find this so offensive that you leave comments that the mods have to remove. Take a chill pill",hardware,2025-10-30 21:25:01,-4
AMD,nmdnvr0,"Cool theory, the practice of working with graphics APIs and drivers is drastically different.  Even code that passes validation layers and is perfectly according to specs may not work on particular driver versions or not perform well. There’s a reason graphics vendors have teams dedicated to AAA games support.  And even old GPUs do receive new API extensions from time to time. You aren’t going to implement them without proper driver support.",hardware,2025-10-31 14:47:23,1
AMD,nmggnsc,framegen on 30 and 20 series is a conspiracy now? so fsr3 and xess fg dont exist in your world then?,hardware,2025-10-31 23:50:37,1
AMD,nm9x9tv,"With this mindset, we deserve sub-par support if none at all then. Amazing. This world is fucked.",hardware,2025-10-30 22:43:00,6
AMD,nmdotct,yeah? then the non-AAA games still happen to work just by magic alone? and the dev is certainly unable to adjust the code themselves to work around bugs in the driver?,hardware,2025-10-31 14:52:06,1
AMD,nm9yh56,"Sure, and with that logic and AMD’s track record we should have all known not to buy RDNA in the first place.  Vote with your wallet, don’t buy AMD again.",hardware,2025-10-30 22:49:59,-1
AMD,nme5v39,"What does it mean ""clarifies""? Was the previous claim of no day zero drivers made up by someone else?",hardware,2025-10-31 16:15:04,245
AMD,nme7mgi,"> New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch.  This is the statement AMD gave to TH. TH is assuming this means day-zero updates and reporting as if that is factually the case.",hardware,2025-10-31 16:23:49,227
AMD,nme81zv,"This to me is just a fine example of corporate weasel wording. ""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch"" does not in any way mean that RDNA1 and RDNA2 cards will get game optimizations. AMD after all can simply deem that ""market needs"" do not include optimizations for ""insert popular game of the year number 2"" , only ""game number 1"" in 2026, and no games after that.   So no, unless they fully commit to providing game optimizations and full game driver support for an architecture that launched five years ago and is still being shipped in mobile and other products in 2025, I don't buy it. It's about trust, and lack of it.",hardware,2025-10-31 16:25:58,179
AMD,nme5d6l,Me when I shoot myself in the foot then realized I may have needed that foot,hardware,2025-10-31 16:12:37,116
AMD,nme6lkh,"To be honest, dropping RDNA1 was fair, but RDNA2 was dumb.  There is a gaming handheld being sold **brand new, right now** that ships with RDNA2, the Xbox ROG Ally. Making a 2nd class citizen out of a device that just launched in late 2025 is just dumb AF.  AMD could have probably avoided this blowback if they were just more aggressive in their GPU strategy for their APUs. Like, there shouldn’t have been a new RDNA2 APU in 2025, full stop. Also, it doesn’t make sense for other modern APUs to still be RDNA3 or 3.5 this late in the hardware lifecycle. If AMD/Radeon were actually firing on all cylinders, RDNA4 APUs should have already released by now.",hardware,2025-10-31 16:18:42,64
AMD,nme4myd,"Aka ""We saw your outrage, and we aren't changing anything, but like uhh yeah market needs and conditions and stuff so like it's fine your less than 5 year old GPU is unsupported"".",hardware,2025-10-31 16:09:03,120
AMD,nmhzl4c,People celebrating when they're clearly weaseling their way out is peak Reddit.,hardware,2025-11-01 07:17:49,11
AMD,nmed5oh,"what the heck does market needs means?  Only if AMD users demand such game optimizations **per title**  > ""New features, bug fixes and game optimizations will continue to be delivered as required by **market needs** in the maintenance mode branch,"" an AMD spokesperson told Tom's Hardware.",hardware,2025-10-31 16:51:02,25
AMD,nme6us7,">New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch,"" an AMD spokesperson told Tom's Hardware  This is just marketing speech, as they still confirm that these drivers are in the maintenance branch.",hardware,2025-10-31 16:19:59,68
AMD,nmeu0ii,"This is what they said before when they put Polaris and Vega to maintenance mode:  > The AMD Polaris and Vega graphics architectures are mature, stable and performant and don’t benefit as much from regular software tuning. Going forward, AMD is providing critical updates for Polaris- and Vega-based products via a separate driver package, including important security and functionality updates as available. The committed support is greater than for products AMD categorizes as legacy, and gamers can still enjoy their favorite games on Polaris and Vega-based products. > — AMD Spokesperson to AnandTech  tldr: this new statement is a nothingburger and doesn't change anything",hardware,2025-10-31 18:14:59,32
AMD,nmeij4y,"I'm not sure qualifying the commitment with ""market needs"" is really going to put peoples mind as ease on this. Market needs are not consumer needs.",hardware,2025-10-31 17:17:19,17
AMD,nme9cn0,I appreciate they didn't wait for so long to scramble their PR stunts.,hardware,2025-10-31 16:32:22,12
AMD,nmhuvd2,"The “clarification” creates more questions than answers. They were quite clear yesterday, only security and bug fixes. This caveated backtrack means nothing. I’m not dropping $1000 for a GPU that’s only going to get 3 years of support again. I’m going Nvidia next time and never trusting AMD again",hardware,2025-11-01 06:24:49,8
AMD,nmel18b,If nothing changed they wouldn't have been put in maintenance branch. It's just that simple,hardware,2025-10-31 17:29:50,12
AMD,nmedjit,"What the hell does “as required” even mean.  I feel something is still off here, but can’t determine it due to their vague backpedaling language.  Screw Radeon. I’d rather pay NVIDIA tax to have confidence in my product being supported for more than 2 years since I bought it NEW from retail.",hardware,2025-10-31 16:52:55,15
AMD,nmerlqc,"I don't really believe it. The text is basically the same as when they abandoned Vega and Polaris.  >The AMD Polaris and Vega graphics architectures are mature, stable and performant and don’t benefit as much from regular software tuning. Going forward, AMD is providing critical updates for Polaris- and Vega-based products via a separate driver package, including important security and functionality updates as available. The committed support is greater than for products AMD categorizes as legacy, and gamers can still enjoy their favorite games on Polaris and Vega-based products.  >— AMD Spokesperson to AnandTech",hardware,2025-10-31 18:02:34,12
AMD,nmffd6r,Maintenance branch is where GPU's go to take a powder.  I can't remember the last time performance got better on a gpu after it was sunsetted.,hardware,2025-10-31 20:06:33,7
AMD,nmgtkz0,We need Vulkan support as well! The notes point to never getting FSR4 for Vulkan and decoder support as well.   Firefox is merging with the Linux version so hardware acceleration might end up relying on it in the future.   Also other apps like DXVK and other layers will be hurt by this segmentation and already dropped old GPUs.,hardware,2025-11-01 01:17:48,3
AMD,nme4tk1,Good. Probably Hardware Unboxed’s video helped,hardware,2025-10-31 16:09:56,13
AMD,nme8axg,They are splitting the driver branches into redstone supported and not supported. An fsr update is happening soon.,hardware,2025-10-31 16:27:11,6
AMD,nme5qe4,I think from next time onwards they will silently stop supporting instead of an announcement. I am never buying an AMD GPU again. My 6800xt is not even 2 years old.,hardware,2025-10-31 16:14:25,13
AMD,nmg28td,They sure romeo-foxed that announcement...,hardware,2025-10-31 22:16:38,2
AMD,nmgjtd4,Well as long as the games work...,hardware,2025-11-01 00:11:49,2
AMD,nmgl7wx,"Okay to translate a bit to what this means: so....They're gonna make sure the games work, but again no optimizations. What does this mean? Well look at the typical amd ""fine wine"" arguments, amd often continues to optimize drivers for games while nvidia will still provide driver updates, but won't optimize for older architectures. That's why the gtx 600/700 series started lagging behind say, amd's 7000/200 series and the 900/1000 series started lagging behind the 400/500 series. Nvidia stopped specific optimizations for older cards while amd continued them. This meant amd cards often started performing better than older equivalent nvidia cards.  Amd won't be providing game level optimizations but will stlil provide driver support to ensure the games work.  To be fair nvidia probably does this too. I could see then optimizing for 4000/5000 series but whatever the 2000/3000 get they get. Nvidia tends to focus on their newest 1-2 architectures and whatever happens to the older ones happens. Doesn't mean nvidia stops providing driver support, they just won't get ""game optimizations."" You know what I'm saying?",hardware,2025-11-01 00:21:19,2
AMD,nmgcbxq,"Maybe ""market needs"" means ""Battlefield 6 runs perfectly fine, so stop bitching about your game-ready driver nonsense.  If a new game or windows change actually makes rdna1 crap out, we'll look into it because that's what maintenance mode has always meant.""",hardware,2025-10-31 23:22:22,3
AMD,nmel31t,"AMD should just open source their GPU driver stack on Windows like they do on Linux, where support for oldish cards isn't a problem.",hardware,2025-10-31 17:30:05,1
AMD,nme9998,"I guess they are planning on exiting the consumer GPU market then.  So that's a solid no buy for me for the next few years at least until they've proven that they support their products throughout their useful lifespan.  I have a 2070 max q laptop that's still well supported after a considerable amount of time, and a 6400 that I just bought for a sff build that is now unsupported.  If I was within a return window for any amd card I'd be looking for a refund immediately.",hardware,2025-10-31 16:31:55,2
AMD,nme3m9d,"Hello -protonsandneutrons-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-31 16:04:06,1
AMD,nmha8wg,"""day zero"" support vs ""zero"" support. Damn AMD",hardware,2025-11-01 03:15:32,1
AMD,nmeka39,"it's exactly the same message, people will still not understand and spread outrage",hardware,2025-10-31 17:26:01,-4
AMD,nme8gnk,"Obvious. Such an overreaction from reddit, probably agents for team green.",hardware,2025-10-31 16:27:58,-26
AMD,nme8g8k,the problem is in both sides 1st the weak comunication from AMD 2nd side the usual clickbait and disinformation from people with lack to understand basic semantics,hardware,2025-10-31 16:27:55,-13
AMD,nmi9j3c,"I don't even think AMD's clarification makes things clear at all.  AMD says ""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch"" but 'market needs' is a loophole you could drive a truck through.",hardware,2025-11-01 09:09:35,31
AMD,nme7dsp,"Nah, they made a mistake in the changelogs. Just a simple typo.",hardware,2025-10-31 16:22:36,105
AMD,nmfi08x,"It doesn't change what was previously reported. RDNA1 and 2 are being moved to the maintenance branch and so won't be updated the same as the main branch. Saying ""game optimizations will continue to be delivered as required by market needs"" is just a cop out.",hardware,2025-10-31 20:20:37,65
AMD,nmvpss7,It was made up by whoever wrote the patch notes.,hardware,2025-11-03 14:05:08,1
AMD,nmg6xil,"Yes it was, somebody heard what they wanted to hear and ran with it.",hardware,2025-10-31 22:47:11,-7
AMD,nmeejcg,And then u/hardwareunboxed (shame on you) fumbled the ball releasing a video with the word “trash” used plenty of times that caused people who have zero reading comprehension skills to sharpen their pitchforks and point it at big bad AMD for this atrocious decision they made unlike Nvidia who gives 10+ year old cards support.   This was a wild day in the pc enthusiast community,hardware,2025-10-31 16:57:44,-85
AMD,nmfcnr5,"imo this is just AMD trying to have their cake and eat it too. they don't want to promise day 1 updates but they don't want to write it off either(because that would really upset folks, not because they actually want to provide them.)  They need to be more clear about what's actually happening. but they don't want to be clear about it because folks won't like the truth.",hardware,2025-10-31 19:52:11,76
AMD,nmfi6o6,> will continue to be delivered **as required by market needs**  What does this mean exactly?,hardware,2025-10-31 20:21:35,18
AMD,nmikfr0,When are they not charitable in interpretation for AMD,hardware,2025-11-01 11:01:55,2
AMD,nmei1i9,"TBF, Market needs implies day 1 drivers, they are sort of important.",hardware,2025-10-31 17:14:54,-11
AMD,nmehnrb,Per market needs = it depends.,hardware,2025-10-31 17:13:01,45
AMD,nmee5ys,"> This to me is just a fine example of corporate weasel wording.  Indeed because the truth is really that easily revealed. If nothing of substance changed the sentence would have read:  ""New features, bug fixes and game optimizations will continue to be delivered **as before**""",hardware,2025-10-31 16:55:56,56
AMD,nmf33x5,unless and until they remerge the driver branches a massive grain of salt is needed,hardware,2025-10-31 19:01:38,12
AMD,nmepw6s,I think what they mean is if game does something extremely poorly and causes significant performance issues and it's easy to fix on both branches they will but they won't bother going out of their way to profile the game specifically for inefficiency on RDNA 1/2 and optimize for it.,hardware,2025-10-31 17:53:58,12
AMD,nmfl14k,"Yeah this is AMD basically saying that they'll do something, without actually committing to it. Those cards are still SOL for all intents and purposes.",hardware,2025-10-31 20:36:56,14
AMD,nmerb1l,"You are still getting general game driver support, you're just not getting any sort of 'extra effort' optimizations that AMD might do on their end for the latest games.  Tweaks and whatnot that can sometimes get you like 5-10% gains if you're lucky.  Usually not even that, though.    This is normal.  This is how Nvidia have done things for a long time now.  Everybody will still be able to run games as they would.  Everybody will still get access to the newest drivers.  It's not actually gonna change much of anything.  AMD have probably already been doing this for a while now, too.  It's just the most sensible thing, since it's unreasonable to ask for game-specific optimizations in driver code for all these different games and with each unique architecture release.  Gotta prioritize.    It's crazy how few people seem to get what's going on here.",hardware,2025-10-31 18:01:03,7
AMD,nmf42fw,This.,hardware,2025-10-31 19:06:38,-4
AMD,nmedyix,"It was a very nice foot, the stump sorta works now, but not nearly as well as the lost foot",hardware,2025-10-31 16:54:57,20
AMD,nmhrmvg,they tried to U-turn without actually committing that u-turn.  NOPE. I will never buy Radeon again unless you have 70% more performance for the same price. (if it means I have to put up with that short software support),hardware,2025-11-01 05:49:24,5
AMD,nmeo40c,It was kinda nonsensical if that's what they were planning.  You can buy a brand new SteamDeck or PS5 with RDNA2. They already have to do a lot of the work to support those systems.,hardware,2025-10-31 17:45:11,8
AMD,nme81mb,*Multiple* gaming handhelds!,hardware,2025-10-31 16:25:55,21
AMD,nmf2sso,"Even then, amd just rebranded some apus with rdna 2 in them as ryzen 100 cpus. my laptop has the 7735hs and a 7700s gpu. smart access graphics which was their competitng solution for optimus no longer works because i need a different driver for the apu and a different driver for the 7700s. my laptop is only 1 year old",hardware,2025-10-31 19:00:03,3
AMD,nmea1rx,Why would it be fair exactly ?,hardware,2025-10-31 16:35:51,-1
AMD,nme78qy,Market needs = We'll make sure it will run GTA VI.,hardware,2025-10-31 16:21:54,48
AMD,nmeruhy,"It doesn't mean GPU's are becoming 'unsupported', it's just no game-specific bonus optimizations via driver tweaks for the latest releases.",hardware,2025-10-31 18:03:49,6
AMD,nmnkt01,Celebrating? I was never dooming. A little disappointed yes but that's mainly because I wish they were more open and clear about their communication. This wouldn't be a conversation if they had a clearly outlined support plan for each card they produce detailing what kinds of driver updates they get and for how long.,hardware,2025-11-02 04:35:47,0
AMD,nme8in6,TBH I want all my drivers to be maintained,hardware,2025-10-31 16:28:15,20
AMD,nmf6vmm,"Which is overblown from poor amd wording. Right now they are the same situation as rtx 30 cards. New game ready updates dont get tailored to old cards. They just run the same dx12 cuda branch that the card bios and its power provides. In fact. New frivers make my and many users old cards slower too so people revert to old drivers or dont take new updates. I still disagree on RDNA2 vendor decision powering handhelds and consoles. But Im not a architecture engineer so this ""maintenance mode"" is a fluke to us?",hardware,2025-10-31 19:21:23,-3
AMD,nmf7b0z,And rx580 still crushes new titles like Bf6. So yes statement doesnt change anything. It just puts a new light on peoples fears that they cant play new games at all. Nvidias naming driver updates game ready drivers misled many people cause old cards dont get any specific tweaks to new games anyway.,hardware,2025-10-31 19:23:38,5
AMD,nmi71gk,People are literally acting like AMD completely stopped all driver updates and their GPUs will immediately explode. AMD’s mistake was even mentioning this whole thing.,hardware,2025-11-01 08:42:01,-4
AMD,nmevnis,Nothing about Day 1 optimizations have anything to do with 'needs' in the first place.    Do y'all really think Nvidia/AMD were out there optimizing EVERY SINGLE GAME for every generation of GPU of the past 10 years? lol    These have always just been like 'bonus optimizations'.  General game support will continue as it always has.  Almost nothing is going to actually change.    People are losing their minds over something they really dont understand.,hardware,2025-10-31 18:23:29,-8
AMD,nmeuxup,"Nvidia literally doesn't do any different.  They are just quieter about it.  They've always focused their Day 1 optimizations on their latest two architectures and that's pretty much it.    And 'as required' means they'll still step in if a specific major game performs unexpectedly poorly for some GPU generation, to where it's a real issue.   These older GPU's will still be 'supported'.  They just wont get like extra effort optimizations on the vendor's part.  They'll still run games just like every other game out there runs without this explicit extra support.",hardware,2025-10-31 18:19:48,1
AMD,nmeggy1,lol you think NVIDIA care about gaming GPUs when it only accounts for 7% of their revenue?,hardware,2025-10-31 17:07:08,-10
AMD,nmfti2o,>  I can't remember the last time performance got better on a gpu after it was sunsetted.  It's a mature driver and GPU. You weren't going to get a major performance uplift on it anyway.,hardware,2025-10-31 21:23:54,2
AMD,nme6ba2,Hardware Unboxed has turned into an outrage farmer.,hardware,2025-10-31 16:17:17,-21
AMD,nme6m00,"No, it seems he just made a big fuss out of something he didn't understand in order to gain clicks",hardware,2025-10-31 16:18:46,-11
AMD,nmgfw5a,This doesn't rule out feature support for older RDNA gens at all.  Day 1 driver optimizations have nothing to do with general feature support.  Whole different things.,hardware,2025-10-31 23:45:34,0
AMD,nmetn98,Just know this has been standard practice from Nvidia for a very long time now.,hardware,2025-10-31 18:13:04,4
AMD,nme8q9y,>My 6800xt is not even 2 years old  It launched more than 5 yrs ago. The date you bought it has no relevance.,hardware,2025-10-31 16:29:18,-7
AMD,nme7mbb,this probably means AMD literally actively wants to exit the consumer GPU market and only make CPU or data centers,hardware,2025-10-31 16:23:47,-5
AMD,nmevr3r,"General driver support will continue, oh my god.   This is about game-specific Day 1 optimizations for certain major new releases.",hardware,2025-10-31 18:24:00,5
AMD,nmoom7t,"…And then abandon it, like they did on Linux this year, leaving valve contractors and volunteers to improve mesa instead?",hardware,2025-11-02 11:03:06,1
AMD,nmvsgh1,AMD drivers are not open source on Linux.,hardware,2025-11-03 14:19:29,1
AMD,nmf5xlp,That is a good idea but the reason they want to end support is so people buy new cards,hardware,2025-10-31 19:16:25,0
AMD,nmg063d,"what kind of room temp iq ahh take is this? They are saying hey, its a well built product and doesnt benefit as much as a brand new GPU would from having day one updates and so to save on support cost we arent going to provide it DAY ONE support, not that they are just letting it die, no...they said they aren't going to provide the updates as if it was a new GPU. Like holy...this has to be ragebait, or you can't read. Most likely both seeing how most americans lack reading comprehension, I promise you your RX 6700 is still going to cook on your 1080p monitor.",hardware,2025-10-31 22:03:31,5
AMD,nmea34q,"Sure, because me, a RX 6950 XT owner is totally an agent for Team Green.  No, sorry that I'm angry that my 3 year old GPU is going to be thrown in the trash to be deemed as a legacy GPU only deserving of maintenance driver support. I still want INT8 FSR4 as the leak shows its a perfectly viable option if AMD actually just finished it up and shipped it officially to these GPUs.",hardware,2025-10-31 16:36:02,18
AMD,nmeack0,"I do think the community here largely overreacted (and I'm still using RDNA 1 myself), but isn't surmising that the angry Redditors are paid off by NVIDIA or something like that a bit much? It's not strange to be unhappy that two- & three-generation-old products are no longer being prioritized in driver updates.  That said, maybe this is a bigger deal for those who only or primarily use Windows than it would be for a Linux user like me, since there aren't wider community projects like Mesa to pick up the slack.",hardware,2025-10-31 16:37:19,-5
AMD,nmjfn2f,"I feel like a jumbo jet would fly through that gap! It's very ambiguous, agreed.",hardware,2025-11-01 14:39:23,7
AMD,nmfbn6k,That's a very expensive typo.,hardware,2025-10-31 19:46:47,81
AMD,nmhftf8,"They typo'd an entire fucking sentence of 60 words and 431 characters? That's one **hell** of a typo there. I'm gonna press X to doubt. This is backpedaling, not a typo.  Just yeah, sorry but no, one does not simply accidentally type out ""In order to focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 places Radeon RX 5000 series and RX 6000 series graphics cards (RDNA 1 and RDNA 2) in maintenance mode. RDNA 1 and RDNA2 graphics cards will continue to receive driver updates for critical security and bug fixes."" or is this the cat walked on keyboard version of monkeys and typewriters?",hardware,2025-11-01 03:58:59,40
AMD,nmiw0ui,> RDNA1 and 2 are being moved to the maintenance branch and so won't be updated the same as the main branch.  But that is standard for all GPUs from 2 gens previous.,hardware,2025-11-01 12:37:41,-6
AMD,nmeg61c,"There's also a chance it was intended, and they backtracked that decision due to backlash.",hardware,2025-10-31 17:05:40,85
AMD,nmeffn4,"You can't seriously blame the tech community for taking AMD by their word, this is solely on AMD.",hardware,2025-10-31 17:02:03,62
AMD,nmgtrzg,I guess we're not calling them amdunboxed today then?,hardware,2025-11-01 01:19:11,3
AMD,nmeuajd,"As someone who is still rocking a 6700xt, I was extremely skeptical of the claim. Glad they were wrong, but that was pretty damaging. So many viewers were claiming they won't buy AMD next over this.",hardware,2025-10-31 18:16:25,7
AMD,nmhcfdr,Since we’re just making shit up - I posit that Hardware Unboxed got them to change their mind and you should be thankful,hardware,2025-11-01 03:31:46,2
AMD,nmeqpf6,HardwareUnboxed's WHOLE BUSINESS PLAN is based on outrage. All they do is whine and complain and attack.  They keep reviewing cards on ultra settings and whine about VRAM when they themselves made a video about how ultra settings are dumb.  Stupid and mostly useless channel,hardware,2025-10-31 17:58:03,-20
AMD,nmgam91,"> They need to be more clear about what's actually happening.  I wonder if they got a call from Gabe.  Jokes aside, the Steamdeck trying to act like a console means that AMD really has to continue RDNA2 support for quite a long time.  This is just one more reason we need a standardized GPU ISA.",hardware,2025-10-31 23:11:18,11
AMD,nmijqe0,It means they'll do shit unless the game crashes at launch and won't run at all.,hardware,2025-11-01 10:55:16,15
AMD,nmvqmtp,It means they can do whatever they want with no consequences and the fans will eat it up.,hardware,2025-11-03 14:09:42,1
AMD,nmeqc23,"They are really not.    I dont know what people think is going on here, but when Nvidia/AMD offer 'Day 1 Game optimizations', they are for specific major releases, where they take some extra effort to try and boost what performance they can from their end of things, via driver tweaks and optimizations.  It usually doesn't even amount to a whole lot, and often only for specific situations.   You do NOT need these 'Day 1 game optimizations' to run games properly.  They are basically a nice little perk.   And the point of only focusing on the latest architectures is that they can put more priority and effort into them, granting some presumably slightly better results in doing so.   It really is exactly what Nvidia have been doing forever.  Many people seem to not understand that just cuz they get access to the latest 'Game Ready Driver' or whatever does not mean there's anything in that driver that will benefit you if you're on a somewhat older generation GPU.  I'm still getting Nvidia driver updates for my GTX1070, but I'd be a fool to think Nvidia were actually still taking any effort to optimize things in the very latest games for Pascal GPU's! lol   It's absolutely insane how people are overreacting to this.",hardware,2025-10-31 17:56:11,42
AMD,nmew3cg,Most  people do not even know these drivers exists for their GPU's.,hardware,2025-10-31 18:25:46,12
AMD,nmfmqky,"They are irrelevant unless a game has severe bugs.  Because Nvidia slapped a marketing term on it (""game ready"") people think hardware has to have updated drivers to support software.  It's completely asinine.  You might as well say you need a new printer driver for every new program you print from.",hardware,2025-10-31 20:46:01,9
AMD,nmvqpkn,Market needs imply they will do whatever they want and call it market needs.,hardware,2025-11-03 14:10:07,1
AMD,nmfs691,AMD still updates the drivers for Vega. You guys just love to doom and gloom.,hardware,2025-10-31 21:16:14,-4
AMD,nmeu6ni,"Losing 5-10% performance is exactly the problem, given how heavy modern games are and how popular RDNA2 is. Sure, RDNA1 doesn't support DX12 ultimate and can't handle mesh shaders, maybe you can make an argument about moving that to maintenance branch. But RDNA2?   I could've begrudgingly accepted this sun-setting had it been done in October of 2027, when the RX6000 series would've turned seven years old. But two years after last RDNA2 dGPU launch? With RDNA2 powered APUs, devices and laptops being launched in 2025? With RDNA2 RX6600 being the most popular AMD card in Steam hardware survey?   Sorry, basic game driver support with AMD's track record regarding maintenance drivers is not enough for me as a consumer at this point in time, and I will absolutely hold it against a company that cuts support far sooner than the competition. AMD can afford to provide full game driver support for an architecture they are using in current products for another year or two.",hardware,2025-10-31 18:15:52,25
AMD,nmglwk9,"Yeah this is my take on it too. 6000 series will get normal driver support, just not the specific optimizations for games. Basically they took away the ""fine wine"" but still support the product generally, kinda like how nvidia does for older architectures.",hardware,2025-11-01 00:26:00,5
AMD,nmeyybq,"I keep seeing AMD doing the hard work then somehow screwing up the last mile - often in communication (be it with consumers or other devs).  There are some simple uncontroversial facts - ""Newer hardware"" has more development focus than older hardware. Some feature work requires hardware support from more recent devices. This is true for every vendor, not really mentioned but generally understood, even if not consciously, by the community. Even if they had a million more skilled engineers eclipsing any possible competition, there would still be that bias in focus.  But then AMD keep shouting those facts in the absolute *worst* way possible, making it easy to dunk on them and get headlines, and any ""clarification"" just sounds like weaselly corporate backtracking.  It's super frustrating for someone who works in the industry (and even used to work on AMD drivers). And honestly, not just this ""marketing""-level communication - I have pretty big issues in other areas that may be less obvious to consumers, they seem to pay for the majority of the work, then spoiling everything in the last mile by doing something dumb like banning interaction with gamedevs not through a single small overworked team that you know will *never* get around to your request, even if it could save *significant* development time.  Maybe AMD just *really* don't want to be a consumer device company? And *really* want to sell IP to others who them productize? But that doesn't seem to match their strengths - it feels like they could do so much better with *so little* effort.",hardware,2025-10-31 18:40:28,2
AMD,nmerxzu,This is irrelevant to consoles.    Steam Deck is Linux.,hardware,2025-10-31 18:04:18,16
AMD,nmec3x7,"Because RDNA 1 is older, nowhere near as widespread (RDNA 1 is only in the RX 5000 series, a fairly small generation, whilst RDNA 2 is in RX 6000, a big one, AND APUs, AND consoles!), and is missing key hardware features (namely mesh shaders and hardware RT) which means it’s aging pretty poorly.",hardware,2025-10-31 16:45:57,42
AMD,nmebfyy,"RDNA1 is old enough at this point, without support for DirectX 12 Ultimate.  Nvidia honestly has the edge in that they’re still technically supporting Turing, but I’m at least willing to let “underdog” AMD let go of an architecture that can no longer keep up with modern gaming feature sets.",hardware,2025-10-31 16:42:42,23
AMD,nmei91m,Lack of DX12 ultimate support,hardware,2025-10-31 17:15:56,6
AMD,nmedddp,"ahhh, now I understand   or if the publisher pays AMD for support...",hardware,2025-10-31 16:52:04,22
AMD,nmf675s,Which no company does after one generation leap to increase specific card performance.,hardware,2025-10-31 19:17:50,6
AMD,nmeep6j,Right? Lmao love this,hardware,2025-10-31 16:58:32,5
AMD,nmfw5h2,"Ah, so if they weren’t doing anything, then why bother making this announcement of change then?     Why bother putting RDNA 2 and 1 into “maintenance” mode if nothing has really changed and this actually doesn’t really save them money?",hardware,2025-10-31 21:39:21,13
AMD,nmhrbtv,RTX20 series is still getting that new DLSS update that improved its quality.,hardware,2025-11-01 05:46:13,14
AMD,nmeh3pc,"I don’t care what NVIDIA thinks. I only care about the value I receive for my investment, and NVIDIA is bodying Radeon in that regard.   If I bought an NVIDIA GPU when I bought my 6700XT I’d now be enjoying DLSS4 and FULL driver support for newly launched games.",hardware,2025-10-31 17:10:16,16
AMD,nmeqx1o,"They at least pretend to care, and have for decades.   Even RTX 2000 series gets the latest DLSS upscaling. AMD legitimately leaked a FSR4 build that's compatible with RDNA2/3 yet pulled it like it never existed.  Also, 7% may sound small, but nVIDIA reported $130b revenue. At 7%, thats 9 billion dollars, which is a lot of money.",hardware,2025-10-31 17:59:07,8
AMD,nmhzvfo,Source: AMD.    Dont see any conflicts of interests here.,hardware,2025-11-01 07:21:05,5
AMD,nme81k4,"No he just accurately reported the info.   AMD just gave a ""well he's right but market needs and blah and blah and blah"" save face attempt.",hardware,2025-10-31 16:25:55,32
AMD,nme8at7,Hard not to when there's so much to be outraged about.,hardware,2025-10-31 16:27:10,9
AMD,nmeenqo,HUB has been an outrage farmer for years now.,hardware,2025-10-31 16:58:20,-4
AMD,nme7dua,*Did you mean*: **Gamers Nexus**?,hardware,2025-10-31 16:22:37,-3
AMD,nmf71k4,How DARE he ATTACK my previous little multi billion dollar company!,hardware,2025-10-31 19:22:15,4
AMD,nmhzs19,Nvidia supports GPUs for much longer.,hardware,2025-11-01 07:19:59,7
AMD,nmebxi1,"They sure as hell didn't stop making them 5 years ago. 2 years ago there was still plenty of stock of RDNA 2 cards left at retailers. Today you can still buy plenty of APUs with RDNA 2 integrated graphics. Hell, Zen 3 APUs come with Vega iGPUs, and those have been on maintenance mode for a while even though you can still easily buy them new.",hardware,2025-10-31 16:45:04,21
AMD,nme8w4x,Umm but it does? They should stop selling them if they no longer plan to support?,hardware,2025-10-31 16:30:06,12
AMD,nmeb5o9,"A more favorable reading is that AMD has finally given in and accepted defeat by acknowledging that nvidia's path they started with Turing was the right and only way forward and they decided to abandon all their half-baked solutions so they can focus on the actually competitive product RDNA4.  However what AMD needs to do, to make that credible is a public 7-year driver commitment to RDNA4.",hardware,2025-10-31 16:41:18,14
AMD,nmgfnvw,Are people actually surprised that they might not optimize four and five year old card drivers for GTA 6 that you’re going to have to run at medium or lower settings without dedicated AI acceleration?,hardware,2025-10-31 23:44:03,-5
AMD,nmh843u,"Thanks for your reassurance random amd person not affiliated with the company, but that still doesn't change the fact that they are making their intentions clear: they will not support these generations of cards at the same level that has been customary in the industry up to this time.  And for that reason their value has declined drastically.  Most folks don't upgrade more than every three generations, and when AMD deprecates their support for a generation that is still on sale right now that's anti-consumer and a bad deal.",hardware,2025-11-01 03:00:02,1
AMD,nmea8j8,"AMD has never been unethical or hostile to consumers, really they deserve the benefit of the doubt here.",hardware,2025-10-31 16:36:47,-25
AMD,nmeb543,only real human beings use AMD,hardware,2025-10-31 16:41:14,-11
AMD,nmeaq47,"Wouldn't be the first time Nvidia paid for astroturfing.  Same shit happened when their Fermi aka Thermi went up in fire, same when GTX900s were catching fire due driver issues, then again with RTX20s memory deaths, then with RTX40/50s melting connectors.",hardware,2025-10-31 16:39:10,-16
AMD,nmhg29s,"And a 60 word, 431 character ""typo.""  Either we have experienced the cat walking on keyboard version of infinite monkeys on typewriters, or there's a bicycle going in reverse at AMD headquarters. I'm going with the latter.",hardware,2025-11-01 04:00:59,34
AMD,nmfuqbw,"I'd hate to work in PR for a major company  imagine sitting and staring at what's supposed to be a routine statement, hoping you understand all the things you're supposed to and have somehow delivered that in clear language to average people, the fear that you could somehow blow up millions of dollars (or more) because you didn't phrase something exactly, *perfectly* right always in the back of your mind",hardware,2025-10-31 21:31:02,23
AMD,nmnqfv1,AI can make much more impresive typos than humans.,hardware,2025-11-02 05:19:29,2
AMD,nmo066m,"That's not true at all. The 3000 series gets the exact same driver updates as the 5000 series and gets them on the same day... actually, this is true for the 2000 series as well. The 1000 series *just* went EOL (i.e, only security updates), and that is with the caveat that Windows 10 users actually still get game ready drivers until October of next year.",hardware,2025-11-02 06:52:49,4
AMD,nmehzvy,They backtracked imo.,hardware,2025-10-31 17:14:41,50
AMD,nmf3nze,They definitely backtracked in my opinion,hardware,2025-10-31 19:04:31,14
AMD,nmemi2v,"possible but unlikely. but even without this new article, some people did understand what they already meant. a bunch of comments on the radeon sub and a few posts too, has been saying, from the first article, it was never said AMD was DROPPING support for those cards, but hbu on the first few minutes of their video and i quote ""They're effectively ending driver support for RDNA1 and RDNA2 graphics cards by no longer providing those models with game specific optimizations"" OPTIMIZATIONS, not support, not drivers, OPTIMIZATIONS.   It's just sad half of the people do not understand the difference.",hardware,2025-10-31 17:37:12,-20
AMD,nmfgkls,I considered hwunboxed response melodramatic and unprofessional.,hardware,2025-10-31 20:12:56,-8
AMD,nmel958,"but they were quick to misconstrued an article they did not fully understand, which in turn, people preaching their word as the truth.",hardware,2025-10-31 17:30:54,-28
AMD,nmevis7,"We need skepticism to be better, and as much as I don’t care about AMDs sales, I care about having options for hardware. Today has been an eye opener, i never knew people could be so loyal to a brand/company or a group of people they don’t actually know apart from daily review videos.",hardware,2025-10-31 18:22:49,4
AMD,nmesow2,"welll, don't know about that, my issue with them is just the misinformation and causing a panic or outcry when there shouldn't be any. AMD fucked up with their announcement and choice of words, and started a flame, then hbu poured gasoline on that thing",hardware,2025-10-31 18:08:10,-1
AMD,nmh12np,"The Steam Deck uses the open source Mesa driver, and does not necessarily require support from AMD",hardware,2025-11-01 02:09:46,24
AMD,nmg2sou,there have been a few games out there where i try running them without the latest drivers and experience a stuttery fuck fest but after installing the drivers get a smooth experience. don’t have any examples off the dome cause it’s been a while since i haven’t had updated drivers but it certainly still happens (4080 so not like im running some shit hardware either),hardware,2025-10-31 22:20:09,7
AMD,nmetbj2,"mate, you need to be at the top of every post relating to this, but i don't think people will still understand how unimportant 'Day 1 game optimizations' really is. oh and, if you've seen hbu's video, you'd understand why these people are overreacting",hardware,2025-10-31 18:11:24,13
AMD,nmfcaoa,"1070 is 9 years old, RDNA2 was refreshed 3 years ago, yes it's not necessary but still a slap in the face, especially considering the overwhelming feature gap between RDNA2 and RTX 30 series and even the older RTX 20 series which got partial support for DLSS 4.",hardware,2025-10-31 19:50:16,12
AMD,nmfkvbz,">They are really not  They really are. Performance optimizations are one thing,  but, unless something changed, these drivers also include a lot of workarounds / fixes for ""quirks"" (e.g. API misuse so driver literally has to ""fix"" it)",hardware,2025-10-31 20:36:03,5
AMD,nmfe92f,">It's absolutely insane how people are overreacting to this  tbf this is just classic AMD being dumb at PR as usual. I don't understand why they didn't just do what Nvidia did and just shut up. They only need to announce something when it's not being updated anymore. Somehow they need to make different wording for some reason.  For example Polaris, RX 580 is still trade blows with GTX 1060, but some people afraid using 580 because its driver isn't updated frequently anymore compared to 1060.",hardware,2025-10-31 20:00:34,3
AMD,nmfhgtt,"You're completely incorrect, your 1070 is absolutely getting optimisations done in modern titles,  the gains may not be as big as newer HW, but there's absolutely a benefit from installing game ready drivers.",hardware,2025-10-31 20:17:43,-5
AMD,nmfrd94,These are installed automatically via windows updates nowdays,hardware,2025-10-31 21:11:37,2
AMD,nmfhw4u,"Laptop users maybe, anyone with a dGPU knows about driver updates, the software gives a notification when there's a new one",hardware,2025-10-31 20:19:59,1
AMD,nmg47do,"Not at all, as a printer either works or it doesn't, they each have their own driver, but they interact with windows in the same way, the same printer popup appears for formatting the print and vinfirming the size, whether it's portrait or landscape, etc.  Each game engine had different demands on the hardware that impact different architectures in different ways.  If it was as easy as you claim there would just be a standard driver for everything and it would automatically run great on everything.",hardware,2025-10-31 22:29:18,2
AMD,nmfhegn,Why not expect the game developers actually doing that work and optimise their code for the target HW?,hardware,2025-10-31 20:17:22,9
AMD,nmgembw,"You're not 'losing' performance that you never had, though.  Another thing that's important to note here is that throughout a GPU generation's lifetime, vendors are constantly trying to improve *general* performance via drivers.  After 2-3 years, these optimizations are usually pretty mature, and thus there's simply much less juice to squeeze out of older architectures with any kind of game-specific optimizations.  As for holding this against AMD, again, this is Nvidia's standard practice for a long time.  It makes perfect sense to prioritize newer architectures in newer games where there's perhaps more juice to squeeze, and can maybe even learn and improve general driver gains as well.  Y'all are so stuck on your initial outrage-based reactions, you're obviously going to keep trying to argue this somehow, though.  Cant admit you're wrong on the internet!",hardware,2025-10-31 23:37:14,3
AMD,nmgm7k7,Yeah I aint super happy about this especially since 6000 series cards were offered next to 7000 series for most of their lifespan but at least it isn't cutting driver support entirely. I would agree 2027 is a more reasonable timeline to start sunsetting rdna2.,hardware,2025-11-01 00:28:06,1
AMD,nmn2rbe,That 'Fine Wine' was always about having poor day 1 driver support really.  And also having forward thinking features that weren't well supported in their day.  And having plenty of memory. lol,hardware,2025-11-02 02:37:51,0
AMD,nmgeob8,"It’s pretty clear that AMD sees the enthusiast graphics card consumer as a test market for newer features they’ll later productize for console and other licensed products. It makes sense too - big orders at higher margins, and lower potential losses (no buybacks) if the end product doesn’t perform well.  Now that PS6 development is in full swing, it makes sense that they’re redirecting their dev resources toward the resources that will actually apply to that GPU.",hardware,2025-10-31 23:37:35,6
AMD,nmgf0cm,AMD of course have plenty of deficiencies compared to Nvidia in terms of software support.   But this example isn't one of them.  They're literally matching Nvidia's standard practice.,hardware,2025-10-31 23:39:46,1
AMD,nmevcrc,"Yeah thanks I'm aware the PS5 doesn't run Windows.  What I'm saying is, this is a supported product in some sense, why drop support solely for Windows?  That's confusing.  Plus they probably have team members actively familiar with RDNA2, surely some of that knowledge is portable across operating systems from an institutional standpoint.  Obviously there is additional Windows specific work that will need to be done, which has a cost, but again probably still easier than if Windows was the only platform these chips were used in.",hardware,2025-10-31 18:21:57,4
AMD,nmekzpl,"Point taken. I'm not sure why this would warrant throwing an entire gen under the bus like that, were the older gens the same ? Not sure.",hardware,2025-10-31 17:29:37,3
AMD,nmeh1zo,"To me it would be the exact contrary especially on these days and ages ( let's not ignore the sad state of that market )  Nvidia can get away because they are leading.   Either way, people are very quick to accept how the news dropped while RDNA 1 having (stated) updates a month ago wasn't even an afterthought, I find it a bit bizarre.",hardware,2025-10-31 17:10:02,5
AMD,nmgfl53,Right.  Nvidia has always 'supported' Day 1 optimizations for their two most recent generations.  And that's it.  Exact same thing.    It would take insane amounts of human resources to optimize every game for every generation of GPU with very unique and game-specific Day 1 driver optimizations.  Of course they have to prioritize.    This probably isn't even a new practice for AMD.,hardware,2025-10-31 23:43:33,-3
AMD,nmik4cx,"No, you are making that up",hardware,2025-11-01 10:58:56,5
AMD,nmnllqs,"Maintenance means the driver is considered stable and will not receive updates introducing new functionality, instead maintaining the existing functionality and making fixes for any significant problems that happen in the future. If a new game comes out that is unplayable for a non-hardware issue then it would be the job of the remaining maintenance team to work on a fix. The rest of the people who used to work on developing the drivers for these cards are likely being moved so they can focus their efforts on improving newer cards.",hardware,2025-11-02 04:41:30,0
AMD,nmn18fg,"That's a feature, not a Day 1 game-specific driver optimization.  It's very depressing that y'all dont understand the difference.  It shouldn't even need explaining.  The ONLY thing AMD talked about here was about the Day 1 Game-specific optimizations.  Not feature support.  Or even general driver support.  Not sure I've ever seen a bigger example of making a mountain out of a molehill in the tech community.",hardware,2025-11-02 02:28:38,0
AMD,nmev6pc,"If you bought an Ampere or older generation, you would not be getting any Day 1 optimizations, either.  Something like DLSS4 is general feature support and not what is being talked about here.  AMD can still offer such feature support for older GPU's where applicable.",hardware,2025-10-31 18:21:04,-3
AMD,nmeoxw1,You would have 8gb of vram tho instead of 12,hardware,2025-10-31 17:49:15,-9
AMD,nmf6jth,"Oh look here, appears you guys are raging over nothing.🙄  https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games",hardware,2025-10-31 19:19:40,-4
AMD,nmft13h,> No he just accurately reported the info.   They said AMD was ending hardware support on RDNA1 and 2. Even if you read the initial announcement from AMD. They never said they were ending support. They just said they moved RDNA1 and 2 to the maintenance branch.  Most readers now think RDNA2 is SOL. When this isn't the situation at all.,hardware,2025-10-31 21:21:10,-4
AMD,nmfsrvz,Let's not pretend like these guys don't love drama. We are in a quiet period as far as hardware releases were concerned. They jumped on this as soon as they could.,hardware,2025-10-31 21:19:41,0
AMD,nmgfl4x,I don't even have a PC mate stop choking  the drama farming's dic,hardware,2025-10-31 23:43:33,-1
AMD,nmn0yxl,"With Day 1 game-specific optimizations?  No, they really dont.    You guys are pretty much all massively misled on this.",hardware,2025-11-02 02:27:01,0
AMD,nmeaapn,There are still GTX cards sold in the online stores. Does that mean that each and every card which still might be sold has to be supported?  But I still don't like the fact that 5 years are said to be  enough for full support. The 6000 cards are fine as they are and would easily have 2-3 more years of life in them for great gaming experiences,hardware,2025-10-31 16:37:04,-1
AMD,nmebkds,then why havent they said this? this is just conjecture,hardware,2025-10-31 16:43:18,1
AMD,nmgg6zw,"Again, all these GPU's are still gonna get driver updates.  Just not Day 1 optimizations for the latest games.    It's no different than Nvidia.",hardware,2025-10-31 23:47:31,-1
AMD,nmvskzi,>Are people actually surprised that they might not optimize four and five year old card drivers for GTA 6  Then will be when the competition will optimize their 8 year old cards (2000 series).,hardware,2025-11-03 14:20:09,1
AMD,nmgn8ug,It's clear from the first topic about this that a large majority of people in this sub dont have the first clue how any of this actually works.  Even at a fairly basic level.,hardware,2025-11-01 00:35:13,-1
AMD,nmf77pt,"Lmao what?  AMD is barely any better than Nvidia.  Actually scratch that, their naming scheme for laptop CPUs is downright predatory.",hardware,2025-10-31 19:23:10,8
AMD,nmecpps,Huh? They very much have! There was e.g. that incident when they tried to [restrict](https://web.archive.org/web/20200519220759/https://images.anandtech.com/doci/15774/Ryzen%203_B550_Press%20Deck_NDA%20Until%20May%207th-page-008_575px.jpg) Zen 3 to B550 and X570 chipsets. They only walked back that decision after an intense community backlash.,hardware,2025-10-31 16:48:54,22
AMD,nmgs7fo,Now I have to wonder who's the agent here...,hardware,2025-11-01 01:08:21,5
AMD,nmhlno2,LOL  It's a corporation. Fucking over the consumer is profitable. They all do it.,hardware,2025-11-01 04:50:10,4
AMD,nmhjbxj,Bingo,hardware,2025-11-01 04:29:01,2
AMD,nmgjfdq,"Well, this is AMD we're talking about, their PR people are probably 3 unpaid interns and one WW2 vet, all furiously knocking back black coffee and chain-smoking.",hardware,2025-11-01 00:09:11,26
AMD,nmojwtf,"Hmm, yeah I suppose feeding it into an LLM and not verifying is also another ridiculously stupid possibility.",hardware,2025-11-02 10:17:48,1
AMD,nmo0ui4,Getting the same drivers is not the same thing as those drivers containing game optimisations for those cards.,hardware,2025-11-02 06:59:42,2
AMD,nmfsys9,"I am not sure they even did that.  ""New features, bug fixes and game optimizations will continue to be delivered as required by market needs""  That is typical corporate speak. In no way shape or form did they say that RDNA2 will get similar first day driver support as RDNA3 and newer.   If all they do is fix the worst performance issues and straight up none working games, then that statement still holds true.",hardware,2025-10-31 21:20:47,34
AMD,nmg2ler,"All we can do is assume....... Only they know the truth, so instead of arguing, let's share some love.",hardware,2025-10-31 22:18:50,-1
AMD,nmg2kbe,"All we can do is assume....... Only they know the truth, so instead of arguing, let's share some love.",hardware,2025-10-31 22:18:39,-1
AMD,nmp8u4v,"HU and GN are goats, although some of their stuff has affected others (like LTT) in a negative way but they are mostly good.",hardware,2025-11-02 13:35:37,1
AMD,nmeosh5,"so, as you understand what they meant, can you explain it what they meant, and what the changes for owners will be?  because even with this, I do not understand clearly what they want to say.",hardware,2025-10-31 17:48:30,19
AMD,nmg3woj,"All we can do is assume....... Only they know the truth, so instead of arguing, let's share some love.",hardware,2025-10-31 22:27:21,1
AMD,nmiicbd,"*In hindsight after AMD literally (implied they) backtracked.*  What?  More than that, they aren't promising anything.",hardware,2025-11-01 10:42:05,1
AMD,nmemaue,"Not misconstrued. They either changed their mind, or fixed a mistake or internal miscommunication.",hardware,2025-10-31 17:36:11,30
AMD,nmh6t11,"Last I checked, AMD devs invested millions of dollars in manpower to Mesa. If they dial back all that support, it will certainly make a big difference in real-world support.",hardware,2025-11-01 02:50:34,10
AMD,nmgdyv1,"That's just random driver fuckery, though.  Can happen with older or newer GPU's.  Heck, even among people with the same GPU and the same drivers, some people might experience issues that others dont.    That's not what this is about, anyways.  Obviously if there was some major release and older AMD GPU's were performing way under what should be expected, they can still address that.  That's a different thing.",hardware,2025-10-31 23:32:58,-4
AMD,nmfh9no,"No, he doesn't, to say day 1 drivers aren't important is complete misinformation.  Back when AMD gave more detailed info on their driver pages, there were regularly 10+% gains from day 1 drivers in optimized titles.",hardware,2025-10-31 20:16:39,-2
AMD,nmgckq6,"DLSS is a feature.  It has nothing to do with Day 1 game optimizations.  Feature level support can be backported to older gens where applicable still, they aren't denying that at all.",hardware,2025-10-31 23:23:57,1
AMD,nmgcs0e,And AMD are making it clear that they'll still work on those sorts of issues.  Which of course they would.,hardware,2025-10-31 23:25:16,-2
AMD,nmi220z,"Can you give an actual example of this from the last 5 years? This is just ""trust me bro it works"" stuff, evidence please.",hardware,2025-11-01 07:45:56,-1
AMD,nmgcs3s,There’s literally nothing they could say that wouldn’t have the streamers screeching about “anti-consumer practices”,hardware,2025-10-31 23:25:16,3
AMD,nmfpsel,Nvidia doesn't test game performance on Pascal anymore. Not for a long time. Any performance optimization is just a secondary benefit of optimization targeting newer archs.,hardware,2025-10-31 21:02:36,5
AMD,nmgcxg1,"r/confidentlyincorrect   You have no idea how insane it would be for Nvidia to be still making specific effort to optimize driver code for specific modern games with Pascal GPU's in mind.  Like, you might as well believe that Jesus is looking out for you.",hardware,2025-10-31 23:26:14,-1
AMD,nmi24kj,Can you give one actual real world example?,hardware,2025-11-01 07:46:44,-4
AMD,nmg0ou9,"Windows Update famously replaces new (manually installed) drivers with older ones, especially with AMD...",hardware,2025-10-31 22:06:46,7
AMD,nmhshei,Who doesn't debloat windows and turn off non critical updates?,hardware,2025-11-01 05:58:36,-2
AMD,nmfj5x3,"The vast majority of casual PC gamers, even those with dGPUs, do not update anything unless they are forced to. I have a friend who only updates drivers on his 6700XT when a game flat-out refuses to launch.",hardware,2025-10-31 20:26:48,12
AMD,nmi2cat,Can you give an actual real world example of a game benefiting from a driver update in the last 5 years on a 10's series card?  You are just making stuff up dude.  > If it was as easy as you claim there would just be a standard driver for everything and it would automatically run great on everything.  This is literally the experience we all have every day on AMD and Nvidia GPU's....we all own this stuff you can't lie like this to our faces and get away with it lol.,hardware,2025-11-01 07:49:08,1
AMD,nmftbbf,"In an ideal world yes, developers would fix their software. Given how game development is with layoffs and churn these days, I don't have high hopes for that.",hardware,2025-10-31 21:22:47,13
AMD,nmgetuf,Drivers are black boxes.  Developers have no clue what's going on inside that code.  And there's no such thing as 'target hardware' on a PC.,hardware,2025-10-31 23:38:35,6
AMD,nmgdvcq,"People do expect game developers to optimize their games. However, the GPU manufacturers also release optimized game drivers for their cards. People aren't asking them to start supporting older hardware, they are asking them continuing to do something that their customers expected when they bought the card.   At the end of the day, it doesn't really matter who ""should"" be responsible for optimal performance. GPU manufacturers have built the expectation of proving some of it, so their customers get a bit pissed if they abandon performance optimizations.",hardware,2025-10-31 23:32:20,7
AMD,nmvrdu9,Because they wont and youll be left with your pants down.,hardware,2025-11-03 14:13:46,1
AMD,nmi4srw,"You are very insistent about that nothing has changed, but then why call it maintenance mode? Why not avoid visibly splitting the driver stack and quietly do the same? It's baffling that they would admit to it. Sorry, I can't as a consumer give a billion dollar corporation the benefit of the doubt.  And AMD blindly copying Nvidia only in the negative sense is honestly one of the unstated problems I have with this.   Nvidia wasn't selling RTX3000 based gaming APUs en-masse to be used in devices 2025. AMD is doing that with RDNA2. It doesn't matter that the architecture is old, if you keep using it in products five years after launch, consumers will expect support reflecting that fact.   Meanwhile RTX 2000 series received DLSS4 upcaling support, a GPU series that launched in 2018. AMD leaked Int 8 FSR4 that can run on RDNA2 and RDNA3, which I've tested via third party tools. I rather doubt maintenance mode driver includes support for such improvements.  Bluntly, I expect better support from a company that is selling a less capable product with an inferior feature set, not worse support. If you only get as much game optimization as with Nvidia, supposedly, and no backwards porting of improvements then why on earth would any consumer even consider AMD in the GPU space at this point?   They have worse upscaler, worse game support, worse feature set (ray tracing, streaming, productivity) and worse resale value.   You are framing this as outrage, rather than this being the last straw. For me, it is very much the latter.",hardware,2025-11-01 08:16:28,9
AMD,nmn3rms,Well that and back in the gcn days amd seemed to iterate more and as a result supported their architectures for far longer. Nvidia on the other hand kept reinventing their tech every couple generations leading to more fragmented support. Rdna is more like nvidia support wise and it's harder for amd to support over longer period of time like they did.,hardware,2025-11-02 02:44:13,2
AMD,nmfafg7,"AFAIK Linux doesn't have day 1 game ready driver anyway so it's really Windows specific. And I hate to say this bevause this looks like I'm defending AMD (it's not, also they disabled the usb c for whatever reason) but day 1 game ready driver honestly barely matters.",hardware,2025-10-31 19:40:15,10
AMD,nmgf7ij,They aren't 'dropping support'.    They're just not getting Day 1 optimizations for very specific major releases.  That's it.    AMD is simply more openly admitting what Nvidia have also been doing for like the past 15+ years.,hardware,2025-10-31 23:41:04,0
AMD,nmjbvst,This is false. 3080 got bf6 update,hardware,2025-11-01 14:17:45,3
AMD,nmeqg1e,I’m not sure if that +4GB VRAM outweighing not having DLSS4 and full driver support.  Radeon team are the ones putting that very question into my mind. Nice work by them!,hardware,2025-10-31 17:56:44,12
AMD,nmf69hs,Extra VRAM is pointless if your GPU isn't getting proper updates  8GB you can use vs 12 you can't,hardware,2025-10-31 19:18:10,8
AMD,nmeu3np,What do you mean 'would'?  What do you think has actually changed here?  I think y'all dont realize that most of the *general* optimizing for RDNA2 for gaming has been done by now.  That general driver support is pretty mature.  These Day 1 optimizations are more about game-specific bonus optimizations for major releases.  You'll *maybe* get like 5% more performance or something.  Little to nothing will change in regards to how RDNA2 will run new games.,hardware,2025-10-31 18:15:26,-2
AMD,nmgu1tp,> all furiously knocking back black coffee and chain-smoking.  They don't seem that alert.,hardware,2025-11-01 01:21:04,21
AMD,nmvq5m5,game optimization stopped for 1000 series last week. It lasted almost 10 years.,hardware,2025-11-03 14:07:05,2
AMD,nmg4sl9,I'm sitting here scratching my head over the linked Tom's Hardware article because the title doesn't seem to be supported by anything in the article itself.,hardware,2025-10-31 22:33:09,16
AMD,nmeqwzw,"i commented this on another sub, but in my eyes, these announcements are just AMD saying that they have extracted all they can from RDNA1 and RDNA2 cards, performance and feature wise and will no longer pursue developing new features for it. Which, indeed sucks being it a relatively new, well the RDNA2 ones, but again, they have cheaper products compared to Nvidia so you basically get what you pay for.  The annoying part about this whole thing is that people are acting like children without understanding the reality of the situation.",hardware,2025-10-31 17:59:06,-13
AMD,nmepxtx,"Not miscontrued? hbu straight up said AMD was ""effectively ending driver support"" which hey, they are not, there is a big line between ending driver support vs not developing new features and/or no more game optimization updates, but i guess people are just dumb and make mistakes.",hardware,2025-10-31 17:54:12,-12
AMD,nmnjmw3,"In that case though ""dialing back support"" would be just moving the engineers they were already paying for to working on features for newer cards instead of spreading them out across cards they want to start moving away from.",hardware,2025-11-02 04:27:53,2
AMD,nmg9g25,Trusting those numbers to not be incredibly cherry picked or straight false is crazy,hardware,2025-10-31 23:03:39,2
AMD,nmgxr67,"Yes but is putting RDNA2 in maintenance mode really a good sign it will continue recieving new festures? And keep in mind everyone here is kinda excusing RDNA1 even tho RDNA1 is as old as Turing which is still fully supported, the point is that game optimization is frankly not all that big of a deal, but the fact Nvidia not only provides optimizations for a generation older than RDNA2 but also still giving them brand new cutting edge features really highlights how bad this is. RDNA 1 and 2 launched with a feature gap vs Nvidia that only widened over time and now they're giving up on them sooner too.       Did Polaris or Vega recieve any new features since they were put in maintenance mode like RDNA2 now?",hardware,2025-11-01 01:46:41,7
AMD,nmgrs2k,"I mean yeah, at this point I agree. But AMD doesn't need to say ""RDNA 1 and 2 is in maintenance mode"" if the cards is still supported in the first place.",hardware,2025-11-01 01:05:25,3
AMD,nmg3iyx,"They absolutely do, hence why sometimes when a game comes out, they will note known issues with said HW in that game in the release notes, they only know that because they have tested it.  Happens quite often.",hardware,2025-10-31 22:24:51,0
AMD,nmgiz4o,It's not a difficult task to do when the architecture is so well understood by the driver team,hardware,2025-11-01 00:06:09,0
AMD,nmitqco,Normal people who aren’t conspiracy theorists,hardware,2025-11-01 12:20:46,3
AMD,nmvr3wp,People who want their windows to function properly instead of breaking basic systems then get outraged when something doesn't work.,hardware,2025-11-03 14:12:16,1
AMD,nmfknca,"That's just stubbornness, nothing to do with being casual.",hardware,2025-10-31 20:34:50,1
AMD,nmiqgun,"Every game.  They will all have a benefit even if only slightly.   This is not the experience you absolute halfwit, there's not a standard windows driver that works on both vendors that automatically allows games to run great.  Gpus need their own drivers, and need optimisations within them for each new game and game engine for ultimate performance.",hardware,2025-11-01 11:55:17,0
AMD,nmnk377,Not AMD's Linux drivers. They are open source and can even be contributed to if a developer finds a problem and makes a fix.,hardware,2025-11-02 04:30:50,1
AMD,nmn0r8d,"General driver support is going to continue, regardless of what it's called.    Again, y'all are really massively overestimating the importance and impact these 'Day 1 Game Optimizations' actually have.  That's the real argument here.  Nothing much is actually going to change for users on a practical level.   'backwards porting of improvements'  Any improvements that would be described as backported improvements will still benefit older GPU's.  You're still gonna have access to the same drivers.  Driver support isn't stopping, like some of y'all seem to think.   But most of the 'general' driver optimization improvements for older architectures have already been achieved.  There's just less juice to squeeze from these older GPU's, which is another reason it makes perfect sense to focus such Day 1 game-specific optimizations on newer architectures that dont have the same maturity of drivers.  Aka - more juice to squeeze.   But it sounds like you already had your mind made up about AMD GPU's, and so for you, this is just confirmation bias at work.  Anything to further help justify the judgement you've already made.   And to be clear, I'm an Nvidia owner.  Not an AMD/Radeon stan by any means.",hardware,2025-11-02 02:25:42,1
AMD,nmmxw3a,"Just the 3080?  Not all other Ampere parts, or any Turing parts, or Pascal parts?  Just the 3080?",hardware,2025-11-02 02:07:54,2
AMD,nmer8nl,"It still gets driver support, and +50% vram is very useful, why do you think everybody avoids 8gb gpu's  Running out of vram is the worst thing that can happen as textures has no cost of performance if you have enough vram  But yeah I do agree Dlss 4 is a big + That's why they need to make it official that the 6000 and 7000 series will support fsr4",hardware,2025-10-31 18:00:43,-3
AMD,nmfzrv7,What? Why would you not be able to use? You can still play the games even if you use an older driver lol..  I am just saying that both choice have drawback (3060 ti & 6700 xt),hardware,2025-10-31 22:01:02,-2
AMD,nmhjev9,"After a few decades of it, we're lucky they have a pulse.",hardware,2025-11-01 04:29:44,14
AMD,nmw3gbg,What game optimizations actually happened?,hardware,2025-11-03 15:16:32,1
AMD,nmeu5mb,"that interpretation doesn't match what they said  >  Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.”  To me that sounds like, oooh, your 6950XT have less fps than the 7400 and 9050XT, tough cookies, buy a new one, you get no updates for you card unless it crash the game.",hardware,2025-10-31 18:15:43,15
AMD,nmeqznv,"""RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security and bug fixes. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 is placing Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with targeted game optimizations will focus on RDNA 3 and RDNA 4 GPUs.""  This is the same thing they did to the RX 480/580 a few years ago. No more optimizations. Just critical updates for stability or security. That's what ""maintenance mode"" meant last time.",hardware,2025-10-31 17:59:28,25
AMD,nmg9siw,"They aren't false, they will be from more demanding scenarios obviously, but that's where the performance gain matters the most.",hardware,2025-10-31 23:05:54,1
AMD,nmgd920,This isn't about 'issues'.  It's about driver optimizations for slight performance boosts.  That's it.  AMD is not denying that they'd fix problems in major releases with older GPU's.  Of course they would.  They're not stupid.,hardware,2025-10-31 23:28:19,-1
AMD,nmgntyx,"No man. lol  Most of the general driver optimizations for Pascal were matured a long time ago already.  Room for improvement would be minimal.  And even less than minimal when we're talking about modern releases that demand a number of features or pure grunt that Pascal literally doesn't support at all.  Again, I think you're very much not understanding what the role of these 'driver optimizations' are.  They are 'extra boosts' when possible, and even then, those boosts are often quite limited and/or situational.  Even if what you're saying was true, it would still be crazy to be hiring entire armies of software engineers working on every single unique generation of GPU of the last 5-10 years with each new major release, just for some smallish gains in some situations.  There's no world in which that would be that a justifiable use of resources.",hardware,2025-11-01 00:39:10,1
AMD,nmi286e,Lol....you might as well believe everything works by magic.,hardware,2025-11-01 07:47:51,-4
AMD,nmku7nq,"Lol but windows is far better when you are a ""conspiracy theorist"" I bet you don't turn off ads or any spying telemetry either...",hardware,2025-11-01 19:02:53,1
AMD,nmxl57g,Nothing's broken by debloating lol. Have fun with the shittiest version of windows that everyone hates.,hardware,2025-11-03 19:35:50,1
AMD,nmf1gvq,"It still gets driver support “as required”. That’s very unspecific and worrisome.   Don’t get me wrong, I appreciate not being with 8GB VRAM.",hardware,2025-10-31 18:53:18,3
AMD,nmg3h5l,"Poorly optimised drivers will mean you won't even be able to use full performance of the card and thus, won't be able to fill enough RAM where you had any gains from having more of it.",hardware,2025-10-31 22:24:32,3
AMD,nmevscn,"And thats where AMD fucked up. But like i said in a different comment, other people did actually understand what AMD meant, maybe people who have zero understanding what these technical terms actually mean jumped into conclusions",hardware,2025-10-31 18:24:11,-2
AMD,nmf32bu,Just a simple typo paragraph. We didn’t mean to say all those words. Simply a typo.,hardware,2025-10-31 19:01:25,3
AMD,nmvr5d8,There are no ads.,hardware,2025-11-03 14:12:30,1
AMD,nmg4b1p,"Dude, rx 580 beat gtx 1060 in recent title even if the drivers are way older, rdna 1 & 2 will still receive driver updates  If you think amd drivers are poorly optimized tell me what Nvidias drivers were at the release of the 50 series lol  BTW TEXTURES HAS NO FPS HIT IF YOU HAVE ENOUGH VRAM YOU CAN RUN A GAME ON LOW WITH ULTRA TEXTURES AND IT WILL CONSUME A LOT OF VRAM  Btw im sure the 8gb r9 390 also aged better than the 3.5gb gtx 970",hardware,2025-10-31 22:29:57,-1
AMD,nmew3mw,"those words are from an AMD representative, I mean, if AMD says that in the drivers and an AMD representative confirms it, I do not think it's people misunderstanding them, but dunno.  https://www.pcgameshardware.de/AMD-Radeon-Grafikkarte-255597/News/RDNA-1-und-2-Finaler-Radeon-Treiber-fuer-Battlefield-6-und-Bloodlines-2-1485427/  And the changelog of the latest drivers say that extended vulkan is only for 7000 and 9000",hardware,2025-10-31 18:25:49,14
AMD,nmf8591,"Lol. I was trying to be generous. But yeah, it just looks like backpedaling a bad PR move.",hardware,2025-10-31 19:28:04,6
AMD,nmxkz9z,There literally are lol. The search bar on the desktop has ads by default.,hardware,2025-11-03 19:35:02,1
AMD,nmvs4q3,>rx 580 beat gtx 1060  oh nice it took them two class better GPU to beat a budget Nvidia one.,hardware,2025-11-03 14:17:44,0
AMD,nmvv7gl,"The original Manufacturer's Suggested Retail Price (MSRP) for the AMD Radeon RX 580 was $229 for the 8 GB version, while the NVIDIA GeForce GTX 1060 (6 GB) was $249 for partner cards and $299 for the Founder's Edition.   Okay buddy, 580 8gb was cheaper than 1060 6gb lmao Maybe do some research before talking",hardware,2025-11-03 14:34:13,1
AMD,nm5xsjw,TLDR: Intel and AMD warned their enclave defenses don't defend against physical tampering of the memory/motherboard. The main vulnerability for both of their enclaves was a decision of performance vs security tradeoffs. Yet dozens of companies claim their software running in those enclaves are resistant against physical tampering and thus can run on untrusted hardware.,hardware,2025-10-30 09:56:42,39
AMD,nm7zg25,"> The low-cost, low-complexity attack works by placing a small piece of hardware between a single physical memory chip and the motherboard slot it plugs into. It also requires the attacker to compromise the operating system kernel. Once this three-minute attack is completed, Confidential Compute, SEV-SNP, and TDX/SDX can no longer be trusted.   lol. You need to pull out memory, plug something between the board and the memory, and then somehow magically also modify the OS kernel at the same time. And this is supposed to be easy and quick? What a load of BS.",hardware,2025-10-30 16:56:21,27
AMD,nm63ycj,"Note that these secure enclaves are typically designed pretty differently from the one Apple used in their M and A chips, which is a completely separate CPU.",hardware,2025-10-30 10:49:23,3
AMD,nm6xot2,This is actually a pretty big deal. Effectively the key selling point for TEEs is defective.,hardware,2025-10-30 13:54:14,0
AMD,nm9ldh9,"Many places that operate these machines claim that it protects the customers from the owner of the machine tampering or spying on their data. The companies making these products do not claim this, but this has not stopped companies renting them out from claiming it.  And the kernel that needs to be ""compromised"" is not the kernel of the customer, the person adding the shim can briefly boot the system from their own drive to compromise the system.  In that sense, the attack is not unrealistic at all, the owner of a machine can easily take it offline for half an hour, add the shim, boot it from a portable drive for long enough, and then put it back into operation.",hardware,2025-10-30 21:36:28,18
AMD,nm8zk44,"That isn't particularly relevant here, from my understanding. The attack doesn't care where the data is going to, just whether they can tap into the interface and whether the encryption used is susceptible to replay attacks. The storage still sits in DRAM in the Apple design, and that data has to travel over the DRAM interface, which is susceptible to a physical MITM actor grabbing information. The main differentiator here would be whether or not the encryption is deterministic, which is where this particular vulnerability becomes effective (because once you've grabbed the right transaction, it can be used repeatedly).  From what the researchers are saying, client-side SGX used to use non-deterministic encryption; however, as the scope of protected memory ballooned (esp with TDX's use case on server), that shifted to deterministic encryption to limit the performance impact for memory regions that large. Presumably, Apple doesn't need to contain entire programs within their secure enclave (same as Intel with the original client design), so there's a good chance this attack doesn't work on them for those reasons instead.",hardware,2025-10-30 19:48:45,9
AMD,nm7hvth,"It's only become a key selling point in a specific context. Which is essentially ""people who don't know what they're talking about talking to people who lack the expertise to verify their claims"".   These kind of TEE type countermeasures, like any countermeasures, always have a specific threat model and specific security guarantees. Which, as the article points out, never really included physical tampering attacks or even side channels.  The problem is once it stops being security experts talking to security experts those details are almost immediately lost. People find it all very niggly and boring and overly detailed and full of caveats and very few concrete guarantees.   So it gets simplified. The caveats get forgotten. The details get smoothed out. Things that were ""Guarantee X under condition Y assuming 1, 2, 3, 4"" become guarantee X.  Aaaaand you get this sort of thing where some researcher proves that you can do nasty things outside of condition Y or when 3 isn't true or something. People cry foul ""but you guaranteed X under all conditions!"" and the people in the know go ""no we fucking didn't"" and point to a security spec that literally nobody ever read besides them. Everyone is annoyed and thinks someone else is to blame.",hardware,2025-10-30 15:33:04,15
AMD,nm59u2d,">Meanwhile the AMD Ryzen Zen 6 ""Medusa"" openSIL code will come in the first half of 2027.  So Zen 6 is delayed",hardware,2025-10-30 05:55:36,-12
AMD,nm5n1rp,">They plan to open-source their Venice openSIL code around one quarter after those 6th Gen EPYC CPUs ship. They say that open-source will be in 2026, so that would mean Venice is launching in Q3 or earlier.  Nah medusa's mobile which launches at ces 27. According to the same slides venice's launching q3 or earlier which confirms a 26 launch for zen6. Amd also said that its helios rack with venice and mi450 is launching around q2-q3 26 at their dev event.",hardware,2025-10-30 08:09:50,12
AMD,nm6su2b,"Medusa is the mobile lineup. i.e. presented at CES, available a few months later. Always has been.",hardware,2025-10-30 13:29:04,4
AMD,nm5j1rn,Not necessarily. They might just focus on the servers and when they have figured that out they will work on client. Might be comparable to ROCm where clients have for a long time been 2nd class citizens.,hardware,2025-10-30 07:28:36,2
AMD,nmshedr,5800xt/5600x/5700x,buildapc,2025-11-02 23:25:23,4
AMD,nmsjutn,[https://ca.pcpartpicker.com/product/g94BD3/amd-ryzen-5-5600x-37-ghz-6-core-processor-100-100000065box](https://ca.pcpartpicker.com/product/g94BD3/amd-ryzen-5-5600x-37-ghz-6-core-processor-100-100000065box)  Update the BIOS now while the 2600 is still installed.,buildapc,2025-11-02 23:38:52,3
AMD,nmsiz2a,"Keep an eye out for a sale on a 5700 or 5700x, else a 5600/5600x.  I've been rocking a 5600X/6750XT combo for just over two years and been a great performer for 1440p gaming.  I got CPU on sale at Best Buy and is under $200.",buildapc,2025-11-02 23:34:01,2
AMD,nmssx5m,Yes go for 5600/X.,buildapc,2025-11-03 00:30:05,1
AMD,nmspo72,DO NOT buy a 5700.  They have half the l3 cache of other mainline 5000 series CPUs and as a result have much poorer gaming performance.,buildapc,2025-11-03 00:11:10,3
AMD,nmss6sg,I did not know that.,buildapc,2025-11-03 00:25:50,1
AMD,nlu0exx,7800X3D,buildapc,2025-10-28 14:25:03,8
AMD,nlu132q,"The 7800x3d is the better pick between those 2 cpus for mostly gaming, and can also handle those other tasks like streaming and multitasking no problem. The x3d v-cache is worth more than just $20, for that price difference its a bit of a no brainer.  For the parts you mentioned there really isn't any compatibility issues or bottleneck problems, the only thing I would say with AM5 is to make sure you get only 2 sticks of ram, Am5 can get unstable with 4 sticks and generally speaking you have to down-clock the ram speeds to get good stability.",buildapc,2025-10-28 14:28:29,6
AMD,nlu3xbm,"7800X3D  So long as you don't buy a really low end motherboard, you'll be fine. For memory, any 2x16gb 6000 CL30 kit would be ideal. 6000 CL36 is fine if 6000 CL30 is too expensive.",buildapc,2025-10-28 14:43:07,2
AMD,nlwg09h,"If fast ram was cheap and you were into overclocking/productivity, you could make the case for the 9700x, but it looks like the X3D chip is the better option here",buildapc,2025-10-28 21:31:58,2
AMD,nlu3tgt,7800x3d is up to 30% faster at gaming.,buildapc,2025-10-28 14:42:36,4
AMD,nly92km,The 9700X is only about 5% faster than 7700X  Meanwhile 7800X3D can show improvement of 15-30% over 7700X,buildapc,2025-10-29 03:43:39,1
AMD,nluovli,Straight to the point,buildapc,2025-10-28 16:25:14,1
AMD,nlup2jz,Thanks you told me what i need to know but still I find people who disagree idk why,buildapc,2025-10-28 16:26:10,2
AMD,nlwjtot,Yeha i will never like overlooking or ill turn the settings down or buy new parts,buildapc,2025-10-28 21:51:27,1
AMD,nlypfrz,Yeha that's why im get the x3d,buildapc,2025-10-29 06:03:46,1
AMD,nlv349o,"The main reason you would hear some people say the 9700x would be you aren't gaming but still do have heavy multicore workloads like video editing, 3d modeling, CAD software ect.   For mostly gaming use cases the reality is anyone who says the 9700x is better than the 7800x3d is just flat out wrong.",buildapc,2025-10-28 17:33:12,3
AMD,nlv3k0e,Yeha the x3d is the best bet for me,buildapc,2025-10-28 17:35:18,1
AMD,nloed2w,"The 9600XT will likely be a bottleneck for the 7800X3D in a fair amount of cases. But that is actually the *situation that you want.*  Proper attitude about bottlenecks isn't about removing them (because realistically, they can't be. At SOME point, you WILL have a bottleneck in the machine). It's about managing them. You want the bottleneck to be appropriately sized for your needs, and placed where you want it (i.e. the performance disparity between a 9060 XT 16GB and a 7800X3D on a 1080p display is reasonably small, and you want the graphics card to be the limitation when possible).  Oh and because you didn't specify, I would make sure that this is the 16GB version of the 9060 XT.",buildapc,2025-10-27 16:57:20,20
AMD,nloeje0,"Its a decent combo, I think the 7800x3d is a bit overkill for the 9060xt personally, but if you are playing E-sport titles and plan on pulling 200-300 FPS it can be a good match.   Generally, I would say to go with the 9600x and a 5070/9070 that should net a better gaming experience overall.",buildapc,2025-10-27 16:58:11,7
AMD,nloq2p5,Hot take: 9070xt & 7600x instead,buildapc,2025-10-27 17:54:08,17
AMD,nloe8am,It won't,buildapc,2025-10-27 16:56:42,2
AMD,nloed5a,What Hz monitor will you be playing on and what sort of games? Mainly competitive shooters or single player AAA titles?,buildapc,2025-10-27 16:57:21,2
AMD,nlpyek2,I am on a 9060XT and a Ryzen 5 9600X.  Has been about 3 or 4 weeks with the build now and no bottlenecking at the CPU.,buildapc,2025-10-27 21:39:56,2
AMD,nlp2mfx,unless if u are into some competitive game u have too much cpu for ur gpu choice,buildapc,2025-10-27 18:57:28,2
AMD,nloeiq6,It's fine for 1080p and definitely won't bottleneck. Make sure you get the RX 9060 XT 16 gb for the best performance and future compatibility.,buildapc,2025-10-27 16:58:05,1
AMD,nlog3oz,It's fine.  Yes you will have bottlenecks as it's impossible not to.  For most gaming the rx9060xt will be the bottleneck.,buildapc,2025-10-27 17:05:47,1
AMD,nlrh3w0,"1080 p ?      it's sufficient but nowadays 1440p monitors are cheap. And the GPU is kinda weak for that cpu.    If i were u, i would buy a 1440p monitor and buy a 9070xt.",buildapc,2025-10-28 02:46:42,1
AMD,nlsd51f,7600 + 5070/9070 is just going to be way better.    Also bite the bullet and upgrade to a 1440p monitor. They're like $150-200 for a good 1440p 180hz IPS panel,buildapc,2025-10-28 06:48:07,1
AMD,nlsidd5,the 7600x is more than good enough for your gpu choice,buildapc,2025-10-28 07:38:28,1
AMD,nlsoshx,Drop down to a 9600X and up the GPU to a 9070 XT.,buildapc,2025-10-28 08:45:56,1
AMD,nlszjal,I'm still enjoying my Ryzen 5800 XT and RX 6650 XT don't let the haters tell you its a bad rig.  That thing is a 1080p Ultra quality beast.,buildapc,2025-10-28 10:30:04,1
AMD,nlodf70,No not at all,buildapc,2025-10-27 16:52:47,0
AMD,nloihai,will what bottleneck?,buildapc,2025-10-27 17:17:18,-1
AMD,nlof0ax,Exactly. Play the 9060xt until the wheels fall off of it. Then get a better GPU later on,buildapc,2025-10-27 17:00:27,3
AMD,nloix2v,cool thanks! and yes i’m planning on getting 16GB,buildapc,2025-10-27 17:19:25,2
AMD,nloqy8r,Youre mostly right if were talking cpu heavy games. But for more gpu demanding games the 7800x3d will probably not see much utilization.,buildapc,2025-10-27 17:58:19,3
AMD,nlottc3,Not a hot take for sure,buildapc,2025-10-27 18:12:34,7
AMD,nlrym19,the price jump from a 9060 to a 9070 is pretty insane for me so i might have to stick with 9060 as i am on a budget. will the 7600x work well with a 9060?,buildapc,2025-10-28 04:45:30,2
AMD,nlq4uuj,Actually less FPS in many e-sport games with this combo.,buildapc,2025-10-27 22:15:10,-2
AMD,nlojyti,what Hz would you recommend? the monitor i’m thinking of buying is 180hz but I’m not sure if that will be enough,buildapc,2025-10-27 17:24:29,2
AMD,nls0b3o,what kind of games do you play?,buildapc,2025-10-28 04:58:45,1
AMD,nlsnlcu,"Stick with the 7800X3D, a lot of games profit from it and depending on which games you play some are very heavy on the CPU, for me thats the case with Star Citizen.",buildapc,2025-10-28 08:33:08,2
AMD,nlsffe0,What's the price difference between a 7500f & 9070(non XT) Vs your original combo?,buildapc,2025-10-28 07:08:21,1
AMD,nlrs3q8,"Well, that’s only a problem if you’re playing e-sports. If you’re playing more graphically demanding AAA titles, you’ll be benefiting.",buildapc,2025-10-28 03:57:01,1
AMD,nlolsvx,"I can see the difference between 144 Hz and 240 Hz, some people say they can’t see any difference beyond 144 Hz. It depends on you. 240 Hz is a good option, and they’re pretty cheap nowadays.",buildapc,2025-10-27 17:33:29,2
AMD,nlommuy,okay cool i will look into getting 240 hz then thanks,buildapc,2025-10-27 17:37:32,2
AMD,nlrs7wa,"160-180hz is plenty imo. Go to an actual store like Best Buy and try out the monitors there for yourself to get an idea of the differences. I personally can’t tell the difference, but everyone is different.  What games will you be playing with this system?",buildapc,2025-10-28 03:57:48,1
AMD,nlv4rei,"What are the rest of the PC specs?  The 6500xt is a terrible card especially with lower PCIE slots.    Googling the 6500xt t5 legion model it has 8gb of RAM and a 5600g, which are all terrible components.  It needs at least 16gb of RAM to even play most games without being crashy and even then the best it's gonna handle is games like 8-10 years old at this point.",buildapc,2025-10-28 17:41:02,1
AMD,nlv529k,Is this a new problem or have you always had poor performance with this PC?,buildapc,2025-10-28 17:42:28,1
AMD,nlv5jqn,"While your system is pretty low end/weak, game performance will depend on the resolution and quality settings, and it should be able to play an 8 year old game like RDR2 at 1080p especially.  What resolution and quality settings are you playing the game at?  Are you sure the monitor is plugged directly into your GPU and not into the motherboard video port?",buildapc,2025-10-28 17:44:47,1
AMD,nlvh8on,Can you run Speccy and copy/paste the output here?,buildapc,2025-10-28 18:41:21,1
AMD,nlvp3q6,"Unless you have a significant CPU bottleneck, you should be able to get around 90 fps at 1080p in rdr2 with a 6500 XT, so slightly higher FPS at your resolution. You CPU being limited to PCIE 3.0 and your GPU only using x4 pcie lanes will likely reduce performance noticeably though.   Check for CPU bottlenecking: [https://www.pcworld.com/article/1955495/pc-bottlenecks-cpu-or-cpu-limiting-gaming-performance.html](https://www.pcworld.com/article/1955495/pc-bottlenecks-cpu-or-cpu-limiting-gaming-performance.html)",buildapc,2025-10-28 19:19:57,1
AMD,nlv6aof,"Thank you so much for the quick reply! As I mentioned I'm super new to anything in this manner and while I've heard my graphics card is extremely poor, I actually don't even know how to/what to look for in terms of PC specs. Idk what a PCIE slot is, I feel embarrassed lmaoo",buildapc,2025-10-28 17:48:19,1
AMD,nlv8rmu,I think it might also be worth noting that I added more RAM to the PC so it now has 32gb. Idk if that helps.,buildapc,2025-10-28 18:00:01,1
AMD,nlvivpd,"Sorry for the late reply, it's been like this since I bought it in 2023.",buildapc,2025-10-28 18:49:20,1
AMD,nlv82k3,Right now it's set to 1760 x 990 with quality set to medium.   Also how would I check to see if my monitor is plugged into my GPU? I feel so dumb for not knowing this,buildapc,2025-10-28 17:56:46,1
AMD,nlx97rm,"Understood, I'll give this a try. Thank you so much!",buildapc,2025-10-29 00:10:04,1
AMD,nlxe1kk,After running benchmark tests on RDR2 I don't believe it's indicating a CPU bottleneck.,buildapc,2025-10-29 00:37:13,1
AMD,nlveqr7,"It'd help somewhat but it depends on how you added it.  if it's one whole 2x16gb kit that's fine, if it's an extra 3 8gb sticks then that would be problematic.",buildapc,2025-10-28 18:29:02,1
AMD,nlviyoy,That tells me it's just kind of a slow PC.,buildapc,2025-10-28 18:49:44,1
AMD,nlv9mqe,"The GPU ports are lower down on the back of the pc in one of the horizontal slots. Your GPU is the graphics card, the big component installed lower down in the motherboard. The monitor video cable should be plugged directly into it through the back of your case.",buildapc,2025-10-28 18:04:08,2
AMD,nlvf6wm,"Oh lordy, I did 4 8gb sticks and I didn't even know that would be problematic.",buildapc,2025-10-28 18:31:15,1
AMD,nlvh3qj,When I bought it each box had two sticks.,buildapc,2025-10-28 18:40:42,1
AMD,nlvjk1k,"Gotcha, is there anything specifically I should upgrade? If so what would you recommend?",buildapc,2025-10-28 18:52:36,1
AMD,nlvbpt0,My monitor is connected to my PC via HDMI cable. Does that change anything?,buildapc,2025-10-28 18:14:13,1
AMD,nlvkf2n,"Depends on your budget. But, the whole thing is kind of slow. It's not like there's one part you could upgrade to dramatically increase your performance.",buildapc,2025-10-28 18:56:48,1
AMD,nlvdrxn,"No, it doesn't help. Take a picture of the back of your PC, post it to imgur, and link it here.",buildapc,2025-10-28 18:24:16,1
AMD,nlvkrv5,I understand. Do you know roughly what parts you would upgrade if I had a budget of around $300-$400?,buildapc,2025-10-28 18:58:30,1
AMD,nlvgqqk,Bet I got u right here: https://imgur.com/a/E6XTduv,buildapc,2025-10-28 18:38:56,1
AMD,nlvlbqt,"At that budget, I would recommend upgrading your GPU to the Radeon RX 9060 XT 16GB (not the 8GB model). If you decided to upgrade the rest of your PC later you could bring the new GPU over with it.",buildapc,2025-10-28 19:01:13,1
AMD,nlvh4mb,"Well, the monitor is plugged in to the right place at least.",buildapc,2025-10-28 18:40:49,1
AMD,nlx9576,"Understood, thank you so much for your input I really appreciate it!",buildapc,2025-10-29 00:09:40,1
AMD,nlxelry,"Just to make sure, is this the one you're talking about?   [https://www.amazon.com/PowerColor-Reaper-Radeon-9060-GDDR6/dp/B0F9QM1M6R/ref=sr\_1\_8?crid=29ZDNRSKPQP5S&dib=eyJ2IjoiMSJ9.fY1Dv66Z4ThRO27-QRzz49QM2zHY3Vfwgm\_7zRcnyw5HyBwLmN824PmiyYkoh-Ct8J3AGHXgBjRNtihfAls90BWDnKsUES3113BaAjwRBrJItHd4iJYXe-RqTw2l9UkaC9Om8RXFDiRgCcdXQvhJipYoxezJJQGA4OmEJePiG3FB6TyXR4jcKy5PM7F7rsSnx\_hS3x8WU3k0GpLBuva6X21bLrqJJ0kqRDnmWA\_yl-Q.eqWaiWQ32Vk5qqIgcEp2aCwwX6SDgk1cNYIrE5s-kTY&dib\_tag=se&keywords=AMD%2BRadeon%2BRX9060%2BXT%2B16G&qid=1761698370&sprefix=amd%2Bradeon%2Brx9060%2Bxt%2B16g%2Caps%2C172&sr=8-8&th=1](https://www.amazon.com/PowerColor-Reaper-Radeon-9060-GDDR6/dp/B0F9QM1M6R/ref=sr_1_8?crid=29ZDNRSKPQP5S&dib=eyJ2IjoiMSJ9.fY1Dv66Z4ThRO27-QRzz49QM2zHY3Vfwgm_7zRcnyw5HyBwLmN824PmiyYkoh-Ct8J3AGHXgBjRNtihfAls90BWDnKsUES3113BaAjwRBrJItHd4iJYXe-RqTw2l9UkaC9Om8RXFDiRgCcdXQvhJipYoxezJJQGA4OmEJePiG3FB6TyXR4jcKy5PM7F7rsSnx_hS3x8WU3k0GpLBuva6X21bLrqJJ0kqRDnmWA_yl-Q.eqWaiWQ32Vk5qqIgcEp2aCwwX6SDgk1cNYIrE5s-kTY&dib_tag=se&keywords=AMD%2BRadeon%2BRX9060%2BXT%2B16G&qid=1761698370&sprefix=amd%2Bradeon%2Brx9060%2Bxt%2B16g%2Caps%2C172&sr=8-8&th=1)",buildapc,2025-10-29 00:40:26,1
AMD,nlvhd92,"Gotcha, thanks for confirming :)",buildapc,2025-10-28 18:41:58,1
AMD,nlxhfc4,Yes,buildapc,2025-10-29 00:56:13,1
AMD,nm28eu8,Phantom Spirit or Peerless Assassin are both very good,buildapc,2025-10-29 19:21:54,3
AMD,nm28g69,Best possible? Noctua NH-D15 G2 LBC.,buildapc,2025-10-29 19:22:04,1
AMD,nm29d02,Royal pretor 130,buildapc,2025-10-29 19:26:27,1
AMD,nm2bi1z,"Peerless Assassin 140 if you can fit it in the case, otherwise the 120 will do fine, just slightly louder.",buildapc,2025-10-29 19:36:47,1
AMD,nm28ymt,The 120 or the 140? Or is there really any difference?,buildapc,2025-10-29 19:24:33,1
AMD,nm28w9c,"I have seen a lot about Noctua, which is definitely a strong contender. But the only thing kinda holding me back is the color. The brown and tan color scheme really puts me off.",buildapc,2025-10-29 19:24:14,1
AMD,nm29gr3,"They perform pretty similarly, but the bigger fan should be able to spin slower which would make it slightly more quiet.  Make sure your case can fit the 140 though, that's likely the deciding factor",buildapc,2025-10-29 19:26:58,1
AMD,nm29z0n,Noctua will likely eventually release a Chromax black version of the cooler but I can't promise when that will happen. The Thermalright Phantom Spirit is only a few degrees hotter than the Noctua.,buildapc,2025-10-29 19:29:22,1
AMD,nm40zt4,They're literally designed with it in mind.,buildapc,2025-10-30 00:51:05,207
AMD,nm41dmo,a $35 dual tower air cooler like a thermalright ps120 is more than enough for any amd cpu,buildapc,2025-10-30 00:53:14,411
AMD,nm42s0a,"A peerless assassin can cool any am5 cpu well  Gone are the days of the shitty overheating intel 13 and 14th gen i7's and i9's that needed high-end motherboard and AIO just to survive. Just your average b650/b850 mobo and air cooler will keep the temps way down. And good air coolers are very affordable nowadays, like the peerless assassin for instance",buildapc,2025-10-30 01:01:07,121
AMD,nm4cy3y,"Lol, BeQuiet! Blackrock Pro5 user here for my 9800X3D. Cool temps through and through. AIOs are silly.",buildapc,2025-10-30 01:58:48,41
AMD,nm4herk,"I do custom watercooling because... It's fun and it looks cool. But air cooling is way more affordable, reliable, and almost as good (and sometimes better) than AIOs and even custom loops. Just dust it out every 3-6 months and you're golden.",buildapc,2025-10-30 02:24:27,30
AMD,nm4721f,Yes.  AIO is such a waste and dumb for 98% of cases,buildapc,2025-10-30 01:25:31,88
AMD,nm45ata,"Yes, in a lot of ways it’s the golden age of air coolers. There are a ton of relatively affordable air coolers that trade blows with noctuas best.",buildapc,2025-10-30 01:15:28,13
AMD,nm443eg,It's a 120w TDP CPU. And they're not getting hotter. The ryzen 9 7900x was 170w TDP.  A DeepCool AK400 would likely be enough.,buildapc,2025-10-30 01:08:37,12
AMD,nm4fxyz,"With how efficient modern air coolers are, main difference between an air cooler and aio is pretty much purely cosmetic. An air cooler will work perfectly fine.",buildapc,2025-10-30 02:16:06,7
AMD,nm4dtzk,Arguably air cooling and heat sinks are so much more efficient now that water cooling isn’t nearly as needed,buildapc,2025-10-30 02:03:52,5
AMD,nm45l7j,The marketing is working really well it seems,buildapc,2025-10-30 01:17:07,12
AMD,nm4db3u,"I'm air cooling a 9950X3D with a D15S and have thermal headroom to spare, even with PBO. You're good. These chips running hot is pure myth. They're incredibly efficient.",buildapc,2025-10-30 02:00:50,6
AMD,nm7bw1j,"Im on a like $40 air cooler with a 9800x3d, its completely fine.",buildapc,2025-10-30 15:04:27,3
AMD,nm62m0m,I had an AIO burst and leak. Corsair was super shitty about replacing parts.  Never water cooled again. I have a phantom spirit with two noctuas slapped on. Idle 43C.,buildapc,2025-10-30 10:38:25,2
AMD,nm64zii,"9950x3d with a Phantom spirit and two noctua fans. With a 5090 dumping heat in the case too.  The thing's never gone anywhere near to 90 C, even when I was doing some prime95 stress testing.",buildapc,2025-10-30 10:57:33,2
AMD,nm6mt9u,"No amount of cooling will ever be ""enough"", I have the same CPU with an AIO and it keeps boosting until it trips a temperature limit (saw it go 200W+). For normal use and air cooler should be fine. But if you're getting a high end CPU, why not get a good cooler as well? Good AIOs can be gotten for less than 100",buildapc,2025-10-30 12:56:14,2
AMD,nm5m6vi,>Is air cooling still enough  Always was,buildapc,2025-10-30 08:00:55,4
AMD,nm469ma,Boy do I got news for you.  There’s been some advancements so yes.  The Royal Pretor 130 is the current top dog,buildapc,2025-10-30 01:21:01,2
AMD,nm4g5fh,"I'm easily cooling a 9900X with an ID Cooling Frozn A720, and I'm blasting 190 watts peak through it with PBO on. The worst it gets is like a very brief peak of like 90C on occasion. Very far from chip killing.",buildapc,2025-10-30 02:17:21,2
AMD,nm4h2c2,My be quiet 5 keeps my 9800x3d at 40°c to 50°c at 40% to 80% utilitization,buildapc,2025-10-30 02:22:31,2
AMD,nm46lni,It looks like a liquid loop with insufficient mass actually works _less_ effectively than air cooling. At least from what I gleaned poking around about a 9800 build.,buildapc,2025-10-30 01:22:55,1
AMD,nm65898,"4080super and 9800x3d here, love my peerless assassin 120",buildapc,2025-10-30 10:59:27,1
AMD,nm66w23,"then how can i fix my r9 5900x with noctua nh u 12a reaching 76c in bf6 on full hd reso with rx7800 xt?  I.ve fiddled with fan curves and still not enough , changed thermal paste , updated driver, bios..  Any clues or improvements?",buildapc,2025-10-30 11:12:24,1
AMD,nm6w7eh,"I bought the same CPU as you did, and I'm getting a Peerless Assassin SE 120 it's great bang for the buck",buildapc,2025-10-30 13:46:44,1
AMD,nm6ylym,"I have a 9950x with mild overclocking and a Noctua NH-D15. It was sufficient but louder under load (gaming) and couldn't keep my CPU cool under heavy artifical loads (Prime95). One challenge is the video card (5080) exhausts heat up and into the Noctua intake fan, making it harder to cool during gaming.    I switched to a Artic Cooler Liquid Freezer 360. It runs cooler and will not throttle even under Prime95 loads. It is quieter under heavy loads and doesn't speed up/slow down the fans nearly as much as the Noctua. I am much happier using the AIO than the Noctua and would recommend this to others with similar hardware.",buildapc,2025-10-30 13:58:48,1
AMD,nm6zc89,A peerless assassin 120 been keeping my 9800x3d cooled no issues.,buildapc,2025-10-30 14:02:24,1
AMD,nm725nr,Yes,buildapc,2025-10-30 14:16:29,1
AMD,nm7cbpi,Short answer? Yes. Long answer? Yeeeeeeeeeeeeeeeeeeeeeeeeees.,buildapc,2025-10-30 15:06:35,1
AMD,nm7dcat,Yes,buildapc,2025-10-30 15:11:31,1
AMD,nm7djdw,Yes. I've seen multiple videos that show air cooling is every bit as efficient as water cooling. Water cooling looks cool but you don't need it.,buildapc,2025-10-30 15:12:26,1
AMD,nm7ga8l,I just built a Ryzen 9 9950X3D system with a Noctua NH-D15.  Runs at lower temps than my AIO cooled 7900X3D,buildapc,2025-10-30 15:25:29,1
AMD,nm7kz7r,"Build ducting so your CPU cooler has separate intake and exhaust ducts with at least 2 fans. One blowing in and the other blowing out. This allows low fan RPM and quiet operation.  Same for your GPU. Pipe in cool outside air. Use a 3d printer or heat bend thin sheet plastic with a heat gun.  Water cooling is for people who don't understand how to move air in a case properly. Move the air in one direction, in from the bottom or front, out in the top or the rear.",buildapc,2025-10-30 15:47:38,1
AMD,nm7m0in,My 16 year old Noctua DH-14 handles a Ryzen 9 5900X like a champ.,buildapc,2025-10-30 15:52:29,1
AMD,nm83wr7,Yes.  /thread,buildapc,2025-10-30 17:17:57,1
AMD,nm8l993,I run a Noctua NH-D15 on my 13700K and I stay under 82° C,buildapc,2025-10-30 18:40:19,1
AMD,nm8s7ww,9900x isn't high end. It's mid range. X3d is the high end.,buildapc,2025-10-30 19:13:11,1
AMD,nm8tg7j,"Air cooling is great once it is installed even now, especially with silent enough fans and proper airflow. But the heatsinks got so large and heavy that installing them is not fun at all. I went to the shop to install it the last time and will do it the next time, if they screw up something they are gonna fix it themselves right then and there.",buildapc,2025-10-30 19:19:05,1
AMD,nm90r1n,"Yep.  Running a Noctua U12S from years ago on a 200W 5900xt with an OC/UV. No issues.   However, case and fans are more important.  Air coolers need an  uninterrupted path of air. Modern fishtanks don't work right. And cases without proper airflow struggle. A shit case can work with a giant 360mm AIO. Not with an air cooler.",buildapc,2025-10-30 19:54:35,1
AMD,nm9qgpa,"Yes, a Thermalright Phantom Spirit is cheap and will work just fine. And of course Noctua NH-D15 at twice the price. They're about the same performance.  That's all assuming you're not going to do any real overclocking that require more power.",buildapc,2025-10-30 22:03:47,1
AMD,nm9xjgu,"My last PC had a small but notable decrease in temperature when I switched from a cheaper AIO liquid cooler to a Noctua 2×120mm air cooler, with a modest overclock.",buildapc,2025-10-30 22:44:32,1
AMD,nmajmeu,My thermal right PA120 is doing a great job on my 9800x3d. Most of the time playing it sits at 60C and at 100 load it goes to about 70C range which it rarely does except when prerendering a game for example. I also have exceptionally bad airflow into my case.,buildapc,2025-10-31 00:51:37,1
AMD,nmalx0e,Average air coolers are on par with average AIO's  Nowadays just comes down to what you want for your build aesthetically.,buildapc,2025-10-31 01:04:50,1
AMD,nmathr6,"Fine as in “not thermal throttle”? If that’s the expectation most reputable brand’s dual tower air cooler will do.   However, AIO, especially 280/360, have significant advantages when it comes to noise to performance under sustained heavy load. If you don’t mind your PC sounds like a jetliner taking off, or you wear headphones anyway, you typically don’t need AIO.  But some of the thermalright 360 AIO are surprisingly price competitive even up against their own air cooler, can’t speak for its longevity but performance wise they are definitely worth considering.  Just avoid 120 AIO as they sometimes perform worse than air cooler, unless you are building a SFF build and short of space for air cooler.",buildapc,2025-10-31 01:49:54,1
AMD,nmbqr9q,I switched from AIO to air cooling and got better temps on my 9800X3D by about 10°C. Very much can handle high end CPUs.    I had a Corsair 280mm Titan AIO and moving into new home with poor AC in the office caused hotter room ambient temps which resulted in higher CPU temps. I looked into and after some research landed on a ThermalRight Phantom Spirit EVO 120mm air cooler and plugged it in and immediately saw about 4°C cooler. After some fan curves adjustments I got it down to as much as 10°C than what I could get my AIO.    Noise is nearly the same with the new fan curves as my AIO was which is basically silent even under full load and only real difference I noticed was my AIO would regulate temps as it got hotter or cooler but took longer to cool down whereas the air cooler has more temp spikes but cools a lot faster.,buildapc,2025-10-31 05:57:14,1
AMD,nmghybn,"If you're gaming and it isnt a 4090 or 5090, just use the 9700X. Save money, and you'll never notice a difference. Even in non gaming, you'll probably never notice unless youre doing the high end stuff.",buildapc,2025-10-31 23:59:14,1
AMD,nmiydu3,"Usually yes, 9000 series are pretty cool cpus",buildapc,2025-11-01 12:53:48,1
AMD,nm4sd1n,I mean sure you can make anything work but honestly a 360 AIO is just so much less headache and noise. Plus you never really have to worry about cooling and they tend to take up less room in the case all things considered.,buildapc,2025-10-30 03:32:05,1
AMD,nm5lidt,"Been running an air-cooled 9800X3D since Christmas, so all through the seasons now I guess, and other than the usual hyperfixation on every little thing when it was a new build, I've never once even *thought* about CPU temp, let alone actually worried about it.  Cooler is a little fancy, a noctua nh u12s chromax black, but its not _outrageous_ imo.",buildapc,2025-10-30 07:53:56,1
AMD,nm4bysk,yes it is: [https://www.techpowerup.com/review/amd-ryzen-9-9900x/25.html](https://www.techpowerup.com/review/amd-ryzen-9-9900x/25.html),buildapc,2025-10-30 01:53:21,1
AMD,nm4d7or,"Well, I'm using a 9900X in my Fractal Terra with a NH-L12S cooler, so I hope this is enough, lol. So far, no issues.",buildapc,2025-10-30 02:00:18,1
AMD,nm5s3et,"I got a Deepcool AK400 Zero Dark with my 9800x3d, and it is doing just fine.",buildapc,2025-10-30 09:01:08,1
AMD,nm5u2c0,My 9800x3d barely gets over 70C with a Noctua in single fan mode. Never even took the 2nd fan out of the box.,buildapc,2025-10-30 09:20:59,1
AMD,nm4kif6,"It depends on what you do, but yes... air cooling can absolutely be viable on the 9900X or even the 9950X.",buildapc,2025-10-30 02:42:32,0
AMD,nm4x1f4,Yes - my 9950x runs almost as fast ( just -.2 GHz slower on all core workload) with my peerless assassin than it did with a MORA IV 400 and optimus sig v3 water block.,buildapc,2025-10-30 04:05:22,0
AMD,nm51z22,"Yes I have the 9900x All Core at 4.9hz &  Phantom Spirit SE, & max temp is literally 65c-7c while gaming and streaming.",buildapc,2025-10-30 04:44:18,0
AMD,nm55b2z,"9800x3d running deepcool ak620 dual tower but i only have one center fan installed. Runs perfectly, no issues.",buildapc,2025-10-30 05:12:59,0
AMD,nm5bmvj,"the peerless assassin 140 keeps things pretty cool on my 5800xt, so far. No OC except to the 4.9ghz it clocks to natively. No adjustment to fan curves or other.  The phantom spirit is a slight upgrade to the peerless assassin. totally worth their money.",buildapc,2025-10-30 06:13:24,0
AMD,nm5dfse,"LMAO, buddy, most of the time unless your water cooled setup is custom and insane, air cooling will always be better...",buildapc,2025-10-30 06:31:24,0
AMD,nm5ec4e,100% fine. Cpu temp's being hotter is really not something to be concerned about its all a mind game at this point between years ago and newer tech today. There is a performance aspect too it but in the end 99.99% of cpu's that have issues down the road will be because voltage/bios/setting issues or physical parts like the vrm's failing and killing the cpu then riding the thermal limit of the cpu,buildapc,2025-10-30 06:40:33,0
AMD,nm5hgbm,"I only have an AIO cooler to give me access to components on the motherboard. I got man hands. 🙌   Otherwise air coolers are totally fine for most use cases.  edit: Eh, rough crowd. I get no respect.",buildapc,2025-10-30 07:12:06,0
AMD,nm5punr,Yes.,buildapc,2025-10-30 08:38:46,0
AMD,nm5u3qp,"It's enough, but you'll have to use the beefy / dual tower ones. Personally I don't like the look of them hence I favor AIO for higher-end chips. But if all you care is performance or you like the look of massive cooler on your motherboard then go for it.",buildapc,2025-10-30 09:21:23,0
AMD,nm45a8a,Yes,buildapc,2025-10-30 01:15:23,0
AMD,nm4jnuv,Yes,buildapc,2025-10-30 02:37:33,-1
AMD,nm4elkt,14900k would like a word,buildapc,2025-10-30 02:08:16,-149
AMD,nm73b01,Is it enough just for gaming or is it enough for something that's much harder on a chip like video transcoding?,buildapc,2025-10-30 14:22:13,12
AMD,nm8b11g,>any amd cpu  On AM5 anyway. AMD does make 500W CPUs (with 128+ cores).  :P,buildapc,2025-10-30 17:52:18,2
AMD,nm83cvv,That's exactly what I'm running ATM and temp currently sitting at 40~C. High-end gaming around 60~,buildapc,2025-10-30 17:15:19,1
AMD,nm8n9ub,"Any AM5 cpu, maybe",buildapc,2025-10-30 18:49:43,1
AMD,nm7qp7x,Even a Threadripper Pro 9980x. Wow the dual tower is so good,buildapc,2025-10-30 16:14:37,1
AMD,nm6vz6t,"What GPU and what blower style does you have? I feel like that matters a lot for air/water cooling decisions.  I've used air cooler for as long as I can remember (over two decades at this point) but recently had to upgrade to water cooling for the CPU as I upgraded the GPU, thermals in my whole box started going crazy and once water cooled, everything went back to normal levels, and GPU runs cooler too :)",buildapc,2025-10-30 13:45:34,-3
AMD,nm45vdx,"You can lose a bit on multicore if you don't have liquid, but its like 5%   Its not worth it   Aircool anyways    Its more than enough",buildapc,2025-10-30 01:18:44,-176
AMD,nm46k1j,Agreee with the Peerless Assassin,buildapc,2025-10-30 01:22:40,32
AMD,nm7r90t,"Phantom spirit is the newer upgraded version, almost identical but slightly better temps at about same price",buildapc,2025-10-30 16:17:13,7
AMD,nm6c5pu,The PA has been the top tier sinflce I can remember.,buildapc,2025-10-30 11:50:42,3
AMD,nmewygw,Even hot intels work normally with air cooling :D,buildapc,2025-10-31 18:30:14,2
AMD,nmvnhrl,I have a peerless on my 9800x3d   It works amazingly,buildapc,2025-11-03 13:52:26,1
AMD,nm4vfdi,stupid sexy AIOs. Really just an aesthetic choice imo,buildapc,2025-10-30 03:53:34,28
AMD,nm5u0qw,"The AIO mafia can pry my NH-D15 out of my cold, dead, hands.",buildapc,2025-10-30 09:20:32,9
AMD,nm6vvhe,Same chip in an NR200 (ITX) and close enough HSF which was ‘just for the build’.   It has been some time. AIO is still in the box and has not been a priority.,buildapc,2025-10-30 13:45:02,1
AMD,nm5zvea,What temps are you running?,buildapc,2025-10-30 10:15:19,1
AMD,nm4quy4,"Plus custom loops are more silent and more maintainable in the long-run.  These days custom loops don't even cost that much more as Bykski, Freezemod and Icemancooler brought prices down to a competitive level. Should only cost around ~$200 USD for a custom CPU loop with one rad :D",buildapc,2025-10-30 03:22:00,-11
AMD,nm6b4fu,AIO is just air cooling with an extra step.,buildapc,2025-10-30 11:43:33,8
AMD,nm5bqep,"Liquid cooling is often quieter under load, so there's that if we're not only talking about cooling performance.  I'm personally running peerless with my 7800x3d, but it gets loud at full speed. Curves can only do so much in the end. But for the money, it's definitely in the top range.",buildapc,2025-10-30 06:14:22,20
AMD,nm6wisa,"I think what else you have in the chassi matters more for water cooling of the CPU, than the CPU/motherboard itself. If you have something that has the traditional blower style of having both intake and outtake inside the chassi (like a modern GPU), then water cooling the CPU can make quite the difference for both the GPU and CPU.  Literally had this issue myself when I upgraded my GPU this summer, CPU temperatures (and GPU) ended up way too high until I got a AIO :)",buildapc,2025-10-30 13:48:21,3
AMD,nm4zw10,"What wattage/TDP setting in PBO do you have it set at?   If i set my 9700X at 105W TDP (142W) with simple PBO and no tweaking, it shoots up to 80C almost instantly under load. Seems excessive. I keep it set to stock 65W TDP (88W) with PBO.and undervolt, and temps are good under load (55C). I'm just surprised how hot it gets so fast at 105W TDP. Doesn't seem usable unless I thermal limit it. I have a Phantom Spirit 120 and good case cooling.",buildapc,2025-10-30 04:27:08,3
AMD,nm7nl65,"Because it is an unnecessary complication which has a much much shorter lifespan than air coolers. My experience with dozens, (if not hundreds) of personal and server compuers tells me not to rely on liquid cooling for a 24/7 on machine.",buildapc,2025-10-30 15:59:51,1
AMD,nm742dq,***Bang!***,buildapc,2025-10-30 14:26:01,1
AMD,nm4k1lg,"Well yeah. Any cooler with insufficient mass isnt going to work as well. Mass affects heat transfer. Thats why people dont use tiny heat sinks for most cpus anymore.  More mass + more surface area means a cooler is likely to handle spikes in heat better, which is pretty much the only thing you need to worry about with when cooling and x3d cpu.",buildapc,2025-10-30 02:39:48,4
AMD,nm5eqg4,Im looking for a similar setup (case/cooler) - how is the noise lvl?,buildapc,2025-10-30 06:44:36,0
AMD,nm657st,"It's not that we like the look, it's that we don't look at it once it's installed, so we don't care.",buildapc,2025-10-30 10:59:21,2
AMD,nm4gltk,14900k isn't an amd cpu. This entire post is asking about amd cpus not intel.,buildapc,2025-10-30 02:19:55,126
AMD,nm7fwna,14900k cant have a word because it killed itself,buildapc,2025-10-30 15:23:41,10
AMD,nm5eta2,"Your brain has lost its connection, please reconnect!",buildapc,2025-10-30 06:45:23,28
AMD,nm5ruet,I’m pretty sure yall missed the sarcasm in his post.  (Downvotes engaged),buildapc,2025-10-30 08:58:37,-13
AMD,nm7jppt,"A dual tower, dual fan air CPU cooler will generally have the best performance-to-cost value",buildapc,2025-10-30 15:41:41,14
AMD,nm7bl0b,Air is fine for all tasks.,buildapc,2025-10-30 15:02:57,44
AMD,nmakmpm,"I have a 9800x3d with exactly that cooler. If video transcoding is as heavy as you say it is, thermal throttling is likely going to be an issue.",buildapc,2025-10-31 00:57:27,3
AMD,nm9rhwp,"> video transcoding  You'd ideally do that on a GPU today, magnitude difference in performance.",buildapc,2025-10-30 22:09:47,1
AMD,nm6zcfo,"I have a 7950x3d with a thermalright rk120se, and a 3080  the only fans that ever ramp up are my gpu, and I probably just need to repaste it since it's been used consistently for 5 years now  and I keep my case on the ground *gasp in horror*",buildapc,2025-10-30 14:02:26,1
AMD,nm47a9c,You only lose performance if you thermal throttle which most modern dual tower aircoolers don't on a 9900x,buildapc,2025-10-30 01:26:47,111
AMD,nm4ah78,Most good air coolers are better than a lot of crap liquid coolers so this statement really needs some clarification.    I would also argue that a good air cooler doesn't throttle any am5 cpu.,buildapc,2025-10-30 01:45:01,38
AMD,nm4cliw,"Absolute hogwash. I'm air cooling a 9950X3D, and very much get all the performance across all 16 cores in the workloads I do. I'm hitting up to 5.9GHz across most cores with PBO.",buildapc,2025-10-30 01:56:52,22
AMD,nm4ll3p,I’m pretty sure the only you really needed an AIO for was a 13900k or 14900k and that’s because of how inefficient they were with their power draw. I don’t think even Intels current gen Core Ultra 285k needs it. An AIO saves space inside the tower though so it can make it potentially easier to work on.,buildapc,2025-10-30 02:48:53,4
AMD,nm6j44o,"This is a problem with your setup , not with the air cooling. I'm running the exact same processor, and the thing never goes over 75, and that's on heavy load.  At this point, I think i've done close to two dozen builds with that processor using air cooling benched all of them, and not a single one of them have had any problems.  That processor does not, nor has it ever needed a aio or custom loop.  One of the greatest strengths of the processor is , the fact that it requires such cheap cooling solutions.",buildapc,2025-10-30 12:34:55,1
AMD,nm5gaca,"Got one paired with a 5800x3d and can confirm Also cooled two other systems with an ak620 and  7800x3d and 9800x3d without any problems. So yes, air cooling is sufficient enough.",buildapc,2025-10-30 07:00:09,6
AMD,nm6q0q9,"Haven't exceeded 60C, I also have a Fractal Torrent full size case and my home doesn't deviate from 20C.",buildapc,2025-10-30 13:14:00,1
AMD,nm4txf1,"Unless the pump(s) and radiator fans are in another room, there are still sources of noise to be hunted down",buildapc,2025-10-30 03:42:58,9
AMD,nm6wbfh,Air cooling is just water cooling with extremely less humidity involved.,buildapc,2025-10-30 13:47:19,6
AMD,nm7pmlk,All cooling on earth is just radiative cooling with extra steps.  :-),buildapc,2025-10-30 16:09:33,2
AMD,nm5cryz,"Ya, AIOs can be a bit quieter, and have a different aesthetic look.  IMO they’re probably not the smartest choice, but I get them anyway. 🤷",buildapc,2025-10-30 06:24:48,10
AMD,nm5t8kh,"> Liquid cooling is often quieter under load, so there's that if we're not only talking about cooling performance.  Which is great, but when you're gaming with headphones on who really cares about that?  Idk, I get the quiet thing. There's a reason why I built my server and HTPC to be as quiet as possible. And if my gaming rig was also a workstation that actually needed that power then I would probably do the same. But for just a straight-up gaming rig? Crank those fans curves and make tons of noise, your components will thank you later.",buildapc,2025-10-30 09:12:42,3
AMD,nm6xx1u,Then there’s me who got a shitty AIO that is louder than the peerless assassin I sold,buildapc,2025-10-30 13:55:22,1
AMD,nm5g104,"Thermal limit is 95c and they're designed to actually sit at 95c 24/7.   being worried about temps sitting in the 80s and set lower thermal limits, is not normal.",buildapc,2025-10-30 06:57:36,5
AMD,nm6moap,"Silent during idle, audible during gaming, but not to a worrying degree. Gets the hottest when I do AI art-creation. I do have the 9900X limited to 65W TDP, but if you look at experts' research videos on youtube, they found that you barely lose any prowess, but get the max temperature down a lot (my 9900X never gets hotter than 69-72°).  And I could further optimize it, haven't yet done all I could. I think it's a nice, quiet system.",buildapc,2025-10-30 12:55:28,0
AMD,nmakutu,"I need to remember this   > A dual tower, dual fan air CPU cooler will generally have the best performance-to-cost value > a $35 dual tower air cooler like a thermalright ps120 is more than enough for any amd cpu",buildapc,2025-10-31 00:58:49,1
AMD,nm9q5t2,"...which is a different answer, since I was asking about that specific one.",buildapc,2025-10-30 22:02:03,-22
AMD,nman2my,"That's exactly the reason I asked, although I have used a NH-D15 on a 5700X for transcoding - but that's a 65W chip in comparison.",buildapc,2025-10-31 01:11:35,2
AMD,nm9syh0,"No, for offline transcoding - for storage purposes.  GPUs are fine for on-the-fly.  Even after the RTX quality upgrade with the 2000 series it's still worse looking than CPU transcoding.  I'm not talking about watching it as it is happening or Twitch or whatever.",buildapc,2025-10-30 22:18:20,2
AMD,nm72t5l,"Hah, yeah, I still have a 5950X and used to have 3090ti, temps were all good with air cooling everything. But as mentioned, upgraded the GPU this summer, and *had to* move to AIO as the new GPU runs *very* warm and made both the CPU and GPU temperatures way too hot. AIO solved this problem immediately :)",buildapc,2025-10-30 14:19:44,2
AMD,nm4jefl,My 9800x3d will throttle on a phantom spirit se stock   It doesn't throttle with curve optimizer though   Edit: wtf is with the massive downvotes   I lose 10% cinebench score pbo + stock vs pbo + co + 200 with ps120   I throttle unless I use co,buildapc,2025-10-30 02:36:02,-85
AMD,nm4ig0l,What wattage does this pull at full power (like a handbrake CPU encode)?,buildapc,2025-10-30 02:30:28,3
AMD,nm4j8ji,With curve optimize I assume,buildapc,2025-10-30 02:35:06,1
AMD,nm5s871,"I still think you probably *should* liquid cool a 285k, but you don't *need* to like you basically were forced to on the 13th/14th gen i9s. Some of the beefy thermalright and noctua air coolers definitely can handle the 285k. Although if you're doing really heavy multicore work for hours at a time, it will probably push into the 90s on air cooling.",buildapc,2025-10-30 09:02:28,1
AMD,nm4uo0s,That much is true. However not all pumps are made equal.   Most D5 pumps are better built and have a lower noise output than AIO pumps. Even a $50 Freezemod D5 pump will run significantly more quieter than any premium AIO pump on the market right now.,buildapc,2025-10-30 03:48:11,1
AMD,nm4wesm,Yeah I have coil whine in both my GPU and power supply lol,buildapc,2025-10-30 04:00:44,1
AMD,nm5zoiz,"Yeah, but 13 fans going 30% will be quieter than 3 fans going maximum overdive",buildapc,2025-10-30 10:13:38,1
AMD,nm94ce9,"I have an old gtx660ti 2 pipe heatsink and as an experiment I decided to hold a dual jet lighter against it (you know them lighters which make nice blue cones of flame).  The water from the combustion in the flames condenses onto the heat spreader about 5mm away from the edge of the flames. No amount of time holding them flames to the surface will cause the water to evaporate off again.  I can immediately take the flame off and put my hand to the point I'd been trying to heat for minutes and it does feel like it's warmer than air temperature but it's by no means ""warm"".   And that's without any fans on the heatsink.  Air coolers are just evaporation coolers really.  What matters is keep the air in your case cool. Which is why I have a positive pressure fine nylon mesh filtered 4 in, 1 out (to encourage exhaust out of the case on the peerless assassin 120 I use) mesh case. Dust finds it hard to get in, and it's easy to blow out, all the extra fans intaking make sure the case is always at room temperature and the exhaust encourages the airflow to go through the cooler even without the CPU fans.",buildapc,2025-10-30 20:12:03,1
AMD,nm5u0t3,"> Which is great, but when you're gaming with headphones on who really cares about that?  People who don't use headphones",buildapc,2025-10-30 09:20:33,23
AMD,nm7faa8,"I use open back headphones exclusively, even when producing. I don't mind the noise so much, but i could see how the noise from air cooling a 9950x3d would be a little loud for some",buildapc,2025-10-30 15:20:44,4
AMD,nm7vz7r,Open back headphones exist,buildapc,2025-10-30 16:39:51,1
AMD,nm6wk54,"The GPU is probably the loudest fan in a gaming system anyway, so having a whisper quiet CPU while under load is kind of pointless unless you also liquid cool the GPU.",buildapc,2025-10-30 13:48:33,-1
AMD,nm9uvuw,"No it’s not. He said air is fine for all tasks, which would include video transcoding.",buildapc,2025-10-30 22:29:22,19
AMD,nmkoqb5,Was it worth it?,buildapc,2025-11-01 18:34:14,1
AMD,nmbyyjy,"Yeah, 9800x3d in particular is a chip that can run pretty hot. I have a good thermal setup (good case, good fans, good location for the case, cold country etc.) and whenever I run a stress test and run all cores at 100%, the cooler does really well, but I always see the temperature go up to around 84 degrees and sloooowly keep climbing until it throttles itself.  If you expect offline video encoding to rival a heavy stess test, maybe get a good watercooler. If not, it should do fine.",buildapc,2025-10-31 07:21:11,1
AMD,nmacl5k,"I still use NVENC for storage/rendering purposes of videos too, but most of the material I deal with is in 4K so maybe it's a bit harder to notice the CPU vs GPU difference compared to what you're handling. Otherwise for archival I'll use whatever format I received it in + smaller versions (again NVENC).",buildapc,2025-10-31 00:10:00,2
AMD,nm5d520,"Thats strange, my 7800x3D will hit all core 4.85 GHz on a phantom spirit SE and top out at like 83 without curve optimizer. Its mid to high 70s with a -10 offset. I thought the 9000 series was supposed to be more power efficient and run cooler",buildapc,2025-10-30 06:28:26,10
AMD,nm4k5c0,9900x doesn't have the 3d part and has a different CCD layout.,buildapc,2025-10-30 02:40:25,25
AMD,nm6isxd,"You either didn't install the cooler correctly, or you did a shit job of applying thermal paste.  A 9800X3D will not throttle with a PS120.",buildapc,2025-10-30 12:33:02,3
AMD,nm5fkku,Strange... I have a 9800X3D.   I cool it just fine with a Phantom Spirit 120 SE and the fans it came with.   Temps never go above 65c when gaming and never had to worry about using Curve Optimizer. And this is with a +200Mhz core offset.  Same goes for my friends 9950X3D with the same cooler.,buildapc,2025-10-30 06:53:03,9
AMD,nm678n8,My 9800x3d hasn't gone past 64C on a thermal right phantom spirit with no custom fan curves,buildapc,2025-10-30 11:15:01,1
AMD,nmb55kf,You're getting dunked on because one bot failed basic reading comprehension then people blindly followed lmao.,buildapc,2025-10-31 03:01:27,1
AMD,nm4wy27,"Yup, fun times after paying the Noctua tax lol",buildapc,2025-10-30 04:04:40,0
AMD,nm84tfs,"I don't even have my fans on a curve, basically 30% until something reaches 80° which at that temperature, something has gone really, really wrong.",buildapc,2025-10-30 17:22:19,2
AMD,nm6cmk5,"Hell, some systems are loud enough at full load that you hear them through open backed headphones.",buildapc,2025-10-30 11:53:47,9
AMD,nm75616,and people who play games that are heavy loads but aren't loud.,buildapc,2025-10-30 14:31:28,1
AMD,nm815vk,> the noise from air cooling a 9950x3d  Never in a million years would anyone convince me to stick an air cooler on a 9950x3D - Liquid Freezer III Pro 360 or go home.,buildapc,2025-10-30 17:04:41,2
AMD,nm9y1xi,You would be correct if I was asking about tasks - I meant that specific Thermalright cooler to the person I asked the question.,buildapc,2025-10-30 22:47:32,-19
AMD,nmkqimm,Absolutely.,buildapc,2025-11-01 18:43:44,1
AMD,nmbz9nv,"Typically mine hovers around 84° or so but I've hit thermal limits now and then depending on the weather, even on the 5700x using the NH-D15, and it was transcoding jobs.  I have a 5900x that has a 360 rad AIO and it is much harder to hit limits.  Actually come to think of it I think the 5700x is 95° while the 5900x is only 90°.  Weird.",buildapc,2025-10-31 07:24:29,1
AMD,nmajeym,"That's probably true regarding the 4K, I agree.  Typically CPU rendering ends up with a smaller file compared to NVENC, with higher quality to boot.  I keep falling back to Handbrake over alternatives, perhaps because I have custom presets I've made that I like.",buildapc,2025-10-31 00:50:25,1
AMD,nm6431p,"It's not more efficient. The arrangement of 3d cache makes cooling more effective and allows higher max temps, but that is offset by using more power (for perf). Similar cooling scenario in the end.",buildapc,2025-10-30 10:50:24,5
AMD,nm74pir,"I get 10% higher cinebench scores with pbo, curve optimizer, and +200 vs pbo + stock   With pbo+co+200, I hit 5425mhz on all cores at like 83c",buildapc,2025-10-30 14:29:12,2
AMD,nm5j1ux,he said “any” amd cpu btw,buildapc,2025-10-30 07:28:38,27
AMD,nm4t90q,"The title says modern high end cpus, then like 9900x   That can include everything 2 chiplet basically",buildapc,2025-10-30 03:38:13,-24
AMD,nm5imw9,"While I agree that the PS is usually more than enough for a 9800x3d, there are of course many other factors.  - Room temperature   - airflow and number of fans in the case  - A founder edition Nvidia 5000 series GPU blows its hot air up to the CPU instead of out the back. That could make a huge difference",buildapc,2025-10-30 07:24:15,5
AMD,nm746mj,"I only see limitations in cinebench, never in gaming.   If I use curve optimizer, pbo, and +200, I can get 5425 on all cores with 9800x3d   But if I just use pbo, I end up losing 10% performance vs pbo, co, and +200",buildapc,2025-10-30 14:26:36,1
AMD,nm5p1p6,"> Temps never go above 65c when gaming  I mean, good for you I guess? When I bench CPUs I'm using Cinebench and Prime95.  I know that I will never achieve that kind of wattage when gaming, but I at least want to validate it to make sure it's not throttling under *any* conditions. Otherwise I wouldn't consider the cooler adequate.",buildapc,2025-10-30 08:30:29,1
AMD,nm73fcs,Try cinebench,buildapc,2025-10-30 14:22:50,1
AMD,nmb6tdj,Yeah I don't get it. I have a fairly level headed opinion here.  Bots! Bots everywhere!,buildapc,2025-10-31 03:12:36,1
AMD,nm89w8f,"I have mine on a curve but they rarely ramp up very high at all. I'm also on a custom loop with triple 360s and the loudest noise it makes is the occasional PSU fan firing up. I'm sure turning the front fans up a hair would fix it entirely, I just can't be bothered",buildapc,2025-10-30 17:46:56,3
AMD,nmjgqan,"Hell, I heard my old pc thought my closed back headphones",buildapc,2025-11-01 14:45:34,1
AMD,nm9ybd7,"To which they already said, is more than enough for any amd cpu.",buildapc,2025-10-30 22:49:03,14
AMD,nmhseoy,You are illiterate,buildapc,2025-11-01 05:57:46,4
AMD,nmd22zu,"> Typically CPU rendering ends up with a smaller file compared to NVENC, with higher quality to boot.  That is true, in the end it's a tradeoff, like most things in engineering. Want 50x faster transcoding time? That'll cost you space and small quality difference :) For some, it's worth it, and that's OK! Personally I can't stand the hours long encoding jobs anymore, mostly why I went GPU-only for almost everything, but definitively understand people still using CPU for it too.",buildapc,2025-10-31 12:51:21,1
AMD,nm6kltf,"Yeah, 7800X3Ds were extra efficient because they kinda had to be, since the thermal path from cores through the cache chips to get to the heat spreader and eventually the heatsink wasn't great, therefore had to run somewhat conservative clock speeds and voltages compared to the non-X3D 7xxx models (and was also locked so you couldn't even attempt overclocking). 9xxxX3D has better thermal contact from cores through to heatsink so can run higher speeds and therfore worse out of the box efficiency with the higher power, but it can get more performance out of it.",buildapc,2025-10-30 12:43:44,1
AMD,nm75u7s,Damn that's pretty good,buildapc,2025-10-30 14:34:50,1
AMD,nm5vm1e,"""I'm planning to upgrade from my current AM4 socket CPU PC to a new one on the AM5 socket with the AMD Ryzen 9 9900X. """,buildapc,2025-10-30 09:36:14,-11
AMD,nm5cl5q,"To add onto this, the comment that started the chain said ‘any AMD CPU’, they didn’t specifically say ‘non X3D AMD CPU’.",buildapc,2025-10-30 06:22:56,14
AMD,nm5whm2,"Agreed, but these are fundamentals.  You may as well say air coolers air fine for all AMD CPUs as long as they have access to air for the cooling.  Even with the hottest of FEs exhausting straight at my dual tower I have no problems in my case. Best advice is to consider these factors and cooling method when buying the case, and vice-versa.",buildapc,2025-10-30 09:44:33,4
AMD,nm6j2d8,Just the 5080 and 5090.  5070 FE doesn't blow air upwards.,buildapc,2025-10-30 12:34:37,-1
AMD,nm7tkyq,> limitations in cinebench  And rightfully so... its a synthetic benchmark that will stress however many cores are available.   Temps on non-synthetic are always going to be lower.,buildapc,2025-10-30 16:28:21,1
AMD,nm5ruui,I do that too and I never reach anything even remotely close to throttling temperatures with my 7800x3D,buildapc,2025-10-30 08:58:44,4
AMD,nm76xxz,"With ps120 and gelid phase change pad, co-30, pbo mobo, and +200 is 10% higher score than stock+pbo with 9800x3d for me   Final oc r23 of 24422",buildapc,2025-10-30 14:40:19,1
AMD,nm7u9c8,"Max my 9800X3d ever saw in Prime95 was 85c with the PS120SE.   So you either have a low-binned chip, or something else is wrong like fan curve or something in the BIOS could be tweaked to better improve temps. Ambient temps also play an important factor. I keep my house sitting at 72F.   My max fan usage is 80% when temps hit above 75c. Between 20-50c, fans are 40% and 50% from 50-75c. Fans are barely audible.",buildapc,2025-10-30 16:31:36,1
AMD,nm74zd1,Nah I use real world work loads,buildapc,2025-10-30 14:30:33,1
AMD,nm8d4db,"Yeah I have a 420 and 280, as I love 140mm fans. My old EVGA power supply was the loudest fan but it was 10 years old so I replaced it with a fresh 10 year warranty EVGA unit and this one has a much quieter fan as it's 140mm as well lol",buildapc,2025-10-30 18:02:06,1
AMD,nm9z88m,"No, they said air is fine for all tasks instead of the specific thermalright ps120 I was asking about.  It can be fun to argue sometimes, I realize.",buildapc,2025-10-30 22:54:13,-14
AMD,nmhtxtn,"Yes I'm not:      ""Has anyone really been far even as decided to use even go want to do look more like?""      “You’ve got to be kidding me. I’ve been further even more decided to use even go need to do look more as anyone can. Can you really be far even as decided half as much to use go wish for that? My guess is that when one really been far even as decided once to use even go want, it is then that he has really been far even as decided to use even go want to do look more like. It’s just common sense.”",buildapc,2025-11-01 06:14:31,0
AMD,nmeccma,It's in realtime or near-realtime for me for permanent storage and can be queued so it is worth it to me.,buildapc,2025-10-31 16:47:07,1
AMD,nm76ip6,R23 score if 24422 which is top 1% for the chip   Gets loud with default fan curves though,buildapc,2025-10-30 14:38:13,2
AMD,nm638hu,“a $35 dual tower air cooler like a thermalright ps120 is *more than enough for* **any amd cpu**”,buildapc,2025-10-30 10:43:33,14
AMD,nm74aq7,Thank you,buildapc,2025-10-30 14:27:10,1
AMD,nm7u307,"gaming is mostly single threaded, with one heavy render thread, and many lighter threads  cinebench is actually a benchmark to replicate the maxon rendering engine, which is used for cpu rendering of stuff professionally  It's a benchmark, but it replicates a specific real world workload",buildapc,2025-10-30 16:30:45,1
AMD,nm75a7b,Cinebench is actually a real world workload turned into a benchmark   Its rendering using the maxon rendering engine,buildapc,2025-10-30 14:32:03,1
AMD,nm97f5x,"I have one of those Super Flower ""High Density"" PSUs, so it's all packed in there, which isn't great for airflow, but it fits in smaller spaces. Whole thing's only 130mm long",buildapc,2025-10-30 20:26:54,2
AMD,nma1oe2,"> a $35 dual tower air cooler like a thermalright ps120 is more than enough for any amd cpu  This is the comment you are referring to, is it not? They specifically, word for word, said  ""is more than enough for any amd cpu""",buildapc,2025-10-30 23:07:53,13
AMD,nmiy7no,Tldr please,buildapc,2025-11-01 12:52:39,2
AMD,nmecmn5,"Yeah no, makes sense when you don't need to transcode 20 hours of video in 2 hours :) Everyone uses what's best for them, and it's great we have choices.",buildapc,2025-10-31 16:48:29,1
AMD,nma7nz8,"No problem, though I am baffled at the upvote/downvote counts on this comment chain. You're at like -20 right now, and even though I'm backing what you're saying I have +15? lol. Reddit is so bizarre. Glad IDGAF about the karma system, would be miserable if so.",buildapc,2025-10-30 23:41:31,1
AMD,nm7zwnh,"> specific real world workload  Yes, an all-core rendering workload.   Its not entirely representative of CPU performance for everybody. Especially not for gaming.   Its mostly used as a test to check for stability - checking for any abnormal behavior after an overclock/undervolt etc; and temperature - testing if your fan curves are set the way you want them for the temps you're looking for in an all-core scenario.  Something as simple as a change in temp of 1-2 degrees and your Cinebench score can swing a few hundred points, upwards of 1000 or more depending on how heatsoaked the PC is.",buildapc,2025-10-30 16:58:34,1
AMD,nm77e9n,"Last one I used was r20 years and years ago, what would you recommend for 2025?",buildapc,2025-10-30 14:42:32,1
AMD,nma37ym,"...which is why I asked for task clarification and the person replied.  If you are new here, sadly the majority of the community mistakenly thinks gaming is the end-all be-all of anything.  Gaming is a freaking joke compared to some tasks.      I'm glad you've had a chance to catch up.",buildapc,2025-10-30 23:16:32,-8
AMD,nmkqdx4,Yes,buildapc,2025-11-01 18:43:02,1
AMD,nmedpyy,"True, but in your example, needing 20 hours of video in 2 hours is an interesting scenario.  What was it about?",buildapc,2025-10-31 16:53:47,1
AMD,nma89xc,Thats bizarre lol,buildapc,2025-10-30 23:44:56,1
AMD,nm82hoq,"""Its mostly used as a test to check for stability""   It's a piss poor test for stability. You want linpack for stability testing. Linpack can find instability in 30 seconds that passes 24 hours of cinebench.",buildapc,2025-10-30 17:11:11,1
AMD,nm78cws,Cinebench is still fine for benchmarking,buildapc,2025-10-30 14:47:14,1
AMD,nmz80tz,"GT is the newer refreshed version with higher clock speeds.  You should be aware that the onboard graphics of these CPUs are not really capable of any modern gaming. They don't even have up to date drivers, and were already kinda old when they launched 5 years ago.   Any sort of used video card from the last 10 years will probably be in the order of 5-20x as fast as the onboard graphics. If you can find something like a $30 GTX 970, then that is already way better. I would recommend a cheaper CPU like R5 5500 and any cheap old used graphics card if you intend to play any games on this machine.",buildapc,2025-11-04 00:41:16,3
AMD,nmz9yjo,"No 5500, 5600, 5700 second hand market?",buildapc,2025-11-04 00:52:34,2
AMD,nmzaoq9,"firsthand can tell you that you that you can play Minecraft, CS, tons of 2D games on that thing if you haven't tasted 144hz yet  unless you don't have room for a GPU or your PSU is fucked up it is worth your while to use the cheapest dedicated graphics you can find anyways",buildapc,2025-11-04 00:56:47,1
AMD,nmz9dqe,+1 GTX or Polaris GPU is 10x integrated and are Ewaste prices,buildapc,2025-11-04 00:49:11,1
AMD,nmtqcta,"Your mistake is thinking you can reuse the RAM, you can't. They are physically different. You need DDR5.",buildapc,2025-11-03 03:59:54,1178
AMD,nmtqmi8,"1: If your main priority is gaming, then definitely don't waste the extra cash on a 9950X3D. The 9800X3D will perform basically just as well for gaming.  2: You can't re-use the RAM from your old build. You'll have to buy a DDR5 kit.  3: That 5070ti seems overpriced. They should be pretty readily available at or close to MSRP, although it could be different if you aren't in the US.",buildapc,2025-11-03 04:01:47,186
AMD,nmtqfnk,"you can't reuse your RAM, am5 only supports ddr5",buildapc,2025-11-03 04:00:26,139
AMD,nmtqj4h,Get the ryzen 7 9800 3xd and get the lower end rtx 5080. You will end up at the same price with 15% better fps. About to do a build like this in 2 weeks with black friday deals finally dropping priced again.,buildapc,2025-11-03 04:01:07,349
AMD,nmts9m4,"IMO you’re overspending in places. $240 for a AIO seems like robbery. The 9800x3d is perfectly fine for a 5070ti especially if you’re mostly gaming. Lian Li is good, especially when it’s a good price. It’s usually not a good price.   If you drop the cpu to 9800x3d, drop the Lian Li tax, you’d be able to get a 5080 in there, and save money. That will be better for gaming in the long run.",buildapc,2025-11-03 04:13:14,77
AMD,nmttw3c,Gotta love when people buy first and then post to this sub questioning their choices. I feel like half these posts are rage bait.,buildapc,2025-11-03 04:25:13,61
AMD,nmtupqs,"https://pcpartpicker.com/list/RRCGyW  This is very close to the same performance, but much cheaper. Also has the DDR5 ram you need.",buildapc,2025-11-03 04:31:24,19
AMD,nmtqeyj,"Ddr4 ram wont be compatible to start, you also posted 2 very different builds here lol.",buildapc,2025-11-03 04:00:18,38
AMD,nmtqg8v,Can't reuse your DDR4 with AM5. Requires DDR5.,buildapc,2025-11-03 04:00:33,12
AMD,nmtqocv,"Could have saved money on CPU, case fans and cooler to get a 5080 and ddr5 ram. Why spend so much and stay with ddr4?",buildapc,2025-11-03 04:02:10,27
AMD,nmts32x,Not super into PCs and drops $2700. That is ludicrous.,buildapc,2025-11-03 04:11:56,75
AMD,nmtqhk3,DDR5 is not compatible with DDR4. You can't reuse your AM4 RAM on an AM5 build.,buildapc,2025-11-03 04:00:48,7
AMD,nmtt6wl,Bruh you can’t reuse the Ram. Did you not do any research before hand? You need ddr 5,buildapc,2025-11-03 04:20:01,7
AMD,nmtr8vy,"Everyone has covered the memory incompatibility. If you have already ordered all that, i'm not sure why you are posting about it. If it was me I would have gone for 9800X3D, that CPU you have is way OP for gaming, it will run worse than 9800X3D for more money. Motherboard you could have gone B850 to save money. That saved money could have been then put towards 32GB kit of DDR5.",buildapc,2025-11-03 04:06:01,10
AMD,nmtsnyh,1.38$ a day for 10 years for solid gaming,buildapc,2025-11-03 04:16:10,5
AMD,nmts7cp,"i would argue that there is a good deal of wasted money here but i wouldn't call any of it a ""mistake"" apart from the DDR4, you need DDR5. definitely go 9800x3d, and if you want to save money then go 9070xt as it's basically a 5070ti but $150 less. you don't really need x870/x870e, so just get the cheapest board that you like the look of, has the features you need, and has good power delivery.",buildapc,2025-11-03 04:12:46,3
AMD,nmtt0wb,Why go 9950 with that video card.... 9800x3d and save 300 bucks. Or get a 5080 at minimum,buildapc,2025-11-03 04:18:47,3
AMD,nmttkbc,"That cpu is super overkill, go 9800X3D. And you have to have DDR5. I  would say keep rolling with the 2060 and wait for the upcoming (when? idk) 5000-series Supers to drop and get a 5080 Super. But 2060, woof! 5070 Ti not bad but with the money saved on cpu, you can afford the regular 5080.",buildapc,2025-11-03 04:22:47,3
AMD,nmtuz3s,"Bro, you made a mistake. If your main purpose is gaming, you overspend on cpu. A 9800x3d or 7800x3d would be more than enough.  Your motherboard - an X-series mobo is a waste unles you intend on overclocking. A B-series mobo would have been fine.  A $250 360 aio is WAAAAY overpriced. A thermalright 360 aio @ $60 would have been more than enough. Heck even a 240mm aio would be fine.  RAM, you would need DDR5 for any amd cpu after 5000 series.  The case is sensible, the psu is sensible.  Not sure what specs your monitor has, but with the money saved you could have gotten a 5080 and an OLED 1440p monitor.  Also, you may want to ditch the HDD and get an nvme SSD.",buildapc,2025-11-03 04:33:24,3
AMD,nmtt5am,"Too expensive CPU.  Too expensive mobo.   And yes, you made a mistake because DDR4 can't be reused.",buildapc,2025-11-03 04:19:41,7
AMD,nmtub7i,"If it makes you feel better, I recently spilled a Miller Lite into a brand new thirty five hundred dollar tower right through the top fans and radiator.",buildapc,2025-11-03 04:28:21,2
AMD,nmtx80d,"You could probably just toss a 5800X3D CPU upgrade after updating your motherboard BIOS. You should double check compatibility between the motherboard and CPU before going this route.      Then swap the graphics card. This is an option if you'd rather put off the upgrade. The DDR5 prices are insane atm.  The other option would be to pick up a used 5XXX generation Ryzen CPU and motherboard combo, reusing your other components aside from the GPU.",buildapc,2025-11-03 04:50:45,2
AMD,nmub1rz,"I don't get why people ask this after they spent the money.  Btw I just dropped over 1k for changing my PC without any performance gains. Just different Mainboard (which is even a technical downgrade from X870E to X870), different case and different cooling. It's hobby and it's fun.  If you like what you bought and enjoy it so be it.",buildapc,2025-11-03 06:55:57,2
AMD,nmuhsfi,You made a few mistakes.   1. DDR4 RAM already noted in the top comment.   2. Overspent on CPU.   3. Overspent on CPU cooler.   4. Massively overspent on case fans.   5. Underspent on GPU.   6. Underspent on storage.,buildapc,2025-11-03 08:04:23,2
AMD,nmuknks,Why would you ask this *after* buying everything? Bro is just attention farming,buildapc,2025-11-03 08:34:27,2
AMD,nmvbul4,The TUF 5070 ti is awesome though. No need to replace that.,buildapc,2025-11-03 12:41:31,2
AMD,nmvgvc5,"I would have taken the 5070ti, with a cost of more than 700 I expect the best and most supported tech and therefore Nvidia for life, the raster performance will also be 5% less but I don't care, we are in 2025 and tech like the dlss 4 are indispensable for me in this price range",buildapc,2025-11-03 13:13:18,2
AMD,nmttl35,"If you live near a micrometer you can get a cpu/mobo/ram bundle for way cheaper than what you spent (and also compatible). You also don’t need x870e for gaming, don’t need an AIO, don’t need so many case fans, don’t need a 1000W PSU, and don’t need that expensive of a case. However, aside from the RAM, it should be fine. You could probably build something with almost exactly the same performance for ~$1800.",buildapc,2025-11-03 04:22:56,1
AMD,nmtul7c,Its a mistake only if you cant afford this kind of expense at this moment.,buildapc,2025-11-03 04:30:28,1
AMD,nmtushg,Its a good PC. You may have spent abit more but its good overall,buildapc,2025-11-03 04:31:59,1
AMD,nmtv134,Why would 7 year old ram still be the standard...,buildapc,2025-11-03 04:33:49,1
AMD,nmtvedt,Dear God this is bad,buildapc,2025-11-03 04:36:37,1
AMD,nmtw627,"For what you are buying, I would check FB marketplace. I have seen comparable mobo, processors, cooler, fans all for 50-70% (sometimes better if willing to drive around collecting the parts). You should be able to walk away with the same or similar build for less than half your planned investment... which should make you feel a lot more comfortable with your build choice",buildapc,2025-11-03 04:42:29,1
AMD,nmtwarl,"If longevity and VMs matter, the 9950X3D path is fine, but fix DDR4 issue with 64GB DDR5 6000 and add a NVMe",buildapc,2025-11-03 04:43:28,1
AMD,nmtwsha,"Get the 9800X3D. That AIO is crazy expensive, the Arctic Freezer III is better and cheaper. You need DDR5 memory, get 2x16GB DDR-6000 cl30 or cl32.",buildapc,2025-11-03 04:47:18,1
AMD,nmtxscj,What kind of VMs are you running?,buildapc,2025-11-03 04:55:16,1
AMD,nmty2k1,"Unless you're going for a total showcase PC, you've grossly overspent on fans and AIO. You also don't need the 9950X3D. Others have also already mentioned your RAM. Can't use your old kit.",buildapc,2025-11-03 04:57:32,1
AMD,nmtyyrq,"As others are saying is swap the 9950x3d to 9800x3d and swap the aio and fans for something else, either just other brand or in running a Noctua U12A chromax on my 9800x3d.  But it depends, you want the specific look thats cool, but its somewhere you can save some money.",buildapc,2025-11-03 05:04:56,1
AMD,nmtzuuz,"your use case is confusing. you're not super into PCs, but you are running multiple VMs. why? for what purpose?",buildapc,2025-11-03 05:12:20,1
AMD,nmu01yk,"I build my pc with 7600x and tomahawk b650 32gb ddr5 I play triple A games with no issues. Don’t cheap out on the graphics card. But I think you could save so much money on CPU and mobo by lowering them in my opinion. My Pc cost me $1,000 and been going strong with no issues",buildapc,2025-11-03 05:14:01,1
AMD,nmu04jp,That’s a strange build. Especially for gaming which is not as much CPU dependent you can get a mid-range CPU and motherboard. But you got really high end CPU and motherboard but a mid range GPU and extremely expensive fans and cooling equipment for your CPU. Oh and again crazy thinking you want the highest end CPU but use old RAM which would heavily bottleneck it if it was even compatible in the first place,buildapc,2025-11-03 05:14:37,1
AMD,nmu0txy,Honestly I’d upgrade your current system with a 5800X3D and then a nice GPU. Then down the road you can keep the GPU and get a new machine for it,buildapc,2025-11-03 05:20:39,1
AMD,nmu0xoq,Overspending on a lot of things that don't really matter.  9950x3d could be 9800x3d  Cooler can be 100 bucks cheaper  Mobo can be a lot cheaper. Good ones are around 200  5070 ti for 850 is expensive. Add 150 more you can get 5080 or you can save 50-100 bucks for cheaper models.,buildapc,2025-11-03 05:21:33,1
AMD,nmu2od3,Seems fine for general gaming and such. I wouldn't worry.,buildapc,2025-11-03 05:37:03,1
AMD,nmu2rmf,The RAM Memory tipped me off you said 3200 MHz? I recommend 6000 MHz plus first check your Motherboard RAM Memory QVL,buildapc,2025-11-03 05:37:52,1
AMD,nmu31m3,"Damn the cpu cost even more than a ps5, pc gaming is really for people who are rich.",buildapc,2025-11-03 05:40:23,1
AMD,nmu31x1,That aio is massively overpriced and not needed. A $40 Thermalright cooler is good enough for am5. Put that money into your monitor.,buildapc,2025-11-03 05:40:27,1
AMD,nmu3knh,Classic move to get the absolute max cpu and then a mid gpu,buildapc,2025-11-03 05:45:10,1
AMD,nmu3yt5,Overpriced af,buildapc,2025-11-03 05:48:44,1
AMD,nmu404y,I agree with most comments. I havent seen one about a monitor. If I was spending alot on my PC I would also go for an amazing QD-OLED Monitor.,buildapc,2025-11-03 05:49:03,1
AMD,nmu4vpj,2TB 5400RPM HDD? What's on there? Photos and videos?  Games should be on at a minimum an SSD in this day and age.,buildapc,2025-11-03 05:57:11,1
AMD,nmu4wvk,Why are you ordering a V2 flow and all of the TL fans?  TL fans are a waste of money if you aren't getting at least a high end 5080,buildapc,2025-11-03 05:57:29,1
AMD,nmu5ffc,"I mean you definitely fucked up but the other comments have told you why lol. If you don't want to return anything for the better deals then the only thing you'd likely want an upgrade for is an NVME SSD, 2tb drives have gotten pretty cheap and there are many games that feel significantly slower when running from an HDD.  Keep the HDD for storing backups but your best bet would be to get a 1-2tb for a boot drive and use your current 500gb SSD as a secondary drive once you've transferred all your stuff (or wipe it and use the 1-2tb for a games drive). A fresh Windows install would also be good since you've swapped close to everything.",buildapc,2025-11-03 06:02:21,1
AMD,nmu699e,Why are you getting aio? Want it to leak and fuck your hardware up? Literally get peerless assassin or phantom spirit heatsink..  Aios are overrated shite with very limited longetivity,buildapc,2025-11-03 06:10:09,1
AMD,nmu73lp,I still understand why so many builds here have an cpu gpu ratio that is so far off. That cpu is so much ahead of the gpu. Could get one 200 dollar cheaper and get a 5080y,buildapc,2025-11-03 06:17:57,1
AMD,nmu7dtl,9800x3d I think is better for gaming,buildapc,2025-11-03 06:20:37,1
AMD,nmu7v0u,You can get a cheaper CPU and get a 5080,buildapc,2025-11-03 06:25:09,1
AMD,nmu88gx,"as others have stated, AM5 is not compatible with DDR4, AM5 needs DDR5",buildapc,2025-11-03 06:28:41,1
AMD,nmu8bmm,"The CPU is overkill, get a 9800X3D, and the cooler seems to be way overpriced, you can get a 360 rad AiO for half that that is better performing and get a better GPU",buildapc,2025-11-03 06:29:31,1
AMD,nmu9e6d,"You really didn't need a 9950x3d, and save 200 bucks there. With some other adjustments you could probably get a 5080",buildapc,2025-11-03 06:39:56,1
AMD,nmu9ek8,"Why are you spending $240 on an AIO? You could buy an air cooler for the 9950 and it will likely work fine, or you can get an Arctic Freezer III 360 for like $90 which is one of the best AIOs. I'm assuming purely cosmetics? You could save $200 there and put it towards a 5080 or something.   Other than that, you spent a ton on the fans, can get some for much cheaper, and I would get a better monitor than the one in your first setup given how nice your GPU is. I would personally get an OLED like the QD OLED you mentioned.",buildapc,2025-11-03 06:40:02,1
AMD,nmua63i,"I definitely like the second build better, but realise that you're spending ~$350 on aesthetics that couldve gone to a oled monitor in the first build or a 5070ti on the second build. Spending some money on aesthetics is totally okay, but $350 is a lot",buildapc,2025-11-03 06:47:26,1
AMD,nmuam7g,Don’t do like me. Save money on water cooling and fans and get a new ssd. I went from a 970 to a 990 and the saving in load times for the games I play and work I do paid for itself in a day (mostly the work but the games feel much more smooth in some cases)  That hard drive will hurt boot times.  Depends what you play. My steam deck some games play off and an sd card,buildapc,2025-11-03 06:51:47,1
AMD,nmuamnt,I bought my first PC in 1984 for $3000.  Every PC I have bought since was about $3K.  I have never been disappointed.,buildapc,2025-11-03 06:51:54,1
AMD,nmubqzg,You’ve spent like $500 on fans/aio when you could just get something like the peerless assassin cooler for $50. 9800x3d is better than 9950 as well.,buildapc,2025-11-03 07:02:50,1
AMD,nmuc7br,"Waow alright. Lot of misconceptions about what makes a pc perform well. But if you don't know you don't know.  Here is my 2cents.  Air cooling is just as good as water cooling for a gaming session when considering performance and temperature. Air coolers last longer and are cheaper. Aio coolers are generally way quieter and will make the system cooler on short gaming sessions (not going into the specifics here but basically, good air cooler means same fps as good aio).  The best gaming cpu is the 9800x3d, it's also a good productivity cpu unless you do professional speicfic things at which it's bad at, which you don't.  The best bang for buck gpu without compromise is the rtx5080 or one of the AMD new releases if you Don't care much about RTx or Cuda. There is, mark my words, and listen to me instead of all those who are marketing victims, there is NO benefit in terms of performance to buy an expensive GPU like a RTX 5080ti pure power giga buff quad fan gaming special edition. Just buy the cheapest 5080 from a known brand. The cooler and the sticker on it won't make a big difference. The only things.to consider are : does this card make coil whine, is it loud, is the warranty good.  Ram speed matters for ryzen. When you buy DDR5 there is a sweet spot in ram speed that you need to consider if you want your build to last. Research the appropriate OC ram speed for your CPU.  I bought a high end rtx4080super computer a couple years ago and it was way cheaper than your current build and compared to it, it's more powerful for its time, and similar power now. I think you are overspending 700bucks at least.",buildapc,2025-11-03 07:07:17,1
AMD,nmud284,"If anything I Made the mistake. I should’ve waited, but I didn’t wanna, so I got a 9800x3d 4080 super combo 100$ more than this build. A couple months ago Early 2025. March or April I forget. Not sure how, don’t even remember where the receipt is.",buildapc,2025-11-03 07:15:49,1
AMD,nmuej49,"300$ ish motherboard for what? Im all for high end myself but realistically a 120$ board will do just fine, then you can get the leftover cash to grab a DDR5 RAM(Gskill Trident Z5 6000 CL30)  Edit : If you wanna save some money on the AIO, then check put the Arctic Liquid Freezer III, its only 100$ and its argueably the best on the market right now. That should save you another 130bucks for something else",buildapc,2025-11-03 07:30:41,1
AMD,nmufk0s,"Another thing that I want to add is Flow version of mini v2 case comes with 5 pre-installed fans too and they are really good. If you wana save cash, you can either return the lian li fans (except one for exhaust) or get a non flow version.",buildapc,2025-11-03 07:41:24,1
AMD,nmug03s,QLED and OLED are two different things,buildapc,2025-11-03 07:46:01,1
AMD,nmugvpp,I might even use all those cores of a 9950x for rendering or possibly cpu based AI tasks.  Still I opted for a 9800x3d.  If I am going to render with the cpu I need to plan accordingly.  So what if it takes 20 minutes instead of 10.,buildapc,2025-11-03 07:55:05,1
AMD,nmuh28k,"If you need VMs its always cheaper to buy separate PC than messing with gaming PC and turn them on/off. I bought used 6core 32gb ram office pc from Micro-center for 300$. It’s always on, and has what I need.  I have Ubuntu server there with CasaOS, and bunch of stuff in Docker like N8N, Redis, Grafana, Prometheus, and some other stuff. Windows VM with corp VPN.",buildapc,2025-11-03 07:56:55,1
AMD,nmui6if,Get an OLED monitor. Its worth it.,buildapc,2025-11-03 08:08:26,1
AMD,nmujja9,"If you're happy with the computer you've built, that's the most important thing. When you've put your heart into putting it all together, you can't go wrong.",buildapc,2025-11-03 08:22:34,1
AMD,nmukq27,"since no one has pointed it out, playing games off a hard drive is very suboptimal in 2025. get a second nvme ssd or a sata ssd at the very least",buildapc,2025-11-03 08:35:12,1
AMD,nmul1s4,"7800x3d is basically the same speed as 9800x3d even with a 5090 at 1080p - at higher res and with a slower GPU the difference is non-existent. So you may want to consider the 7800x3d and invest the additional ~$100 in another GPU upgrade. Or the RAM that you will need.   X870 and B650 are the same chipset - X870 just has two of the same chip in series to give you some more PCIe, M.2, and USB slots. If you don't use them you are paying for them for no reason.   Others already pointed out you can't reuse the RAM. Additionally I want to add, do not use four sticks on AM5. If you need more than 32GB, get a 2x24GB kit. Only if you really need 64 or more, get a 2x48GB kit. DDR5's high frequency is difficult for the AMD memory controller (7000 and 9000 series btw use the same memory controller). Four sticks are the hardest to handle, dual rank sticks (32 or 48 per stick) the second hardest, newer memory (24 or 48 per stick) is easier on the controller than older (16 or 32). So ideal for speed is 2x24, ideal for speed and capacity is 2x48. If you get 2x16 or 2x32, definitely get 6000CL30, that CL will be Hynix chips which are much better than Samsung or Micron for that generation of memory (for the 24Gbit chip generation I don't believe there is nearly as much of a difference, so you can just buy according to your budget).",buildapc,2025-11-03 08:38:42,1
AMD,nmul4pc,"You need DDR5, not DDR4. Suggest 6000 CL30.",buildapc,2025-11-03 08:39:35,1
AMD,nmumguw,"That's one overpriced motherboard,",buildapc,2025-11-03 08:53:40,1
AMD,nmun3tz,You can't put DDR4 on DDR5 slot and you picked the worst time to buy RAM ugh,buildapc,2025-11-03 09:00:22,1
AMD,nmuputc,"Your mistake is that 500gb SSD. I'd get a 2tb one cause there will be games that will run a tad faster with it, and get a newer spinning rust drive for a tad older games, or a sata ssd like 2tb instead of spinning rust.  Up to you.",buildapc,2025-11-03 09:30:18,1
AMD,nmuqunf,"9800x3d is blown way out of proportion. If you are worried about cost, get the 9800 NON X3D, AKA the RYZEN 9700X. It's like nearly 200 dollars cheaper. Also don't overspend on memory either. Cheap SSD and ddr5 will do just as well as the expensive stuff. Also also that 360 cooler is extremely overpriced. Get something from thermalright for less than half the cost. Also grab a 5080. It's well worth the premium over the 5070ti. Another thing you can overspend on is motherboards. I'd suggest something in the B850 range. Even I went for an X870 and haven't really seen the advantage.",buildapc,2025-11-03 09:40:56,1
AMD,nmutuj0,I think you’re still aiming too high with your processor. I have a 7600x paired with a 5080 graphics card and the graphics card is still the bottleneck.,buildapc,2025-11-03 10:11:15,1
AMD,nmuxxjf,"I would've probably upgraded the existing build to a 5800X3D/5700X3D + 5070 Ti / 9070 XT + 32GB RAM if its still running 16 - given the PSU allows for it. Whole point of building on an upgradable platform is, well... being able to upgrade components instead of building a whole new system.   You've already received decent feedback on your new components list. Have fun, also pcpartpicker is your friend. (:",buildapc,2025-11-03 10:50:49,1
AMD,nmvbgqr,"You spent about $1000 more than if you bought a prebuilt with similar specs.  Yea, your parts are better, but will that matter in 5 years when the prebuilt in question and your pc are both running fine but outdated?",buildapc,2025-11-03 12:38:59,1
AMD,nmvbxjz,Literally wait a month and get the monitor on Black Friday.  That CPU is way too much for a 5070Ti.,buildapc,2025-11-03 12:42:03,1
AMD,nmvc8l6,Damn lmao yeah you can’t reuse ddr4 but it’s all good,buildapc,2025-11-03 12:44:02,1
AMD,nmvd6qw,"I haven’t purchased battlefield 6 yet but I remember when I tried to play battlefield 2042 I kept lag spiking. I did some research and turned on Re-Bar in the bios. Night and day difference, made the game run so smooth. So if you haven’t done that maybe consider it.",buildapc,2025-11-03 12:50:09,1
AMD,nmvfurg,Get at least a descent mini LED monitor.,buildapc,2025-11-03 13:07:01,1
AMD,nmvg1vj,"You could have ""downgraded"" the CPU to a 9800X3D and still been completely fine since for gaming they are pretty much the same.  The CPU AIO is also ridiculously overpriced. You can get something much cheaper and will still work just fine. Mine only cost like $150 and I can overclock my 9800X3D no problem.  As others have said, you will need all new DDR5 memory. 32gb is enough for gaming.",buildapc,2025-11-03 13:08:14,1
AMD,nmvg8ah,If you’re really hurting for wanting that 5080. Newegg is offering daily deals and one is as was a $1539 5080 dropped down to about $899.  [NewEgg](https://www.newegg.com/promotions/nepro/25-1035/index.html?utm_medium=Email&utm_source=promo&utm_campaign=EMC-Automation110225-usa-_-EMC-110225-Index-_-doorbuster-_-Top-_-E1A&cm_mmc=EMC-Automation110225-usa-_-EMC-110225-Index-_-doorbuster-_-Top-_-E1A&&cvtc=14030345&tp=i-1NGB-Q7F-FOa-K7JU3T-2D-wrvt-1c-K7IA8G-lCWK3H0QJz-2HjStn&pi=vVYHFhsmmJC4-HDM-QKu6NqKChSgxnaTHA4DinpB2iI),buildapc,2025-11-03 13:09:20,1
AMD,nmvgpv0,"It seems like no one cares about the VMs part. The 9950x3D could be a lot better for that use but I can't really say outright if it would be. Do you need a lot of cores for those VMs? If you do, it is understandable to go for that CPU.",buildapc,2025-11-03 13:12:21,1
AMD,nmvka8k,"There are no mistakes, only ~~happy~~ expensive accidents",buildapc,2025-11-03 13:33:55,1
AMD,nmvn2r0,"That Second list looks pretty good, however with A 9700 xt you could save even more with a 7800x3d. Unless you plan to play at 1080p 90 percent of the time, you can save about another 100 bucks.",buildapc,2025-11-03 13:50:07,1
AMD,nmvn3uq,All that power just to play silksong,buildapc,2025-11-03 13:50:17,1
AMD,nmvnq7t,The new build is much better. I went with a 9800x3d and a 5080 about a month ago. You'll be pleased with the performance. That is a powerful setup.,buildapc,2025-11-03 13:53:45,1
AMD,nmvp7jc,"RAM is not compatible. Get DDR5 6000 CL 30. And idk where you’re located but here in America, I built basically the same PC(9070 XT) for like $1700-$1800.😬",buildapc,2025-11-03 14:01:52,1
AMD,nmvrh01,"Just some thoughts, I upgraded my fans to Noctua (amazon),  a bit more expensive. But very silent and a reputation of quality  (MTTF of 150.000houres). And if you case can fit them, 140mm fans are more silent as well with great airflow.  Amd you can re use the fans for a new build as well.",buildapc,2025-11-03 14:14:14,1
AMD,nmvrzy7,Am I wrong in feeling that a 5800x3d/5900x and like a 9070xt/5070 ti/5080 would have sufficed here?,buildapc,2025-11-03 14:17:02,1
AMD,nmvsb0b,"Just to give you an idea this is what i built recently  SKU	Description	Quantity	Price Per	Total Price 732115	LIANLI LANCOOL 207 BLACK ATX 1	89.99	89.99  596155	MSI MAG A850GL 80+G ATX 1	119.99	119.99  739615	THERMALRI PEERLESS ASSASIN 120SE 1	32.90	32.90 Match Competitor  865840	MSI PRO X870E-P wifi  1	191.92	191.92  774919	AMD AMD RYZEN 7 9800X3D WOF 1	408.07	408.07  440792	G.SKILL 32G 2X D5 6000 C36 FX B 1	120.00	120.00  516120	SAMSUNG E 2TB 990PRO NVME GEN4 SSD	1	159.99	159.99  762963	SAMSUNG E 1TB 990 EVO PLUS M.2 SSD 69.99	69.99  531830	MICROSOFT WIN HOME FPP 11 USB FLASH 1	139.99	139.99  800128	ASUS RTX5070TI PRIME 16G 1	749.99	749.99  603969	TWG TWG-2YR REPLACEMENT 1	24.99	24.99 026369	TWG TWG-2YR PROT PLAN 1	89.99	89.99 Subtotal »			$2,197.81  Sorry for the horrible formatting",buildapc,2025-11-03 14:18:41,1
AMD,nmvt5xm,Me personally I would have gone with a ryzen GPU as they just work better with the ryzen CPUs but other than getting new ram you should be fine,buildapc,2025-11-03 14:23:13,1
AMD,nmvuxux,"I don't think it's possible to gage how long a PC will last. I've seen some go 10 or more years on older hardware and I've seen some last less than 10 months. Just depends on usage and quality of the parts. Also, BF6 is already pretty optimized and can be ran on some pretty old hardware.  Per Google here's it's minimum requirements:  > Intel Core i5-8400 or AMD Ryzen 5 2600 processor, 16GB of RAM, and an Nvidia GeForce RTX 2060, AMD Radeon RX 5600 XT, or Intel Arc A380 graphics card. You will need Windows 10 64-bit, DirectX 12, and 55GB of free storage space (on an HDD) to run the game at 1080p with low settings.   Guessing you just wanted to get the best performance in-game since you were already meeting these specs...  Looking at what you picked out tho, you should be good. Just make sure to get good DDR5 RAM to go with it. You have plenty of RAM at 48gb, and both the CPU and GPU are solid choices for day to day task. I don't know jack about monitors (I just use my TV on a switch to swap between my PC, consoles, and TV system). Personally I'd opt out of an AIO and just spend the money on a good Air Cooler but that's just from a bad experience with a past AIO I had in a system.    Regardless, it's your rig, your money, so long as your happy with it that's all that matters dude. Hope you enjoy it.  SPS (Smiles Per Second) over FPS",buildapc,2025-11-03 14:32:45,1
AMD,nmvwlkh,You are overspending on AIO.  Just get an inexpensive 360 one.  And get a decent 2 or 4 TB SSD if prices aren’t crazy.    EDIT: I see you made a decent change.,buildapc,2025-11-03 14:41:33,1
AMD,nmvxewu,"That system will last you 5 years easily as hardware progress has slowed significantly.        I just wanted you to know lso had the option to upgrade your old system much cheaper (CPU, GPU and Monitor).  If you worry about money a lot, I recommend doing more value based builds and just upgrading more frequently.",buildapc,2025-11-03 14:45:48,1
AMD,nmvxhta,Just for reference I bought your second build at Best Buy for 1700. 9800x3d with the 9070. Obviously that’s including your extra purchases monitors etc. btw it’s a beast of a computer. I swapped out the 9070 for my gtx 4090 from my last build but I ran it a couple of days with the 9070 and it was so capable of running everything I wanted. I favored the 32gb of ram 6000mhz at 30cl which I saw as the most favorable timing for that set up but beyond I think you will be super happy with the build.,buildapc,2025-11-03 14:46:14,1
AMD,nmvz7sg,I don't see why you get the 9070xt now when on your og budget you could afford a 5080.,buildapc,2025-11-03 14:55:03,1
AMD,nmw5mbr,"You should get a 9800x3d instead of the 9950x3d. Even a 5090 will bottleneck before a 9800x3d for gaming, so spending a few hundred dollars more on a beefier CPU won’t actually benefit you in any way. You could use that money to grab a 5080 over the 5070ti which would give you significantly better performance.",buildapc,2025-11-03 15:27:13,1
AMD,nmw7nj7,"I didn’t even notice you were missing RAM in the first list. I was wondering why it was so cheap, haha.  FWIW, cosmetics are nice, but if you don’t want to spend a ton of extra money (sounds like you don’t) dropping $300+ on cosmetics is a bit rough.",buildapc,2025-11-03 15:37:08,1
AMD,nmw9afs,Looks much better. I’d get the Meshify 3 case tho,buildapc,2025-11-03 15:45:06,1
AMD,nmw9i5x,"I mean, yes, you did. If you split that into two computers you could have a quite good one now and then a *better* one for years 3-5.  Also a $300 motherboard and $700 for a cpu is just lol. Like what are you doing.",buildapc,2025-11-03 15:46:09,1
AMD,nmwfk2q,Buying 9070 XT instead of 5070 Ti for new $2'000+ build is delusional. Missing DLSS4 and FG won't help your OLED monitor,buildapc,2025-11-03 16:15:42,1
AMD,nmwh0kf,Bro got bullied into getting an AMD card,buildapc,2025-11-03 16:22:56,1
AMD,nmwksmy,"Just like me fr. I would’ve stuck with the GPU tbh. From the looks of it Radeon isn’t gonna give the same level of support as NVIDIA will to their GPUs. In other words, your card will be relevant for longer given its efficiency and technology that still surpasses Radeons. Radeon is good and catching up. Don’t get me wrong, but they’re just not there yet. At least with this generation. If all you care about is fps then go with Radeon I suppose, which only slightly edges out the 5070ti in SOME games.",buildapc,2025-11-03 16:41:31,1
AMD,nmwnkto,This amount of money and ur even considering less than 1440p🤣🤣🤣 This is a joke right?,buildapc,2025-11-03 16:54:47,1
AMD,nmwsbty,Honestly man you could fit a 5080 in that hudget. I know its not an oled monitor but heres what i wouldve done  https://pcpartpicker.com/list/Xvh4xg,buildapc,2025-11-03 17:17:12,1
AMD,nmwsma7,Check your ram. I was running 32gb of DDR5 but thanks to AM4 not liking 4 sticks and DOCP not working because of that I was only getting 3600mhz. Removed two sticks and hit 4800. Enabled DOCP and it finally hit 5800. No crashes since,buildapc,2025-11-03 17:18:34,1
AMD,nmwuzlk,Yes,buildapc,2025-11-03 17:29:50,1
AMD,nmww2x1,"Meh, I'm an IT guy who does light gaming and I just picked up a complete tower, used, b550 ac board, 3080, r9 5900x, 32GB DDR4 RAM for around 350 bucks. The CPU water cooler fan controller was bad, so I just moved the fans over to the motherboard and control them from that. So far so good and it is light years better than the 8350 with a 480 I've been rocking since 2015. It works for me and I'm happy with it, longevity it will probably be fine for 5 years for me.",buildapc,2025-11-03 17:35:03,1
AMD,nmx03co,"Also, if you still have time to return em, parts will definitely have sales on black friday/cyber monday. I know AMD GPUs get a lot of praise but... honestly you should just get an Nvidia gpu if you're going to spend that much. AMD is more for the cheaper builds. A 5070 Ti or 5080 for your budget range. And yea, the Ryzen 9800 x3d is perfect and will definitely last you 5 years   Again, definitely check out sales later this month, will save ya a good chunk of cash.",buildapc,2025-11-03 17:53:46,1
AMD,nmx2n0j,"Careful with those Sapphire Nitros 9070XT, lotta cases of them burning the pcie connector on Reddit, they use those new PCIE 5.0 instead of the usual 8-pin and apparently it's possible to cause failure on the future. I'd refrain but it's your choice",buildapc,2025-11-03 18:05:47,1
AMD,nmx6qyi,Jeez PCs really are cheap in America.,buildapc,2025-11-03 18:25:32,1
AMD,nmxfbs2,Everyone is so blinded by ram speeds they never stop to check the clocks. DDR5 is no practically different than ddr4. Reddit has convinced you to throw your money at a problem that isn't there.,buildapc,2025-11-03 19:07:07,1
AMD,nmxhfsx,"Ram needs to be upgraded to DDR5, and my personal experience has been that DDR5 is very unstable in a four-slot configuration, so just do 2-slot, and stick with 6000mhz as the higher clock speeds are also unstable. Even at 2x32 my PC does ram training every 12 reboots or so, but my coworker who went with 4 slot and 6400mhz couldnt even get it to work. I think he ended having to run them at 5600 or something in the end.      keeping your HDD sounds good on paper for just general storage, but my experience has been that the computer constantly indexing the damn thing and spinning it up for no reason when it's not even being used gets on my nerves, and actually slows down the computer for a second when it happens. Get an external HDD that you can unplug when you dont need it, 14TB is relatively cheap by comparison to SSD, so you can dump all the data you could ever dream onto it and just turn it on when you actually need it. My two cents. WD Blue is their garbage tier drive anyway. They liked using shingled storage on these too, meaning that you'd only get full performance on the first 75% of the storage and then it would turn into a turd because they're like writing another layer of storage over the first one or something. They're not actually 2TB; their more like 1.5TB and they're squeezing extra storage out of it.       9950X3Ds are weird in that they have separate gaming cores and efficiency cores, and it will park either set based on what you're doing. Which is great if you jump back and fourth, between gaming and work, but my understanding is that for specifically gaming the 9800X3D might do a better job, it will just be less efficient outside of gaming, but like, most people dont really do like a bunch of rendering where they'll notice that efficiency loss. Pretty sure that's how it works, but I have the 7800X3d so that might be outdated info.",buildapc,2025-11-03 19:17:30,1
AMD,nmxlsgf,Your RTX 2060 isnt giving you 90 fps on high settings?,buildapc,2025-11-03 19:39:00,1
AMD,nmxnp8d,"Too much, same performance for less then 2k",buildapc,2025-11-03 19:48:24,1
AMD,nmy1zye,You definitely could have saved a lot of money and still “future proofed” your system.,buildapc,2025-11-03 20:57:00,1
AMD,nmy2gr7,Pics of build?,buildapc,2025-11-03 20:59:13,1
AMD,nmy486d,Whats the deal with these ryzen 3d chips? double to triple an i5 14600kf and really no faster....  I could build a PC as fast or faster than this for 1/2 the cost!,buildapc,2025-11-03 21:07:45,1
AMD,nmy48mg,"Okay so I have a 9070 XT. When you get the PC built update your chipset drives and regular drivers.  I've had a lot of issues recently, and that seemed to fix it for me",buildapc,2025-11-03 21:07:49,1
AMD,nmy675h,Seen some people say get a 9800x3D and a lower 50s card and while I agree on cpu. Disagree on gpu. Get a 9070xt. It's basically a 5070ti for less money. If you go down to the 5070 the 9070xt is faster by a bit and relatively similar in price,buildapc,2025-11-03 21:17:25,1
AMD,nmy7jnv,"While playing BF6 did you use an resource monitor/overlay to see where your bottleneck is? My rtx 5060 ti with a circa 2020ish Ryzen 7 runs that game on max settings (not 4k) smooth as butter. My guess is you just needed a slightly better GPU (~300$). If you found that the CPU was maxing then a slightly newer AM4 CPU (like the on i have) would have been fine (~150$).  Basically, if you just wanted to be able to play the latest games like BF6 then IMO you could have gotten away with spending only a few hundred bucks on upgrades (and of course if you ebay'd your old parts that would have defrayed the cost) buttt now you've got something that should hopefully last another 4 or 5 years i would hope.",buildapc,2025-11-03 21:24:02,1
AMD,nmy7pcc,You can save $300 by switching cpu to 9800x3d. I bought mine for $300  And you can buy a better gpu for similar price.   You need better monitor And you can save $ on cooler,buildapc,2025-11-03 21:24:49,1
AMD,nmy89ee,"I would get a 9800x3D instead for the CPU, and you dont need the AIO cooler for that.  Just get a thermalright phantom spirit for like 1/10th the price.  You will need DDR5 RAM and i would wait for the prices to drop.  Dont get more than DDR5 6000 as it will force your infinity fabric to run 2:1 instead of 1:1.",buildapc,2025-11-03 21:27:35,1
AMD,nmybx4f,"9800x3d is better at gaming than 9950x3d. But, if you run multiple VMs at the same time, you need to allocate cores, so maybe 16 core 32 thread of 9950x3d might be better than 8 core 16 thread 9800x3d",buildapc,2025-11-03 21:45:41,1
AMD,nmyeoyt,Nice to see your revised build. You're on the right track I think. Good luck bro,buildapc,2025-11-03 21:59:28,1
AMD,nmykv12,"I actually went all-in with ram and now my system has 192gb. This is mainly for llms, but it might be useful for VMs too. But this option hinges on your ability to find a good 4x48gb ram pack for a good price. The recent hike in ram prices changes this option from being slightly eccentric and unreasonable into very costly and unreasonable.",buildapc,2025-11-03 22:32:17,1
AMD,nmyvii2,"I would step up to the MSI Mag B850 Tomahawk Max. It is newer, very close to X870 features and comes with 9800x3d compatibility out of the box. Not sure the 670 will need a bios update before that CPU will work.  Also, b850 will be slightly. More future proof for AM6 over that 670 board.",buildapc,2025-11-03 23:30:12,1
AMD,nmywxeu,Wait for Black Friday things to get good deals.,buildapc,2025-11-03 23:37:55,1
AMD,nmzksye,I will have to say a 9950x3d is a college waste of money. You could get a 7800x3d and be able to use it for at least 5 years easy.,buildapc,2025-11-04 01:56:11,1
AMD,nmzo3t3,"Idk if this is a bait post or not but 2700$ on a single PC is straight up crazy, giving you are not even super into PCs. Some above 500-600$ would have been enough. A well-designed system lower than 1000$ can run games buttery smooth, even the heaviest ones like CP77.   I mean if I saw someone spending BAGS on just a PC, I would assume they are either professional developers (especially AI/ML or game dev.) devoted to their work, or a popular streamer. Even if you were really into PCs 2700$ would be super weird imo, but you spent all this money just for daily usage and games? Crazy. If I were you I would  find my ways to refund it and make a logical research before buying something valuable.",buildapc,2025-11-04 02:15:10,1
AMD,nmzro7l,You’re gonna play bf6 right? 7500f and 9060XT PC for about~900$ would’ve been enough for this game. You’re wasting your money.,buildapc,2025-11-04 02:36:05,1
AMD,nmzu7gx,Look for deals on Newegg this month you might get lucky,buildapc,2025-11-04 02:50:52,1
AMD,nmzysnx,2600 on a rig but only 169 on a monitor?,buildapc,2025-11-04 03:18:25,1
AMD,nmzzueo,honestly overpaid a bit for some of these parts like the 5070ti and aside from needing new ram bc u cant reuse that one? nah. Ur not gonna need an upgrade for a looong ass while dude. Enjoy.,buildapc,2025-11-04 03:25:03,1
AMD,nmvkk83,Replacing the 5070Ti with a 9070XT is just a terrible choice. The rest is great.,buildapc,2025-11-03 13:35:34,1
AMD,nmtr8z0,"Me walking into my hour long 9am staff meeting at 9:47  “Thanks for waiting folks, let’s get started!”",buildapc,2025-11-03 04:06:02,0
AMD,nmtsjpx,I got a 12900+3090+128gb ddr4 barely used for $1300 so yes u messed up,buildapc,2025-11-03 04:15:18,0
AMD,nmtt713,You’re wasting a bunch of money. I built a pc that does 4k for 1400 dollars. And I’m sure that thing will last 10 years,buildapc,2025-11-03 04:20:03,0
AMD,nmtudyl,Why won’t BF6 play on your previous PC? it’s the most optimized game in a while,buildapc,2025-11-03 04:28:56,0
AMD,nmtvvi7,I spent $3800 on mine lol couldn’t be happier,buildapc,2025-11-03 04:40:14,0
AMD,nmtwk3r,$250 for an AIO and another $200 for fans is...interesting choices,buildapc,2025-11-03 04:45:29,0
AMD,nmu0zs7,"The second config imo is way better than the first. The first build will work but it's not optimized at all.   -9800x3d is all you need   -b650 or b650e is all you need     -that 5070ti is overpriced   -aio and fans are very pricey but that's personal preference (you could save a ton by getting a cheaper aio or air cooler and a case with included argb fans and achieve the same matching look, see montech)   -you can't reuse your ddr4 ram, get ddr5   -using a 500gb SSD and 2tb HDD in 2025 is rough, I'd understand if you were on a low budget but there is no reason a larger SSD can't fit into this build   -OLED alone is a gamechanger  Edit: Do you live near a microcenter? You could save a decent bit on the CPU, motherboard and GPU if you do.",buildapc,2025-11-03 05:22:04,0
AMD,nmu1yn6,The chances of an AIO lasting you 5 years are very low.,buildapc,2025-11-03 05:30:35,0
AMD,nmu2gn1,"Do you live within one or two hours drive from a MicroCenter? If so, make the drive. They usually have combos for a Mobo / CPU at a significant discount. They also have all types of discounts if you are building a full PC.  I got a this bundle for $500 bucks:  1. Ryzen 9 9900X 2. MSI Tomahawk 670e 3. 32GB Corsair Vengence RGB RAM  I also got a MSI 850w PSU, 2TB Hynix m.2 drive, Cooler master AIO, and NZXT H9 Flow Case for another $500.  The big expense was adding an AMD Gigabyte 9070XT OC w/ 16GB for $599.  The system is a beast. Just about every game gets 150fps at ultra settings at 1440p. You dont need to spend that much for a self-built PC.  Also, you gotta buy Windows. I bought OEM from MicroCenter. That added like another $100, but you can buy it other ways online way cheaper.   It was kind of a waste as I have switched to Linux and run all my games (so much better). However, I cannot speak to if BF6 runs in Linux or not. I don't play competitive shooters so it isn't an issue for me.   With the money savings, also buy an AMD 9060XT (16GB Ver) and keep that old system for another room. Maybe put it on a TV. It will still play a ton. Even with buying a second card, you are saving on that $2700.",buildapc,2025-11-03 05:35:05,0
AMD,nmu4ikg,Rx 9070 xt is cheaper and better then 5070 ti,buildapc,2025-11-03 05:53:48,0
AMD,nmvtyrm,"In your new build consider to get 9070xt for MSRP or add like 50 bucks and get 5070ti.  Also, pity to see that you dropped nice looking fans for cheap alternatives. This case deserves better.",buildapc,2025-11-03 14:27:28,0
AMD,nmtwine,"I'll get DDR5 then, any suggestions?",buildapc,2025-11-03 04:45:10,157
AMD,nmurcoq,"I bought this RAM KD5KGUD80-60A300G. Kinda expensive but it's the better looking one in my opinion.   I replaced the Lian Li tax for a Thermalright FW360 AIO/fans that is like $120 total. I also changed the CPU to a 9800x3d. Do you think it's a bad idea on getting a Sapphire Nitro+ AMD Radeon™ RX 9070 XT ($720) + AOC Q27G4ZD 27"" QD OLED ($450) or getting a GIGABYTE GeForce RTX 5080 ($999) and just sticking with my current 1080p 240hz monitor? I honestly prefer the former because the GPU looks better and I feel like the experience with OLED monitor would be awesome. I just don't know if it's a bad idea or not.  I also updated my post to reflect my current build if you wanna take a look.",buildapc,2025-11-03 09:46:11,3
AMD,nmvbtwo,I just bought second hand unused 64gb rams trident neo 6000 32cl for 230 euros. Insta bought them since they are 500 to buy right now,buildapc,2025-11-03 12:41:23,1
AMD,nmtxert,The CPU cooler seems over built for 9800X3D.,buildapc,2025-11-03 04:52:15,50
AMD,nmuaa13,"The guy said he uses the machine to run a couple of VMs, so it really depends on how many are run at the same time and how many cpus are allocated per machine. In my case I have a 7900x at work, and the 12 core is the good amount to run 4 semi loaded machines simultaneously. Edit: made a typo on a model",buildapc,2025-11-03 06:48:29,20
AMD,nmtuy9k,"Yeah for gaming 9800x3d, i would stick with 5070ti, 5080 is just too much money, you can buy really good monitor and other peripherals for that much difference",buildapc,2025-11-03 04:33:13,64
AMD,nmybo5i,He uses VMs. Maybe he needs the cores?,buildapc,2025-11-03 21:44:26,2
AMD,nmurlaa,"Do you think it's a bad idea on getting a Sapphire Nitro+ AMD Radeon™ RX 9070 XT ($720) + AOC Q27G4ZD 27"" QD OLED ($450) or getting a GIGABYTE GeForce RTX 5080 ($999) and just sticking with my current 1080p 240hz monitor? I honestly prefer the former because the GPU looks better and I feel like the experience with OLED monitor would be awesome. I just don't know if it's a bad idea or not.  I also updated my post to reflect my current build if you wanna take a look.",buildapc,2025-11-03 09:48:37,3
AMD,nmyktpd,Whats a “lower end” 5080? are they not all the same 5080 gpu?,buildapc,2025-11-03 22:32:05,1
AMD,nmu0yme,"Depends on OPs use case. If they are using programs that can take advantage of all the cores, then that CPU might be the one they should have.",buildapc,2025-11-03 05:21:48,-1
AMD,nmuq172,"I replaced the Lian Li tax for a Thermalright FW360 AIO/fans that is like $120 total. I also changed the CPU to a 9800x3d. Do you think it's a bad idea on getting a Sapphire Nitro+ AMD Radeon™ RX 9070 XT ($720) + AOC Q27G4ZD 27"" QD OLED ($450) or getting a GIGABYTE GeForce RTX 5080 ($999) and just sticking with my current 1080p 240hz monitor? I honestly prefer the former because the GPU looks better and I feel like the experience with OLED monitor would be awesome. I just don't know if it's a bad idea or not.   I also updated my post to reflect my current build if you wanna take a look.",buildapc,2025-11-03 09:32:13,4
AMD,nmv87gu,"Buyer's remorse is a real thing. I've been there, maybe we all have after an expensive purchase.  The trick is to do enough research so you can justify the money spent. And buying a system with no usable RAM means you obviously haven't. But that's fine. Return policies are pretty good nowadays. Just... ask for help beforehand next time. There are a ton of people here willing to give you a hand.",buildapc,2025-11-03 12:16:08,13
AMD,nmw2z9h,"I see it happen all the time, including in my line of work. People drop thousands on something and *then* message asking if it's the right thing. They're impulsive and lack some level of self control (sorry OP), or don't know how to deal with delayed gratification. If we wanted to dive in real deep it's probably how they were raised. Also, sales and marketing is designed specifically to get people to do this.  The benefit is we can sometimes get a great deal on an open box part. I picked up my 9700X a few months ago for $160 (+tax) since it was an Amazon Resale open box. Likely someone that got talked into a 9800X3D and returned the 9700X.",buildapc,2025-11-03 15:14:09,1
AMD,nmu0hm2,This is a much better build. OP would be smart to look at this.,buildapc,2025-11-03 05:17:40,3
AMD,nmtqy5z,"DDR4 isn't supported on AM5, so they won't be staying with DDR4.",buildapc,2025-11-03 04:03:59,33
AMD,nmvn4o8,">Could have  I'm so used to people saying ""could **of**"" that this threw me off for a second",buildapc,2025-11-03 13:50:25,3
AMD,nmtwfp5,I mainly chose the AIO because it has short hoses and makes the build look better. Same with the fans.,buildapc,2025-11-03 04:44:31,-10
AMD,nmtuil7,"This is not real , It is only to get a response",buildapc,2025-11-03 04:29:55,18
AMD,nmw096m,"What? It doesn't run ""worse"" when running correctly, in some scenarios using Process Lasso to force the correct CCD - but still a very bad idea for Op who claims to not really be into PC's.",buildapc,2025-11-03 15:00:20,1
AMD,nmu6r2f,Tbh x870 tomahawk wifi is an amazing mobo (the one without E at the end),buildapc,2025-11-03 06:14:45,1
AMD,nmtuwoe,Miller cooling,buildapc,2025-11-03 04:32:52,3
AMD,nmwymo5,He's playing BF6. You don't want FG in a fast-paced multiplayer FPS.,buildapc,2025-11-03 17:47:00,0
AMD,nmy2ktu,I got everything on low and my games stutters and has 40fps,buildapc,2025-11-03 20:59:45,1
AMD,nmtvfse,A 2060 at 1440p isnt going to be a great experience.,buildapc,2025-11-03 04:36:55,2
AMD,nmtwu14,The PC looks nice with these options,buildapc,2025-11-03 04:47:38,1
AMD,nmtwsk9,Klevv 6000 cl30. But ddr5 prices literally just went up like 50% last week...,buildapc,2025-11-03 04:47:19,249
AMD,nmtx4c9,haunt r/buildapcsales for ddr5 deals,buildapc,2025-11-03 04:49:56,18
AMD,nmtwyzs,"Sorry to say but ddr5 ram prices are jacked up rn, average 32gb ddr5 kit sitting 140$+ rn",buildapc,2025-11-03 04:48:44,49
AMD,nmu5ydi,I recommend kingston fury 64gb ddr5 6000mhz cl30 - model KF560C36BBE2K2-64  Ive got the same mobo as you and this ram works flawlessly,buildapc,2025-11-03 06:07:17,6
AMD,nmu05xy,"Whatever RAM you buy, check the motherboard manufacturer website and find that board QVL to make sure the RAM is tested to work on that board.",buildapc,2025-11-03 05:14:57,13
AMD,nmu2y52,Yes and has to be at least 6000 Mhz or check your Motherboard latest QVL btw how many RAM slots 2 or 4?,buildapc,2025-11-03 05:39:30,2
AMD,nmu23gi,"Make sure the RAM speed is not over 6000 whatever you do. 6000 is good, anything over is not.",buildapc,2025-11-03 05:31:47,3
AMD,nmuemlv,Viper Elite 5 CL30 2x16gb DDR5 6000mhz  Klevv Crass same but CL28 good luck finding this one   And any to behonest with this spec,buildapc,2025-11-03 07:31:43,1
AMD,nmujjgq,"Im pretty happy with mine, you can check them out. Lexar Ares 6000MTs cl28-36. They've been running well on expo tweaked with my rog strix x870e-e.",buildapc,2025-11-03 08:22:36,1
AMD,nmumio1,"check your mobo specs and see what ram mhz is compatible,  just know it got worse with ddr5 ram prices  which have spiked within the last month. Unless you are working with heavy software with loads of work you could save yourself some money and get 9850x  cpu instead.   Also that AIO is over kill save yourself more  money by getting Thermalright Peerless Assassin, and if you want to spend your budget get better m.2 storage.  and the price of the lian li fans could get you a 140 Noctua fans which are some the of the quietest fans on the market.",buildapc,2025-11-03 08:54:12,1
AMD,nmvypnd,64gb 2 sticks if you want it 5+ years,buildapc,2025-11-03 14:52:30,1
AMD,nmxrm5s,"I'd recommend the trident neo from skill, rock solid in my rig",buildapc,2025-11-03 20:07:23,1
AMD,nmzdaol,I suggest you keep the 5070ti. Return the cpu and get that 9800. People recommend cl30 ram but its probably going to be double the price of some cl36.,buildapc,2025-11-04 01:12:04,1
AMD,nmwk9k5,"Nitro+ uses the 12vHFR (high failure rate) power connector. Get any 9070xt that is less expensive and doesn't use the 12vHFR.  9070xt MSRP was supposed to be $600, so that should be your reference point for 9070XT pricing.",buildapc,2025-11-03 16:38:57,2
AMD,nmy1tgw,That 9800 X3D likes to run hot though.,buildapc,2025-11-03 20:56:10,1
AMD,nmue4nf,"> In my case I have a 9700x at work, and the 12 core is the good amount to run 4 semi loaded machines simultaneously.  The 9700X has 8 cores, it's the 9900X that has 12.",buildapc,2025-11-03 07:26:32,3
AMD,nmu9flr,"On the other hand, they're building a PC from scratch. The fps % increase will be larger than the total price increase if they choose 5080 instead of 5070ti.",buildapc,2025-11-03 06:40:19,33
AMD,nmzkyne,I built a new PC over the summer and went with the 9800x3D and an OC 5070ti. I game in 1440 ultra wide and haven’t ran into a single game that it can’t handle. After using a 9700k/1080ti machine for the last 6 years it was an excellent upgrade.,buildapc,2025-11-04 01:57:04,2
AMD,nmuew50,"Agreed, the 5080 is such a bad value card, its only a bit faster than the 5070ti and it costs waaaay more",buildapc,2025-11-03 07:34:31,2
AMD,nmu9war,Aside from random deals people really seem to not factor in price to performance ratio. The 5080 is a terrible value as you noted. Just get 5070 to and upgrade like half a year soon with the money saved. Slotting in a GPU is super easy.,buildapc,2025-11-03 06:44:49,-3
AMD,nmuxi63,"Go for the 1440p oled, its the current sweetspot and will be for awhile since you need constant upgrading down the line for 4k gaming. I also like a future proof build knowing I can run al games max settings in the upcoming 5 years without having to worry about fps. Got for the rtx cards sinds dlss just has a better upscaling methode imo.",buildapc,2025-11-03 10:46:50,11
AMD,nmvkzhx,"dont even think of buying a 5080 if you will be playing in 1080p, 5080 is made for 4K gaming",buildapc,2025-11-03 13:38:04,2
AMD,nmv6pnw,"2 years ago, I went from 3700X and 2080 Ti to 5800X3D and 7900 XTX (had decent deals on both, today I think you would get as much value from a 5700X3D and 9070XT), kept everything else I already had (SFF PC so ITX X570 motheboard, 32 GB DDR4 RAM 3600, nvme gen 4 SSDs x 2, SATA SSDs x 2, Corsair SF750 Platinum).  I treated myself with a Samsung G8 32"" 4k QD-OLED 240 Hz monitor that cost me only € 330  through a corporate deal (kept my 27"" 144 Hz 1440p IPS monitor as 2nd display). You might want to stay with a 1440p max display for BF6 though and that AOC monitor will be more than fine.  \- 5800X3D € 260  \- 7900 XTX Nitro+ € 890  \- Samsung G8 4k OLED € 330  Total € 1480 tax incl. of which only € 260 spent on the EOL AM4 platform  I think this platform is good enough to wait out until AM6 lands (after 2027). It runs very well.  So you might reconsider moving to AM5 already.",buildapc,2025-11-03 12:05:12,1
AMD,nmx7g02,"Make sure you don’t buy an OLED sight unseen.  They can have funky artifacts around curves in text. If all you’re doing is gaming on it, it shouldn’t be a problem. If you’re going to be reading on it a lot, you may want to have a look at one in a store before you buy to make sure that it won’t bother you.",buildapc,2025-11-03 18:28:55,1
AMD,nmz0lj4,For some reason there are 1000€ rtx 5080’s and 1700 € 5080’s. Has to do with cooling and factory overclock. Im going for the zotac version since it had 4 cooling fans which makes it perfect for overclocking,buildapc,2025-11-03 23:58:12,1
AMD,nmuqyuu,5080 + 1080p monitor makes 0 sense. Absolutely get a high refresh rate 1440p monitor.,buildapc,2025-11-03 09:42:09,27
AMD,nmwkd71,Just a heads up I do repairs and AIOs fail often I prefer a good air cooler. Noctura is probably the best.,buildapc,2025-11-03 16:39:26,3
AMD,nmwdlqh,"9070xt is good. If it's between 9070xt/5070ti + a new monitor Vs Getting a 5080 -new monitor, Id probably go for the GPU+Monitor.",buildapc,2025-11-03 16:06:01,1
AMD,nmwa9g4,"Yes it's a real thing, but not even looking up or knowing about DDR4 is not gonna work with the most expensive and top of the line AM5 cpu? lol",buildapc,2025-11-03 15:49:47,3
AMD,nmtx81o,Spending that much on fans and an AIO is ridiculous though. I could’ve gotten nearly identical looking ones from Thermalright.  Stop paying for name brand slop.,buildapc,2025-11-03 04:50:45,26
AMD,nmu4sle,"You can get way cheaper aio's as well as cases with included rgb fans. You could even swap out the aio rad fans to match the case fans. People aren't criticizing the aesthetic choice to go with the AIO and fans, but the money spent relative to the other components. I'd pour more money into the computing components, specifically the SSD or your display (the OLED).",buildapc,2025-11-03 05:56:23,5
AMD,nmu6mgp,"You rather buy significantly more expensive aio and fans that bring no performance difference over cheaper fans and arguably better heatsink like phantom spirit or peerless assassin which would save you probably enough to get rtx 5080 instead? Thats a mistake, also 9800x3d instead of 9950",buildapc,2025-11-03 06:13:33,1
AMD,nmuxdzj,The DDR4 ram certainly makes it feel that way but I’ve met some people who have no problems spending (even if they don’t have it) money and no sense to go with it.,buildapc,2025-11-03 10:45:47,6
AMD,nmtwnpn,It's real. Some parts come tomorrow others till Friday.,buildapc,2025-11-03 04:46:17,-14
AMD,nmwtlll,It will run worse because no shot OP is going to use process lasso. Hell I'd be surprised if they enable XMP,buildapc,2025-11-03 17:23:12,1
AMD,nmx1rj2,"I have just seen many comments about issues with parking, which the 9800x3d won't have. The power draw is also much less on the 9800x3d from what I've read. i don't tune my rig at all so it would just be one less thing I have to be on top of. Both are great chips.",buildapc,2025-11-03 18:01:35,1
AMD,nmxby8h,"Yes you want it to play 240 Hz, you are ultra wrong, don't try",buildapc,2025-11-03 18:50:48,1
AMD,nmy2pqj,Sounds like a dying gpu then. I get 90 and with frame gen 120-140ish,buildapc,2025-11-03 21:00:24,1
AMD,nmv12go,"If you like it then it is not wasted, just bear in mind that a good chunk of the money you spent is on the looks",buildapc,2025-11-03 11:19:27,1
AMD,nmw1tn4,Holy shit no kidding. I got a KLEVV 32GB 6000 CL28 for $110 less than a month ago.,buildapc,2025-11-03 15:08:22,32
AMD,nmurews,"I bought this, it looks nice. [KD5KGUD80-60A300G](https://www.amazon.com/dp/B0DC5X39PH?ref=fed_asin_title)",buildapc,2025-11-03 09:46:48,10
AMD,nmuw8et,Why did prices go up?,buildapc,2025-11-03 10:34:46,2
AMD,nmu9ymt,"Bro is considering a $700 CPU, I don't think he cares about RAM being $50 higher than usual.",buildapc,2025-11-03 06:45:27,55
AMD,nmugh58,Wtf where? Im in EU and u can get decent 6000mhz cl 32 for around 100€,buildapc,2025-11-03 07:50:54,-2
AMD,nmu7z05,"Just commenting to add that I just upgraded my RAM recently with this exact model! 10/10, highly recommend!",buildapc,2025-11-03 06:26:11,2
AMD,nmuebpa,Why is not good? Useless because of minimal returns or is even worse?,buildapc,2025-11-03 07:28:34,1
AMD,nmuu264,That is not how timing works,buildapc,2025-11-03 10:13:20,1
AMD,nmwsc0k,Honestly likely won't even be XMP'd,buildapc,2025-11-03 17:17:14,1
AMD,nmue8z6,"Made a typo, it’s 7900x, my bad",buildapc,2025-11-03 07:27:48,5
AMD,nmug9pp,"I agree, but also, that's only the case *here* if the OP isn't already paying a premium for aesthetics, with some 5080s down to MSRP. 5070 Ti for 850 is a lot to pay when $150 more gets a significant boost in raw horsepower, and turning the price difference from ~getting a 9800x3d and shaving off aesthetics to ~9800x3d or 9900x3d and heavily shaving aesthetics. If OP wants to build around a budget, going for 5080 will get the most gaming mileage out of the current build IMO",buildapc,2025-11-03 07:48:48,7
AMD,nmz1azv,"Buy’s gpu for 700 now, 2 years later it lost 300 in value. So you want to buy a better new card that costs you 1000/1200 so you lost 600/800 in the process… better to go 5080 now and enjoy the high fps/settings without having to upgrade.",buildapc,2025-11-04 00:02:13,2
AMD,nmvishe,Spending $2700 and buying a 5080 to play in 1080 is a criminal offense lmao,buildapc,2025-11-03 13:24:55,27
AMD,nmv6oth,What about 4k high refresh rate?,buildapc,2025-11-03 12:05:01,3
AMD,nmweebn,"Or, honestly, it's a 9070 XT.  Absolutely 4K capable.",buildapc,2025-11-03 16:09:58,1
AMD,nmuifkf,Your mistake is thinking he was asking for advice when he asked for advice,buildapc,2025-11-03 08:11:00,1
AMD,nmu6h1s,"This post is bait.  If you actually cared about your build you'd have asked for feedback before buying.  You want attention. That's all this post is for.  And yes, you did make mistakes, but you're better off living in ignorance if this is your approach in life",buildapc,2025-11-03 06:12:11,35
AMD,nmwuksu,"Lol yea. 9950x3d definitely isn't a processor for the average person, and especially Op.  But games don't run ""worse"" in most scenarios, and especially if you know what you're doing.",buildapc,2025-11-03 17:27:53,1
AMD,nmxe8pq,I used it for six hours and it was very noticeable. I turned it off and it felt way better. You don't want extra latency in an FPS.,buildapc,2025-11-03 19:01:47,1
AMD,nmy4798,I get 150fps on other games like r6 and cod modern warefare 2,buildapc,2025-11-03 21:07:38,1
AMD,nmx20pf,I got some T create for 90€ in july. they are 165 now.   edit: 230 now.,buildapc,2025-11-03 18:02:48,10
AMD,nmxuqm5,Some prices are absurd. I got 64GB DDR5 6000 for $130 back in 2023 as part of a microcenter bundle.,buildapc,2025-11-03 20:22:19,3
AMD,nn002d9,Did you select just based off ddr5 and how it looked? If so you lucked out cause those are ideal timings for the cpu lol,buildapc,2025-11-04 03:26:26,1
AMD,nmv2e7f,"It seems likely due to AI data centers needing ram, therefore production and demand are more prioritized for them, meaning prices go up for these rams too.",buildapc,2025-11-03 11:30:52,8
AMD,nmv22qh,"Lot of AI data centers trying to pop up round the country, combined with politics.",buildapc,2025-11-03 11:28:10,15
AMD,nmv2ktq,AI data centers more than anything,buildapc,2025-11-03 11:32:27,6
AMD,nmuz8fw,Politics.  A bunch of Americans voted for this.,buildapc,2025-11-03 11:02:53,90
AMD,nmviy7d,Large increase of ai demand,buildapc,2025-11-03 13:25:53,1
AMD,nmwvuxd,"Datacenters and limited production capacity, mostly  - https://digitimes.com/news/a20251014PD209/dram-hdd-nand-price-capacity-production-adata.html  - https://www.sammobile.com/news/samsung-sells-out-of-2026-hbm4-supply-as-memory-resurgence-continues",buildapc,2025-11-03 17:34:00,1
AMD,nmwzhs7,Can’t remember the company but an AI company is buy up like 50% of all the ram chips being made right now,buildapc,2025-11-03 17:50:59,1
AMD,nmubban,"Depends. Personally I also buy these big CPUs, currently sitting on 7950X3D, but I still care for memory prices and these are insane currently.",buildapc,2025-11-03 06:58:31,5
AMD,nmuhino,"Not where I live in Europe, or Germany and Spain which I've checked this week.  Where do you live where team is still cheap? It's skyrocketed everywhere else in the past month.",buildapc,2025-11-03 08:01:36,6
AMD,nmumb4c,Nahh,buildapc,2025-11-03 08:52:02,1
AMD,nmugkec,WTF IS A KILOMETER 🦅 🦅 🦅,buildapc,2025-11-03 07:51:51,-5
AMD,nmuimib,"It has to do with how AMD chips run timing with their RAM. To make a long story short, over 6,000 simply isn't compatible.  I can tell you 6k is great, I'm running that myself on my new built that I built a month ago.",buildapc,2025-11-03 08:13:03,3
AMD,nmwllwr,Yeah but getting a lower end 5080 could be more costly in the future. Idk I think it’s best not to cheap out on the card. You’re paying more for certain tiers for a reason. Given TUFs cooling capabilities in comparison to something from Venus or wind force then I think you’re getting good value,buildapc,2025-11-03 16:45:23,1
AMD,nmva7qj,Ye that works too,buildapc,2025-11-03 12:30:23,3
AMD,nmuh27i,"Eh I kinda agree but it's very possible he's starting to feel buyers remorse on some parts, or just is antsy waiting for parts and started thinking deeper about it all.",buildapc,2025-11-03 07:56:54,13
AMD,nmwcubi,Cause everyone runs to Reddit before buying a pc,buildapc,2025-11-03 16:02:14,0
AMD,nmy4avt,"No, it's not noticeable if you have like 140 FPS without FG",buildapc,2025-11-03 21:08:07,1
AMD,nn01zy5,Yeah just based on looks,buildapc,2025-11-04 03:38:48,1
AMD,nmv1zyd,To be fair memory was always set to rise cause of the shift into AI,buildapc,2025-11-03 11:27:31,47
AMD,nmweiqz,"lol no open ai signed a deal to buy 40% of global production. Literally once that deal was signed prices started creeping up. Also ram goes in cycles. Do tariffs help? No, but that's not the true reason.",buildapc,2025-11-03 16:10:34,15
AMD,nmvk0jk,thats not the reason. the reason is shortages due to others prioritizing high speed ai memory production.  nvidia google aws and open ai are willing to pay more for this to be built so its taking priority...,buildapc,2025-11-03 13:32:19,24
AMD,nn00spv,Pretty sure computer parts are exempt from tariffs so it’s just AI buying up everything,buildapc,2025-11-04 03:31:02,1
AMD,nmv3llx,We think they are naive in Russia.,buildapc,2025-11-03 11:40:55,-5
AMD,nmw7vav,"Do you use the extra cores though? Sounds like dude doesn’t do anything on the PC but game, so the extra cores would be wasted.",buildapc,2025-11-03 15:38:10,2
AMD,nmuppxj,"Okay edit, just checked and cheapest are 140€ like whats going on  Edit 2. Ddr4 16gb for 70€? Did something heppened?",buildapc,2025-11-03 09:28:50,-1
AMD,nmupm6r,Slovakia. Bought from marketplace 32gb 6000mhz but cl36 for 60€ 2w ago,buildapc,2025-11-03 09:27:42,-2
AMD,nmuqqme,"Yep sorry, checked again and same shit is here. Even 16gb ddr4 for 65€",buildapc,2025-11-03 09:39:45,1
AMD,nmuh7jh,It's a kilo of meters.,buildapc,2025-11-03 07:58:25,11
AMD,nmut02r,"Only the rest of the world knows, it's our secret.",buildapc,2025-11-03 10:02:47,6
AMD,nmux2bq,I have met some impulse buyers who fit that scenario.,buildapc,2025-11-03 10:42:43,5
AMD,nmz8hvc,You don't need to go to reddit before buying a PC   But you definitely don't go to reddit after buying one and then ask if what you got was good.  That specifically is the key detail you're ignoring.,buildapc,2025-11-04 00:44:01,5
AMD,nmy44j8,Right? And they give terrible advice,buildapc,2025-11-03 21:07:16,1
AMD,nmy5h1j,He's not getting that with a 9070 XT.,buildapc,2025-11-03 21:13:52,1
AMD,nmzmsei,damn. sound like what happens when scalpers were hitting the memory market. 🤣🤣,buildapc,2025-11-04 02:07:35,1
AMD,nmwk93c,"So... tariff's aren't real?  I called them a multiplier, not the sole cause.  I also said I thought the world economy was a tinderbox long before he took office.  The least you could do is admit that Tarrifs are doing nothing to help and making a bad situation worse.  But then we would agree.",buildapc,2025-11-03 16:38:53,-3
AMD,nmvl2qi,Tariff's.  I am talking about Tariff's.,buildapc,2025-11-03 13:38:35,-4
AMD,nmwo205,Aint no one using DDR5 for highspeed AI,buildapc,2025-11-03 16:57:01,-3
AMD,nmwhs82,"Can't speak for OP but I rarely use the cores as long as the cpu sits in my daily/gaming PC but I move ""old"" hardware at some point into my server PC and there every core and thread counts. Old means usually 2-4 years in my case. That's definitely not the usual customer, but I don't think I am alone.  Actually also interesting will be the future especially regarding local AI. Smaller models on cpu are getting better and at some point it might be used for NPCs in games and suddenly cores and memory will become more important.",buildapc,2025-11-03 16:26:46,1
AMD,nmutz2v,"> Did something heppened?  Datacenters, mostly AI, gobbling up all the RAM Samsung, SK Hynix and Micron can make, driving up prices for all.",buildapc,2025-11-03 10:12:29,7
AMD,nmwfrry,Open ai signed a deal to buy 40% of global production a month ago.,buildapc,2025-11-03 16:16:45,1
AMD,nmy5uu4,Indeed,buildapc,2025-11-03 21:15:43,1
AMD,nmwusm1,"Your reading comprehension is lacking. I acknowledged tariffs and said it is not helping, but you are so adamant in your position that Trump is the reason for everything, it's clouding your ability to read and think.  Additionally, I don't know what else you said in other comments so you can spout off and try to claim the moral high ground. As if there is any when discussing ram pricing and global economics.",buildapc,2025-11-03 17:28:56,4
AMD,nmwzi2p,Memory is exempt.,buildapc,2025-11-03 17:51:02,1
AMD,nmw75p3,"Tariffs play a part, but as stupid as I think they are, they are not really the main component here.",buildapc,2025-11-03 15:34:43,5
AMD,nmvqk18,You're both correct.,buildapc,2025-11-03 14:09:16,12
AMD,nmvux9g,"Prices are also up here in the EU, it's not just tariffs.",buildapc,2025-11-03 14:32:40,12
AMD,nmx0e52,say it with me folks: apostrophes aren't for pluralizing.,buildapc,2025-11-03 17:55:11,2
AMD,nmwe0mr,if tarrifs are the reason why has it also gone way up in other countries.....,buildapc,2025-11-03 16:08:05,2
AMD,nmwpfy2,"... no they arent.... they are made in the same factories... so instead of making (made up number) 10,000 u its a day of ddr5, these factories are making 5,000 units of ddr5 and taking the reasourses to make ai memory",buildapc,2025-11-03 17:03:33,8
AMD,nmwudgg,"Ya, that makes sense.  IF CPU thread count suddenly becomes tremendously important for AI features, it'll be cheaper to upgrade your CPU at that point than to constantly overpay ~$200-300 for your CPU each upgrade cycle ""just in case"" it happens on this upgrade cycle, lol.",buildapc,2025-11-03 17:26:55,1
AMD,nmwvhve,"I called tarriffs a multiplyer, said world economics where a powderkeg before he lit the fuse but generally if you believe there is a lot going on and tarriffs are making things worse then we we agree.  What exactly is your fucking problem with me?",buildapc,2025-11-03 17:32:14,0
AMD,nmw7on5,But they are a multiplier.  From my perspective the economy has been a powderkeg for some time and when the orange one was crowned his foolishness lit the fuse.  Whatever stability we could manage was helping the world economy putter along.  We were no longer stable.  The king is mad and gives no shits about the economy or starving people.,buildapc,2025-11-03 15:37:17,-2
AMD,nmwfe72,Because orange man bad. This is reddit. Take everything you read here with some salt.,buildapc,2025-11-03 16:14:54,0
AMD,nmwrwuk,"Ahh. Yep, you're correct",buildapc,2025-11-03 17:15:16,2
AMD,nmwjb6l,Ok bro chill out. Can’t believe people actually think Trumps a king LMAOOOOO,buildapc,2025-11-03 16:34:17,3
AMD,nmwq7ls,found the wokie loon,buildapc,2025-11-03 17:07:11,-1
AMD,nmock5z,A 9060xt 16gb is around 350 and will be perfect for your system.  You will also be able to upgrade to stronger cpu's in the future like the 9600x since the 9060xt is just really well rounded.,buildapc,2025-11-02 09:03:11,2
AMD,nmocp2i,"9060XT 16GiB or the NVIDIA equivalent, 5060Ti 16GiB.  I would be a tad worried about PSU but a bit late there I guess.",buildapc,2025-11-02 09:04:36,1
AMD,nmogavg,Which psu do you have? Might be what killed your 3060. It's not a good idea to keep it if it's an unreliable model.,buildapc,2025-11-02 09:41:51,1
AMD,nmok1r8,I would say a 9060 because it uses the classic pcie 8 pin plug. Dont go for the 5060 if you are using the old gpu. I think its better to have a dedicated 12v cable directly from the psu than to use the adapters.,buildapc,2025-11-02 10:19:11,1
AMD,nmqa55b,"might be something helpful on gputiful for comparing those options   a 3060 ti could be a decent pick, butcheck if your 500w psu can handle it.",buildapc,2025-11-02 16:53:36,1
AMD,nmofk98,for a 9060 xt my 500w psu would be good or do i buy a 600w,buildapc,2025-11-02 09:34:26,1
AMD,nmog8g9,"for this 9060 xt, is my psu good",buildapc,2025-11-02 09:41:11,1
AMD,nmogvbx,i will tell u when i get back home but i have a old chieftec,buildapc,2025-11-02 09:47:36,1
AMD,nmoq0gb,"do I buy the sapphire nitro+, pulse or the gigabyte oc one",buildapc,2025-11-02 11:16:09,1
AMD,nmomoef,450w us the minimum for a 9060xt but to be safe 550w or above will do fine.,buildapc,2025-11-02 10:44:43,0
AMD,nmoqd4u,I would recommend the xfx ones. Or the pulse. The nitro uses the 12v connector that burns itself.,buildapc,2025-11-02 11:19:24,1
AMD,nmoqh9j,wdym by xfx ones,buildapc,2025-11-02 11:20:26,1
AMD,nmoqnry,AMD Radeon RX 9060 XT 16 GB XFX OC EDITION 3 fans for example?,buildapc,2025-11-02 11:22:05,1
AMD,nmoqrdb,Yeah,buildapc,2025-11-02 11:23:01,1
AMD,nmopnwi,What about the CPU cooler and PSU? Do you already have those?,buildapc,2025-11-02 11:12:55,2
AMD,nmourup,Oh the processor comes with a free msi mag 360mm cooler. I may just sell it and get a grand vision though. I'm planning on reusing my 750w psu from my last system for now.,buildapc,2025-11-02 11:57:56,1
AMD,nmoxjjg,"This is a great build, and it's very cheap too if you're getting it built and shipped for only $1622.",buildapc,2025-11-02 12:19:24,1
AMD,nmsfs2f,For about two years and but I've been running a 5600X and 6750XT combo for 1440p.  My current plan is hang on for a few more years before upgrading to AM5 (and hopeing for a more affordable X3D chip and DDR5 RAM).  5600 or 5600x even use would be a substantial improvement for a modest investment.  The time to buy a 5700X3D was a year ago as prices are 25%+ higher if you can even find them.,buildapc,2025-11-02 23:16:22,16
AMD,nmsl2xa,"A 5600X, 5600X3D, 5700X, 5800X, 5800XT, would all serve you just fine. Compared to a 1600X, any of these would be a massive leap in performance",buildapc,2025-11-02 23:45:34,13
AMD,nmsexwy,"A Ryzen 5 5600 would be a huge upgrade on its own. You're currently severely bottlenecking that GPU. The 5600 will have a much better time feeding it. If you want a 5800X3D, I would just look at a 5700X3D used on eBay for around $250 instead. I would do that instead of going AM5 unless you really want to, but you're going to be spending a lot of money since decent AM5 board are near $200. The 9800X3D is stupid expensive, and I would tell you to just get the 7800X3D instead since they're so damn close. Also, DDR5 has skyrocketed in price due to AI.  It's up to you, but I think a used 5600 or the 5700X3D is a better option for you for minimal money.",buildapc,2025-11-02 23:11:50,11
AMD,nmshaaa,">I can afford a high-end option like a 9800X3D and the accompanying parts but I'm not sure if a purchase at or above $1000 is a good use of my money.  Well you would be going from a mid-range CPU from 2017 to the best gaming CPU in 2025.  If you want to save some money, even Ryzen 5600 would be a day and night difference for you.",buildapc,2025-11-02 23:24:47,9
AMD,nmsgw4w,"Would a 5800X or similar part do what you need it to do? If so, just grab that and ride out your current setup for a while longer IMO.",buildapc,2025-11-02 23:22:35,4
AMD,nmsigf6,"it REALLY depends on what games you're playing. its all relative.   a Ryzen 5600x would be a huge jump and can be had for $100-$120 second hand... and you can use the same motherboard & ram  it wouldn't be worth upgrading on AM4 past the 5600x imho, after that you're getting into AM5 territory in price",buildapc,2025-11-02 23:31:08,3
AMD,nmskk92,"Get a cheap am4 cpu + cooler if you need it, no reason to upgrade now imo",buildapc,2025-11-02 23:42:47,2
AMD,nmslku9,"Even a 7600X matches or exceeds a 5800X3D in all but pure multicore workloads, simply because it has two fewer cores. However, even then, it's still extremely close. And, then, you can always upgrade to something better down the line.",buildapc,2025-11-02 23:48:17,2
AMD,nmsee61,"I think it is because I've just upgraded from a 5600X + 3080 to a 9800X3D, kept the 3080. If you want a cheap temporary solution you could get a used 5600. Otherwise yeah any jump in performance beyond that will cost quite a bit more",buildapc,2025-11-02 23:08:50,1
AMD,nmsei7v,[https://www.microcenter.com/product/682196/amd-ryzen-7-5800xt-vermeer-am4-380ghz-8-core-boxed-processor-wraith-prism-cooler](https://www.microcenter.com/product/682196/amd-ryzen-7-5800xt-vermeer-am4-380ghz-8-core-boxed-processor-wraith-prism-cooler)     Possibility?,buildapc,2025-11-02 23:09:26,1
AMD,nmsfdvb,If you upgraded to am5 and kept your gpu that would be a huge upgrade.   If you live near a microcenter they have good bundle deals even with the expensive ram   I personally recommend the 7600x3d bundle. That’s the one I got for like $329.99. It was a much better deal when it came with ram but even now it’s a good deal,buildapc,2025-11-02 23:14:14,1
AMD,nmsh202,"If you plan on playing any Unreal Engine 5 games in the future, it absolutely is. UE5 uses a LOT of CPU.",buildapc,2025-11-02 23:23:31,1
AMD,nmsmejk,Yeah it’s worth it.  You will see an enormous increase in FPS going to 9800x3d or 9900x. I wouldn’t upgrade to 5700x3d. Not that it’s a bad cpu. But imo worth it to just spend the money on a newer platform that will still get support for even longer.,buildapc,2025-11-02 23:52:52,1
AMD,nmsuc29,I really think you should go to say 5600 till the DDR5 ram prices come back to normal.,buildapc,2025-11-03 00:38:27,1
AMD,nmswvlo,I have a Ryzen 7 5800x with a rtx 4070.    It does wonders for a budget mid tier.,buildapc,2025-11-03 00:53:43,1
AMD,nmszspd,I upgraded my 1700 to a 5700x3d. No complaints paired with a 5070,buildapc,2025-11-03 01:11:34,1
AMD,nmt044r,You're pretty severely bottlenecked by your Ryzen 1600. That CPU just doesn't hold up in 2025.  Hard to say if AM5 is worth it for you since you haven't given any indication of what you use the PC for. You could be fine with a Ryzen 5600 or even a used 3600 if budget is a big issue.,buildapc,2025-11-03 01:13:31,1
AMD,nmt0viq,"Like others have said, you can get a used 5600 or 5600x for $100 or less on eBay. Pop that in and see how things perform. If it's still not good enough you can always upgrade to am5 and resell the used 5600.",buildapc,2025-11-03 01:18:09,1
AMD,nmt5gyj,"If you want a more low commitment upgrade, the 5700xt or 5800xt are usually around the $150 range if they go on sale.",buildapc,2025-11-03 01:46:20,1
AMD,nmtcgwb,"Personally I would start by putting a better AM4 CPU in.  I've done that myself, most recently upgrading from a 3700X to a to a 5700X3D. No regrets.",buildapc,2025-11-03 02:28:22,1
AMD,nmtdgo1,"Instead of the binary ""is it worth it"", I like to evaluate PC options in terms of ""how long do I expect to be happy with this before I want to upgrade again?"". If (to throw out some arbitrary numbers) I can afford to go up 20% on the budget to get a part that I'm likely to keep for 50% longer, I think that there's a meaningful sense in which the longer upgrade cycle is the better deal.  I bought my 1600X at release, so I'd been running it for about eight years before I got tired of Microsoft nagging me about Windows 11. Based on that experience, I decided what I'd be willing to pay for a Zen 5 system on the assumption that I'd keep it for another 8-10 years.",buildapc,2025-11-03 02:34:25,1
AMD,nmtwyjz,You can definitely squeeze more life out of the build with a cpu upgrade. The 5600x and 5800xt are both solid performers and below 200 . Fellow AM4 who upgraded cpu and loved the result.,buildapc,2025-11-03 04:48:39,1
AMD,nmuh1g4,You could simply buy a second hand 5600x and get a pretty massive performance increase,buildapc,2025-11-03 07:56:41,1
AMD,nmst50e,"Just get a 5600 or a 5700 or something. You'll forget about wanting to upgrade for a good while, and with good reason.",buildapc,2025-11-03 00:31:22,0
AMD,nmse2u0,"The 9800X3D is the best CPU for gaming, so if you can afford it, IMO it’s worth the money.  With that being said, investing in AM4 now, even with the crazy DDR5 prices, is just wasteful.",buildapc,2025-11-02 23:07:07,-1
AMD,nmsm3yt,"if you are content with what you have, then stay with it until you cant any longer.  I assume you play esport titles which dont really require all that much.",buildapc,2025-11-02 23:51:14,-1
AMD,nmte0lw,I have a 5600x($115) with a tuf gaming wifi II B550($120ish) and 2x32gb DDR4 3200 @ 3600 16-20-20-38 1.4v(before memory price spike $120).  To replace a 3770k z77 Gigabyte board with 4x8gb ddr3 2400 10-13-13-38 1.65v. Been using cpu and board since 2013. Kinda lost what to do with it now.,buildapc,2025-11-03 02:37:52,2
AMD,nmsnf6a,"Yep, OP should go this route and wait it out a bit longer. Would still be a massive leap and might be plenty enough of an upgrade for some years. Save a nice chunk of a cash and get a better upgrade later   Worked out well for me I'm just going to chill until AM6.. or maybe Intel will be competitive again by then who knows!",buildapc,2025-11-02 23:58:25,3
AMD,nmvv2pj,I recently upgraded from a 2700x to a 5600 and had a very noticeable performance increase. Namely 1% lows and tighter frametimes.   I have a 6700 XT and play at 1440p. Next upgrade might be a 9070 XT and then AM5 next year(or the following).,buildapc,2025-11-03 14:33:30,1
AMD,nmwx3c7,A 5600x shouldnt be much more expensive new.,buildapc,2025-11-03 17:39:53,1
AMD,nmtee9x,Yeah the 7700x produces a ton of heat. Kinda wish I got a 7600x instead back in 2023. But I wouldn't have gotten a X670E chipset tho.  The 7700x does better or slightly better than 5800X3D. Same goes for 7600x on certain titles.,buildapc,2025-11-03 02:40:16,1
AMD,nmsf8m6,"IIRC aside from power efficiency 3080 and 4070s are similar. What's a ballpark estimate on how much improvement you got form 5600x -> 9800x3d, and what is your monitor(s) resolution and hertz?",buildapc,2025-11-02 23:13:27,2
AMD,nmt0ik9,"5700**X**, not the regular 5700. The 5700 is a gimped CPU that basically has no reason to exist.",buildapc,2025-11-03 01:15:57,2
AMD,nmsfnvl,"Yeah, I didn't mention it in my post but I am wary of doing AM4 since it could very well by a ""Poor man's boots"" problem. I also saw the crazy RAM prices but $80 won't stop me like the batshit insane GPU prices. (Off topic but I got MSRP 4070 Super in Nov 2024, very nice)",buildapc,2025-11-02 23:15:45,1
AMD,nmsfaeh,"If DDR5 prices are crazy, why would DDR4 prices be any different? They all use the basically the same components. Circuits may be wired differently, but basically the same.",buildapc,2025-11-02 23:13:43,0
AMD,nmzbu2z,i couldnt find any in stock anywhere,buildapc,2025-11-04 01:03:29,1
AMD,nmsfjxx,I have a 1440p 165hz monitor. I've only tried the 9800X3D out in Fallout 4 and Fallout London so far and the difference is massive. But those games are cache sensitive single threaded nightmares,buildapc,2025-11-02 23:15:09,1
AMD,nmt1zpw,Fair enough and point taken.,buildapc,2025-11-03 01:25:02,1
AMD,nmshypc,He already owns DDR4….,buildapc,2025-11-02 23:28:25,4
AMD,nmpzlgu,I have saved you 250 euros on getting you better value components while still getting good quality components for future upgradability. If you only want to buy from this store I can understand it but you could definitely do better and get access to more variety if you source from multiple places.  https://www.pccomponentes.com/configurador/06F79195E,buildapc,2025-11-02 16:00:04,2
AMD,nmq1q2c,That gpu is great for the price to performance for 1440p,buildapc,2025-11-02 16:10:50,1
AMD,nmpzw7z,Is the choice of GPU correct? I don't understand a lot and I saw you didn't change it. And should I get extra fans for the pc case?,buildapc,2025-11-02 16:01:33,1
AMD,nmq1wth,Yes it's a good choice because the 5070ti (nvidia equivalent which is slightly better due to features and raytracing performance) costs 230 euros more which is way too much.,buildapc,2025-11-02 16:11:47,1
AMD,nmqa28w,"and whats the difference between different companies' 9070 xt?, because the price difference is big. The one i chosewas the cheapest",buildapc,2025-11-02 16:53:12,1
AMD,nmqbnde,"Very minimal performance differences and different coolers by default, but some offer a higher power limit for overclocking as they have 3 8 pins or a 12vhpwr cable. There are some other differences like hynix vs Samsung ram but you really don't need to worry about that if you don't plan to tinker with clock speeds.",buildapc,2025-11-02 17:00:58,1
AMD,nmrjjcm,Thanks!,buildapc,2025-11-02 20:31:07,1
AMD,nlpz2jj,"compatibility looks fine. You may wanna bump the PSU up in wattage if you think you'll upgrade the GPU past the 9060 XT, you could probably get a cheaper SSD with comparable specs, if you have aceess to microcenter you should look at their bundles.  those are all nitpicks though. if you ordered it rn it'd be fine",buildapc,2025-10-27 21:43:31,5
AMD,nlpyscx,How much is that total and where are you located?,buildapc,2025-10-27 21:42:00,3
AMD,nlpz8fi,Ok thank you!,buildapc,2025-10-27 21:44:23,3
AMD,nlpyyvo,This costs around $1350 and I am located in Eastern Europe,buildapc,2025-10-27 21:42:58,3
AMD,nlpzlzq,"Alright.  Overall it looks good. If you can save a little by going with another RX 9060 XT 16 GB then do that. Also consider spending a little more on maybe an 750 or 850W Gold rated PSU, that leaves more room for potential upgrades.  Might also be able to save a little on another SSD like WD SN7100, those Samsung PRO SSDs tend to be a little on the expensive side with no real benefit.",buildapc,2025-10-27 21:46:23,2
AMD,nlq0w98,Noted.   Bumped the PSU to 750W (Corsair CX750).  The only available WD SN7100 is 4TB at the moment so alternatively there is a Kingston KC3000 that would save me around $35 over the Samsung,buildapc,2025-10-27 21:53:18,1
AMD,nmppcmx,"Not a very high end case, but I believe the Montech Sky 2 GX has hinges.",buildapc,2025-11-02 15:07:50,2
AMD,nmpquhv,I don't mind budget brands at all! Thank you!,buildapc,2025-11-02 15:15:39,1
AMD,nmo57d6,I’ve used the 14600k and 9600x and they’re pretty equivalent in performance in most situations. You could check benchmarks for your programs that you use.  The benefit with the 9600X is that you get AM5 upgradability so you can upgrade your cpu in a few years on AM5 without replacing motherboard or ram. That’s a value.  The 9600X is also more power efficient and doesn’t need as much cooling.  They’re both going to work fine though.,buildapc,2025-11-02 07:46:01,11
AMD,nmorswr,"Multicore performance on 14600kf > 9600X. But for video editing, get 14600K to benefit from the quicksync feature.  9600X on other hand will give you upgrade path if you are someone that upgrades quite often",buildapc,2025-11-02 11:32:29,1
AMD,nmp0u9m,"The 14600kf is probably better for productivity because of the ecores. AM5 has some benefits though like some room to upgrade if you ever feel like you need to. But the 14600kf is pretty good for a midrange cpu, and comes with a 5 year warranty. As long as your needs don't change too much, you shouldn't need to upgrade for a while. Just make sure you get the new microcode, which you can get through a bios update.   There's probably some tasks where the 9600x would do better, say tasks that can use avx512 instructions for example. So its not exactly an easy question. But yeah you'd be fine either way but I would lean to the 14600kf if you don't plan to upgrade short term, 9600x if you do.",buildapc,2025-11-02 12:43:28,1
AMD,nmpce7p,"I don't understand why people are saying stuff like ""you can upgrade your 14600k/f with a 14900k"".. it's way more power AND cooling hungry.. it's not nearly as efficient. Plus, by time you upgrade it'll be several years old. It already is. Price might go down a fair bit but the efficiency will remain what it is.  Also, the point of ""you'll get quick sync"".. sure, it's true, but i hope you'll be using your gpu (which has cuda since it's nvidia) to render/encode video.. right?  I personally also don't like the way intel is going for the E cores/P cores thing. The amount of cores does NOT always mean extra performance in everything, since the cores aren't equal. It's only true in some contexts.  On top of that AM5 will get more support for future upgrades. Not only have they already announced it, but they're still releasing CPUs on the AM4 platform which is kinda unheard of in this market. I'd get the 9600x.  Happy shopping! 🛒   Edit: paragraph splitting for better readability",buildapc,2025-11-02 13:57:02,1
AMD,nmo1f90,"go intels new socket for upgrading room, 14xxx is dead",buildapc,2025-11-02 07:05:41,1
AMD,nmo6nb0,Go 14600Kf. Amd is currently overrated overpriced and overhyped. Don’t forget that 14900K still exists and intel will continue to manufacture it for a long tome. Well tuned 14900K outperforms the latest 285K in most real world tasks. What I am trying to say is that you are not really buying a dead platform you can always upgrade your 14600Kf to 14900K. Just buy moderately decent motherboard.,buildapc,2025-11-02 08:01:05,-3
AMD,nmo17mk,"For your use case the 14600kf is better.   The 9600x has better single core performance.   However, the 14600kf has 8 E cores giving it a significant advantage in multi threaded tasks.",buildapc,2025-11-02 07:03:30,0
AMD,nmo8n9s,Well for gaming ryzen will be better. If intel then intel ultra only but ryzen still will be better in gaming.,buildapc,2025-11-02 08:22:12,0
AMD,nmo5n3u,"Thank you, this been helpful",buildapc,2025-11-02 07:50:42,2
AMD,nmoqai8,for video editing the 14600kf does get quicksync,buildapc,2025-11-02 11:18:43,1
AMD,nmo1v55,"Truth to be said, the current Arrow Lake platform also don't got any 'upgrading room'. It's also dead as all it's gonna get is a minor refresh of Arrow Lake, and that's it. Intel will be releasing an all-new platform for the next-gen CPU.  You shouldn't be getting any Intel platform for purpose of 'upgrading / future-proofing'. Get Intel because you need to, not because you're looking for future upgradeability.",buildapc,2025-11-02 07:10:17,6
AMD,nmo1nho,"Exactly can't afford them for now, I thought I'd buy this Ryzen for now and then I upgrade, I liked the fact that Ryzen is future proof! What u think?",buildapc,2025-11-02 07:08:03,2
AMD,nmp8rnz,Yep. I have the 14900k and am building another rig with similar. A beast. Be sure it is new and get the microcode upgrade.,buildapc,2025-11-02 13:35:12,2
AMD,nmpifa8,"The 14600K does, the 14600KF doesn’t have the iGPU which is the where the QuickSync block lives.  None of the -F client CPU SKUs have the iGPU.",buildapc,2025-11-02 14:30:48,1
AMD,nmo2gzq,"ah okay, didnt realise the new intel socket was also eol",buildapc,2025-11-02 07:16:37,1
AMD,nmplpk4,"Right now, the main reasons to go Arrow Lake are because you aren’t overly concerned with the gaming delta between it and Zen 5 and. . .    - you want more cores at the lower price point,    - you want some of their motherboard features (read: Thunderbolt/USB4 without compromising PCIe lanes because ARL has that on the CPU),    - you want to play around with very high speed memory and see your performance go up instead of down,    - you have an app that loves QuickSync,    - or you caught a good sale. At $250 the 265K suddenly looks a lot better.      &nbsp;  If you’re above 1080p then the gaming differences are significantly smaller unless you’re comparing to the X3Ds.  If maxing frames is the goal, AMD by all means.  But at 1440p with reasonable cards ARL and Zen 5 both get you to GPU bound most of the time.",buildapc,2025-11-02 14:48:36,1
AMD,nmo2fns,"yes then ryzen is definitely the better option, 14600k will give you better performance today, but you will have a more future proof platform",buildapc,2025-11-02 07:16:14,1
AMD,nmo85xb,Pretty much. Arrow lake and arrow lake refresh is what it's getting.,buildapc,2025-11-02 08:17:05,1
AMD,nmut082,"It would be significantly faster, but still AM4 (like one of the fastest AM4 Cpu). It probably will still work, but you might also choose to buy it used since your next upgrade is going to AM5 anyways?",pcmasterrace,2025-11-03 10:02:50,6
AMD,nmvbvy7,"Considering the crazy prices for DDR5 RAM, staying on AM4 seems a sensible choice to me.",pcmasterrace,2025-11-03 12:41:46,3
AMD,nmvkfox,"Yeah 5950x would be a pretty good upgrade. You have a decent mobo as well, go for it!",pcmasterrace,2025-11-03 13:34:49,2
AMD,nmwphci,I’d go to am5 if I were you,pcmasterrace,2025-11-03 17:03:43,1
AMD,nn03pli,"If you do software development, yes the 16 cores is not overkill, even a ThreadRipper is not overkill for giant games. Get it while its cheap!",pcmasterrace,2025-11-04 03:49:50,1
AMD,nmutxx1,"I probably won't be upgrading to AM5 for a long time. I just so happen to run into a little extra money here recently. So, looking for something that's gonna last me awhile. My Ryzen 7 3700x surprising was pulling its own weight. However, am looking for a more responsive snappier experience without upgrading to a AM5 motherboard.  I sort of stopped focusing on building computers, so my knowledge has fallen off. I use common sense, but I don't really know what to get anymore. I checked the used market and these are being listed for around 250$, so not really worth saving 40ish$ over brand new. Although, I assume you recommended that because I said ""Is there something cheaper that will do almost the same thing? Looking for some pointers before I drop 300$ on it. :D"".  I think I might go ahead and buy it, I was just checking reddit before I made a stupid decision. I know I'm going to have to update my bios, my motherboard makes that easy to do :D",pcmasterrace,2025-11-03 10:12:09,3
AMD,nn03x74,"that is dumb right now with ram prices, 5950x is good enough for his case, a 16 core am5 chip to expensive with ram prices new mobo fuq that! he can get 16 cores for 250 bucks right now dude.",pcmasterrace,2025-11-04 03:51:14,1
AMD,nmvbi7d,Does your cooler have a double heatsink? If not you might want to get a new beefier one,pcmasterrace,2025-11-03 12:39:15,1
AMD,nmminpn,"> Nonetheless, we don't know what is AMD's definition of what the ""market needs,"" so we have to follow and wait for future driver updates before we can rule out these GPUs completely.  Yeah nothing has changed",pcmasterrace,2025-11-02 00:39:06,123
AMD,nmmsfpa,"yeah, ""miss-understanding""   ![gif](giphy|gKHGnB1ml0moQdjhEJ)",pcmasterrace,2025-11-02 01:35:22,56
AMD,nmo7bq7,People act like the cards wouldn't suddenly stop working.... Stop being so incredibly dramatic. It will still work absolutely fine regardless. Heck even if the drivers never changed again you would be fine. Specific game optimisations are nice but they are not a night and day difference.,pcmasterrace,2025-11-02 08:08:14,30
AMD,nmoc7t4,"Can't remember how often I've used GPUs that weren't supported anymore, and by that I mean no new drivers at all, not even a legacy branch and surprise they still worked, even with games that were released after the last driver. Yes it's stupid by AMD but not ""I need to sell my GPU asap""-stupid",pcmasterrace,2025-11-02 08:59:37,11
AMD,nmnc8cy,Hasn't this been known since day 1 and it just got drowned in totally not suspicious posting?,pcmasterrace,2025-11-02 03:39:24,18
AMD,nmo2732,"I didn't even hear of this news, had to look it up, I love AMD, but this is beyond stupid of them. Gonna have to rethink on how I look at them in the future now..... Hope there will be some modded drivers that prolong these perfectly capable cards still. My old 5700xt and Vega 64 are still kicking and holding their own. my 6800xt is still doing great... What a shame....",pcmasterrace,2025-11-02 07:13:43,4
AMD,nmn9qtn,"True, it feels like we're in a waiting game. Just hope they pul through with those updates!!",pcmasterrace,2025-11-02 03:22:11,1
AMD,nmwoiqt,I think they backtracked.  They tried it to see how it would go.,pcmasterrace,2025-11-03 16:59:10,1
AMD,nmnnotf,AMD not giving older cards features is enough to set the hater bandwagon into overdrive despite Nvidia doing the exact same. proof of the shitty double standard the nvidia fanboy brainlets in this sub hold.,pcmasterrace,2025-11-02 04:57:05,-7
AMD,nmoxqv3,"The ""game ready"" patches situation was clarified hours after the first statement in AMD Discord. Surprised how many people didn't notice, despite have been told so many times in this sub. Anyway, that's not how you use this meme template.",pcmasterrace,2025-11-02 12:20:58,1
AMD,nmmv7b4,Bah.  It's ok to not get new features after some time. Is the latest dlss really useful for a 1080 or 2080? Framegen isn't available on all models iirc.,pcmasterrace,2025-11-02 01:51:44,-15
AMD,nmobnwc,I'm never going to trust AMD again,pcmasterrace,2025-11-02 08:53:57,-5
AMD,nmtwymd,"Honestly it just sounds like teams aren’t communicating properly like in every company, like the drivers team said “hey we gotta focus more on newer cards in preparation for udna” and the news people heard “we’re dropping old cards”.",pcmasterrace,2025-11-03 04:48:39,2
AMD,nmw8uci,">Yeah nothing has changed  Yes, they are just going to do what they've always done: fix bugs and performance issues with older cards but no longer put effort into actively optimizing.  That's been basically the standard practice of nvidia and AMD/ATI for as long as they've been making gaming GPU's.",pcmasterrace,2025-11-03 15:42:54,1
AMD,nmmryxl,"""There's a problem with your heart, it's not working correctly""  ""I have heart failure?""  ""Not at all. To a degree, maybe""  ""I don't know what you mean by ""to a degree"", though""  Saying nothing has changed is, while based on reasonable expectations, isn't factually correct.",pcmasterrace,2025-11-02 01:32:37,-53
AMD,nmou3du,We didn’t want to rob you of the sense of pride and accomplishment that comes along with creating your own day one patches for the video games you buy.,pcmasterrace,2025-11-02 11:52:20,7
AMD,nmnxce7,"Fucking true, I love my AMD card, but ffs don't fucking find excuses when the company is fucking it up big time like this. Call them out, they got to do better, and it will not be better for the customer if the customer allows shit. No fucking company should be allowed, you don't own any of them anything.",pcmasterrace,2025-11-02 06:24:08,6
AMD,nmobv1c,Its not about them stopping working its that the 6000 series cards are still very capable of playing new games and not releasing drivers just Hurts them for no reason.,pcmasterrace,2025-11-02 08:55:58,6
AMD,nmq8ian,Dropping game ready driver support for a card that was for sale less than a year ago is very much a dick move. If Nvidia dropped game driver support for the 20-series and 30-series people would be flipping out.  AMD is just letting everyone who bought Nvidia over AMD know they were right.,pcmasterrace,2025-11-02 16:45:30,1
AMD,nmobzkk,If the first day game support remains idk what else to want from them since security updates are also there,pcmasterrace,2025-11-02 08:57:15,1
AMD,nmtxtnl,From my opinion it’s most likely due to an architectural problem that makes drivers more resource intensive while also trying to make drivers for udna. It’s a shame but at least amd is open sourced enough to where third party drivers can be made and can perform exceptionally well.,pcmasterrace,2025-11-03 04:55:33,1
AMD,nmwbf0m,"I'm unsure what you actually think they did here though.  They're not doing or saying anything here that nvidia isn't also doing. nvidia isn't actively optimizing for the 20 or 30 series anymore either. They also only fix bug and provide security updates. and new features are added only if they require no extra effort from them (so CNN model yes, new framegen no).",pcmasterrace,2025-11-03 15:55:18,1
AMD,nmod4xo,Modded drivers? For what? 6000 isn't getting fsr 4 so what else is there to give it?,pcmasterrace,2025-11-02 09:09:15,-5
AMD,nmnu83d,"Friendly reminder that RTX 20 series, first released in 2018, got DLSS 4.",pcmasterrace,2025-11-02 05:53:42,26
AMD,nmmxpav,"It's not even about the features though? Just receiving constant updates so that the card performs well and is optimised for newer games is more than enough most of the times and nvidia has a pretty good rep when it comes to giving long term driver support so let's leave them alone here credit where it's due nvidia has a lot better driver support even for their older gpus compared to amd, they only fumbled recently with the 50 series launch with some buggy drivers that they released multiple patches to fix and for most it's fixed already.  However bad nvidia's sale tactics are with expensive gpus and low vram, they are still the best when it comes to driver support for GPUs",pcmasterrace,2025-11-02 02:06:42,7
AMD,nmolh07,![gif](giphy|VEhWqu9nJHzOPKFsVA),pcmasterrace,2025-11-02 10:33:07,4
AMD,nmoq5du,said a guy who probably never had AMD gpu in his life.,pcmasterrace,2025-11-02 11:17:25,0
AMD,nmphqmj,Then you'd be in for a ride 😂  What's your next CPU going to be?  Intel or AMD?,pcmasterrace,2025-11-02 14:27:00,0
AMD,nmr8s8r,Still cant beleive EA pulled that fumble?,pcmasterrace,2025-11-02 19:40:04,1
AMD,nmod08d,Does it though? I usually lag a few months before upgrading graphics drivers to make sure I dont get any glitchy ones and I haven't noticed any real issue playing a new game at launch without updating drivers. Its just being blown out of proportion.,pcmasterrace,2025-11-02 09:07:51,8
AMD,nmpgdxy,"For ""no reason?"" you don't seem to understand how development works, for new drivers to be made for the older cards, someone still has to make them. This is not ""for no reason""",pcmasterrace,2025-11-02 14:19:33,0
AMD,nmwacxz,"AMD never said, in any way shape or form, they were ending driver support though. No message from them said that.  What they are doing is exactly the same thing nvidia's is doing. They aren't actively optimizing for the 20 or 30 series either. Which is the same thing the whole industry has done since gaming GPU's have existed.",pcmasterrace,2025-11-03 15:50:15,1
AMD,nmtydwp,"Yup, I agree. I remember using modded drivers for my old HD6990 when AMD/ATI dropped support for it, Kinda makes me want to install that card again and just see what it can still do if I can find a modded driver new enough.",pcmasterrace,2025-11-03 05:00:08,1
AMD,nmoddq0,"Game support, modded drivers already been doing that on unsupported GPUs, who cares about fsr4, open your mind, drivers are not for just features.",pcmasterrace,2025-11-02 09:11:49,7
AMD,nmwc2tv,Because that worked out of the box with no extra effort on their part. just Like AMD and FSR3 support.  20 and 30 series didn't get the new frame generation tech though. Because that would have required extra work to get working.,pcmasterrace,2025-11-03 15:58:29,1
AMD,nmofnpq,"20 series already has hardware that supported DLSS, the same way RDNA3 supports FSR4. DLSS4 is just an upgrade for it. 20 series also did not get FG or MFG. 30 series didn't get Smooth Motion either.  RDNA 1 and 2 on the other hand clearly don't have the capabilities to run FSR4 well. I get the complaints about Day 1 optimization but not for features like these lol.",pcmasterrace,2025-11-02 09:35:25,1
AMD,nmnuk1f,"and the fuck did that do for 20 series? didn't make it worth buying over anything else, did it?",pcmasterrace,2025-11-02 05:56:53,-26
AMD,nmphwea,"Wrong, I was very happy with my HD and R9 cards",pcmasterrace,2025-11-02 14:27:52,2
AMD,nmpi333,AMD obviously... For some reason they act like Radeon is a complete joke and put no effort into it,pcmasterrace,2025-11-02 14:28:54,0
AMD,nmoeg64,Well the drivers are supposed to optimize your game more,pcmasterrace,2025-11-02 09:22:58,5
AMD,nmosh1k,"Unless you're really paying attention to version numbers, I don't think this strategy does anything.  Say a new driver comes out September 1st, I'm gonna wait a month before I update.  New driver drops September 29th.  October 1st, ok I'm going to install that September 1st driver, well, I'm now downloading the Sept 29th latest, unless I was really paying attention, so now I'm on the latest drivers, not the last one I was intending to upgrade to.  And who knows if that Sept 29th driver was a bug fix or new release. Basically, I thought about it and just update when I see a new update, I'm not reading version numbers and patch notes.",pcmasterrace,2025-11-02 11:38:32,0
AMD,nmtxidg,"I second this, with plans for udna there’s most likely a development reason why making drivers for older rdna is more resource intensive",pcmasterrace,2025-11-03 04:53:03,0
AMD,nmtyij4,"I doubt there’s any windows drivers but there might be some Linux to where you can do… something. Idk what, but something",pcmasterrace,2025-11-03 05:01:11,1
AMD,nmodo9t,"I go weeks to sometimes months without updating my graphics driver, playing new games all the time no issue. If I do have an issue it's simply that whatever game is too demanding for my 6700xt.",pcmasterrace,2025-11-02 09:14:55,3
AMD,nmp28ay,"There's the int-8 version of FSR-4 that has been leaked and shown to be running on BOTH RDNA 2 & RDNA 3 cards. It's not as great as the version of fsr4 that runs on the 9000 cards, but it's a heck of a lot better than FSR 3.x. They could've thrown a bone to older card users and just made the release official... Just call it FSR 3.7 or whatever; make it a manual driver override feature using adrenaline software if they didn't want the hassle of maintaining yet another upscaling solution. Unfortunately, AMD isn't great at supporting their older cards  https://www.youtube.com/watch?v=2TLDQHvMBrQ - ‼️epilepsy warning - stop playing from 4:45 onwards‼️",pcmasterrace,2025-11-02 12:53:02,2
AMD,nmo0qes,"yep, the 20 series is good to have if you've bought it long ago.     also why are you getting downvoted?",pcmasterrace,2025-11-02 06:58:31,-5
AMD,nmof414,Thats a fair point. I haven't done anything detailed but I have tested a few games before and after doing a driver update after forgetting for almost a year and a half. Not much of a difference. Even when testing games that weren't out yet when I updated last.,pcmasterrace,2025-11-02 09:29:56,-2
AMD,nmoog2c,"it often doesn't make much of a difference at all, maybe 5/10% at the very best, people are acting like stuff is unplayable without specific optimizations, the vast majority of lesser known games never get any such thing and are absolutely fine.",pcmasterrace,2025-11-02 11:01:30,-4
AMD,nmodugf,Ok and what does that have to do with future titles?,pcmasterrace,2025-11-02 09:16:37,0
AMD,nmp4ouq,"iirc benchmarks show that while RDNA2 could run it, the performance uplift was a lot less than RDNA3. With that I mind I can see if AMD wouldn't want to support it.  Not to mention the reason why they still didn't add official support for RDNA3 was because they're still working on improving the performance gains. If RDNA2 cards would have even worse gains than RDNA3, AMD themselves might conclude it's not worth it.",pcmasterrace,2025-11-02 13:09:19,1
AMD,nmods9c,because people can't accept that Nvidia is in dire need of criticism and a huge downfall.,pcmasterrace,2025-11-02 09:16:01,-5
AMD,nmoqss9,Its not unplayable at 5/10% less FPS but its kinda unplayable if your game crashes,pcmasterrace,2025-11-02 11:23:23,4
AMD,nmoev9l,It means it's more likely the future titles won't run well due to them being demanding more so than lack of proper driver support.,pcmasterrace,2025-11-02 09:27:22,2
AMD,nmoft31,"Games will run even if you don't have the ""optimized and new drivers"" installed. Do you think newer games will just not run at all if they're not installed? That's what he's referring to.  If they weren't going to run, then AMD will release fixes for it. That's essentially what maintenance mode is.",pcmasterrace,2025-11-02 09:36:54,2
AMD,nmoibsb,Nvidia is getting plenty of deserved criticism all the time. But your statement was still just straight up wrong. They did give new features to older cards. So maybe don't spread misinformation if you want to be taken seriously,pcmasterrace,2025-11-02 10:02:08,4
AMD,nmoex0m,"No, it’s because you’re acting like a dick just because people don’t agree with you.",pcmasterrace,2025-11-02 09:27:54,5
AMD,nmpmnyz,"ah, so its ""what ifs"" you are worried about .... makes sense ... stop worrying about something that may happen and start worrying about things that actually have happened.",pcmasterrace,2025-11-02 14:53:40,0
AMD,nmtx9yg,"I believe those unplayable bugs would still have gotten updated in what and originally stated. Mostly just optimizations, performance and feature updates won’t happen",pcmasterrace,2025-11-03 04:51:11,-1
AMD,nmog54d,"Spoken like a little pissant nvidia fanboy ""MY TRILLION DOLLAR CORPO NNNNGNGNGNGGNGNNNNNGN!""",pcmasterrace,2025-11-02 09:40:16,-4
AMD,nmq1jjf,well bf6 crashed for me bc of drivers a few times. so i would say it has happened,pcmasterrace,2025-11-02 16:09:54,2
AMD,nmse8ul,"damn, wild to think someone actually typed this",pcmasterrace,2025-11-02 23:08:02,1
AMD,nmummm0,if people don't wanna be mocked they better not be a fuckwit. simple as that,pcmasterrace,2025-11-03 08:55:21,0
AMD,nmz1zp8,Make sure the hdmi is plugged into the graphics card,pcmasterrace,2025-11-04 00:06:11,36
AMD,nmz30w0,That thing will rock 1440p high at over 100 fps on just about anything short of a couple newer games.  1.)Make sure your monitor refresh is actually set to 180 Hz\ 2.)Use Radeon Chill to set your FPS limit to 177 FPS. Set both limits to 177 FPS\ 3.) Set “Wait for VSync” to “always on”\ 4.) Set AMD FreeSync to either AMD Optimized or Always on\ 5.) Enable FSR4\ 6.) Enable AMD Image Sharpening. I leave it at the default 50 setting.  Do not enable VSync within the graphics settings of any game. Ever.  Set the graphics settings of basically any game to High or Very High. I typically don’t go over High because the visual quality difference typically minimal but the compute resources cost is huge.,pcmasterrace,2025-11-04 00:12:07,8
AMD,nmz1ffl,"Looking for advice on settings? What games are you playing? Use the settings presets in the graphics menu and choose the setting that looks/plays/feels best, there's really not much else to it. Enjoy some games!",pcmasterrace,2025-11-04 00:02:56,3
AMD,nmzb8ra,"Look up how to disable GPU driver updates from Microsoft. Windows has a persistent problem where it rolls them back for AMD. Install and use the AMD Adrenalin program to update drivers. AMD Adrenalin also has a sound filtering function, which is great if you play online games with a mic.  If your display has HDR, you need to also enable it in Windows and you may need to also calibrate it. If your monitor has Freesync/Vsync, enable it in GPU settings. Cap FPS at 10-15 below refresh rate for smoothest performance.   Use Steam and GOG for games. Wishlist anything you're interested in to be notified when it goes on sale.   Nexus Mods is the go-to site if you want to explore mods. Being able to customize games is one of the fun things about PC.   If you still prefer using a controller, look up one with Hall Effect or TMR sticks. Way better than the default ones, although they lack the dualsense haptics.",pcmasterrace,2025-11-04 00:59:59,2
AMD,nmzuef2,"Isthereanydeal.com is a good way to find the cheapest price currently for a game you want right now.  Steam wishlist is a great way to just kinda have a running list and something to shop from when the sales hit. Steam has a solid sale every season or so.   You console controller will probably come in handy.  I don’t have a mouse pad, but one of those pads that cover half the desk, including the keyboard. It’s more comfortable to me.  Get a solid chair. Look up good keyboard ergonomics. Carpel tunnel is better prevented than treated.  My opinion, $30 cheap speakers and a solid wired headset, and a decent stand alone mic will go far. All my wireless gaming headsets all died In like 18 months, even the $300 turtle beaches. When gaming with your boys headset is in. When you’re relaxing on YouTube, the little speakers will be more relaxing. My headset is a like $60 Phillips and it has lasted years, comfy and sounds nice for me. I think the surround sound headsets are a gimmick but your mileage may very.   Enjoy brother! Go look up satisfactory!  If you don’t have a discord account, look it up. Almost all games have a discord server and great place to meet people to play with or learn games.  Okay I just reread your post. lol this is everything other than settings advice. Sorry, maybe something in here is useful so I’ll leave it.",pcmasterrace,2025-11-04 02:51:59,2
AMD,nmzb40l,"not sure about amd gpu stuf but for windows  win+s, start up apps , turn off everything you dont need   win s, background apps, same thing  you can add your games to high performance in windows (win+i, display, advanded graphics settings, set game to high performance if it aint, seems like win11 has most of them at high already but 10 didnt)   set your power plan to the highest: win s, power options, set to high or ultimate (if you dont have ultimate look up how to unhide it, i forget how)  you can also set your game to high cpu priority, go to task manager, right click game (running), go to details, right click and set priority to high - if it says you cant youll need to go into reg editor and set it manually, which i have written down already, but youll need the .exe file path of the game which ill let you figure out how to get:  careful in regedit tho  press:  windows key+R  type:   regedit  hit enter   highlight the address bar in the registry bar, and paste the address above   Computer\\HKEY\_LOCAL\_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Image File Execution Options  hit enter  right click the ""image file execution options"" on the left hand side  new - key  then name it exactly as the .exe appears (so like   hit enter to confirm the name  then right click that file you just made  new - key  name it:   PerfOptions  then Left click that  then right click the mostly empty white area on the right  new - DWORD (32-bit) Value  name it:  CpuPriorityClass  then click it and make the value data:   3  hit ok  heres what it looks like with marvel rivals, notice the file name marvel-win64-shipping.exe, this will just be the game you want as high cpu priority (dont select realtime in task manager or 4 here)  my marvel rivals exe is in this folder C:\\Program Files (x86)\\Steam\\steamapps\\common\\MarvelRivals\\MarvelGame\\Marvel\\Binaries\\Win64 - then the .exe  , and your games may be in a similar spot in file explorer (which opens with win+e btw)  if you cant find just run the game, right click on taskbar, properties, and you should see the file path   i also use ryzen master to oc my cpu - i think.. its been awhile since i actually opened it     https://preview.redd.it/6ihaukrc15zf1.png?width=852&format=png&auto=webp&s=11630991576214103fa598491a4dbd6cdfd691cd",pcmasterrace,2025-11-04 00:59:14,3
AMD,nmz1gsk,"Just set it to low settings, trial and error. Play it, raise settings, keep playing until you're happy with graphics fidelity settings and high fps settings",pcmasterrace,2025-11-04 00:03:10,1
AMD,nmz1on1,Use Radeon chill to cap the fps @ 190 or so.,pcmasterrace,2025-11-04 00:04:25,1
AMD,nmzaygm,Don't do the PCMR thing. It started as a joke that these idiots didn't understand and ran with. Loads of PC guys in a circle jerk wanking each other off about 2 grand systems whilst consoles just ignored them.  It was maybe funny for 2 days but it's just embarrasing. Also don't call your pc a rig. You only get to say that if you made the case yourself or you bolted it to your car.  I'm full PC now but back when PCMR was a serious thing they'd shout at the clouds and my friends and I would just ignore them.   basically just this  https://www.youtube.com/watch?v=aDMsGl_XxTk,pcmasterrace,2025-11-04 00:58:20,1
AMD,nmz2bh3,ur shit brolic bro I don't think settings really matter for you for the next few years unless you're playing ue5 slop,pcmasterrace,2025-11-04 00:08:04,-3
AMD,nmza326,Also make sure in the settings that the monitor is using the correct fps and not something lower.,pcmasterrace,2025-11-04 00:53:18,10
AMD,nmz949y,"NGL, I committed this error on my PC, and spent questioning myself why I was getting 10 fps on a 12 year old game",pcmasterrace,2025-11-04 00:47:39,3
AMD,nmzcznz,Thankful the pc had a giant sticker on the back ro let me know hdmi goes directly into the GPU,pcmasterrace,2025-11-04 01:10:16,3
AMD,nmzmhcs,![gif](giphy|tnYri4n2Frnig),pcmasterrace,2025-11-04 02:05:49,1
AMD,nn03u3l,Data port not hdmi.,pcmasterrace,2025-11-04 03:50:39,1
AMD,nmzdem8,This! This is the help I was looking for. Thank you.,pcmasterrace,2025-11-04 01:12:42,4
AMD,nmz6fld,Why 177 ?,pcmasterrace,2025-11-04 00:31:56,3
AMD,nmzhyo7,Radeon Chill.. Hmm. This is the best approach nowadays? I'm out the loop.,pcmasterrace,2025-11-04 01:39:46,1
AMD,nmzbi9o,"This then turn off motion blur, vignette, and film grain",pcmasterrace,2025-11-04 01:01:32,2
AMD,nmzcb5y,"Right now gears of war, vigor and ready or not. I wanna get into the nitty gritty.",pcmasterrace,2025-11-04 01:06:16,2
AMD,nmzg3ql,Thank you!!   So far I have steam set up but had no idea about GOG. I do still use controller so that's a big help. Ill look up how to disable. This was the kind of help I was hoping to get. 1st time pc gaming and its incredibly intimidating.,pcmasterrace,2025-11-04 01:28:39,2
AMD,nmzys25,I appreciate the answer though. This is such a big world it can be a big much so any help is good help.,pcmasterrace,2025-11-04 03:18:19,2
AMD,nmzfd2k,"Thank you , I really appreciate you writing it out for me. Not all heros wear capes",pcmasterrace,2025-11-04 01:24:17,2
AMD,nmz9ej7,With a PC that good and 1440p it's better to start at high settings and adjust down.,pcmasterrace,2025-11-04 00:49:19,1
AMD,nmzcreg,Thanks Ill look into that,pcmasterrace,2025-11-04 01:08:55,2
AMD,nmz2ju2,Why would you cap the frame rate above the monitor refresh rate?,pcmasterrace,2025-11-04 00:09:25,3
AMD,nmzen4f,"Got it.  Im not full fledged into pc, I still have my Xbox and ps5. I just wanted to use language that would likely get me some help. I stay away from the ones who call their desktop rigs.",pcmasterrace,2025-11-04 01:20:01,1
AMD,nmzd752,I dont understand,pcmasterrace,2025-11-04 01:11:29,3
AMD,nmzd2x1,1st thing I did once my monitor was set up.,pcmasterrace,2025-11-04 01:10:48,4
AMD,nmzclf8,"blurbusters tested end to end input latency and found that typically 3-4fps under the refresh rate is enough to keep input latency at a minimum while using vsync. If you don't use a frame cap with vsync you will incur a large amount of input lag if your card can render the game above the refresh consistently.  Nvidia will enable its own frame cap when you use reflex with vsync, they use 4-9% under the refresh (larger for higher refresh) rather than a fixed number. At 144hz they use 138, about 4% lower, at 240hz they use 225 which is 6% lower, at 360hz they use 328 which is 9% lower. Higher refresh doesn't scale how you think, going from 144hz to 240hz (adding 96hz) is 2.7ms smaller frametimes, but going from 240hz to 360hz where you add 120hz is only 1.4ms smaller frametimes, that's probably why nvidia doesn't just do ""3fps below"", they are making sure a minimum number of milliseconds is held before the next frame comes in even though blurbusters finds input lag reduces at only 2fps below even up to 240hz https://blurbusters.com/gsync/gsync101-input-lag-tests-and-settings/5/",pcmasterrace,2025-11-04 01:07:56,3
AMD,nmzccxu,Ill make sure does options are off,pcmasterrace,2025-11-04 01:06:34,1
AMD,nmzdqte,"Gotcha. Check out r/OptimizedGaming , the [PCGamingWiki](https://www.pcgamingwiki.com/wiki/Home) for essential fixes and information, Alex over at Digital Foundry updates [this playlist](https://youtube.com/playlist?list=PLZmp67znVJLU31dS6-H92ZeawijtxQ_Py) with title-specific optimized settings as he covers new titles, those are some good resources to get started with!",pcmasterrace,2025-11-04 01:14:42,2
AMD,nmzcm27,I maxed out all settings inside gears of war. That was wild,pcmasterrace,2025-11-04 01:08:02,2
AMD,nmz6dfi,Because frames will sometimes drop by a few so you give it a little room to not drop below your refresh rate,pcmasterrace,2025-11-04 00:31:35,2
AMD,nn0bxm9,It's really embarrassing. The entire PCMR joke is a throwaway joke from Yatzee. Literally one sentence. Some idiots on reddit took it to mean PC is better than console. They conveniently ignored all the years we got out shit pushed in.,pcmasterrace,2025-11-04 04:47:25,1
AMD,nmzd8yv,Make sure you check it periodically after windows updates. Mine has never changed back but I have heard it happening to others.  Speaking of which it’s been awhile since I’ve checked mine…,pcmasterrace,2025-11-04 01:11:47,2
AMD,nn03xgj,"Interesting, so when I play fortnite with my kids my frames cap at 225fps on my 240hz monitor and I have been wondering why.",pcmasterrace,2025-11-04 03:51:17,1
AMD,nn01cxd,To be clear - those are personal preference. You can enable or disable them in any game you want depending on how you like things.,pcmasterrace,2025-11-04 03:34:41,2
AMD,nmzgeki,Thank you,pcmasterrace,2025-11-04 01:30:27,2
AMD,nmzfkb4,Great feeling isn't it?,pcmasterrace,2025-11-04 01:25:28,2
AMD,nmzciur,"If you're using a variable refresh rate monitor, you want your frame rate below your max refresh rate. If your refresh rate is fixed, you ideally want it to match. If you're just trying to get the lowest latency possible, then you don't want to cap it.",pcmasterrace,2025-11-04 01:07:31,1
AMD,nmzgidh,Hope it didn't revert back!,pcmasterrace,2025-11-04 01:31:05,1
AMD,nn0831p,I know but I like to start with preferences and then making changes as I go,pcmasterrace,2025-11-04 04:19:16,1
AMD,nmzg9pa,It was amazing! It breathed new life into an old game.,pcmasterrace,2025-11-04 01:29:39,2
AMD,nmzcunx,My monitor does support vrr,pcmasterrace,2025-11-04 01:09:27,1
AMD,nmzdv01,"Then you'd want to enable freesync and vsync, and cap your frame rate around 170 for the best balance of latency and image quality.",pcmasterrace,2025-11-04 01:15:24,2
AMD,nmyf7t8,an used 5600 and 6700xt should be a big upgrade while not being too expensive.,pcmasterrace,2025-11-03 22:02:09,2
AMD,nmyff38,Depends on the budget  I would just max am4 out with an 5700x3d and a 9070xt  Otherwise the other guy is right with s 5600 6700xt,pcmasterrace,2025-11-03 22:03:13,1
AMD,nmd01ir,"Hope we push back enough they reverse it. 6000 series is not old by any stretch, it's inexcusable.",pcmasterrace,2025-10-31 12:39:19,850
AMD,nmd3x48,"5000 series perhaps, but 6000? if i bought a 6700xt or up back then im going to be pissed off.",pcmasterrace,2025-10-31 13:01:43,399
AMD,nmd0hic,AMD ruining any shred of positive momentum they had as fucking always,pcmasterrace,2025-10-31 12:42:01,479
AMD,nmd056f,"RDNA1 is somewhat reasonable (Lack of DX12U and 6 years old). RDNA2 ain't   You can still buy RDNA 2 GPUs new, and AMD are still releasing RDNA 2 APUs",pcmasterrace,2025-10-31 12:39:57,176
AMD,nmd8esa,AMD always manages to snatch defeat from the jaws of victory.,pcmasterrace,2025-10-31 13:26:48,92
AMD,nmd92qa,"This is unacceptable. RDNA 2 gpus still have great performance. Also does this mean that the new xbox rog ally wont get game optimized updates, unlike it's main competition?",pcmasterrace,2025-10-31 13:30:30,48
AMD,nmd4xy0,Da fuck  I just paid full price 5 months ago for my RX 6650XT    It's not fucking supported anymore?,pcmasterrace,2025-10-31 13:07:32,61
AMD,nmdauy1,Ye so next card is not going to be amd,pcmasterrace,2025-10-31 13:40:16,32
AMD,nmd5k33,"If it's just like the previous generation then they've already began de-optimizing it and removing features. Actually, I already know this is true, they began removing features from RDNA2 well over a year ago.  e.g  [https://github.com/ROCm/ROCm/discussions/2867](https://github.com/ROCm/ROCm/discussions/2867)  [https://github.com/ROCm/ROCclr/commit/16044d1b30b822bb135a389c968b8365630da452](https://github.com/ROCm/ROCclr/commit/16044d1b30b822bb135a389c968b8365630da452)",pcmasterrace,2025-10-31 13:10:58,19
AMD,nmdayli,"""It's a shame the 6000 Series is also affected by this shift. Its performance is still capable of handling the onslaught of unoptimized games this year.""  higher end 6000 series can handle unoptimized games this year.  mid to lower 6000 series running at 1080p might struggle.  something like bf6 that is optimized well runs ok.  but, i don't think driver features and game patches are going to make a big different to my 6650xt.  stalker 2 still runs poorly (unoptimized/buggy game).",pcmasterrace,2025-10-31 13:40:49,9
AMD,nmddjkk,"Although I don’t play new games I find it really weird they are already pushing the 6000 series to not have day 1 optimisation, the card is like 5 years old and hell I have my 6700xt for 3 years and it still running nicely, are they like trying to push people to go from red to green?",pcmasterrace,2025-10-31 13:54:26,7
AMD,nmdk4vb,I bought a 6900XT about 2 years ago and was pretty happy with my purchase   But no way I'm going back to AMD after this sh!t,pcmasterrace,2025-10-31 14:28:17,16
AMD,nmdx056,I jist bought a rx 6800xt wtf,pcmasterrace,2025-10-31 15:32:20,5
AMD,nmeip61,I busted my ass for 2 months of work in where i live to buy my rx 6600 and owned it for barely a year and didnt even enjoy it to its fullest yet and now this.....    i am screwed big time and it hurts me a lot... words cannot describe how pissed off and angry i am...,pcmasterrace,2025-10-31 17:18:08,3
AMD,nmdgdyz,... Really AMD? Nvidia gave us 9 years of full GTX 1000 support and y'all are pulling support for the 6000 series? Like... Bro.. the 6000 series is where I was gonna go when I could afford too.. grab a 6800 XT to replace my 1080.. guess I'm gonna have to go Nvidia 3000 now? Like... Ugh.. c'mon guys..,pcmasterrace,2025-10-31 14:09:06,13
AMD,nmduzx3,I’ve paid 1300€ for a 6900XT in 2022. not even 3 years ago. And now I don’t get updates ? That feels like fraud ngl.,pcmasterrace,2025-10-31 15:22:45,8
AMD,nmfmzji,It's funny that driver support for my GTX1060 from 2016 and my RX6700xt from 2021 ended practically at the same time.   How am I supposed to take Radeon GPUs seriously when even AMD doesn't do that?,pcmasterrace,2025-10-31 20:47:21,4
AMD,nmd5l4z,"I bought a 6700 XT last year, fu** AMD, never buying an AMD GPU again.",pcmasterrace,2025-10-31 13:11:08,15
AMD,nmdddn5,"Shame, this makes me want to pay the Nvidia tax for my next card. My 6700xt is still a great gpu, it's unacceptable to be sunsetting rdna2 this soon.",pcmasterrace,2025-10-31 13:53:35,8
AMD,nmdkyvb,Currently on a 7900 GRE. At this point im considering switching it out for a 50 series card because im sure that supports likely not lasting long.,pcmasterrace,2025-10-31 14:32:34,3
AMD,nmdlkma,Yeah screw my 1 year old 6950xt i guess.,pcmasterrace,2025-10-31 14:35:41,5
AMD,nmd5sw0,"Not defending the move, but with a small market share and even smaller profits it is hard to justify the cost of ongoing support as a business expense. Looks like a purely business decision.",pcmasterrace,2025-10-31 13:12:19,10
AMD,nmeaa23,# AMD clarifies that RDNA 1 and 2 will still get day zero game support and driver updates — discrete GPUs and handhelds will still work with future games  [https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),pcmasterrace,2025-10-31 16:36:59,14
AMD,nmdknkd,I'm still getting updates and DLSS4 on 2060,pcmasterrace,2025-10-31 14:30:58,2
AMD,nmdtcgp,Me sitting here with my 1600x,pcmasterrace,2025-10-31 15:14:44,2
AMD,nmegfh3,Huge storm in tiny little petit glas,pcmasterrace,2025-10-31 17:06:57,2
AMD,nmeicur,"Former HD4850/4870 owners:  *""First time?""*",pcmasterrace,2025-10-31 17:16:27,2
AMD,nmesqah,"At the start of the year I was looking for a gpu, It was a choice between a 4060ti 16gb or a cheaper 6600xt.  Man, I'm glad I decided for the 4060ti and dodged a bullet.",pcmasterrace,2025-10-31 18:08:22,2
AMD,nmf6gmw,"I thought they clarified this is for software updates, like offering the 7000 series and the latest GPUs newer versions of FSR.   Still garbage communication though",pcmasterrace,2025-10-31 19:19:13,2
AMD,nmi1vis,[no they're not](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),pcmasterrace,2025-11-01 07:43:55,2
AMD,nmdssop,"As much as I hate to say it, I am glad I chose the 5070 Ti over the 9070 XT. The 5700 XT I very briefly owned (which was promptly returned for a 2060 Super back then) soured my experience so much with AMD GPUs that I've sworn them off on my main PC.",pcmasterrace,2025-10-31 15:12:01,4
AMD,nmdimx5,"Oof. I'll stick to Nvidia, they're slimy as well but damn.",pcmasterrace,2025-10-31 14:20:38,2
AMD,nmdln4c,Wtf? 6000 series isn't even that old,pcmasterrace,2025-10-31 14:36:01,4
AMD,nmd0nyr,AMD is one of the worst companies ever but I will get downvoted by AMD shills,pcmasterrace,2025-10-31 12:43:05,7
AMD,nmd0q1r,From FineWine to SourWine…,pcmasterrace,2025-10-31 12:43:26,3
AMD,nme9pob,"This is a pure dick move AMD.  I didn't get a 5 year shelf out of my GPU.   I will definitely not be purchasing another one of their releases.  You wont even support your own product for 5 fucking years, that's a pure PhD move. They burned a bridge they cannot get back.",pcmasterrace,2025-10-31 16:34:11,2
AMD,nmeebm5,"6750xt here, kinda lame but the jump in frame gen & rt from 6>7 series is insane, kinda makes sense. Oh well, bought the 9070xt that’s on woot right now for 650.",pcmasterrace,2025-10-31 16:56:42,2
AMD,nmfalre,"As someone who owns an AMD card in their gaming laptop, and an Nvidia card in their desktop, this is part of what makes people move to Nvidia. Yes, Nvidia has its fair share of sin, there's plenty of blame to go around but if you're trying to undercut your competition, phasing out support for a relatively recent and prevalent gpu series that is quite competent is not a good way to guarantee future business. Meanwhile my 3090 was still getting new features as of a few months back. Really messed up.",pcmasterrace,2025-10-31 19:41:13,2
AMD,nmfpduy,"Imagine if Nvidia did this. Whole sub would be shitting on them, 10k+ upvotes. I get that they suck but AMD suck more .",pcmasterrace,2025-10-31 21:00:19,3
AMD,nmd7duc,"I do wonder if they intentionally want the gaming GPU side to tank, because they can make so much more money selling AI GPUs i.e. much bigger return on wafers if they dedicate it to AI",pcmasterrace,2025-10-31 13:21:07,1
AMD,nme6gar,The dumbest move . Now nvidia is better,pcmasterrace,2025-10-31 16:17:59,1
AMD,nme9mlk,So dumb,pcmasterrace,2025-10-31 16:33:45,1
AMD,nmecun6,they gave free advertisement to Nvidia. buncha idiots ruined their locomotive train of success.,pcmasterrace,2025-10-31 16:49:34,1
AMD,nmeekj4,"Feels like my rx 580 lasted forever. My old hd5870 lasted a long ass time too. Now the 6950xt I bought two years ago is going out of date? I’m a lifelong AMD gpu buyer, but I’m about to go to Nvidia if this is the case.",pcmasterrace,2025-10-31 16:57:54,1
AMD,nmefs9b,Bought a 5070ti and resold my 6800xt earlier this month. Feeling a bit of survivors guilt out here,pcmasterrace,2025-10-31 17:03:46,1
AMD,nmehl9h,I just upgraded to a used 6800xt a few months ago... feels bad man.,pcmasterrace,2025-10-31 17:12:40,1
AMD,nmeip1g,lmao my 6950xt is barely 2 years old :( WTF AMD ??,pcmasterrace,2025-10-31 17:18:07,1
AMD,nmekhvj,There goes my RX 6600 that I bought 2 years ago. What's the best upgrade from this I wonder?,pcmasterrace,2025-10-31 17:27:07,1
AMD,nmelar3,What would be an upgrade on the NVIDIA front for the 6650xt? currently just playing BF6 on 1080p,pcmasterrace,2025-10-31 17:31:07,1
AMD,nmesl52,"Was gonna get a 9700xt this holiday season, but really makes me question the decision.",pcmasterrace,2025-10-31 18:07:38,1
AMD,nmevxf0,so that means we won't have optimazitoions for gta 6 even the gpu could handle it.,pcmasterrace,2025-10-31 18:24:54,1
AMD,nmey2hr,Noooo not my 5600xt!!,pcmasterrace,2025-10-31 18:35:55,1
AMD,nmeyrug,"Between this and Windows 11, is this a sign I need to update at last?",pcmasterrace,2025-10-31 18:39:33,1
AMD,nmf1905,"I've only had my GPU for 4 years, and I can run every game I want at a reasonable framerate, yet AMD has to ruin it.",pcmasterrace,2025-10-31 18:52:12,1
AMD,nmgaxoi,My 6800 XT is staying decently afloat at 1440p. No reason for all this garbage other than to force users to buy new cards faster. Disgusting behavior.,pcmasterrace,2025-10-31 23:13:20,1
AMD,nmgl2fh,Linux is still supported FYI.  https://www.gamingonlinux.com/2025/10/linux-users-have-no-reason-to-worry-about-recent-amd-gpu-driver-changes/,pcmasterrace,2025-11-01 00:20:15,1
AMD,nmgzn0l,[MFW the fact that the drivers on Linux for these cards is open means that the community can continue to make them better while Windows languishes.](https://i.imgur.com/1MPPGiB.gif),pcmasterrace,2025-11-01 01:59:42,1
AMD,nmh7se4,"i got my first amd card after upgrading from a 2080ti to a 6900xt.  there is still plenty of life in that damn card, i am definetly not buying another amd card after this bullshit.",pcmasterrace,2025-11-01 02:57:38,1
AMD,nmho8af,Thank god I didn't opt for 6750xt 2 years ago. This is why I'm still not convinced to opt for amd in gpu over nvidia.,pcmasterrace,2025-11-01 05:14:33,1
AMD,nmhu8vw,I have a 6700xt and am hoping to have it a few more years :/,pcmasterrace,2025-11-01 06:17:57,1
AMD,nmhxvqp,"I may have accidentally chosen the perfect time to upgrade to a 9070xt then lol, was not aware of this..",pcmasterrace,2025-11-01 06:58:33,1
AMD,nmhxyyc,"**Selling RX 6800 😣😖 switching to Nvidia, I would never buy an AMD Radeon**",pcmasterrace,2025-11-01 06:59:32,1
AMD,nmhz6e0,Bruh,pcmasterrace,2025-11-01 07:13:08,1
AMD,nmhzy27,"If 6000 series is EOS, does that mean my PC bricks without a new GPU?  I left my PC with my son when I split with my ex, so basically I'm asking if I need to prioritize a new GPU in my budget for him to be able to continue using the machine when his 6700xt hits EOS?",pcmasterrace,2025-11-01 07:21:53,1
AMD,nmi5zqh,"I have a 6700 10GB and run everything fine, I don't want to spend money on a new GPU when there is no reason for it",pcmasterrace,2025-11-01 08:29:58,1
AMD,nmicsv6,Bro i just bought rx 6700xt 😭,pcmasterrace,2025-11-01 09:45:19,1
AMD,nmidwls,not allowing 6900xt to play new games is just another level of disrespect,pcmasterrace,2025-11-01 09:56:53,1
AMD,nmiidcr,I just got an rx 6700 xt last month bruh...,pcmasterrace,2025-11-01 10:42:22,1
AMD,nmj8t2t,So my 6800xt is just borked? Love spending $400 on a card just to have it not be supported anymore,pcmasterrace,2025-11-01 13:59:45,1
AMD,nmjejry,https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games,pcmasterrace,2025-11-01 14:33:12,1
AMD,nmkiv5x,Is it worth getting a RX 6700XT instead of a 3060 or a B580 after AMD shut down the driver support for Ryzen 5000/6000 series?   I have been planning my build for months and finally is going to build my PC but suddenly Ram prices went crazy and now driver support for this card is also on maintenance.   What should I do? Should I get a 6700XT or a 3060 or a Arc B580 Index?,pcmasterrace,2025-11-01 18:03:57,1
AMD,nml09uk,I wish it's a joke ? if that is right i will never buy an amd product again...,pcmasterrace,2025-11-01 19:34:55,1
AMD,nmmuye5,I literally just bought a 6950xt 2 weeks ago for 380euros.💀💀,pcmasterrace,2025-11-02 01:50:16,1
AMD,nmozef3,amd moment,pcmasterrace,2025-11-02 12:33:13,1
AMD,nmvbign,"Broooo i just went from 5700xt to 6700xt like a year ago. Wtf arw they thinking? The 6000 series isnt that old, i could understand stopping the support for the 5000 series but the 6000 series is unexcusable. They really removed the mask and showed us who they really are. I mean we all nvidia is not a saint either but at least they support their gpus for years to come",pcmasterrace,2025-11-03 12:39:17,1
AMD,nmxkk0x,This would explain all the 6\*\*\* series GPU's I'm seeing up for sale on HWS,pcmasterrace,2025-11-03 19:32:55,1
AMD,nmdnb1x,Glad I didn't pay for my 6700xt back when new.. side trade... But still it was £700+ when new during lockdown.   It's a shit turn for AMD to drop support on those that backed them by spending soo much on their product when they needed sales.,pcmasterrace,2025-10-31 14:44:30,1
AMD,nme1b77,i wondered why lately i had so many issues...  no more amd,pcmasterrace,2025-10-31 15:52:59,1
AMD,nmftnwr,"I have an RX6600, great card, served me well for the last 3 years, couldn't see a reason to complain or to upgrade. My next one is gonna be from NVIDIA.",pcmasterrace,2025-10-31 21:24:50,1
AMD,nmdibqt,"F\*\*ks sake, take some copium. It's not the end of the world.",pcmasterrace,2025-10-31 14:19:03,0
AMD,nmd798o,Is Optimization support that shit that installs with the driver so they can market you? Like GForce Experience?,pcmasterrace,2025-10-31 13:20:25,0
AMD,nmddvay,5000 probably it’s time is up. My office computer has a 5700xt so it is what it is but the 6000 should still be supported at least till the next gen RDNA.,pcmasterrace,2025-10-31 13:56:06,0
AMD,nmdxmg0,https://preview.redd.it/0yv4eqadvgyf1.jpeg?width=500&format=pjpg&auto=webp&s=ef37d6257c55a6b1fcc337b056c7326b983d6043,pcmasterrace,2025-10-31 15:35:21,0
AMD,nme0drd,Planned obsolescence.,pcmasterrace,2025-10-31 15:48:35,0
AMD,nme7skm,"Tbh i always found it so weird that drivers have game specific optimizations, i though that was just a marketing gimmick",pcmasterrace,2025-10-31 16:24:40,0
AMD,nmd59cd,Bro is using Internet Explorer,pcmasterrace,2025-10-31 13:09:18,-1
AMD,nmd9paz,My next card will be intel,pcmasterrace,2025-10-31 13:33:58,-1
AMD,nmec8id,https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games   Can we move on now?,pcmasterrace,2025-10-31 16:46:34,1
AMD,nme1ovk,Sheesh this is awful. I was thinking of upgrading my rtx 2080 to an AMD card too ...,pcmasterrace,2025-10-31 15:54:47,0
AMD,nmebjsp,What optimizations are you expected in a 6000 series GPU? They're as optimized as they're going to get.,pcmasterrace,2025-10-31 16:43:13,0
AMD,nmedi65,Nvidia still doesn't support RT50 series so I'm not sure who's worse.,pcmasterrace,2025-10-31 16:52:43,0
AMD,nmdx5ay,My 6650XT seems to be dead (I need to test it s'more) and I was gonna buy 9070 XT. But not sure if I should because they might not support it in October 2026,pcmasterrace,2025-10-31 15:33:02,-1
AMD,nmdx5n6,nvidia it is for my next gpu,pcmasterrace,2025-10-31 15:33:04,-1
AMD,nmd4zgl,In fact the latest RX 6*50 GPUs was released 2 years ago. Also  odern APUs with the same RDNA 2 architecture are affected by this too.  Edit: [AMD says GPUs based on RDNA 1 and RDNA 2 will still get zero day game support.](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),pcmasterrace,2025-10-31 13:07:46,263
AMD,nmd57sy,"I have a 7000 series and this shit pisses me off. They charge premium prices for these cards, customers deserve premium support. I paid $750 for mine. I expected to get at least 5 years out of this thing. Not to mention other devices such as Steam Deck and PS5 that might be affected.",pcmasterrace,2025-10-31 13:09:04,72
AMD,nmdp4ai,"I do have a 6650XT, bought together with Starfield. If AMD really ends their game-ready driver support this quickly, then that's my first and only AMD graphics card.",pcmasterrace,2025-10-31 14:53:37,18
AMD,nmde3ht,"Everyone complained when they killed Vega early, but it didn't change anything",pcmasterrace,2025-10-31 13:57:17,11
AMD,nmdrh5r,"Even if they reverse it, AMD has shown you what they wanted to do and who they are.   I have a 7900 XTX, and it's an insanely nice GPU, I game on 4k with. It's not being affected, but I'm still pissed, because it's a massive slap to the face. I know Nvdia isn't a saint either, but honestly they at least support their GPUs. I'm switching back to Nvdia for 6000 or 7000 series, I'm sure a lot of others will too.  AMDs Radeon department makes the most asinine choices when it comes to GPU decisions.",pcmasterrace,2025-10-31 15:05:24,12
AMD,nmk2jux,Seriously.  I bought a 6800 last year because I knew it was a beast that could still handle all modern games.,pcmasterrace,2025-11-01 16:41:06,1
AMD,nmkld88,Im rly amazed how much effort AMD keep putin into alienating the rest of the gamers aswell. No wonder NV is over 90%,pcmasterrace,2025-11-01 18:16:45,1
AMD,nmdeqwe,They didn't drop support people are promoting lies like they did woth copilot always on then when amd clarifies support isnt dead people will cheer we did it reddit we made them go back.,pcmasterrace,2025-10-31 14:00:40,-11
AMD,nmevdr2,"I thought AMD is consumer friendly and love you r/pcmr folks, what happened?",pcmasterrace,2025-10-31 18:22:06,-4
AMD,nmde0yo,I bought a brand new 6750 xt eight months ago.  Imagine my face right now.,pcmasterrace,2025-10-31 13:56:55,95
AMD,nmd84wi,Yup! have 6900 XTX. Paid like 1200 retail when you couldnt find any gpu. It still clobbers games at 4k how I use it. AMD hasnt given a compelling GPU release for 6900 7900 owners to switch to!    Why would i buy say a 9070xt it makes little sense yet,pcmasterrace,2025-10-31 13:25:17,97
AMD,nmeo9g2,[https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),pcmasterrace,2025-10-31 17:45:56,9
AMD,nmf0fpp,total is misleading it's still getting support just not priority because those likely aren't have major issues when new games out with trash codes,pcmasterrace,2025-10-31 18:48:04,8
AMD,nmlvjrz,"I built my first PC at the start of September- fully AMD with a 6700xt in it. I was feeling good until now. I don't understand why they are doing it, I'm guessing money, but isn't the card is only 4 years old? It is perfectly capable of running new games.",pcmasterrace,2025-11-01 22:25:00,1
AMD,nmd4syl,AMD never misses the opportunity to miss an opportunity,pcmasterrace,2025-10-31 13:06:46,261
AMD,nmd6brv,I think the community needs to get over thinking they're somehow the savior. They're just like Nvidia and if they were larger they would be doing business exactly as Nvidia does. They're just doing it on a smaller scale because that's what they have to work with.,pcmasterrace,2025-10-31 13:15:14,92
AMD,nmdcenu,Remember when they stopped the production of the 7800x3d months before the 9800x3d to drive up the price of the older chip so people would buy the new one?  AMD is still a business with a goal of making as much money as possible. You better hope Intel get their shit together and Nvidia going stronger on the low/mid range GPU. Nothing good happens when anyone stays on top.,pcmasterrace,2025-10-31 13:48:31,16
AMD,nmddr0a,"As Steve always says, AMD never fails to snatch defeat from the jaws of victory",pcmasterrace,2025-10-31 13:55:30,16
AMD,nmd9ut6,"Was is ever any different?    This was always the case, just look at Polaris or Vega AMD never really supported hardware over long periods of time.",pcmasterrace,2025-10-31 13:34:48,5
AMD,nmdnzur,"Aren't we living in a great GPU world where your choices are: - A good GPU with good VRAM for a ""reasonable"" price but you get only 3 years of drivers supports  OR  - An expensive brick that could die at any moment you start putting a 12HWR in them for 30-40% more money than the first choice and less VRAM but you get 10 years of driver supports",pcmasterrace,2025-10-31 14:47:58,6
AMD,nmdjo6s,"Yep, unless they change course, guess I'll unironically be looking for Intel hopefully high end next gen XDDD",pcmasterrace,2025-10-31 14:25:58,1
AMD,nmevipk,"maybe stop fangirling for a corporation like a bunch of sheep, r/pcmr is a joke     thinks AMD is somehow 'for the consumers' unlike literally every other company",pcmasterrace,2025-10-31 18:22:49,1
AMD,nmd14g1,RTX 3090 guys laughing all the way to the bank at the poor bastards that got RX 6900XT,pcmasterrace,2025-10-31 12:45:48,82
AMD,nmdfgqc,Support isnt dead they are deprioritizing specific fixes for older cards. So you get generic driver fixes and game profiles    Nvidia litterally does this day 1 when new cards release and has since Kepler 15 years ago.,pcmasterrace,2025-10-31 14:04:22,5
AMD,nmenayd,current gen consoles use RDNA 2 there is literally no excuse,pcmasterrace,2025-10-31 17:41:14,2
AMD,nmd5jew,6 years old? RDNA 1 is 8 years old.  RDNA 2 is 6 years old.,pcmasterrace,2025-10-31 13:10:52,-9
AMD,nmdrenw,"Only because people are delusional enough to believe they were anywhere near ""victory"" in the first place.  Their market share is single digit, their software features are behind in every metric, they do not compete in the high end at all unless you count super cherry picked 9070xt benchmarks, and their pricing isn't really that competitive.  This isn't snatching defeat from the jaws of victory, this is just yet another example of why Nvidia is the dominant force in the market to begin with.",pcmasterrace,2025-10-31 15:05:03,22
AMD,nmew1k8,what victory did they have? the reddit victory?   Being fawned over by a bunch of sheep in r/pcmr means jack shit for market-share,pcmasterrace,2025-10-31 18:25:30,2
AMD,nmdihmm,"Yeah the standard Xbox ROG Ally is RDNA2 APU and is on Windows, so out of the box it needs the AMD drivers that are now being discontinued right as the thing released, what a bloody joke and scam to those purchasing it.   Steam Deck meanwhile is on Linux where the community ran drivers will keep support, in fact even ancient GCN Radeons still get updated on Linux.",pcmasterrace,2025-10-31 14:19:53,21
AMD,nmeshl3,Its a clickbait headline. Thats not what AMD actually said.,pcmasterrace,2025-10-31 18:07:07,-3
AMD,nmd5s2f,"It is supported.   AMD has just stopped doing day 1 game optimisations. You still get driver updates, bug fixes and security patches.  You will literally notice NO DIFFERENCE.",pcmasterrace,2025-10-31 13:12:11,-35
AMD,nmesew4,All these articles are literally clickbait deliberately misinterpreting AMD’s actual announcement,pcmasterrace,2025-10-31 18:06:44,9
AMD,nmeo7m0,[https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),pcmasterrace,2025-10-31 17:45:41,8
AMD,nmeo6xo,[https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),pcmasterrace,2025-10-31 17:45:36,2
AMD,nmfo6tf,"Great card man. Enjoy it, I’d say 🙂. Don’t worry about all that nonsense being spread around",pcmasterrace,2025-10-31 20:53:50,2
AMD,nmevz9d,You should never buy 6000 series to begin with in 2024. DLSS was already taken off and preset E was in general giving people better than native quality starting from 4k performance mode. Now preset K is getting previously quality level image with performance mode and it even runs in a potato RTX2060 thanks to its tensor core support.  A weak RTX4060 have double the matrix compute performance of a 7900XTX is a huge red light.,pcmasterrace,2025-10-31 18:25:10,-3
AMD,nmfosq4,"Man, I built a top of the line AM4 build with a 5700X3D and a 6800 for way less than that. CPU + GPU were just €598 combined, new.",pcmasterrace,2025-10-31 20:57:07,2
AMD,nmeo657,[https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),pcmasterrace,2025-10-31 17:45:30,2
AMD,nmlfi0g,"There isn't any Nvidia tax. You get better and more features with a reasonable increase in price. It's only VRAM that is the issue, but people vastly overstate how important it is.",pcmasterrace,2025-11-01 20:55:35,1
AMD,nme9h7w,If I didn't play games that played very well with Linux I'd be fuming right now....,pcmasterrace,2025-10-31 16:33:00,1
AMD,nmfodb0,How’s it been performing and what’s the power draw?,pcmasterrace,2025-10-31 20:54:48,1
AMD,nmdfz0v,"they need to fire their business suits because they must be retarded thinking pulling the plug on 5 year old products will grow their business in the future.   also, the 7000 series most likely sold worse than the 6000. shouldn't they stop support that one instead by your logic?",pcmasterrace,2025-10-31 14:06:59,23
AMD,nmfctpb,"It's likely just a guy profiling new releases for shader bottlenecks.  Nvidia moved it over to AI, which is why the drivers on their side are sucky now too.  AMD just has the dude taking 3 hour lunch breaks just to be spiteful.",pcmasterrace,2025-10-31 19:53:04,1
AMD,nmekun7,">""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch,"" an AMD spokesperson told *Tom's Hardware*.  This is meaningless until they define what ""market needs"" means.",pcmasterrace,2025-10-31 17:28:54,11
AMD,nmebu30,thanks for posting this time to end all the rage baiting.,pcmasterrace,2025-10-31 16:44:36,1
AMD,nmi3bzw,"Most making said storms don't even update thier drivers on purpose due to a certain game they like performing better ok an older version or have windows update/auto update on at best, they won't even notice it, nor will they notice the less frequent updates.  This will cool down for sure, they probably think older Nvidia cards are getting the same full updates that new gen cards are receiving 🤭, both Nvidia and AMD do this, it's normal, it's just something AMD could've avoided by wording it better.",pcmasterrace,2025-11-01 08:00:14,2
AMD,nmftcqu,lol amd has already traumatized many of us before,pcmasterrace,2025-10-31 21:23:01,1
AMD,nmd2d0x,I mean both AMD and Nvidia have pretty awful anti consumer practices with their products.,pcmasterrace,2025-10-31 12:52:54,48
AMD,nmd2cdv,"AMD is **Just** good in CPU, but **Not** in GPU.. am currently use 5700 XT and 6600 must change to latest one",pcmasterrace,2025-10-31 12:52:48,3
AMD,nmecsi5,I hope you know that OP is very misinformed and they are not stopping support... Just read the new article and move on,pcmasterrace,2025-10-31 16:49:16,1
AMD,nmg3de0,AMD retracted the RX 5000/6000 statement via tomshardware.,pcmasterrace,2025-10-31 22:23:52,2
AMD,nmgw8or,6700 XT still kicking here so fuck em,pcmasterrace,2025-11-01 01:36:23,2
AMD,nmhyq0t,I think you should just sell this card and upgrade when the super series launches at least support is somewhat guaranteed for some time. Really disappointing behaviour from AMD regardless.,pcmasterrace,2025-11-01 07:07:58,1
AMD,nmhymbf,"Yeah exactly prople here dont seem to understand that a similar situation may happen to non UDNA cards 2 years down the line. Are they going to still keep defending AMD, I hope not.",pcmasterrace,2025-11-01 07:06:49,2
AMD,nmhyi58,"Then they may cut support of this card 2 years time. I dont understand how could people support this behaviour unless they change cards very frequently, are ok with non day 1 optimisations or just dont care and can live with this. I wouldnt have bought a 9070XT for my sister knowing that this is a possibility in the future as the 5070Ti is its slightly more expensive but at least I can rest easily eith driver updates and such.",pcmasterrace,2025-11-01 07:05:29,1
AMD,nmfu54y,or Intel,pcmasterrace,2025-10-31 21:27:35,1
AMD,nmd7zb4,Optimization is them making the card run better for specific games. The software that installs with it if you choose to install it is called Adrenaline.,pcmasterrace,2025-10-31 13:24:26,4
AMD,nmdsdhl,5700XT Pref is same as 1080TI And 3060 AND EVEN THE 5050 and the b570 which are still enough for modern standards even at 1440p   indiana jones that requires RT can still run Medium settings At 1080p when using RT emulation bypass on linux. can confirm it works as 5600XT owner. would expect at least 2 more years then it would be resonable. but damn thats a shitty move,pcmasterrace,2025-10-31 15:09:54,2
AMD,nme54s2,And then in 5 years Intel will drop the GPU side since their CPUs are frying themselves,pcmasterrace,2025-10-31 16:11:28,1
AMD,nmft49t,"nope, that doesn't really change anything and they're still ambiguous with their language, ""market needs"" wtf does that even mean...   the reality is what matters, and the reality is that they already moved these cards into another branch that won't get the same amount of work and support as before, as long as missing out on new vulkan extensions and versions, and also won't get fsr4 lite or whatever, which was already proven to work completely fine  it's really just amd trying to save face, while saying a whole load of nothing",pcmasterrace,2025-10-31 21:21:40,1
AMD,nmdmzux,"Yep the 6750 XT GRE 10GB released October 18, 2023. I bought the 6950XT on launch day for GPU bubble prices back in May 2022.  Boneheaded decision by AMD.  I bought a 7900 XTX on the 9th of this month. I'd return it, but the closest Nvidia equivalent of what I bought is twice as expensive (a white, RGB 4080 for my wife's all white, all rgb build). It's like AMD knows its positioned to screw its customers over. For now.",pcmasterrace,2025-10-31 14:42:56,87
AMD,nme59q4,"Meanwhile I am playing on my 1070. If I remember correctly, it is over 2 years since release.",pcmasterrace,2025-10-31 16:12:09,9
AMD,nmjeoyp,https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games,pcmasterrace,2025-11-01 14:34:01,1
AMD,nmenibk,6900xt is 5 years old..,pcmasterrace,2025-10-31 17:42:16,0
AMD,nmdh3z9,steam deck won't be affected since the latest Linux amd gpu driver fully supports GPUs as old as 2012,pcmasterrace,2025-10-31 14:12:48,18
AMD,nmdewya,PS5 wont be affected what are you on about?,pcmasterrace,2025-10-31 14:01:32,7
AMD,nmh2s9p,You do realize your gpu will still work right? I dont even update my graphics driver half the time as usually it ends up causing more issues as of late.,pcmasterrace,2025-11-01 02:21:52,1
AMD,nmjfust,https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games,pcmasterrace,2025-11-01 14:40:35,1
AMD,nmdh94o,You still get support just not prioritization.   Nvidia cuts prioriety support day 1 of new arch.,pcmasterrace,2025-10-31 14:13:32,-5
AMD,nmdhwdj,<Points at all the threads where I've told people the 9000 series is a better long term option>  As for the Steam Deck and PS5 - irrelevant. Consoles work on the concept of fixed specifications.,pcmasterrace,2025-10-31 14:16:50,-7
AMD,nmeslf2,I dont understand why youre mad. its for the 5000 and 6000 series. the 7000 series will still recieve support.,pcmasterrace,2025-10-31 18:07:40,-2
AMD,nmg013e,"I feel the same, I own a 7000 series card, but this news leaves a bad taste in my mouth.  If they are doing this to cards that can be considered recent, then the 7000 cards might be on the block sooner rather than later.",pcmasterrace,2025-10-31 22:02:40,4
AMD,nmhvvje,"Vega was a failed product, can't compare it to the likes of rx 6600.",pcmasterrace,2025-11-01 06:36:04,1
AMD,nmjglfz,https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games,pcmasterrace,2025-11-01 14:44:48,1
AMD,nmjgmhe,https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games,pcmasterrace,2025-11-01 14:44:58,1
AMD,nme5ugv,"6700 XT here. Bought back in 2023. It's literally one of AMD's most popular RDNA2 cards and is still relevant for modern 1080p/1440p gaming at 50-60 FPS medium-high settings, especially with the 12 GB VRAM.  They have all the sales data yet they choose to make the worst decision possible.",pcmasterrace,2025-10-31 16:14:59,44
AMD,nmdguis,I bought a new 6750XT 2 years ago and even that feels like a joke.,pcmasterrace,2025-10-31 14:11:27,47
AMD,nmdmuut,T_T,pcmasterrace,2025-10-31 14:42:14,5
AMD,nmffr84,"Litterally same here, the GPU works on Litterally every game on the market right now without issues",pcmasterrace,2025-10-31 20:08:36,1
AMD,nmfjb53,Im in the same boat.,pcmasterrace,2025-10-31 20:27:35,1
AMD,nmeb0oo,I bought a 6800XT >5 years ago. I replaced it this year.  I'm not really too surprised about a 5 year old GPU that is in the grandparent generation of modern cards being put from mainstream to support to extended/security updates only support. It's not like it's dead or being totally unsupported. In a couple of years it'll be very long in the tooth anyway and we'll probably have the next generation of cards available.,pcmasterrace,2025-10-31 16:40:36,-5
AMD,nmdh9k7,"Not only that, but AMD basically killed any chance of you reselling the card and getting your money back if you decided to sell.",pcmasterrace,2025-10-31 14:13:36,64
AMD,nmessbz,6900XT  The 6900XTX was never released.,pcmasterrace,2025-10-31 18:08:40,9
AMD,nmfhfvt,"Water-cooled 6900XT owner here. Yeah, I'm not upgrading for at least another 3 years",pcmasterrace,2025-10-31 20:17:34,2
AMD,nmewnv2,I overpaid for my 6900xt in the second mining craze and paid like $2500cad when it was new. Was gonna run that sucker into the ground but now the ground seems shorter than it should be. Thanks AMD >:(,pcmasterrace,2025-10-31 18:28:43,1
AMD,nmeen0m,Sometimes I wonder are they on drugs?.,pcmasterrace,2025-10-31 16:58:14,19
AMD,nmda2wu,It's funny that nvidia gets most the hate but also provides the best long term support of any hardware manufacturer. But we don't say that here because nvidia is evil.,pcmasterrace,2025-10-31 13:36:03,57
AMD,nmd81ve,"funny thing is that nvidia offers software support for even the 10 series at this point, which is almost 10 years old, while AMD is killing off a 2 generation old product  the fact that AMD only charges like 50 bucks less than nvidia for comparable cards while killing them off in half the time is pretty damn gross",pcmasterrace,2025-10-31 13:24:49,39
AMD,nmendmc,Ahh yes good guy AMD,pcmasterrace,2025-10-31 17:41:37,2
AMD,nmdn27u,AMD is the savior because if it wasn't for AMD and their Vega propaganda we would have never gotten GTX 1080 Ti 11gb or we would have gotten it at like much more expensive price  Because of AMD we have a really cheap really powerful legendary card that is the GTX 1080 Ti,pcmasterrace,2025-10-31 14:43:16,-6
AMD,nmf2e5d,"you're far too optimistic with it  the options are either A: An overpriced brick with a fire hazard connector but with 10 years of driver support  or B: the same fucking GPU but without the fire hazard connector, less software features and 4 years of driver support for 50 to 100 bucks less  remember that with the 90 seres AMD decided to go with the same amount of VRAM as nvidia",pcmasterrace,2025-10-31 18:58:00,7
AMD,nmd3rn9,> better fps/watt than RDNA4   Bruh,pcmasterrace,2025-10-31 13:00:51,0
AMD,nmeyjnc,"I have never fangirled for AMD, maybe you should check out the goomba fallacy",pcmasterrace,2025-10-31 18:38:22,1
AMD,nmdj041,"Great resale value too. Buying 24GB+ Nvidia cards is smart, you can still get £600+ off your next purchase many years later since demand for lots of VRAM shows no sign of abating.",pcmasterrace,2025-10-31 14:22:31,20
AMD,nmd30ap,"The 30 series also won’t get DLSS frame gen nor smooth motion, and game optimizations hardly happen on anything but the newest gen. Really, everyone is crying.",pcmasterrace,2025-10-31 12:56:32,-24
AMD,nmd61f1,"RDNA 1 released in 2019, 6 years   RDNA 2 released in 2020, 5 years",pcmasterrace,2025-10-31 13:13:39,8
AMD,nmdz6iw,"Afaik 9070 xt doesn't come close to competing with the high-end and is instead a VERY good competitor to the current generations mid-tier with the 5070 ti from Nvidia.    I'd say it's mostly because the high-end doesn't sell as much and it's more the profit from such inflated prices that makes the 5080 and 5090 viable for Nvidia whilst it's been consistently shown, with some outliers, that mid-tier cards sell the most.",pcmasterrace,2025-10-31 15:42:48,3
AMD,nmetwts,"And they are a bad value on productivity. To the point pudgetbenchmarks doesn't even use them to compare. And on 3D...worst... I am surprised, my A770, the first Gen of Intel dGPU, with hardware limitations, is faster than a 9060XT on blender benchmark.  Mind you, both are slower than a 4050M.",pcmasterrace,2025-10-31 18:14:27,2
AMD,nmds20y,Yeah that's what I was thinking as well. Steam Deck seemed like a better deal from the begging but now it sounds like it's even better in comparison to the ally,pcmasterrace,2025-10-31 15:08:19,2
AMD,nmglnre,You can put steam is on the Xbox rog ally...hell my rog ally Z1E has bazzite on it,pcmasterrace,2025-11-01 00:24:19,1
AMD,nmetyrp,Did they make a statement? The driver patch notes still imply that only 7000 and 9000 series will receive new game ready driver support  https://preview.redd.it/ltmko8usnhyf1.jpeg?width=892&format=pjpg&auto=webp&s=fa85224a1bcdbcbd2c9e4d32178d864cb0300fb1,pcmasterrace,2025-10-31 18:14:44,5
AMD,nmdjvx2,"bro stop it, sounding like an amd fanboy over here. its bullshit to drop support for 5 year cards.",pcmasterrace,2025-10-31 14:27:02,18
AMD,nmddck0,"what do you mean with ""no difference"" if we won't be getting game optimisations anymore ? lol  You're simply contradicting yourself.",pcmasterrace,2025-10-31 13:53:26,46
AMD,nmfnvj3,That’s what I believe too,pcmasterrace,2025-10-31 20:52:08,3
AMD,nmfrdjj,doesn't change anything really,pcmasterrace,2025-10-31 21:11:40,2
AMD,nmepg6z,"That doesn't mean shit, fuck does market needs mean?",pcmasterrace,2025-10-31 17:51:46,1
AMD,nmi78ix,They'll stop optimizing drivers. This is f\*\*\*ed. I paid good money for that 6900xt,pcmasterrace,2025-11-01 08:44:15,1
AMD,nmfrhsg,to me that only says that the decision was made and nothing is gonna change,pcmasterrace,2025-10-31 21:12:21,1
AMD,nmfrsq6,"yeah, amd really spreads lots of nonsense",pcmasterrace,2025-10-31 21:14:04,-1
AMD,nmfyxm1,"Yeah … but the 6800 is also two tiers (or 18% - https://www.videocardbenchmark.net/compare/4322vs4314/Radeon-RX-6900-XT-vs-Radeon-RX-6800) Slower than the 6900XT.  And the time of purchase also plays a role. I bought the 6900XT during the Cryptocrisis for 1300€, when it went for 1500€ usually, assuming rising prices. When I checked, the 6800XT (didn't check for non XT) was 1100-1200€ street prise. Scalpers drove the prices crazy high. If you got your 6800 XT in late 23, when the craze was over and the 7000 Series was released, it obviously was way cheaper.",pcmasterrace,2025-10-31 21:55:53,1
AMD,nmepyxg,"This makes sense. I've been perplexed by this whole thing since early yesterday. AMD literally has RDNA2 products on shelves still, they can't possibly be moving it to a state where it won't get game optimizations already. They'd probably end up getting hit with a class action from people actively buying hand helds lol",pcmasterrace,2025-10-31 17:54:21,5
AMD,nmijwzk,"It's one hell of a card, can play almost everything on ultra 1440p without framegen/upscaling above 90-120fps. My plan when i bought it was to use it until it breaks actually or wont reach ~60fps at low/medium anymore. Powerdraw is kinda high with 330-550w. Sometimes i run undervolt setting because of temps rising above 90 hotspot especially in bf6.",pcmasterrace,2025-11-01 10:56:58,2
AMD,nmdgyw6,"Don't give them ideas.  The 7xxx series is still in production so it won't be dropped, yet.",pcmasterrace,2025-10-31 14:12:04,-1
AMD,nmfccd9,"It means when they blackmail enough people to putting them back at at least 10% market share, they'll start to become consumer friendly again.",pcmasterrace,2025-10-31 19:50:32,1
AMD,nmfs2zz,"this literally doesn't change anything tho, these cards are still gonna be moved into another maintenance branch that won't ever get the same amount of work or support as before, and also won't get fsr4 lite of whatever that was already proven to work just fine. rage still 101% deserved",pcmasterrace,2025-10-31 21:15:43,-2
AMD,nmdesx6,But AMD has all the good side of their CPU market. Still releasing AM4 CPUs that kick ass at affordable prices.,pcmasterrace,2025-10-31 14:00:57,7
AMD,nmd2xu8,"And you must change because of what? Vega 64 is not getting updates for past 2 years and it runs modern games just fine, it's still as fast as GTX 1080 despite GTX 1080 ""getting"" updates.",pcmasterrace,2025-10-31 12:56:09,9
AMD,nmf4dbc,Me and my money will move on.  AMD dropped the new article only because of the backlash.,pcmasterrace,2025-10-31 19:08:14,2
AMD,nmfsiun,"not misinformation, nothing really changed",pcmasterrace,2025-10-31 21:18:15,1
AMD,nmin50d,"might do that, the plan was for a 9070xt ...but not anymore lol",pcmasterrace,2025-11-01 11:26:40,1
AMD,nmdtgyb,Maybe we have different standards is probably why. I am going to retire this card once I get my 5080 at the end of this year and upgrade my main PC. And move over my 3080 over to my office build. But once I see a gpu cant run games at 1080p high Imo it’s time to let go. I play BF6 at 1440p on the 3080 still 100 FPS+ high but this 5700xt all settings low at 1080p and barely getting 100fps+ with FSR quality on. I prioritize high fps. Yeah that’s why I said it’s time to let go for newer games. I think what’s confusing people is its support for newer games. Older games that ran fine will still run good. But I’m not playing games at low settings just me,pcmasterrace,2025-10-31 15:15:21,1
AMD,nmfvicq,"You're in denial, great.  If you have to quote the article, quote it completely.  >""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch""  Let me break it down to you, barney style:  >New features, bug fixes and game optimizations will continue to be delivered  Those are the keywords.  But suuuure, you clearly know better than AMD themselves.",pcmasterrace,2025-10-31 21:35:35,0
AMD,nmdyhb2,"Unless the NVIDA decided to follow the steps of AMD to EOS the GPU of 3 years or oldest to maximum their profits while hurting the loyal customers. But that's the future I hope not to come.  AMD is cheapest but most likely to EOS after 2-4 years.  NVIDA is expensive but likely to still support the GPU close to 10 years. Even RTX 2000 series and some oldest gpu are still being supported.   Take your posion, Ig",pcmasterrace,2025-10-31 15:39:28,20
AMD,nmeogsq,Arc B770 if it ever happens.,pcmasterrace,2025-10-31 17:46:56,1
AMD,nmg8j4l,AMD always seems to fumble the bag somehow,pcmasterrace,2025-10-31 22:57:38,1
AMD,nme63m6,"GTX 1000 series support will end this month (October). But keeping security updates each quarterly. Now Nvidia is basically only optimizing drivers for RTX 2060 and above, which makes sense for games that have DLSS and ray tracing.",pcmasterrace,2025-10-31 16:16:14,1
AMD,nmer65v,RX 6750 XT and RX 6650XT were released just 3 years ago.,pcmasterrace,2025-10-31 18:00:23,6
AMD,nme5pzb,Why are you making shit up?,pcmasterrace,2025-10-31 16:14:22,2
AMD,nmdijrz,"Stream Deck is not a console. I’m not sure if they are affected or not but they are rdna 2 so I just said maybe they would be. Also, the 9000 series did not exist when I bought my card.",pcmasterrace,2025-10-31 14:20:12,0
AMD,nmeufcg,First they came for the 5000 and 6000 series and I did not speak out for I did not have a 5000 or 6000 series. Then they came for me…,pcmasterrace,2025-10-31 18:17:07,4
AMD,nmgiois,But going by this I can expect them to stop supporting the 7000 series sometime in the next 2 years  I really need to put bazzite on my main computer,pcmasterrace,2025-11-01 00:04:12,1
AMD,nmjtk96,It's AMD trying to save face cause they know they fucked up.  They've shown where they stand.,pcmasterrace,2025-11-01 15:54:21,1
AMD,nmjgoop,https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games,pcmasterrace,2025-11-01 14:45:19,1
AMD,nme8sgr,"I bought an open-box 6750xt for my wife 2 years ago on a great deal and even I feel slighted. I could understand them announcing that they're sunsetting the 5000 series in 6 months or something like that, but kneecapping the 6000 series is just bonkers.",pcmasterrace,2025-10-31 16:29:36,14
AMD,nmef2u6,"The 6650, 6750 and 6950 were released in 2023",pcmasterrace,2025-10-31 17:00:21,1
AMD,nmdnjco,"Didn’t even think of that, good point.",pcmasterrace,2025-10-31 14:45:39,26
AMD,nmfu5pf,To be fair RDNA 2 is 5 years old at this point. I wouldn't expect much in retail value the time to sell them was after RDNA 3 launch. Now its 2 Gen's old I don't know many people in the used market that would buy a used one in 2026+. You are already better off buying something from the current generation.,pcmasterrace,2025-10-31 21:27:41,1
AMD,nmew9dj,There is a binned XTXH which is what I have it came in red devil and handful of others top end skus. Forgot about the H lol   Nobody was paying the steep price of red devils most expensive variant so it was the only 6900xt model to buy at the time lmao,pcmasterrace,2025-10-31 18:26:39,2
AMD,nmeeobp,Their pricing just sucks. That’s the only issue I have with them. their support is great.,pcmasterrace,2025-10-31 16:58:25,18
AMD,nmee06e,Hell even my 1080 got a battlefield 6 driver!,pcmasterrace,2025-10-31 16:55:09,4
AMD,nmddf7e,"Well, they also manufacture product shortages because they favor commercial business over the consumers who basically made them in the early days. 🤷🏻",pcmasterrace,2025-10-31 13:53:49,-14
AMD,nmd952g,"They *just now* killed 10 series support. But it's been 8 years, not 4.",pcmasterrace,2025-10-31 13:30:51,25
AMD,nmfj2cl,Or you can get a RX 9070 XT WITH 12PWR connector because you love chaos.,pcmasterrace,2025-10-31 20:26:16,1
AMD,nmd3gct,>The 30 series also won’t get DLSS frame gen nor smooth motion    They do have DLSS 4 transformer upscaling that runs well  >game optimizations hardly happen on anything but the newest gen   Based on?,pcmasterrace,2025-10-31 12:59:05,50
AMD,nmdeald,"It does have AMD frame generation, though and does get access to the transformer model of DLSS upscaling.",pcmasterrace,2025-10-31 13:58:18,3
AMD,nmd3ntn,"But they *will* get game support   Which one is more important? Fake frames, or the game straight up buggy? Yeah, that's what I thought",pcmasterrace,2025-10-31 13:00:16,1
AMD,nmdhrvc,"Having a better upscaler is better than fake frames, plus if he wants frame gen, he can buy lossless scalling, which only costs  $7 to get frame gen.",pcmasterrace,2025-10-31 14:16:11,2
AMD,nmdu1ia,"Tbh, dlss frame gen is a fat piece of shit",pcmasterrace,2025-10-31 15:18:07,1
AMD,nme0m25,"If you overclock the shit out of it and cite very specific benchmarks the 9070xt trades punches with the 5080 on raster situationally.  It's a whole thing people point to when talking up the 9070xt.  Mind you I'm not complaining that the card can do that, I think it's pretty damn neat, but it's just an interesting fact more than a compelling reason to recommend the card.  What a GPU can do 1% of the time isnt as relevant as what it does on a daily basis.",pcmasterrace,2025-10-31 15:49:40,3
AMD,nmfiopt,https://preview.redd.it/lt3s4eivaiyf1.jpeg?width=1320&format=pjpg&auto=webp&s=c0c0d3da3ad6f7930a3ad0bdf523c72084beb753  They will continue to be updated but not receive new features,pcmasterrace,2025-10-31 20:24:16,1
AMD,nmehm6i,"The 6x50 cards were released in May of 2022, they're 3 year old cards. The 6x50 GRE cards were released in Oct of 2023.   2 fucking years old and getting full support neutered is insane work by AMD.",pcmasterrace,2025-10-31 17:12:48,2
AMD,nme1r7l,"Support HASNT been dropped. Get this into your thick skull. Dropped support means no updates, that is NOT happening here.",pcmasterrace,2025-10-31 15:55:05,-5
AMD,nmdgmfe,You get game optimizations for overall driver stuff they just wont bother with hard to fix rdna 2 specific bugs. Basically and is killing fine wine on rdna 1 and 2.   Nvidia does this day 1 on new gen.  So bf6 has some minor graphic issues on 5000 series nvidia nvidia will fix this if 6000 series was out they wont because its low priority but if it affected all nvidia cards or 5 and 6000 series it gets fixed.,pcmasterrace,2025-10-31 14:10:18,-12
AMD,nmjs59m,"THE ENTIRE ARTICLE STATED THAT THEY WILL NOT STOP DRIVERS, JUST STOP ADDING NEW FEATURE SUPPORT",pcmasterrace,2025-11-01 15:47:04,1
AMD,nmfzgd2,"Yes, of course I understand. It was a completely different time period and market back then.",pcmasterrace,2025-10-31 21:59:04,1
AMD,nmdxj2v,they still selling rdna2 gpus today   they launched new APU line with rnda2 this year   the xbox ally base with the Z2A chip is rdna2... being in production doesnt mean anything apparently,pcmasterrace,2025-10-31 15:34:54,6
AMD,nmfw7cv,"there is no point having this conversation until everyone understands.  ""Maintenance mode implies ""No new features"" not ""No support""",pcmasterrace,2025-10-31 21:39:40,3
AMD,nmi2xeb,"The fear mongering is incredible, minor updates are still updates, and as you said yes, many don't even update thier drivers unless they let Windows update do it.",pcmasterrace,2025-11-01 07:55:42,1
AMD,nmdtz4m,True. Good point,pcmasterrace,2025-10-31 15:17:48,1
AMD,nmfxuwf,"they already said they won't be updating the vulkan driver or giving out any new vulkan extension, learn what happened to Polaris with vulkan games or emulators like baldur's gate 3 that constantly crashes, or rpcs3 that just updated their minimum requirement implying it's no longer supported on polaris. does that mean fsr 4 is coming then? :p",pcmasterrace,2025-10-31 21:49:25,1
AMD,nmeu4p3,"The problem is the Radeon option isn't *that* much more expensive than the comparable Nvidia option in most cases.  For the 9070XT you can find the 5070TI within $100 or less of it usually, the biggest difference is on the low end with the 9060XT 16 $80 cheaper than the 5060ti 16.  Idk, if AMD was consistently 30-40% cheaper it might make sense to go with it.  But as things stand you might as well just pay the extra Nvidia tax of $100 or whatever so you know you have long term driver support.  Plus even though FSR4 is improved a lot DLSS is still better and has way wider game support.",pcmasterrace,2025-10-31 18:15:35,3
AMD,nmgzr74,>Unless the NVIDA decided to follow the steps of AMD to EOS the GPU of 3 years or oldest to maximum their profits while hurting the loyal customers.  Nvidia kind of doesn't have to if only having 8GB of vram motivates people to upgrade on their own.,pcmasterrace,2025-11-01 02:00:32,1
AMD,nmdt4sl,It's not effected because Linux is not effected by this change at all. Amd drivers on linux are open source and are not only developed by amd. Valve and many other companies contribute as well. On linux even Amd GCN cards (from around 2012 iirc) still receive relatively regular updates,pcmasterrace,2025-10-31 15:13:41,5
AMD,nmdj5sf,"That can be argued. It's irrefutably a console like experience. All the examples of a model are the same, like a PS5. Unlike a ""PC"" where there's millions of permutations.  I guess you meant Steam Deck. Stream Deck is a multi-button control surface. 😉",pcmasterrace,2025-10-31 14:23:19,0
AMD,nmeg1dx,"The 6750XT was released Mar 3rd, 2022.  The 6800XT was released November 2020.",pcmasterrace,2025-10-31 17:05:02,0
AMD,nmeih0o,For a 3-5 year old GPU? No lol.  A 30 or even 20 series Nvidia card still gets support and just became a much better 2nd hand option,pcmasterrace,2025-10-31 17:17:01,2
AMD,nmehzho,6000 series were still being sold brand new 2 years ago. It's not old nor that used.,pcmasterrace,2025-10-31 17:14:37,2
AMD,nmfplt3,At least they don't lie about msrp. $600 9070xt still ultra rare,pcmasterrace,2025-10-31 21:01:34,2
AMD,nmleoym,"Pricing is incredibly fair and generous for what you get; the only reason people complain about it is because they compare it to other competitors that are only as cheap as they are because they lack the features Nvidia has, and are always one step behind.",pcmasterrace,2025-11-01 20:51:19,0
AMD,nmdgp3z,"All 5000 series cards hit MSRP before the AMD 9000 series did. The 5070/5070ti were below MSRP in many countries. Also, Im yet to see the 9070xt hit below MSRP",pcmasterrace,2025-10-31 14:10:41,18
AMD,nmdhjgg,AMD has been FAR worse when it comes to product availability and princing (relative to MSRP) than nvidia in most of the world with the 9000 series,pcmasterrace,2025-10-31 14:14:59,7
AMD,nmdeqb5,Who does that?,pcmasterrace,2025-10-31 14:00:35,1
AMD,nmdajib,"And it makes sense too. Since the 1000 series pretty much disappeared from system requirements of new releases for the most part. RX 6000 still has a good few years with how performant it is - what's killing it is not the hardware aging, but AMD themselves.",pcmasterrace,2025-10-31 13:38:34,24
AMD,nmdadgr,"I like how he mentions frame gen when it's the *least important* DLSS/FSR/XeSS feature of any of them, atleast for desktop cards.  >Based on?  Based on what he just pulled from his ass.",pcmasterrace,2025-10-31 13:37:42,32
AMD,nmdj50i,"Yeah but if you just make things up you see, Nvidia bad.",pcmasterrace,2025-10-31 14:23:13,4
AMD,nmdfv07,Nvidia stops optimizing for past gen day 1 of new architecture people used to attribute half of that to and fine when when you compare amd and nvidia gens 2 years after release but if you look similar nvidia performing cards fall down because they only do generic driver fixes for old gen.   Basically amd is stopping fine wine on older cards.,pcmasterrace,2025-10-31 14:06:23,-3
AMD,nmd447p,"> But they do have DLSS 4 transformer model-  DLSS 4 depends on the game implementing it. Going by outer worlds 2, fsr4 works the same way.  > Based on?  Past experiences and release notes.",pcmasterrace,2025-10-31 13:02:51,-17
AMD,nmd63av,"First, we don’t know how often they will be releasing drivers. Second, most people sat on 566.36 for months when Nvidia was releasing dogshit drivers.   Point being, not getting day 1 driver updates and optimizations  won’t suddenly make the gpu unusable in games.  Again, not justifying AMD, just saying this isn’t much better.",pcmasterrace,2025-10-31 13:13:56,-6
AMD,nme4jcr,"Idk what you're going on about, but DLSS frame gen is absolutely fantastic in single player games and is easily the best implementation of frame gen currently available",pcmasterrace,2025-10-31 16:08:34,0
AMD,nme18dj,"I'm not really talking about that though. I mean sure overclockers like to push GPUs to the brink, doesn't mean it's stable in games just in benchmarks.    It's a mid-tier card at an attractive price point and amazing performance.",pcmasterrace,2025-10-31 15:52:36,1
AMD,nmfnwjz,Honestly this seems like a reaction to the pushback and conflicts with previous statements.   Anyways it's too early for the 6000 series to stop receiving both features and game optimization updates,pcmasterrace,2025-10-31 20:52:18,2
AMD,nmdicw4,"Yeah, that means AMD may never bother fixing these specific bugs for 6000 and 5000 cards at all because they don't have to! which doesn't make sense because these GPUs are still being sold in the market in new conditions, and they are relatively young of age.",pcmasterrace,2025-10-31 14:19:13,18
AMD,nmfxg0d,"we understand, still doesn't make it any better, it's like the minimum expectation",pcmasterrace,2025-10-31 21:46:56,-1
AMD,nmh02gu,It's crazy how opposite it is compared to AMD versus Intel when it comes to CPUs. Specifically how long AMD keeps their sockets relevant.,pcmasterrace,2025-11-01 02:02:43,5
AMD,nmfh61b,"So a solution here, is to install Linux if you've got one of these cards",pcmasterrace,2025-10-31 20:16:06,2
AMD,nmdo5ri,Steam Deck is closer to a laptop than a console. It does all the computer shit and runs Steam instead of a console store. You can run anything on it that will run on Linux including emulators and stuff from epic and GOG.,pcmasterrace,2025-10-31 14:48:48,1
AMD,nmemz56,I'm surprised reddit didn't think of a reason why 20 series getting DLSS 4 so late in its life was a bad thing.,pcmasterrace,2025-10-31 17:39:35,1
AMD,nmfpqaj,That’s true. Nvidia fucks you but at least they tell the truth.,pcmasterrace,2025-10-31 21:02:16,2
AMD,nmgjsfx,Hell I paid 670 for a XFX AMD Radeon RX 9070 Swift like a dumbass a couple of weeks after release for a computer I use maybe once a month,pcmasterrace,2025-11-01 00:11:38,1
AMD,nmlg1jp,"calling literally any aspect of the current GPU market ""fair and generous"" is genuinely insane  nvidia's pricing is in line with their competitors when taking into account the extra features that you get, but that's because both players in the market have dogshit pricing",pcmasterrace,2025-11-01 20:58:28,2
AMD,nmlewgg,Nvidia cards aren’t worth the cost. They’re grossly overpriced.,pcmasterrace,2025-11-01 20:52:25,0
AMD,nmejcnr,That's country dependant though. Here in Germany the 9070xt has been below msrp for months now while the 5070ti is around msrp,pcmasterrace,2025-10-31 17:21:23,3
AMD,nmetb9v,Nvidia. They prioritize commercial products.  Edit: anybody who doesn't believe it should look into Nvidia VRAM and how they allocate it between different product lines. Just saying. Seems like a lot of people out here Stan'ing for Nvidia.,pcmasterrace,2025-10-31 18:11:22,-1
AMD,nmdl1nn,"The 6000 series is some of AMD's best work in the graphics department, killing RDNA 2 support is completely and utterly retarded. To this day, it's AMDs only modern ""full stack"" lineup (meaning bottom of the barrel sub 75W cards to 90 class cards)  and they still don't have a replacement for lots of their lower tier cards. This was an efficient, and effective gaming architecture that was reasonably priced and didn't have completely dogshit drivers like 5000 and 7000. This is a sad day for 6000 series users.",pcmasterrace,2025-10-31 14:32:58,12
AMD,nmdbdhf,"Side note, but I think FG will soon be seen as essential as upscaling       CPU progression has slowed to a crawl, but games want more  CPU power   You see in DF coverage capping the framerate to keep the CPU from maxing has huge consistency wins   FG is going to be pretty much the only way to access consistent high framerates going forward",pcmasterrace,2025-10-31 13:43:03,-2
AMD,nmdgghi,">Nvidia stops optimizing for past gen day 1 of new architecture   Again, based on?   >similar nvidia performing cards fall down    Different games have different architecture preferences",pcmasterrace,2025-10-31 14:09:27,6
AMD,nmd4ew3,">DLSS 4 depends on the game implementing it.   You can globally override it in the nvidia driver, doesn't need game implementation      >fsr4 works the same way.     There isn't an official way to run FSR 4 on RDNA 2. You have to use the opticaler workaround, and even then it runs slow   >Past experiences and release notes.    So assuming Nvidia might be doing what AMD are telling you they're doing, not really equivalent",pcmasterrace,2025-10-31 13:04:33,14
AMD,nmd6kyl,">Point being, not getting day 1 driver updates and optimizations won’t suddenly make the gpu unusable in games.    Which is not what anyone is saying (It could happen for new releases, but it's extremely unlikely to)    Worst case. Nvidia give you the same service level AMD will. But it'll likely be better    >Again, not justifying AMD,   You're downplaying the situation, and trying to act like Nvidia do the same thing as if that's a defence",pcmasterrace,2025-10-31 13:16:40,5
AMD,nme1jlk,"I understand that.  I said they don't compete in the high end unless you count cherry picked benchmarks, you replied saying as far as you know it's not competing in the high end, so I expanded on what I meant by mentioning it in the first place.",pcmasterrace,2025-10-31 15:54:05,-2
AMD,nmg00lz,you just quoted 2 different texts saying new features will continue to be delivered,pcmasterrace,2025-10-31 22:02:35,0
AMD,nmholqx,It's like AMD cpu and gpu divisions are 2 very different companies.,pcmasterrace,2025-11-01 05:18:19,3
AMD,nmhy5yc,Nova Lake should have multigenerational sockets while being able to compete with AMD hopefully. Then back to status quo in the 2010s I guess for CPUs.,pcmasterrace,2025-11-01 07:01:42,2
AMD,nmfjt9g,If you want game ready updates then yes,pcmasterrace,2025-10-31 20:30:16,1
AMD,nme6m2d,"So what's a PS5? It's got an APU with Zen 2 cores and RDNA2 graphics, so surely it's just a Desktop PC that hasn't been cracked yet? 😂  Which would make the Xbox 360 a PowerMac G5 in disguise.",pcmasterrace,2025-10-31 16:18:46,1
AMD,nmfpycm,5070 ≠ 4090 though so don't give them too much credit,pcmasterrace,2025-10-31 21:03:32,3
AMD,nmligcj,"GPUs aren't just the equivalent of CPUs. They are also the motherboard and ram as well.   I don't think you should be paying thousands for a 5090, but that option only exists because people still want it; you don't need to buy the flagship models. The 5060 alone can play pretty much any game comfortably, especially with DLSS and its features, for ~$300. For VRAM you could get the 16GB Ti model for ~$400. You could also get the 5050 if price is really a problem, but that is more controversial.   This is also ignoring the fact that the super models still are yet to arrive, which often provide the same price for more performance. I used to think the models were overpriced, but you are literally paying for the new features that are the key points. Frame gen is amazing and so is DLSS.",pcmasterrace,2025-11-01 21:11:29,1
AMD,nmlgmyl,"I used to think that but they really are worth it. You pay a bit more for vastly better and more features that make a significant impact. I would pay ~$35-$45 more for DLSS alone over FSR 4, let alone everything else. People just compare only the raw performance to competitors who are always a few steps behind so they don't have to be the ones doing the actual innovation Nvidia does, and they still lack the features.",pcmasterrace,2025-11-01 21:01:40,0
AMD,nmlfsc1,prioritizing commercial products is not the same thing as manufacturing scarcity,pcmasterrace,2025-11-01 20:57:05,-1
AMD,nmdd343,"Frame gen really need get dual card support to remove the overhead like how it's done in LSFG. Lower latency delay and sell more cards for Nvidia, win win.",pcmasterrace,2025-10-31 13:52:05,0
AMD,nmdh4jd,Based on performance and bugs in games.,pcmasterrace,2025-10-31 14:12:53,-4
AMD,nmdfe8r,"the guy he is replying to is literally claiming games will be ""buggy"" because amd stops ""game support""",pcmasterrace,2025-10-31 14:04:00,2
AMD,nmhy1tc,Both are bad really when compsred to the gains frok ARM. Hope Nova Lake and Celestial come sooner for AMD to understand that it should earn our money rather than sell at Nvidia - 50.,pcmasterrace,2025-11-01 07:00:25,2
AMD,nmlhe3f,“A bit more” and it’s 20-30%,pcmasterrace,2025-11-01 21:05:43,2
AMD,nmmg4sm,You really think 2gb of vram is worth $300+?,pcmasterrace,2025-11-02 00:24:19,0
AMD,nmddf5t,"Dual GPU FG would go the same way as SLI, almost nowhere",pcmasterrace,2025-10-31 13:53:49,7
AMD,nmdhqnt,So assuming Nvidia do what AMD are explicitly telling you they do,pcmasterrace,2025-10-31 14:16:01,8
AMD,nmdgnuk,"There will likely be cases of that   Both are arguing the extreme. The bulk of games will work fine. But driver optimisation does make a difference, just not all the time",pcmasterrace,2025-10-31 14:10:30,2
AMD,nmhz35m,>Both are bad really when compsred to the gains frok ARM.   What do you mean by this?,pcmasterrace,2025-11-01 07:12:07,2
AMD,nmliqib,"In some instances like the 9070 and 5070, but in most instances it trails for far less. 9070s have also not been at MSRP for months and I am still not sure if they are.",pcmasterrace,2025-11-01 21:13:03,0
AMD,nmmghkz,which 2 models that nvidia sells only have a 2gb VRAM difference and are sold at a 300 dollar price difference?,pcmasterrace,2025-11-02 00:26:29,0
AMD,nmdee3o,"SLI had terrible support, required identical cards and offered no real gaming experience improvements outside of 3 benchmarks.  When we get to the point when FG becomes essential and the ""standard"", having a 60 series card doing FG for a 80 series card for a substantial gain and reduction in input delay will become a lot more appealing to people.",pcmasterrace,2025-10-31 13:58:48,1
AMD,nmhzl64,Apple M5 had multi generational gains for both CPUs and GPUs while releasing on a yearly cadence. While we have the same Nvidia and AMD releases with barely over 30% gains over 2.5 years for both CPU and GPU while charging an obsence amount of money for the midrange parts and boosting their high end offerings. Idk about you but I think its really absurd when the higher end offerings for consumer parts are better price/performance than midrange parts because of all the AI BS and that the midrange parts are usually just cut down or binned high end chips. Idk im just sick and tired of all these antics as they continue to please shareholders while affording to alienate us as we need them more than they need us due to games needing more compute for the same visual fidelity since 5 years ago.,pcmasterrace,2025-11-01 07:17:50,2
AMD,nmmlzw6,"RTX 3080 10 GB launched at $699 and the 3080 Ti 12 GB at $1,199. The Ti was only about 14-20% faster but was 71% more expensive. That price gap is mostly market segmentation and demand, not memory cost. But I'm sure you're going to tell me the higher bin accounts for that as well... 🤡",pcmasterrace,2025-11-02 00:58:38,0
AMD,nmdf0y1,>reduction in input delay will become a lot more appealing to people.   I doubt it will be enough to warranty and entire second GPU,pcmasterrace,2025-10-31 14:02:07,4
AMD,nmtombu,"i can build that out of spare parts from FB Market around here for 500$ or less.    There's a 5700X, MoBo and 32GB of RAM for sale near me for 200$ right now.    NO way this is 1000$  Edit:  It occurs to me that this is your existing rig, and you're contemplating a new build?  In that case... nah.    The 5700X is still a perfectly OK CPU for gaming.  Its not going to seriously bottleneck any GPU you'd want to pair with it anyway.    Maybe just get a new GPU, additional SSD storage.    You can grab a 5070 for ~500$ or even less right now (they are going on sale below 500$ since they cant move them) or a 9060 XT (16GB) for 400-ish.    Both would be significantly better than your current GPU.",pcmasterrace,2025-11-03 03:48:02,1
AMD,nmtr08q,"This seems pretty okay. Get more storage maybe, but I run VERY similar specs, including same GPU, and I've been gaming just fine. Considering the budget, you *could* also get a new GPU, but as I said, (assuming you're using this for gaming) this should be able to run anything you throw at it at medium settings at LEAST. If what you're trying to play isn't holding up and you want higher settings, then probably just go for a better GPU, the CPU should be just fine.   But no, absolutely don't go for another desktop, lol.",pcmasterrace,2025-11-03 04:04:21,1
AMD,nmtr58n,"Anything newer than a 5070/ti would be a bit of a miss match..   5700x is a cooler running version of the 5800x but is also faster/has more cores and still can run cooler than a 5600x... no need to upgrade yet*  For $1000 you could go am5 easy,  B650/b670/7500f/7600x/9600x/32gb cl30 6000/1tb ssd m.2/800w psu(or better) re use your case and maybe cpu cooler too saving 1-200 there :)",pcmasterrace,2025-11-03 04:05:19,1
AMD,nmtvhk6,"Yes, I really think I'm playing well. I run Assetto Corsa with Quest 3 at 60 fps (multiplayer), which is a very interesting parameter for me. In this case, I just wanted to improve my VR quality and play some games with slightly higher quality, but I had no idea if the setup itself was still okay to use. Also considering that Black Friday is coming up.",pcmasterrace,2025-11-03 04:37:17,1
AMD,nmtw0u1,Do you have any ideas for GPUs?,pcmasterrace,2025-11-03 04:41:22,1
AMD,nmsg8pf,"Well that sounds good, why did they not say that in the first place.  Some confusing sentences in driver release notes will just cause confusion, as if, something is trying to be hid",pcmasterrace,2025-11-02 23:18:56,32
AMD,nmsp0vi,"Ah thats good.  When i first read the release, I was thinking of selling my backup 6700 core that I keep if something happens to mine.  (I bought it when something happen to my first)",pcmasterrace,2025-11-03 00:07:30,7
AMD,nmsugu5,Good. Good,pcmasterrace,2025-11-03 00:39:15,3
AMD,nmtj6yj,"Just more PR talk, the problem remains the same and they just worded it nicer to sound less bad.  RDNA2 is still being moved into maintenance mode and I doubt it will receive the same kind of support as RDNA3 and 4.",pcmasterrace,2025-11-03 03:11:20,2
AMD,nmw0sf3,"Can everyone stop panicking now? Nvidia users are having to accept the 10 series is losing support, I guess AMD users thought they were immune to it lol",pcmasterrace,2025-11-03 15:03:05,1
AMD,nmu6egd,"It is not, I have a Radeon Sdr agp and no game ready drivers, so the every Radeon is misleading.",pcmasterrace,2025-11-03 06:11:31,-2
AMD,nmsig74,I would assume miscommunication from the marketing division.,pcmasterrace,2025-11-02 23:31:05,12
AMD,nmtctx0,"They are just backtracking, and spinning it to save their pr! Clear as day.",pcmasterrace,2025-11-03 02:30:33,2
AMD,nmtzjgl,"This is most likely due to fsr redstone. Which makes perfect sense, those cards are physically incapable of running the new feature set at acceptable quality. The way I see it, it's just more transparent.",pcmasterrace,2025-11-03 05:09:41,-1
AMD,nmu7iml,In this article I believe they are referring to the Radeon architecture not the GPUs with Radeon in their name. RDNA stands for Radeon DNA so they mean GPUs using RDNA 1-4.,pcmasterrace,2025-11-03 06:21:53,3
AMD,nmsxaq1,"Literally living to their acronym's other meaning.   That being ""Advanced Marketing Department"", but not in a good way. Lol",pcmasterrace,2025-11-03 00:56:16,17
AMD,nmv1ikv,"its literly what happens with every company, and most outlets use that as ""clickbait"" ragebait"" blah blah frankly im tired of ppl doing knee jerk reactions when things like this happens.",pcmasterrace,2025-11-03 11:23:21,3
AMD,nmvnb95,"I don't even see why they would stop supporting the 6000 series though? It's not old, I would just guess this is AMD being AMD with terrible messaging",pcmasterrace,2025-11-03 13:51:26,2
AMD,nmu0yxj,Transparency would be saying OK guys we couldn't make it work at acceptable quality and performance. You're welcome to try it unofficially but no FSR4 for RDNA2   They did NOT do that,pcmasterrace,2025-11-03 05:21:52,3
AMD,nmujlpt,"GTX10 series can't run dlss and all the new feature RTX bring, but Nvidia kept it on single/unified drivers until recently, beside from programming pov, won't it be easier and more efficient to use unified branch rather than separating them if the goal was the same, bringing game optimization for all Radeon products?",pcmasterrace,2025-11-03 08:23:16,1
AMD,nmu3f04,They never said FSR 4 was gonna be on RDNA 3 or 2.,pcmasterrace,2025-11-03 05:43:43,4
AMD,nmu22ag,They're splitting it into 2 tiers. They haven't said jack about fsr yet because they're not ready to unveil redstone. That's pretty clear to me at least.,pcmasterrace,2025-11-03 05:31:30,0
AMD,nmvei56,"Dunno how much they plan on changing but it may end up harder to maintain. Nvidia feature sets have been easy to differentiate between generations since they've been drip fed. But you end up in situations where dlss, rr, fg, and now multi fg not working on previous gen cards.   We won't know if this makes perfect sense till we see redstone.",pcmasterrace,2025-11-03 12:58:31,1
AMD,nmuurnd,Read the post I was replying to,pcmasterrace,2025-11-03 10:20:17,1
AMD,nm88nfc,I was looking at 6700's just recently for a new build. Does this mean it would be a bad idea?,pcmasterrace,2025-10-30 17:40:55,284
AMD,nm8as6l,Can anyone explain like I’m 5  Why did they remove the USB-C functionality and what are the ramifications of this? I’ve never used my 7900XTX’s USB-C port.,pcmasterrace,2025-10-30 17:51:09,650
AMD,nm8hvqp,And I'm still with my RX 6600...,pcmasterrace,2025-10-30 18:24:30,152
AMD,nm8m4o4,Lol at me for buying a 1000€ 6900xt when it came out. No new game support after such a short time. Ridiculous,pcmasterrace,2025-10-30 18:44:24,141
AMD,nm8m2q7,"This makes 0 sense and is such a trash move by AMD as 6800XT and 6900XT are still very capable cards and beloved by the community. These companies and their idiotic decisions in which they think will generate more profit can easily backfire, hope it does.",pcmasterrace,2025-10-30 18:44:09,459
AMD,nm8i0bk,"What the fuck I bought my 6700xt in 2023, this is balls",pcmasterrace,2025-10-30 18:25:06,294
AMD,nm8s0ac,The consoles are still running 6000 series. This makes no sense.,pcmasterrace,2025-10-30 19:12:09,54
AMD,nm8jwkv,"Well, fuck, they ended support earlier than I thought for the 6000 cards. I bought the 6900xt in 2021 lol.   There goes any hope of fsr4 support for us.",pcmasterrace,2025-10-30 18:33:59,94
AMD,nm8kvg9,"Excuse me, what the actual fuck, I upgraded to the Rx 6800 XT from the GTX 1060 6gb and they both loose driver support at the same time? Is this some kind of a Halloween joke? GPU have never really stabilized after the crypto rush and then got into the AI slop era immediately. Gaming is really in the worst place ever.",pcmasterrace,2025-10-30 18:38:32,191
AMD,nm8r6v7,"Wow. thats what i get for supporting the ""underdog""(gpu) when the difference isnt to great. (performance per dollar within my budget and need).  And i dont understand why this isnt getting more traction than it currently is getting, this is setting a really terrible precidence. Im pissed",pcmasterrace,2025-10-30 19:08:15,106
AMD,nm8g80g,"I can understand not perpetually supporting a card with new features, but it seems a little sneaky and very unnecessary to disable existing functionality such as with this USB port.",pcmasterrace,2025-10-30 18:16:42,162
AMD,nm8nl53,Disabling that USB port seems dicky? If it causes issues then it's on them to fix these and not keep it broken.,pcmasterrace,2025-10-30 18:51:10,32
AMD,nm8mf3o,"This seems really premature for the 6000 series cards.   The 5000 series I can kind of get because they lack the ability to do RT and weren't *that* popular.   But 6000 series production only ended what, 3-4 years ago?   The rx 6700 and up are still very capable gpus.",pcmasterrace,2025-10-30 18:45:45,85
AMD,nm8n8ez,"That's disappointing, looking that Nvidia just ended support to cards released between... 2014 and 2016.",pcmasterrace,2025-10-30 18:49:33,29
AMD,nm8p3wr,"It's insane to me that AMD are cutting adrift graphics cards from 2 generations ago (6000 series).  On the CPU side they continue to support AM4, and word is the next generation of CPUs will still work on AM5.  How does this company not have a coherent company-wide philosophy for legacy product support?  This isn't even hardware.  It's software.  There is no technological reason for them to stop supporting these GPUs--only greed and laziness.",pcmasterrace,2025-10-30 18:58:16,66
AMD,nm8q126,Ah yes.. the same AMD who's first dedicated card comes in at *#29...* on the Steam Hardware Survey and who's LATEST GEN Cards arent even registering on the Steam Hardware Survey at all (I know about the bug but they arent even caring to fix that either).  I understand you cant maintain tech forever.. but this seems stupid if you are trying to retain/encourage consumers.  Look at Intel. Forcing people to get new Mobo/CPUs every 2 years drove their customers to the multi year platform of AM4.,pcmasterrace,2025-10-30 19:02:39,48
AMD,nm8npop,"4-5 years is unacceptable for game driver support on the RX 6000 series. If Nvidia can give game support to the bloody GTX 1660 Ti & RTX 2060, why the hell can AMD not be bothered to give it to cards like the RX 6700 (Which is the bloody PS5 GPU) or RX 6900 XT. Reeks of laziness.",pcmasterrace,2025-10-30 18:51:46,62
AMD,nm91uvd,"This is such a shit move by AMD    I guess no matter what GPUs we decide to buy, we still will get fucked by the corpos",pcmasterrace,2025-10-30 19:59:56,20
AMD,nm94ntz,"Seriously? wtf AMD   How to fuck with consumer trust in one simple step. I wonder when they will do the same for 7000 or 9000 series cause imagine buying such card for 3 or more years of usage for pottentially having been cut from features (or worse, drivers if they decide to fuck us even further)",pcmasterrace,2025-10-30 20:13:36,17
AMD,nm8kb0y,"I bought a Waterforce 6900XT last year because it was on sale, my 2080TI crapped out, already had a water block on it, and I wanted to take advantage of resizable BAR with my 5800X3D build.  Well this sucks.",pcmasterrace,2025-10-30 18:35:52,37
AMD,nm8rtks,"It's almost as if this is one of the reasons AMD is In a distant second place, Nvidia JUST dropped the 10 series which is almost a decade old.",pcmasterrace,2025-10-30 19:11:16,34
AMD,nm95uu2,Wow! And here I was floating my next GPU decision between AMD and nVidia. If AMD means I'll need to buy a new GPU every 4 years I'll just stick with the folks still updating my 7 year old 2080.,pcmasterrace,2025-10-30 20:19:20,15
AMD,nm9c61z,I love how all companies trying their best to make me quit this hobby.   My 6700xt will be the last gpu from amd. I may ditch my cpu too.   This gotta be one of the scammest things i have ever seen in my entire life.  Ill gladly to stay away from this company for good.,pcmasterrace,2025-10-30 20:49:53,16
AMD,nm8nngj,"Nah, I just got 6750xt and was talking about how well it runs 1440p, telling my friends to switch from green team. Not anymore",pcmasterrace,2025-10-30 18:51:28,31
AMD,nm8h9qc,"We were receiving new features? As far as I know even rocm still isn't supported on my card.   > new features, like the latest Battlefield 6 update    Oh, they call the bare minimum of maintenance a feature. Fuck amd why did I give them the benefit of the doubt after buying a fucking buggy rx5700xt before.",pcmasterrace,2025-10-30 18:21:38,55
AMD,nm9exhg,Makes me regret buying my 9070xt now...  This is absolutely insane. The card came out recently and can still run modern games. Huh?,pcmasterrace,2025-10-30 21:03:31,12
AMD,nmab8z1,"Because this offended the mods elsewhere for who knows what reason:  Not that I'm team this or that (I don't give a fuck about any of that, I go where the performance is for whatever I'm shelling out at the time) but this definitely pushes me more toward a certain other team.  Because, what the fuck is this? My card (6900 XT) will only just turn 5 in December and it's already being relegated? Meanwhile, Nvidia is still supporting the 900-series cards for the next three years, albeit only with quarterly updates — full-time support ended, what, this month? 11 years of support and this is getting relegated after 5?  Fuck off. Properly.",pcmasterrace,2025-10-31 00:02:07,14
AMD,nm8ksq9,"I get not supporting older cards. That happens, though this does seem a little premature. The 6000 series was released 4 years ago. And on the other side of that, we have the Nvidia 10 series that stopped receiving update earlier this year. They were released 9 years ago. Pretty big difference.   But disabling the USB C port is insane. What reason do they have for that? Just let people use it as a standard USB C port, or continue to use it for DP over USB C for VR if they already are.   Cant wait to see all the AMD fanboys in PCMR flock here to try and justify this one.",pcmasterrace,2025-10-30 18:38:10,45
AMD,nm8m70s,So what does the practically mean? How long until games won’t run on 6000 series cards or they have issues with operating systems?,pcmasterrace,2025-10-30 18:44:43,9
AMD,nm8oxlw,I have a 6800XT and I haven't seen any updates for game compatibilities in more than a year... so this changes basically nothing I guess?,pcmasterrace,2025-10-30 18:57:27,9
AMD,nm9runk,"So this is from AMD's Duscord channel about the USB Type-C on the Radeon RX 7900 series GPU: We’d like to inform you that the release notes for AMD Software Adrenalin Edition 25.10 2 posted included misinformation that has since been corrected. There is no change to USB-C functionality on the RX 7900 series GPUs in the 25.10.2 driver. There was an incorrect line in the originally posted release notes that has been removed, and the release notes have been updated.   We apologize for any inconvenience.",pcmasterrace,2025-10-30 22:11:49,9
AMD,nm8nr5r,This is really bad right? People were irritated that the 10 series was losing support earlier this year and the 6000 series is like half as old even the 5000 series is not particularly old.,pcmasterrace,2025-10-30 18:51:57,15
AMD,nm93jlk,Fuck AMD.,pcmasterrace,2025-10-30 20:08:11,15
AMD,nm8r3bp,"cool, not like I was planning on supporting AMD after my current card.",pcmasterrace,2025-10-30 19:07:46,9
AMD,nm8ti7p,I.... just got my RX 6600 not even a year ago...,pcmasterrace,2025-10-30 19:19:21,9
AMD,nm9931t,"""WHOOOOW WE FUCKING LOVE BEING STUPID NVIDIA DESERVES THEIR 100% MARKETSHARE BECAUSE WE ARE COMPLETELY AND UTTERLY BRAINDEAD FOR LITERALLY NO REASON GET FUCKED IDIOTS FSR 4 ONLY WORKS ON 9000-SERIES CARD WHILE DLSS 4 WORKS ON A GODDAMN 2060 BUT WHO CARES BECAUSE WE'RE STUPID""  Did I understand them right?",pcmasterrace,2025-10-30 20:34:57,8
AMD,nm8stp1,I thought AMD aged like fine wine???,pcmasterrace,2025-10-30 19:16:03,16
AMD,nm8ive2,![gif](giphy|U79sZO1AFMp3DI0b0r),pcmasterrace,2025-10-30 18:29:07,7
AMD,nm8t6h8,"Ok what the actual fuck i have a 6700xt and can run any game pretty decently and most of them at 165fps stable, why they will stop receiving features? Those GPU are more than capable on 2025",pcmasterrace,2025-10-30 19:17:45,7
AMD,nm8rd6r,"We got Nvidia cranking the damn prices to oblivion and we got AMD dropping support for less than 5 year old GPUs that are still in tons of builds.  I can't believe I'd be saying this, but Intel might be the future of the consumer GPU market if they make the right business moves in the next few years and capitalize on this.",pcmasterrace,2025-10-30 19:09:05,14
AMD,nm8ureq,Wondering how much I could get for my 6700xt and grab a 5070.,pcmasterrace,2025-10-30 19:25:25,5
AMD,nm9biml,"Think I might just pay the nvidia tax next time I upgrade my GPU. Still won't do that until games become literally unplayable for me, but yeah, pretty bad look to axe driver support after a measly 4 years.",pcmasterrace,2025-10-30 20:46:47,7
AMD,nm9d4t3,If prior cards are any history the Linux kernel will keep them supported for a few decades.,pcmasterrace,2025-10-30 20:54:36,7
AMD,nm9ohlz,Well that puts the nail in the coffin for me. Only Nvidia cards for me from now on. I have a 6800xt that works great and it's not that old.   Trash behavior.,pcmasterrace,2025-10-30 21:52:45,6
AMD,nm9rsxg,"Yeah these fucking shitstatins can go fuck themselves that's for sure.  My laptop gtx 960m which released in 2015 and is based on a gtx 750 ti is still getting updates, but my 4 year old 6700xt isn't going to be getting shit now?  These fuckers truly never miss an opportunity to miss do they.",pcmasterrace,2025-10-30 22:11:33,6
AMD,nm9w281,"So not only do they essentially end support for 5, 6 series, this is a guaranteed nail that it will not receive FSR4. I got my 6600 in 2022. So 3 years and it originally dropped in 2020? I think. So 5 year support and instantly dropping it.  At least for me.. I'm not sure i want another amd product. Prices have been higher than msrp, drivers are pretty ass, still tons of bugs. They take months! MONTHS! To release stable drivers with bug fixes everyone's been asking for. The program itself is buggy, slow. It constantly kills itself and you have to restart it. Generally all of this is excusable for products under msrp and is within its product limits of hardware.  Yeah big yikes amd... not too sure about your future.",pcmasterrace,2025-10-30 22:36:03,6
AMD,nmc2vdd,"Even as a 9070xt buyer this is absolutely infuriating. Confirms that AMD won't be pursuing long-term support of their products, massively decreasing their resale value. Damn!",pcmasterrace,2025-10-31 08:02:11,5
AMD,nm8tomy,I bought my 6900 XT as a birthday gift with my new job in 2022.  It's already getting dropped? Why??,pcmasterrace,2025-10-30 19:20:12,5
AMD,nm8x2dg,What about the Steam Deck?,pcmasterrace,2025-10-30 19:36:36,5
AMD,nm93js1,"What does that mean for the USB port?  Can you still use VR headsets?   (I never owned one but, AFAIK, this is the use-case for it).  I would be glad that someone explains this to me.",pcmasterrace,2025-10-30 20:08:12,5
AMD,nm96kzh,Haha good thing I didn't spend 300 dollars on a used 6000 series gpu then haha....,pcmasterrace,2025-10-30 20:22:51,5
AMD,nm9je6s,I guess this kills any chances of an official FSR4 version for RDNA2 and RDNA3 GPUs. Booooo!,pcmasterrace,2025-10-30 21:26:17,4
AMD,nm8pm20,I paid £600 for my 6950XT in June 2023 and I don't even get the BF6 update?  My shits lagging af from a memory leak and is unplayable - not sure if that's a driver issue or game issue but I'm quite annoyed over this. It wasn't cheap.,pcmasterrace,2025-10-30 19:00:38,9
AMD,nm8k5uv,"Oh, you mean the peoples champion of value and virtue is not what it seems?  https://preview.redd.it/teug50mjmayf1.jpeg?width=947&format=pjpg&auto=webp&s=33f1600db1cfa55dc2253cadd2879d8c9aee1480",pcmasterrace,2025-10-30 18:35:12,38
AMD,nm8sxx7,"No way, only 4.5 years after release 6000 series lose driver support for new games.  This is akin to nVIDIA abandoning 3000 series cards, meanwhile Rx 6000 series are even 0.5 year newer!  People would not let NVidia live that down! There would be crusades.",pcmasterrace,2025-10-30 19:16:36,2
AMD,nm99esh,Oh okay so between still being on AM4 and using a 6700 I'm double fucked.   Anyone interested in some hairy feet pics? /s,pcmasterrace,2025-10-30 20:36:33,4
AMD,nm9gz2a,I thought my RX 6800 (non xt) was gonna carry me through for a long time. Like the RX 480 and 580.   Does that mean the shelf life for the 7000 and 9000 will last for another year and a half?  Gotta say I'm bummed to buy anything new.,pcmasterrace,2025-10-30 21:13:50,4
AMD,nm9pj4x,"What bastards, what the fuck",pcmasterrace,2025-10-30 21:58:31,4
AMD,nm9zox1,"I bought an expensive card specifically so I would have less to worry about in terms of upgrading. Do I just turn off updates and hope for the best, or lose my USB-C port?",pcmasterrace,2025-10-30 22:56:47,4
AMD,nmat0a1,"Oh I bet this is how they weasel their way out of giving RDNA2 INT8 FSR4 despite modders having literally shown it works and reviewers like HUB testing it out. Dicks, my 6950 XT I bought two and a half years ago and it only released 3 and a half years ago ffs.",pcmasterrace,2025-10-31 01:47:00,4
AMD,nmb2il3,"Don't get too comfortable, AMD",pcmasterrace,2025-10-31 02:44:37,5
AMD,nm8kone,"Me, still on a RX580...  Soooooo do I need to upgrade?",pcmasterrace,2025-10-30 18:37:38,7
AMD,nma8qub,People shit on Nvidia for cutting support for 7 year old cards. This is even worse,pcmasterrace,2025-10-30 23:47:36,5
AMD,nm8nnr0,"AMD really wants to move from their prior gen. As quickly as possible.   They underestimated the market when it came to AI and now realize they can’t ignore it.  The problem is they have been ignoring it for too long and now have prior generations of products that really don’t have a path to fit in with their current model.   Nvidia on the other hand was laying the ground work starting with 20 series and why they can STILL have some carryover support for new features and keep things going. Their whole line up has a single trajectory and don’t require different teams and feature sets because their products are scattered all over the place. It’s why some features DLSS4 went back to supporting 20 series.   AMD can’t do that because they never laid down that groundwork. They’re JUST NOW starting to work the their current gen 90xx cards, that means they need to back away from prior gens as soon as possible to focus on this and future generations.   5000, 6000 now, 7000 probably isn’t that far off either.",pcmasterrace,2025-10-30 18:51:30,8
AMD,nm8vats,"I have an RX 5600 XT, am I cooked chat?",pcmasterrace,2025-10-30 19:28:01,3
AMD,nm8vefg,The fuck? My fiance is rocking a 6700. And that card is only what 4-5 years old? The fuck,pcmasterrace,2025-10-30 19:28:30,3
AMD,nm905eh,"Wait, my 7900XTX has a USB-C port??",pcmasterrace,2025-10-30 19:51:38,3
AMD,nm922kv,"Not sure i understand why tf they would disable the usb-c features. I mean, what did it hurt them to just leave it as it was? Seems kinda like when they nerf a character in a game because they think its too OP. However, nerfing a product that youve paid for is bs. Also, what exactly is “maintenance mode”?",pcmasterrace,2025-10-30 20:00:58,3
AMD,nm9c4z6,Geezus AMD has never been good with GPU support... Kepler got like 13 years of game ready drivers,pcmasterrace,2025-10-30 20:49:44,3
AMD,nm9fmjq,"At this point it's not even worth recommending AMD saying that they have 16GB compared to a 5070, since if they last 2 years of support with 12GB maybe for 2 years you can still play at 1440p, clowns",pcmasterrace,2025-10-30 21:07:03,3
AMD,nm9n55w,Is this the AMD FineWine I keep hearing about? They keep making me regret going with them for my last gpu,pcmasterrace,2025-10-30 21:45:34,3
AMD,nm9sjcu,FML just built a second PC for living room with a 6800XT. This stinks.,pcmasterrace,2025-10-30 22:15:50,3
AMD,nm9xfwk,"AMD seriously reconsider, you'll hurt future GPU sales with this move. Can they open / community source it or something so people don't regret supporting them?",pcmasterrace,2025-10-30 22:43:58,3
AMD,nm9zfuu,AMD decided to make the GPU and CPU parts of the company polar opposite for some reason?,pcmasterrace,2025-10-30 22:55:24,3
AMD,nma186u,"That's really bad, like wtf amd?",pcmasterrace,2025-10-30 23:05:22,3
AMD,nma42zp,"So I have an odd question about the disabling of the usb-c port on the 7900 series. So does it fully not work to display on, say, a usb-c dock at all now?",pcmasterrace,2025-10-30 23:21:21,3
AMD,nma7b0r,From what I've heard Nvidia works right out the box no tweaks yess some have less v ram but the same performance as amd if not better and on the other hand with my personal experience both my cards the rx580 and the rx6800 have crashed my pc multiple times unless I tweak setting this is with 3 different PC setups so imo if this is all true fuck amd cards in the future and watch that end of their company fold just as they were doing pretty well maybe they are just going to focus on cpus and new architecture but to end support for these cards this soon while still selling them new seems like a huge scam,pcmasterrace,2025-10-30 23:39:29,3
AMD,nmb30of,"Shit, c'mon AMD do better. If they're going to be like this my next card I will be going back to NVIDIA. My GTX 960 on an old machine just got another update.",pcmasterrace,2025-10-31 02:47:47,3
AMD,nmbn0dj,"Damn, I guess the wine went off...",pcmasterrace,2025-10-31 05:21:17,3
AMD,nmbuzr4,"Ah, that OpenAI deal really made them stop caring huh. Alright then, we will remember this.",pcmasterrace,2025-10-31 06:40:11,3
AMD,nmci75t,That's a dick move.  And they're still selling those cards??  What tf is that?   As much as everyone bitches about Nvidia.  They at least stick by their products and support them for the long haul.,pcmasterrace,2025-10-31 10:31:54,3
AMD,nmf6ras,"~~maybe a stupid question, but will mesa (linux) driver still have support?~~   **Update - 11:45 am ET, October 31:** In a statement of clarification to *Tom's Hardware*, AMD says that ""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch"" to its RDNA 1 and 2-based GPUs, following community pushback and some confusion regarding its recent decision to put the cards in 'maintenance mode'. AMD had previously stated cards would ""continue to receive driver updates for critical security and bug fixes.""      source: [https://www.tomshardware.com/pc-components/gpu-drivers/amd-decision-to-put-rdna-2-gpus-in-maintenance-mode-could-spell-trouble-for-handheld-gaming-systems-including-rog-xbox-ally-company-backtracks-on-rx-7900-series-usb-c-functionality-but-not-on-rdna-2-support](https://www.tomshardware.com/pc-components/gpu-drivers/amd-decision-to-put-rdna-2-gpus-in-maintenance-mode-could-spell-trouble-for-handheld-gaming-systems-including-rog-xbox-ally-company-backtracks-on-rx-7900-series-usb-c-functionality-but-not-on-rdna-2-support)",pcmasterrace,2025-10-31 19:20:46,3
AMD,nm8jyvd,Does that mean my 7700XT *will* be getting new features?,pcmasterrace,2025-10-30 18:34:17,5
AMD,nma1eu0,"Wtf?  Going green on my next GPU upgrade. They are overpriced by like 50% but at least they get longer support so I save more money on the long run.  AMD, after going through 3 of your GPUs for the past 10 years, you lost me fully with this one.",pcmasterrace,2025-10-30 23:06:24,4
AMD,nm8uhl8,I just got a 7800xt last year. Makes me a little worried how much longer it may have support for😬,pcmasterrace,2025-10-30 19:24:06,2
AMD,nm8yji2,Makes me question if buying the 9060xt was the correct decision. If it’s gonna get support for it dropped in 3-4 years then it kinda sucks.,pcmasterrace,2025-10-30 19:43:48,2
AMD,nm98inm,At this pace of support drop off how likely is it for the 90- series to become unsupported in the next 5 years?,pcmasterrace,2025-10-30 20:32:12,2
AMD,nm9c5c2,"Bought a 7900xtx during a gpu drought… so not only has my card lost features but the generation previous has lost support, signaling to me this card will have its support cut in what, A year or so?   Really looking like I should’ve bought team green.",pcmasterrace,2025-10-30 20:49:47,2
AMD,nm9e2vt,"Ugh, and I’ve been so happy with my 6900. If they don’t reverse this decision, they won’t get my money next time I upgrade. Maybe I’ll give Intel a chance if they’re still making GPUs by then.",pcmasterrace,2025-10-30 20:59:15,2
AMD,nm9ra28,Are these companies allergic to money or what??  They've been hitting it out of the park lately and now they go and fuck it up.,pcmasterrace,2025-10-30 22:08:30,2
AMD,nm9yqwo,Wow. I'm currently rocking a very capable RX 6800. This...honestly might push me to Nvidia for my next card. AMD are *crazy*.,pcmasterrace,2025-10-30 22:51:33,2
AMD,nm9z5wy,guess I'm not switching to amd gpus after all,pcmasterrace,2025-10-30 22:53:52,2
AMD,nma1dbq,"Man, duopoly sucks. I thought AMD would cultivate good will with their current position, but I guess they're just another corpo.",pcmasterrace,2025-10-30 23:06:10,2
AMD,nma1eel,I'm wondering about my fate owning a 7000 series card in the next few years but man this sucks.,pcmasterrace,2025-10-30 23:06:20,2
AMD,nma5z7j,That's really anti-consumer.,pcmasterrace,2025-10-30 23:31:58,2
AMD,nma6h0o,Well that sucks. I'm glad I at least got a decent deal on my 6650xt.,pcmasterrace,2025-10-30 23:34:46,2
AMD,nmaeoud,"That's a shame, I got my sapphire 5700XT right before the pandemic hit (800$ was and still is a huge amount for me to spend) and it's been good to me ever since. Gonna have to stick with older/indie games for another couple years until I can afford to make a new build.",pcmasterrace,2025-10-31 00:22:30,2
AMD,nmakotd,"shit, i was looking to get a rx 6600 to pair with my ryzen 5 5600x, guess ill be looking at Arc now",pcmasterrace,2025-10-31 00:57:48,2
AMD,nmaqffm,Fuck you AMD,pcmasterrace,2025-10-31 01:31:40,2
AMD,nmarc7p,Me with my 7800 XT  ![gif](giphy|55itGuoAJiZEEen9gg),pcmasterrace,2025-10-31 01:37:04,2
AMD,nmatj6w,"Man, I love AMD for years until now. This disappointed me so much considering the fact I owned and still use 6800XT which is 5 years old. I understand that they want to stop support any gpu over 10 years old. Hell, 6800xt is match to match the 9060XT in most games to the point it isn't needed to ""upgrade"". This is a big middle finger to me as a 6800XT and a fan of AMD.  I guess my next upgrade would be NVIDIA.",pcmasterrace,2025-10-31 01:50:09,2
AMD,nmbhzro,guess I won't be buying another AMD card anytime soon,pcmasterrace,2025-10-31 04:36:59,2
AMD,nmbj0p5,"This is exactly why when I sold my 7900XT for lack of FSR4, I moved to Nvidia. Can't deal with 2 scummy companies. Nvidia is more than enough.",pcmasterrace,2025-10-31 04:45:39,2
AMD,nmbv1ab,"So if I buy used, I’ll get like 2 years of support. If I buy new I get less than 5 years of support? This is why people buy green",pcmasterrace,2025-10-31 06:40:36,2
AMD,nmbvour,Wtf amd,pcmasterrace,2025-10-31 06:47:20,2
AMD,nmbxi45,Saltar a Linux y usar los drivers de MESA que tendrán soporte por muchísimos años.,pcmasterrace,2025-10-31 07:06:00,2
AMD,nmbzma5,AMD being AMD,pcmasterrace,2025-10-31 07:28:12,2
AMD,nmcdne3,Looks like im sticking with amd cpus and going nvidia gpus now.,pcmasterrace,2025-10-31 09:51:31,2
AMD,nmcmx3n,sadly typical of amd look at the radeon 7 or for how long you had to use modded community drivers on the rx580,pcmasterrace,2025-10-31 11:10:02,2
AMD,nmd344k,such an L and greedy move,pcmasterrace,2025-10-31 12:57:08,2
AMD,nm8q18s,AMD fanboy : ''BuT aMd DoEsNt DrOp SuPpOrT fAsT''  They did the same shit with first generation ryzen processors and tried to do it to 3rd gen ryzen barely 3-4 years in until community backlash. VEGA gpus got cut 3-4 years in and got 1 update every 2 years after ...  AMD is the most anti-consumer company.,pcmasterrace,2025-10-30 19:02:41,6
AMD,nm8iu6h,"Seems I dodged a bullet, I was torn between a rx 6800xt and a 3080 and went with the 3080 last minute a couple months ago.",pcmasterrace,2025-10-30 18:28:58,4
AMD,nm91ul5,"Keeping up with pc hardware was already impossible, and now they just start crapping out like some damn cheap Motorola phone? Niiiice.   Really happy I chose AMD for that extra VRAM for all those future games that are going to need it :DDD",pcmasterrace,2025-10-30 19:59:54,2
AMD,nm8fnx4,"Jesus, what a garbage generation of gpus... They have aged like milk, If I had bought a 3080 instead of a 6800XT I would not have bought a 4080super to replace It.",pcmasterrace,2025-10-30 18:14:04,3
AMD,nm94glp,"bruh... i was planning to get a rx 9060xt as an upgrade from my rx 6600... reading this changed my mind, i will go with a rtx 5060ti or heck even a used rtx 3080/ti, i cant keep supporting AMD while they keep making such stupid, dumb decisions.",pcmasterrace,2025-10-30 20:12:37,2
AMD,nmaapqz,Can just 1 year go by without AMD's GPUs division shooting themselves in the foot with a poorly communicated decision that defies any sort of logic or reason? Just once please to see what it feels like.  What the hell are they doing over there? It's like they are refusing to even attempt to regain market share and like they're  allergic to good PR that lasts for more than a few months.,pcmasterrace,2025-10-30 23:59:00,2
AMD,nmae3zm,Fuck. I just bought a brand new 6600 like a sucker lmao. Every day I regret that decision more and more lol 💔,pcmasterrace,2025-10-31 00:19:02,2
AMD,nmaxk4m,My 6950xt is cooked 🥲,pcmasterrace,2025-10-31 02:14:27,2
AMD,nmb7tn4,Nvidia set to have 100% market share in that case.,pcmasterrace,2025-10-31 03:19:31,2
AMD,nmb87xv,"Dropping support this fast suggests the company’s financials are completely fucked & they need to risk consumers never buying from them again, by trying to force an upgrade cycle. I’m guessing they’ve done the stupid thing & put shit tons of money recently in Ai without realising the US economy is about to explode & need enough liquidity to survive the collapse.   I love my 6950xt, I love my 6700, I will probably not buy AMD for a while because this kind of nonsense is why I stopped buying Intel products.",pcmasterrace,2025-10-31 03:22:21,2
AMD,nmb8pmt,I just bought my 6700xt. I will never buy any product that they will release.,pcmasterrace,2025-10-31 03:25:51,2
AMD,nmche9a,I'm returning my 9070xt and bu6ing a 5070ti. I'm not willing to find out how long RDNA4 will be supported,pcmasterrace,2025-10-31 10:24:52,2
AMD,nm99t6u,They're still going to fix bugs and security issues. It's not like nVidia adds new features to 5 year old hardware either.,pcmasterrace,2025-10-30 20:38:29,1
AMD,nm9gx9i,"It's fine cause it's AMD, and AMD never does anything wrong whatsoever  Greedy capitalist pigs from NVIDIA on the other dropped support for the 10 years old 1080 TIs because Jensen is that mean of a guy",pcmasterrace,2025-10-30 21:13:35,2
AMD,nm8alay,"Ah yes, good ol' planned obsolescence.",pcmasterrace,2025-10-30 17:50:14,0
AMD,nm8udk5,"Damn. I mean on one hand, I was planning on upgrading to the RX 9070 sometime the next few months anyway, but on the other it’d be nice to continue to get support for a 4 year old card that is still beloved by the community.",pcmasterrace,2025-10-30 19:23:33,1
AMD,nm99jne,Ded card.,pcmasterrace,2025-10-30 20:37:12,1
AMD,nm9a8r5,Imma give Intel just a little longer until I switch my 6700xt.,pcmasterrace,2025-10-30 20:40:33,1
AMD,nm9b6fd,"Wait, what i I have bought 6900 so i can have it for like 2-5 years. What now can someone explain it to me like i; am 2 years old?",pcmasterrace,2025-10-30 20:45:06,1
AMD,nm9f4xf,"I don’t use a usb-c on my 7900xtx, what else does this mean?",pcmasterrace,2025-10-30 21:04:33,1
AMD,nm9l203,So my usb c on my midnight black edition rx 6800xt will be disabled?!,pcmasterrace,2025-10-30 21:34:50,1
AMD,nm9l3z3,I wonder if any of this has any relation to Microsoft's dropping of Windows 10 support? The timing seems rather coincidental to just be a coincidence,pcmasterrace,2025-10-30 21:35:07,1
AMD,nm9q79e,bUy AmD nOt NvIdIa,pcmasterrace,2025-10-30 22:02:17,1
AMD,nm9vitf,And i will never buy amd again gpu wise if the comments are right this time. Lucky i just sold my 6600xt,pcmasterrace,2025-10-30 22:32:59,1
AMD,nm9xigk,"The RX6400 is honestly of my favourite cards ever. £125 back in 2022, and ran everything I needed it to really smoothly. And then FMF was introduced and it felt like I got a huge upgrade. Absolutely goated budget card",pcmasterrace,2025-10-30 22:44:22,1
AMD,nmbd7mo,Legal! End of purchasing amd models for my PC.,pcmasterrace,2025-10-31 03:58:45,1
AMD,nmbi6tk,Does this mean no drivers for future games for my 6900xt wtf,pcmasterrace,2025-10-31 04:38:39,1
AMD,nmbneuf,This may have been the only thing that could have convinced me to switch to nvidia when I upgrade,pcmasterrace,2025-10-31 05:24:59,1
AMD,nmboohf,So is that good news for poor peasants as me to planing buy used one about 200$?,pcmasterrace,2025-10-31 05:36:59,1
AMD,nmbwye3,"what, tf why?",pcmasterrace,2025-10-31 07:00:21,1
AMD,nmc7stq,Thats pretty fucking shitty of them,pcmasterrace,2025-10-31 08:53:21,1
AMD,nmc9bix,Does Mesa open drivers do that?,pcmasterrace,2025-10-31 09:08:42,1
AMD,nmc9lhc,They will backpedal 100%.,pcmasterrace,2025-10-31 09:11:30,1
AMD,nmcaagi,"....Damn, I've had a 6600 for barely 3 years now lol",pcmasterrace,2025-10-31 09:18:35,1
AMD,nmccbl1,"Literally saved 2 months salary to be able to buy RX 6700xt last year, cards in my country are hella expensive, But now i woke-up to the news that they wont be supported for future games anymore ? seriously shitty move AMD i will be moving to team green in the future, you are never getting my money again.",pcmasterrace,2025-10-31 09:38:56,1
AMD,nmcs2hn,They were doing soooo good up until now lol Such a shame really,pcmasterrace,2025-10-31 11:48:03,1
AMD,nmcs3y5,This has just 0 sense. I bought my rx 6600 1 year ago (2024 summer) and it has 4 years since it released now losses game optimisation drivers. This is just so stupid. AMD should add at least one more year of game optimised drivers for the 5000 series and 2-3 years for rx 6000 series,pcmasterrace,2025-10-31 11:48:19,1
AMD,nmd2yd8,"My 6900XT OC still rips most modern games at 4K/60 on ultra settings, and you're telling me it's now OBSOLETE!?  People remember shit like this when they're looking at future upgrades, AMD.",pcmasterrace,2025-10-31 12:56:14,1
AMD,nmdkoyk,"After so little time? Older cards had more time support, typical corporations.",pcmasterrace,2025-10-31 14:31:09,1
AMD,nmdncqg,"mais de 10 anos sendo usuario de GPUs AMD. depois desse anuncio nunca mais AMD, daqui pra frente só GPU Nvidia. Tenho uma GPU 6800XT, isso foi a maior sacanagem dos ultimos tempos com o consumidor.",pcmasterrace,2025-10-31 14:44:44,1
AMD,nme18vc,"I would understand it not receiving new features such as cutting the edge stuff meant for newer hardware, but to actually cut off both lines like no driver support for games that come out is what I'm hearing here? I call it outrageous alright, while I was spreading the gospel to my console friends to turn to PC from consoles due to the recent fiasco it seems my arguments have been silenced by none other than AMD themselves...what a sad day.",pcmasterrace,2025-10-31 15:52:40,1
AMD,nme42al,"This is insane. I'm using the RX 6700 XT, and it's a really great GPU. I'm glad I bought it almost two years ago for around $280 used, and there is virtually no good upgrade option in my country as all new GPUs are inflated way beyond MSRP. This is a kick in the pants.",pcmasterrace,2025-10-31 16:06:16,1
AMD,nmh3jzr,"So, AMD GPUs have gotten big enough for their liking and they are now in milking mode?!",pcmasterrace,2025-11-01 02:27:20,1
AMD,nmh8dre,![gif](giphy|l2JhtKtDWYNKdRpoA),pcmasterrace,2025-11-01 03:02:01,1
AMD,nmhmvx7,"Thanks AMD, guess after the last 15 years of being team red for my builds I’m going to switch to either nVidia or Intel for my next build.",pcmasterrace,2025-11-01 05:01:33,1
AMD,nmhyqg0,Again Nvidia wins by doing nothing  lol,pcmasterrace,2025-11-01 07:08:06,1
AMD,nmj3osp,"Drivers, Drivers, Drivers. 10 times more important for video card manufacturers than specs. Look at intel, should devote 50% of budget to developing drivers.    Greed is killing any goodwill.",pcmasterrace,2025-11-01 13:28:11,1
AMD,nml46z2,What does AMD think at all?    **AMD will ruin itself and lose a lot because of such actions**,pcmasterrace,2025-11-01 19:56:07,1
AMD,nmo78f5,"Honestly it's not a big deal. RDNA 2 based cards are still powerhouse cards in terms of raw performance anyway. Nothing a bit of tweaking won't fix. I literally bought a Sapphire Pulse 6800 yesterday, of course used. The card is emasculate and runs everything w/ out of the box settings and it still performs with current gen cards.   The game industry is changing and AI is taking over, it makes sense for them to shift support from non AI based architectures to focus on the last, current, and future gems where everything is going from hardware intensive to software enabled. That's why they're stressing DLSS and FSR so so much, the future of gaming is gonna be less hardware and more software focused.  Some of those workhorse cards are big as hell due to shere hardware.   Get your money up, get strategic/adaptable with your builds, start thinking ""future of gaming"" instead of  ""good old days"" those days are rapidly coming to a conclusion.",pcmasterrace,2025-11-02 08:07:15,1
AMD,nmsq0rs,Nope,pcmasterrace,2025-11-03 00:13:12,1
AMD,nm8u9vb,The RX 9000 series will suffer the same fate in 4 years.  Curse you AMD!,pcmasterrace,2025-10-30 19:23:04,1
AMD,nmay0zq,They will still be supported and receive new features in Linux.,pcmasterrace,2025-10-31 02:17:16,1
AMD,nmb1p84,Lesson learn; bought an rx 6700 XT two years ago after quitting Nvidia.  Never again ! It's such a sh.. move to your customers.,pcmasterrace,2025-10-31 02:39:33,1
AMD,nmbib9h,"I had my 6950xt for almost 3 years now, i was always so fucking happy with how well that purchase turned out as i skiped a 3080 for this card.  Turns out fuck me i guess i should have gone for the 3080 for 100 bucks more, at least I would still have gaming driver updates.   I'm running this card until i can upgrade and then never toching another amd card ever again, fucking bastards.",pcmasterrace,2025-10-31 04:39:40,1
AMD,nmbpcru,"Hopefully intel can start really competing soon with gpus, amd and nvidia are both so shit lmao",pcmasterrace,2025-10-31 05:43:28,1
AMD,nmccv96,It’s not like the GPUs stop working after that,pcmasterrace,2025-10-31 09:44:12,1
AMD,nm89x31,"Yes. It won't receive support for new games.  From the driver announcement directly: ""New Game Support and Expanded Vulkan Extensions Support is available to Radeon RX 7000 and 9000 series graphics products.""  https://www.techpowerup.com/download/amd-radeon-graphics-drivers/",pcmasterrace,2025-10-30 17:47:03,252
AMD,nm9z17r,"It will be fine. Just because they aren't adding new features doesn't mean that the existing ones won't be supported. Most ""new features"" are very minor tweaks that make very little real-world difference anyway.",pcmasterrace,2025-10-30 22:53:09,10
AMD,nmbeb4m,"I bought an used 6700xt just this week, chose it over similar priced Nvidia models cause of the 12gb vram, thought it'll age better. Shit luck I guess.",pcmasterrace,2025-10-31 04:07:13,2
AMD,nmgu23v,"They've clarified it as a bad press release.  Game ready drivers, bug fixes, and security updates will still be issued but new features won't be backported to 6x series and below.",pcmasterrace,2025-11-01 01:21:08,2
AMD,nmbvcdc,"This is great idea, because this gpu have to cost less now. Also never heard about GPU can't play games because there are no support. Radeon 580 is abandoned 5 years ago and works great for 70$",pcmasterrace,2025-10-31 06:43:45,1
AMD,nmbznqr,Its fine if you can get it cheap.,pcmasterrace,2025-10-31 07:28:37,1
AMD,nmcjojx,Depends on what games you play. Most games will run fine. Its mostly about new games and special features. Still isnt great though and id consider this evaluating a price.,pcmasterrace,2025-10-31 10:44:10,1
AMD,nmdizxc,"I have an rx 6750 xt released 3.5 years ago, support cutoff is too early.",pcmasterrace,2025-10-31 14:22:29,1
AMD,nm8e32a,"The USB-C port is a VirtualLink port that provides 27w of power to anything plugged into it, a DisplayPort (iirc 1.3 or 1.4?) connection, and USB 3.1 support for data transfer. This means you cannot use it for fast charging/Power Delivery, or acting as a regular USB-C 3.1 port once its disabled. It will only be able to drive a montor with DP if the monitor gets power from somewhere else since its now been disabled on the VirtualLink port.  Its a cool little port that could do loads of other things like support and power an external drive bay but unfortunately it died off because VR didn't hit as mainstream as companies liked and valve/htc never managed or bothered to make a Virtual Link adapter that would handle all of your headsets data, display, and power needs. iirc only the PSVR2 makes use of its intended purpose which came out 5 years after we first saw these on the RTX 20 series GPUs",pcmasterrace,2025-10-30 18:06:39,490
AMD,nm9qv2u,"AMD stated on their discord it was an error and that line doesn't show anymore in the change log, not sure if it's true or not because I cannot test it",pcmasterrace,2025-10-30 22:06:05,2
AMD,nm9c8ol,"First time I heard about them having a usbc port, I have an XTX spectral white and mine doesn't have it, nor did any of the ones I saw when looking up prices.",pcmasterrace,2025-10-30 20:50:14,1
AMD,nm8dxmg,Just speculating but most probably to prevent unofficial firmware updates in the future.,pcmasterrace,2025-10-30 18:05:56,-2
AMD,nm8ikou,"I'm pretty sure Steam stats showed that as one of the most common cards out there. Probably why they're giving it the shiv, to get people to finally upgrade to newer cards.",pcmasterrace,2025-10-30 18:27:44,97
AMD,nm8pyrn,"I was on a RX 580 just a month ago, I wonder how many years of useless drivers I had downloaded lol",pcmasterrace,2025-10-30 19:02:21,2
AMD,nm99a65,"copy/paste from another thread  *If you are lucky enough to live in a country with 5 year warranty OR if you bought your 5 or 6 series gpu card, maybe even 7900XTX with the usb-c within your country's warranty limit, go ahead and use it, this is NOT what you paid for!*",pcmasterrace,2025-10-30 20:35:55,1
AMD,nmafqni,yup my 6800xt has been treating me very well,pcmasterrace,2025-10-31 00:28:48,1
AMD,nmal8pj,You definitely got your money out of it. The 9060XT is a pretty good upgrade.,pcmasterrace,2025-10-31 01:00:58,1
AMD,nmbao6p,I still have mine as a backup,pcmasterrace,2025-10-31 03:40:07,1
AMD,nmbxk1b,"yes, and I don't consider it is too old, I am still playing alright with it. But saying that they won't implement new feature doesn't mean that they will stop updating the drivers, no? They just stop including new technologies.",pcmasterrace,2025-10-31 07:06:32,1
AMD,nmby9l5,I'm on my 6800xt. I'll have to use it for quite some years to come as I'm a student,pcmasterrace,2025-10-31 07:13:55,1
AMD,nm9awtg,"Yea same, I love my 6900xt but this is quite a bummer to see. Great GPU with great performance.",pcmasterrace,2025-10-30 20:43:49,35
AMD,nma5usi,"You will get game support i believe,thats maintenance, just no new features",pcmasterrace,2025-10-30 23:31:15,17
AMD,nmexlw6,"It always happens when a company is comfortable with its market position. Zen was priced very well as well as Zen+. Zen 2 had a small price increase but was well regarded as good value. Zen 3 and later increased price a lot and so you're just paying for the performance increase you get.  AMD was in a very weak position coming from years of GCN, so 5000 and especially 6000 series was good. Nowadays they are just copying Nvidia in product positioning with 8GB VRAM SKUs and price it at Nvidia -10%.   And cutting off GPU support early.",pcmasterrace,2025-10-31 18:33:33,2
AMD,nm8tov5,"They’re still being sold too, so you’d expect to have a decent amount of life out of them.  This is a bullshit move to show you’re only providing 3 years of proper driver support from their initial release date.",pcmasterrace,2025-10-30 19:20:14,151
AMD,nm8teyn,Remember how much shit Nvidia got for dropping 1000 series support earlier this year? Cards that are like 9 years old? Imagine if they had dropped 3000 series support.  I can't wait to not see a single tech influencer call this out to any similar degree even though it's way worse.,pcmasterrace,2025-10-30 19:18:55,155
AMD,nm8prdi,"Heck, the RX6600 was still THE budget card for a lot of markets. It was that or a 3050 for a lot of people if they wanted a new card, or maybe an A580/B570.",pcmasterrace,2025-10-30 19:01:21,29
AMD,nm9uz9l,I had a 5700xt and was planning to build a new pc soon and honestly seeing all this is dissuasing me from doing so,pcmasterrace,2025-10-30 22:29:54,1
AMD,nm9v1v5,"I'm pissed off there's nothing wrong with my RX6950xt ... This was my first AMD card. But if they're only supporting their hardware for 3 years then I'll go back to Nvidia, that's outrageous honestly.",pcmasterrace,2025-10-30 22:30:19,1
AMD,nm9yu8q,"My TUF 6800XT actually has been a royal pain in my ass, RMA'd twice and likely dying as it stands so kinda torn between upgrading or just throwing my 2080 Super back in...",pcmasterrace,2025-10-30 22:52:04,1
AMD,nmb7s2q,"Bruh my friends 6900xt goes toe to toe with my 3090. This is fuckin’ bullshit, that thing should still be playing games at LEAST on 1080p medium-high in another 4-5 years from now.   Hell the 1080 TI is almost 9 years old now and still compares to budget GPUs today.",pcmasterrace,2025-10-31 03:19:13,1
AMD,nmbhyf0,they need more money,pcmasterrace,2025-10-31 04:36:41,1
AMD,nmby1jh,I can still run everything with my RX 6700XT on 1080p :(,pcmasterrace,2025-10-31 07:11:35,1
AMD,nm9ir7m,I bought my 6900xt in 2022 and it runs everything I play flawlessly,pcmasterrace,2025-10-30 21:23:00,1
AMD,nm91ktw,My 6950xt still has warranty and still is a fucking beast.   This really sucks...,pcmasterrace,2025-10-30 19:58:35,53
AMD,nm8poq6,"The 6-50 XTs released in october 2022 (3y) and already not supported, while nVIDIA dropped the 1080Ti (a 2017 card) only this year?  Were the drivers too good or what?  GPUs in 10 years will get 1 update on release then become obsolete right after.",pcmasterrace,2025-10-30 19:01:00,123
AMD,nm9b97l,"Bro, I bought my 6900XT in January... 2025",pcmasterrace,2025-10-30 20:45:29,17
AMD,nm8llwi,"I bought my 6800 this year lol, but it's ok, it's only for the new games that will be released from now on. And even if it has no ""support"" you will still be able to play them, people play games on rtx1050 still......",pcmasterrace,2025-10-30 18:41:56,52
AMD,nm9b3lx,"Same here. Man everywhere I turn there is a corporation or some politician saying ""fuck you, what you going to do about it?""  Like this in particular doesn't strike me as the biggest deal so long as the card is still usable for years to come, but it's just yet another unwanted plunge in the behind when I just want a day off from being completely fucked.",pcmasterrace,2025-10-30 20:44:43,3
AMD,nm98yk5,"copy/paste from another thread      *If you are lucky enough to live in a country with 5 year warranty OR if you bought your 5 or 6 series gpu card, maybe even 7900XTX with the usb-c within your country's warranty limit, go ahead and use it, this is NOT what you paid for!*",pcmasterrace,2025-10-30 20:34:21,6
AMD,nma05mz,Got my 6900xt in 2022. This is such a bullshit move by AMD. Wtf.,pcmasterrace,2025-10-30 22:59:23,1
AMD,nm9eulp,"Last time they did this, they dropped all Vega-based GPUs, while still actively selling mobile CPUs with Vega iGPUs.  It's not like they're supporting the consoles either, the console manufacturer gets complete control over the chip there - it's Sony and Microsoft who make new optimizations for them.  EDIT: Oh, it's even worse. This drops RDNA2, so once again, an iGPU that they're putting in new mobile CPUs for next year. The ""new"" Ryzen 3s and Athlons will launch without GPU driver support from day one.",pcmasterrace,2025-10-30 21:03:06,33
AMD,nm8p5e8,Yeah I was still hoping for it but... yeah...,pcmasterrace,2025-10-30 18:58:27,28
AMD,nm8q5ab,"You have like me (7900xt) the int8 FSR 4, which is a massive upgrade from FSR 3. Its easy to install with Optiscaler.",pcmasterrace,2025-10-30 19:03:14,20
AMD,nmb7el1,"Fsr 4 isn't even coming to the 7000 series. You can get it to work because the leaked code/software, but even then, on the 7000 is doesn't work as well as 3.1 because of the overhead.  But yeah, this situation is fucked man, they should've given way more support to these cards.",pcmasterrace,2025-10-31 03:16:36,2
AMD,nm8o1ia,Lol exactly the same: My Rx 6700 XT is 1 year old and it is losing support same time as my 7 year old GTX 1060 6gb.   Also Bf1 randomly started crashing on my PC for no reason. Lovely stuff.,pcmasterrace,2025-10-30 18:53:17,64
AMD,nm8p2iz,I literally just bought a 6800xt to replace my 2070.. might return it for a 7800xt.. just didn't want to spend £75 more for a card that doesn't out perform the 6800xt.,pcmasterrace,2025-10-30 18:58:06,9
AMD,nm9cgiz,"Actually, the 1060 6GB gets support for new games like Battlefield 6 and Bloodlines 2 that the 6800 XT is NOT getting support for.",pcmasterrace,2025-10-30 20:51:17,9
AMD,nm8m4h1,Another reason to accept Nvidia tax. At least they don't fuck you over after just 4 years like AMD,pcmasterrace,2025-10-30 18:44:23,65
AMD,nm9mgf2,I will never understand why people downgrade to AMD.,pcmasterrace,2025-10-30 21:42:01,5
AMD,nm90pwm,"If you are lucky enough to live in a country with 5 year warranty OR if you bought your 5 or 6 series gpu card, maybe even 7900XTX with the usb-c within your country's warranty limit, go ahead and use it, this is NOT what you paid for!  I am lucky enough to live in a country with 5year warranty, if AMD is ending driver support to my XTX within the next 4 years i for sure know the store would have to honour a warranty claim.",pcmasterrace,2025-10-30 19:54:25,22
AMD,nm9n6qg,>And i dont understand why this isnt getting more traction than it currently is getting  Too busy calling NVidia the devil for cutting support on their 12 year old graphic cards.,pcmasterrace,2025-10-30 21:45:48,25
AMD,nmd12qa,"Yup, I'm buying a Nvidia card next. I keep my PCs for too long to deal with this.",pcmasterrace,2025-10-31 12:45:32,2
AMD,nmaliso,Well AMD is known to do this. They don't support their GPUs for long compared to Nvidia.,pcmasterrace,2025-10-31 01:02:35,1
AMD,nmd0vy3,"Sure, but the 6000 series cards aren't old at all. Nvidia is just now dropping support for 1000 series cards.",pcmasterrace,2025-10-31 12:44:25,1
AMD,nm8ztt0,Even the 6600 is still very popular as a budget card,pcmasterrace,2025-10-30 19:50:04,30
AMD,nm9f67f,The 6x50's came out in 2022 and is already being dropped   Next time im paying nvidia tax,pcmasterrace,2025-10-30 21:04:44,13
AMD,nm9dw1i,"The RX 5700XT was very popular. It's what brought RTG back from the grave, and made money for the development of the RX 6000 series - with which AMD's popularity peaked. That was the single out of ""modern Radeon era"", where they were actually somewhat competitive with NVIDIA. The 7000 while decent, was overshadowed again by the Green Side.   Also, this is a bad look either way. Who's to say they won't drop the 7000 series by next year?",pcmasterrace,2025-10-30 20:58:19,18
AMD,nm9xq6y,"I love my 5700xt but I've had it for 6 years now, it's served its purpose though I wish it could get support for a bit longer.",pcmasterrace,2025-10-30 22:45:38,2
AMD,nmbbnmp,"I have a RX 5700 XT and it's still a very capable GPU despite the lack of RT.  They didn't cut support of the RX 6000 series because the hardware was deprecated, they dropped it for no good reason.",pcmasterrace,2025-10-31 03:47:15,2
AMD,nmaqugq,"Whilst I agree dropping support for a 2020/21 product is bad, the AM4 platform has been one of the most widely used in history and honestly don’t see them dropping support for another couple of years.",pcmasterrace,2025-10-31 01:34:10,2
AMD,nmb9cza,The only reason AM4 is still going is as an outlet to get rid of extra server dies. If it didn’t make business sense they probably wouldn’t do it,pcmasterrace,2025-10-31 03:30:27,1
AMD,nm9kvf6,It's possible that this support has something to do with Windows 10 support as well. The timing seems rather coincidental,pcmasterrace,2025-10-30 21:33:54,7
AMD,nm94iyi,"This doesn't mean new games won't start with that card, it means AMD isn't adding new features or updating the drivers to fix bugs for specific games. This doesn't instantly turn your card into ewaste.",pcmasterrace,2025-10-30 20:12:56,6
AMD,nmamoyc,So AMD historically has had much shorter driver support compared to Nvidia. I'm not really surprised.,pcmasterrace,2025-10-31 01:09:22,3
AMD,nmc70re,The last 5000 series model has only recently passed 5 years.,pcmasterrace,2025-10-31 08:45:38,2
AMD,nm8q04f,"There's no concrete timeline, but games as recent as BF6 could technically start having issues.",pcmasterrace,2025-10-30 19:02:32,17
AMD,nmc0by2,Realistically not until support actually ends when they stop driver updates all together. The cards have gone into maintenance for their drivers meaning they will only be receiving fixes and not new features. Games today and into the future should run fine but there might not be any game specific optimisations for small performance boosts on the latest driver version for these cards anymore.,pcmasterrace,2025-10-31 07:35:47,2
AMD,nm99bur,"It just means that they won't be getting any new features in upcoming drivers. They're still supported, and driver updates will contenue for the foreseeable future.",pcmasterrace,2025-10-30 20:36:09,3
AMD,nmc0ajo,"Most likely 10 years. The 500 series, for which support was dropped 4 years ago still does fine in a lot of games.",pcmasterrace,2025-10-31 07:35:22,1
AMD,nma0l3j,Fuck amd and nvidia they both suck and both do shtty things.,pcmasterrace,2025-10-30 23:01:47,9
AMD,nm9zbs7,Guess I'll wait until mine dies to see what upgrade comes next.,pcmasterrace,2025-10-30 22:54:47,2
AMD,nmc0e95,Your GPU will continue to work just fine.,pcmasterrace,2025-10-31 07:36:28,2
AMD,nmam8dl,The Open-source Linux drivers will keep chugging along unaffected.,pcmasterrace,2025-10-31 01:06:42,5
AMD,nmc0ofj,This is the actual fuck you. If they don't release an int 8 fsr4 for no real reason when I've been running it on the 6000 series that's just a massive fuck you.,pcmasterrace,2025-10-31 07:39:30,2
AMD,nmv9yx1,"Hey man are you me? Exact same here. Honestly wtf. No new game support on  3 year old - or 2 years for us - top of line card. I'm so out. Gonna sell it and get a whatever Nvidia card. I put up with quirks / bugs in drivers or issues at times with amd / fewer features than nvidia as it was such a great bang to buck card and just a raster beast, but now fuck that.  ( I dont have the memory leak though that seems unique to you.)",pcmasterrace,2025-11-03 12:28:40,1
AMD,nm8qqf1,People praising one corporate over another until the one they trust fucks them over too. Choosing between two evils.,pcmasterrace,2025-10-30 19:06:03,12
AMD,nm8sfa2,this was a very surprising thread to read on r/ amdmasterrace,pcmasterrace,2025-10-30 19:14:08,6
AMD,nmc0kws,It will. This article is not saying your GPU will not work with new games starting now. This is honestly a PR disaster and AMD needs to address this.,pcmasterrace,2025-10-31 07:38:28,3
AMD,nm8n2vq,"If its running the games you want, then I wouldn't upgrade, just my opinion.",pcmasterrace,2025-10-30 18:48:50,12
AMD,nm8qgn3,"The RX 580 is such a champ though, played so many recent-ish games on that.",pcmasterrace,2025-10-30 19:04:44,6
AMD,nm8sh2z,RX470 here 😂 we don’t need to but it does mean our next upgrade is cheaper now 🤷‍♂️,pcmasterrace,2025-10-30 19:14:22,2
AMD,nmbay61,"If you're on a budget, get the 5060/ti . It's a sizeable upgrade from that gpu.",pcmasterrace,2025-10-31 03:42:06,1
AMD,nm9ber6,7000 series got screwed worse than any other generation... Took 2 years for FSR 3 then dropped them from future FSR versions 6 months later. AMD is a dumpster fire,pcmasterrace,2025-10-30 20:46:14,7
AMD,nm95omz,I have a 5700 xt and I'm with you 💀,pcmasterrace,2025-10-30 20:18:30,3
AMD,nmgtfp9,"I have an RX570 and it still works fine??? There's only like 2 people in this thread that can read apparently. The cards are still getting updates and big fixes, just not working with game devs pre-release on optimizations, which the game devs should be doing anyway. That and not all new features will be ported to it, but I mean when your card came out FSR wasn't even a rumor, now you have FSR3... people aren't buying these cards for features that haven't even been thought of yet, so nothing is changing for anyone.  This whole ordeal makes me want to mute PC subs, people are too dumb.",pcmasterrace,2025-11-01 01:16:48,1
AMD,nmbxaxk,"Well the drivers have been open source for a while, but only for Linux.",pcmasterrace,2025-10-31 07:03:55,1
AMD,nmfarps,GCN was FineWine because it pioneered GPGPU compute features. With ACE and strong reliable GPGPU performance it keep up with the trend of more compute less pixel blending in latest games.  RDNA back paddle on that front and split from CDNA. This was a direct opposite move from GCN philosophy and this is the result of that.,pcmasterrace,2025-10-31 19:42:07,1
AMD,nm8lkfq,lol,pcmasterrace,2025-10-30 18:41:45,9
AMD,nm8pw9p,No it means that next year they will stop updating it and in 2028 they'll stop everything like on that news the bear minimum lol,pcmasterrace,2025-10-30 19:02:01,7
AMD,nm8nzty,"If this is a pattern, its getting axed from all updates in 2 years.",pcmasterrace,2025-10-30 18:53:04,4
AMD,nm8he5c,The 6800XT was considered one of the best cards available for its generation lol,pcmasterrace,2025-10-30 18:22:13,25
AMD,nm8ijyy,Wrong. Thats a great card and was the price performance king for months,pcmasterrace,2025-10-30 18:27:38,11
AMD,nmfbx96,AMD is at its all time peak now. Dropping support is just an inevitable result for poorly developed hardware.  I predicted that this day would come when I figured out RDNA3 still missing AI matrix execution units that was already on CDNA for years.,pcmasterrace,2025-10-31 19:48:17,1
AMD,nmbbndf,Don’t say that. It’s just RDNA 1/2/3 were bad products that have short lifespans. GCN cards were super long lifespans and RDNA4 is also promising. You should not buy a 6700XT in 2025 when FSR4 was released and praised.,pcmasterrace,2025-10-31 03:47:11,2
AMD,nmafvf0,Did you even read this thread? People with AMD cards are pissed and have likely already lost them as future customers.  I got a 6800XT just 2 years ago and was expecting to keep using it for several years. Cutting support so soon is nonsensical.,pcmasterrace,2025-10-31 00:29:35,2
AMD,nm8gsav,"If it was Nvidia doing it, the sub would be bashing Nvidia. Since it's AMD, this is normal",pcmasterrace,2025-10-30 18:19:23,13
AMD,nm8bytt,That’s not what that phrase means.  Planned obsolescence would be AMD designing them to DIE at some point just outside of warranty. This is simply them not supporting the cards with newer features because they lack the hardware.,pcmasterrace,2025-10-30 17:56:41,5
AMD,nm9gpv7,The card won't get new features or a game optimisation driver anymore just security and maintenance patches,pcmasterrace,2025-10-30 21:12:33,1
AMD,nmgxj4n,It's great news because everyone thinks this will change something and sell them,pcmasterrace,2025-11-01 01:45:09,1
AMD,nmf9zk2,They can’t. As those cards lacking hardware performance for matrix FMA which is a key point in AI era graphics rendering.  They will have to drop support and focus on new cards or they are dragging themselves down and wasting their new hardwares.,pcmasterrace,2025-10-31 19:37:53,1
AMD,nmf9n43,You shouldn’t brought a RDNA2 to begin with. A 3060 would runs faster with DLSS.  Fanboys always laugh at DLSS but when AMD dropped FSR4 they just stopped.  They knew the days for older cards are counted.,pcmasterrace,2025-10-31 19:36:01,2
AMD,nmf95u9,They were never doing good since RDNA1 and this is inevitable. RDNA 1/2/3 was missing important GPGPU compute features that defined GCN as FineWine. They even regressed on int8 support on RDNA1.  I always warn people this day would come when AMD have to make this difficult decision to tell them those card was in fact pretty bad and they have to live on.,pcmasterrace,2025-10-31 19:33:28,1
AMD,nma0awp,At least I got 4 years,pcmasterrace,2025-10-30 23:00:12,1
AMD,nmfbc1d,3080 was a lot faster with DLSS. And it has hardware ray tracing while 6950 only runs them with normal shaders and texture units.  It should be a no brainer back then but thanks to AMD marketing and slow awareness of mainstream media this is the result.  Even today we still get benchmark with native resolution that nobody would use.,pcmasterrace,2025-10-31 19:45:09,2
AMD,nm8myzf,Is this not a bit soon? Like maybe I’m wrong here. I’m not really that well-versed in this world of software updating but aren’t Nvidia still supporting the 20 series which came out around the time of the 5000 series if I remember correctly?,pcmasterrace,2025-10-30 18:48:20,268
AMD,nm8ef5l,"You got down voted, but that link has put me over the limit for sure. On to the 7000 series",pcmasterrace,2025-10-30 18:08:13,36
AMD,nm95auq,"The ""New game support"" is some bunk, because in a technical sense the 6000 hasn't worked for many new games which have Ray Tracing as a requirement vs a little feature you can turn on to increase the power draw on your PSU while also making the FPS drop.",pcmasterrace,2025-10-30 20:16:41,11
AMD,nm8anw0,Features not game support.,pcmasterrace,2025-10-30 17:50:34,26
AMD,nm9fvp4,What the fuck ? All the while the current gen consoles use RDNA 2. What is going on,pcmasterrace,2025-10-30 21:08:20,2
AMD,nmch6mm,are you retarded. It just means its not day 0 support. Its still supported.,pcmasterrace,2025-10-31 10:23:01,1
AMD,nmci8zr,there is little to gain in such mature platform  for games tho however its literally the rest that matters especially now that we know fsr 4 can work on rdna 2 it means that they will never officially bring it and im sure if they list nvidia 3xxx cards as compartible there is going to be a nuclear war,pcmasterrace,2025-10-31 10:32:20,1
AMD,nmbp6gp,"In a vacuum, this might be fine. But AMD aren't alone in selling with GPUs.",pcmasterrace,2025-10-31 05:41:47,3
AMD,nmbh6rx,"Was just discussing this with a friend. Its so recent too, I'm shocked. I was reading about Nvidia updates for their 1000 series bro thats like a decade plus old no? What is AMDs reasoning here  Like these RX cards are the best value for Vram in my opinion, anything 12 or 16 gb is pushing 1000 dollars for Nvidia, but only around 400-600 for a 16 gb 6000 RX Card   Make it make sense",pcmasterrace,2025-10-31 04:30:15,1
AMD,nm8mipg,I don't have one but I could see it being useful for a monitor that has USB-C in using the DP mode. Single small connector with no adapters.,pcmasterrace,2025-10-30 18:46:13,138
AMD,nm8yj0q,"Hijacking to get this information to the most people.  If you are lucky enough to live in a country with 5 year warranty OR if you bought your 5 or 6 series gpu card, maybe even 7900XTX with the usb-c within your country's warranty limit, go ahead and use it, this is NOT what you paid for!  I am lucky enough to live in a country with 5year warranty, if AMD is ending driver support to my XTX within the next 4 years i for sure know the store would have to honour a warranty claim.",pcmasterrace,2025-10-30 19:43:44,70
AMD,nmb9fiu,You would think this would lead to lawsuits since they are removing something people paid for.,pcmasterrace,2025-10-31 03:30:58,4
AMD,nm8kn21,Sooooooo non-issue for everyone except for maybe <1% of RX7900 users?,pcmasterrace,2025-10-30 18:37:26,27
AMD,nm8vdtk,huh. Always wondered why my 2070S FE had a usbc port but newer RTX gens didn't. That explanation makes sense.,pcmasterrace,2025-10-30 19:28:25,2
AMD,nmb494z,It was ahead of its time when unfortunately. Really sad it died.,pcmasterrace,2025-10-31 02:55:35,2
AMD,nmbwzpx,"This actually affects me, I pass a card through to a VM and use the USB port for a USB hub for peripherals.",pcmasterrace,2025-10-31 07:00:43,2
AMD,nm995ok,kinda funny this happens right around the rumor of Valve's new VR headset being green lit,pcmasterrace,2025-10-30 20:35:19,1
AMD,nm9qjbl,Is it known if it also disables the USB-C port on my 6900XT?,pcmasterrace,2025-10-30 22:04:12,1
AMD,nma0nj0,Why do it now that more headsets support it? It probably also works with the Bigscreen Beyonds and the Vive Focus Vision.,pcmasterrace,2025-10-30 23:02:09,1
AMD,nmbrgpj,"Well, it can only be disabled if you keep updating the Radeon drivers.   \*wink\* \*wink\*",pcmasterrace,2025-10-31 06:04:21,1
AMD,nmdnrd8,Oh shit so *thats* what that random C port was lmao,pcmasterrace,2025-10-31 14:46:47,1
AMD,nm8fazh,"you can't flash anything onto the GPUs via the USB-C port. it doesn't ""connect"" to GPU memory it passes through to the motherboard, and it would be useless when the GPU is powered off anyways. you'd still need a reprogrammer that physically connects to the eprom on the GPU PCB to flash unofficial BIOS if the on-board security doesn't reject it. These aren't like USB ports on TVs or your phone that let you flash new firmware.",pcmasterrace,2025-10-30 18:12:23,53
AMD,nm8ncm1,What a stupid decision.. highest market share product no longer supported?? Seems like a stupid gut punch to your consumer.  How do you plan to maintain market share at all,pcmasterrace,2025-10-30 18:50:05,89
AMD,nm8ix53,"Well, it's time to save for a new GPU...",pcmasterrace,2025-10-30 18:29:20,8
AMD,nma9vgv,"This actually does the opposite effect since the customer will think that the GPUs only gets 4 years of driver update now, they will switch to Nvidia for better support and long term usage. Not to mention the 2nd hand market value will drop even further when AMD cards are already selling cheaper on 2nd hand market. AMD actually shot themselves in the foot lol.",pcmasterrace,2025-10-30 23:54:07,2
AMD,nmaok0r,"Looking at the Steam Hardware Survey right now, the Top 10 AMD GPUs right now are:  1. ""AMD Radeon(TM) Graphics"" (2.2%) 2. ""AMD Radeon Graphics"" (1.95%) 3. Radeon RX 6600 (0.92%) 4. Radeon RX 7800 XT (0.72%) 5. Radeon RX 580 (0.66%) 6. Radeon RX 6700 XT (0.65%) 7. Radeon RX 580 2048SP (0.61%) 8. Radeon RX 5700 XT (0.61%) 9. Radeon RX 7900 XTX (0.51%) 10. Radeon RX 570 (0.41%)",pcmasterrace,2025-10-31 01:20:24,1
AMD,nmczwiz,"They are pushing me to upgrade to a newer card, that's right, however after being an AMD GPU fan for so many years (simply because of cost/performance) I will finally be buying my first Nvidia GPU",pcmasterrace,2025-10-31 12:38:28,1
AMD,nmc89vm,"Bought mine 2 years ago and I love it, so this is really annoying. I always favored AMD over Nvidia but after this move, that may change in the future",pcmasterrace,2025-10-31 08:58:06,3
AMD,nmdgjst,"Still a crook move from AMD.  I've seen so many people defending AMD against Nvidia like they were for the gamers, but all in all they're here for the profit just like everyone else, and they're moving in a way they hope people will upgrade their GPUs... I wouldn't with that dick move.",pcmasterrace,2025-10-31 14:09:55,3
AMD,nm8v5jp,"literally just bought a 6800 few months ago, pissed off.",pcmasterrace,2025-10-30 19:27:19,74
AMD,nm8v5nb,It genuinely seems like amd is fine treating their discrete gpus as an afterthought and don't really care about increasing marketshare at all.   They already bungled the whole fake msrp thing with the 9070xt earlier this year.   Might as well just pay the extra Nvidia tax of $50-100 for your card so you know it'll at least get more than 3-4 years of driver support.,pcmasterrace,2025-10-30 19:27:20,32
AMD,nmaewdt,This this this. I bought a bnew 6600 like a fucking idiot 😭,pcmasterrace,2025-10-31 00:23:45,1
AMD,nm8zvci,"It's just that Nvidia still actively supports GTX 900 and 1000 cards. There's still gonna be a handful more gpu drivers supporting them.     But yeah, I doubt these threads will reach 1k upvotes or get any media coverage. The underdog can't possibly be held accountable for anything.",pcmasterrace,2025-10-30 19:50:17,13
AMD,nm8zb2d,didn't you get the memo... come on everyone knows AMD is a small startup indie company. That is not only the underdog but is treated like swine by the big players. I mean Intel/Nvidia are totally out to get AMD and do everything in there power to miss treat and destroy AMD reputation every chance they get.   That is why Nvidia keeps support for there cards for so long. Not because there a good company or care about the consumers (They don't care about the consumers but that isn't the point.). They do it to make AMD look bad...   Poor AMD I hope they survive another ten years off there billions of dollars of profit. It must be hard. /s,pcmasterrace,2025-10-30 19:47:32,28
AMD,nmb3hlm,"The double standards from the tech influencers is really disappointing. It feels like they’re too afraid to upset AMD fans to actually tell the truth.  It’s a real shame, because while AMD makes amazing CPUs, their GPU division feels like an afterthought, yet they get glazed to the high heavens. They’re quick to point out any issues or flaws with Nvidia, yet the AMD side barely gets a whisper in comparison.  What’s funny is that despite their rhetoric, most of them use Nvidia cards in their own PCs.",pcmasterrace,2025-10-31 02:50:45,3
AMD,nmaow0t,"From what I know, Nvidia still has not dropped the support. They will droplater next year. Unlike AMD they just gave a years notice.",pcmasterrace,2025-10-31 01:22:24,2
AMD,nm8xrb5,My decision between the RX 6600 XT and the A770 genuinely landed me with an Intel card that is being supported for longer. What a wacky world.,pcmasterrace,2025-10-30 19:39:57,9
AMD,nm8w5vp,"I was about to buy an Rx 6600 when the last unit sold seconds before I got it, And I ended up with an arc A750 because a 3050 6gb was almost 50 bucks more.",pcmasterrace,2025-10-30 19:32:13,2
AMD,nm93wew,I got mine last year in February...   ![gif](giphy|qQdL532ZANbjy),pcmasterrace,2025-10-30 20:09:55,1
AMD,nm9xgja,mine too. this is a brain dead move.,pcmasterrace,2025-10-30 22:44:04,6
AMD,nmeh9ju,"And the 6750 gre came out in october 2023, so literally 2 years ago",pcmasterrace,2025-10-31 17:11:04,1
AMD,nm9oto4,I got a 6700 XT last month lmao,pcmasterrace,2025-10-30 21:54:35,8
AMD,nm8r0tx,10 was is supported thru this month i think,pcmasterrace,2025-10-30 19:07:26,27
AMD,nm8te63,Well you will be able to play most games but I know some anticheat requirements are newer drivers and who knows if it will allow drivers that are not updated regularly.,pcmasterrace,2025-10-30 19:18:49,12
AMD,nm8xve3,No New features. As in new versions of fsr or future technology. Your card will work fine with driver updates,pcmasterrace,2025-10-30 19:40:31,4
AMD,nm9liwg,"there's no rtx1050, you might be thinking of the GTX 1050, which still had full support up to this month, and will keep getting security updates untill october 2028.",pcmasterrace,2025-10-30 21:37:15,2
AMD,nm9t2n9,">*people play games on rtx1050 still......*  GTX 10 series (including the 1050) is only no longer getting game-ready drivers after October 2025, so technically the GTX 1050 from 2016 was supported by NVIDIA longer than the RX 6800.",pcmasterrace,2025-10-30 22:19:01,2
AMD,nmd9b7z,"but what exactly am I going to do, demand return of my money because driver support is over? I'll wait until I heard of someone actually succeeding doing it before I waste my time arguing and not getting anything of it (likely outcome)",pcmasterrace,2025-10-31 13:31:49,1
AMD,nm8qrgv,"Yeah but I'm not going to buy another GPU when the 6900xt is perfectly fine lol...... not looking to upgrade until after the 9000 series.  Edit: oops, misread that reply",pcmasterrace,2025-10-30 19:06:11,15
AMD,nmbuts3,Doesn’t work in games with anti cheat though right?,pcmasterrace,2025-10-31 06:38:32,1
AMD,nm8pr7h,"Yeah, my 6800XT is 2 years old and already loosing support, I guess I will buy Nvidia in time to play the PC port of GTA 6",pcmasterrace,2025-10-30 19:01:19,19
AMD,nmd0n5p,Good thing it will never be fixed now apparently!,pcmasterrace,2025-10-31 12:42:57,2
AMD,nmb9mzt,Have a peace of mind and go with Nvidia. Go with the 5000 series gpu and pretty sure the driver support will last a good number of years.,pcmasterrace,2025-10-31 03:32:29,5
AMD,nmbb3ro,"Well...maybe the 7000 series will be next. Looks like AMD isn't going to support their GPUs for a longer period anymore. If you want to keep your GPU for more than three years, your only option might be to go for NVidia.  I always used AMD GPUs before but now that they are going in that direction, I might switch for long term peace of mind.",pcmasterrace,2025-10-31 03:43:14,6
AMD,nm8pfgv,"Yeah, after this, I'd rather buy even Intel, at least that is affordable and they have to get competitive on the driver field. I play mostly 5+ year old games anyway but I will buy the RTX 6000 series whenever that comes out",pcmasterrace,2025-10-30 18:59:46,12
AMD,nmv6gsi,mate some of the cards being dropped are 2 years old!,pcmasterrace,2025-11-03 12:03:21,1
AMD,nmcz8z6,It won't stop functioning will it? Just wont be optimized for new games day 1. Seriously doubt it could be a warranty claim. I am in the same boat.,pcmasterrace,2025-10-31 12:34:28,1
AMD,nm9rmqd,"If AMD gets away with this shit, what do we think Nvidia will do next? This is horrible, and people dont seem to get it",pcmasterrace,2025-10-30 22:10:33,6
AMD,nm8myon,"Among other things, they're capable of fast charging. That power draw off a GPU could explain that.",pcmasterrace,2025-10-30 18:48:18,19
AMD,nm8qj9q,The USB C port on my RX6800 drives a couple of daisy chained displays through a simple USB-C to DP adapter.,pcmasterrace,2025-10-30 19:05:05,8
AMD,nmb9cnj,"My thinking was it's at least more logical for the 5700xt since it's six years old and can't play some newer forced RT games like doom dark ages (without linux workarounds).   As others pointed out the last 6000 series release was the 6750gre only two years ago, so that's way more of a kick in the dick.",pcmasterrace,2025-10-31 03:30:23,2
AMD,nmbd0gb,"I know the 5700xt is still usable for 1080p, but there'd at least be some logic to discontinuing drivers for it since it's 6 years old now and games with forced RT like Doom dark ages and Indiana Jones are becoming more common.   Discontinuing support for the 6000 series, where the last new gpu only came out 2 years ago is a serious dick move.   Real shame they can't have Nvidia's track record of long support.  Glad I upgraded from a 6600xt to a rtx 5070 this year.",pcmasterrace,2025-10-31 03:57:16,2
AMD,nm9s8xt,Given how piss poor games are at release it kinda turns them into e waste,pcmasterrace,2025-10-30 22:14:09,19
AMD,nm9rjo0,With modern game optimization... well...,pcmasterrace,2025-10-30 22:10:03,12
AMD,nmh73zo,So they should stay useable for quite a while then! Good to know!,pcmasterrace,2025-11-01 02:52:42,1
AMD,nmbv6ua,"Yes, at least in Linuxland we'll get support until the community is tired of RDNA2 (it won't for like the next 10 years) but it's still the shittiest of moves from AMD.",pcmasterrace,2025-10-31 06:42:11,2
AMD,nmvf49o,"I keep seeing comments like 'they're ending support on a 5 year old line, what did you expect?' and it's driving me crazy.  I purchased this NEW less than 3 years ago.   Apparently AMD have walked back on their decision now but their response doesn't fill me with hope. 6000 series cards will get patches based on market needs, what if they deem the market doesn't need more patches? Seems too vague.",pcmasterrace,2025-11-03 13:02:23,1
AMD,nm8x6sb,"Yeah it's just a shame a number of people don't realize this. This whole ""team red"" was just clever marketing. Nvidia does the same stuff. They are corporations trying to make money, not friends or teams. The only friend and team should be your wallet with whatever product has the value you desire at the time.  Still this driver support, or lack thereof, has certainly made me glad I sold my 7900 XTX.",pcmasterrace,2025-10-30 19:37:12,9
AMD,nmdnu51,Thanks. That makes sense.,pcmasterrace,2025-10-31 14:47:10,1
AMD,nm8rg2w,"Seriously, though! I've been running BF6 on it, and it's doing so good! My only delay is that it takes a bit longer to load assets at the beginning of the round. Otherwise, it keeps up with everything just fine!",pcmasterrace,2025-10-30 19:09:27,5
AMD,nma33di,"There’s a reason they lost pretty much all of ATIs marketshare, which at time of acquisition was around 45%+.   They never had a vision or path, just played catch up and even did that poorly, to this day.   AMD fanboys are some of the loudest but most delusional group I’ve ever seen on the internet.",pcmasterrace,2025-10-30 23:15:49,5
AMD,nm8lwiz,6800xt is is still great mid range card apple to compete with 9070's and 5070's,pcmasterrace,2025-10-30 18:43:20,-1
AMD,nmfe8nu,"They’re at a peak when most their consumer base is at a financial low, whereas nvidia did their shitty business practices during the Biden boom, it’s why so many have a 5090.   While the 6000 series wasn’t the best designed, they’re still pretty powerful gaming wise & with my 6950xt, I’ve been able to run everything modern maxed out (minus RT) at 4K at 144hz, the only drawback is that it’s less heat efficient so needs a window open when playing with settings maxed. Dropping it for the 7000 series is also pretty silly, as iirc the XTX has 24gb & is comparable to the 6950XT while being around $300 cheaper.   These practices could mean AMD has done a bad financial decision, they’re cutting staff or they’ve just thrown a massive investment somewhere & need to raise some quick capital. It’s most likely a mix of 1 & 3, as pre-built pc’s aren’t going to keep shipping cards which no longer have new game support, really bad news for that business sector tbh, which will lead to price increases to cover the sudden new stock costs, meaning less consumers buy them, companies buy less new stock, hurting AMD’s revenue.   It’s just such a silly decision, for something that costs peanuts compared to everything else AMD spaffs money towards",pcmasterrace,2025-10-31 20:00:30,1
AMD,nmbfc33,I wonder what else do I need to upgrade in case I will change to 7 or 9 series,pcmasterrace,2025-10-31 04:15:13,1
AMD,nmbhujn,"Ah yeah, one of two times in the past 15 years AMD has competed with NVIDIA seriously and up to the flagship, and it's a bad product now. RX 6800 and up outperform the 9060 XT, no reason to drop support.  Also, RDNA 4 is a carbon copy of the initial release of RDNA 1, 5700 XT and 5700 | 9070 XT and 9070. 5600 XT and 5600 OEM-only | 9060 XT and 9060 OEM-only.  This is 5 year old hardware that's been sold new as recently as 2024 (which is still put into current gen mobile APUs of 2025) being delegated to security updates the same time NVIDIA is delegating Maxwell and Pascal from 2014 and 2016 to security updates.  If they're gonna drop game-ready support on their users after only a few years, then screw Radeon.",pcmasterrace,2025-10-31 04:35:47,0
AMD,nmbio41,"then dont buy amd ever again, problem solved",pcmasterrace,2025-10-31 04:42:41,1
AMD,nmc40js,yes,pcmasterrace,2025-10-31 08:14:09,1
AMD,nm8gged,But them literally killing a port for no reason is making it obsolete. The guy is right 100%,pcmasterrace,2025-10-30 18:17:49,10
AMD,nm8gs4x,"It is planned obsolescence, just by a different manner/technique. What you described is the traditional way, but if a manufacturer says that we will limit X features only to products A and B, C and D will be shut out, then C and D does become obsolete because if you want those features you will be forced to upgrade.",pcmasterrace,2025-10-30 18:19:21,8
AMD,nm8fjmt,"You can plan for the product to fail by design or action. In this case they planned to make the cards obsolete with an update. Pro Apple move, they have sold me the last chip I will ever buy from them.",pcmasterrace,2025-10-30 18:13:30,8
AMD,nm8fscu,In turn intentionally making them obsolete. It's the same outcome. He's right.,pcmasterrace,2025-10-30 18:14:39,7
AMD,nmgqaz6,"Back then and still now i didn't care for dlss or fsr, all i just wanted was rasterized performance beast as i also did not care for RT, so $100 less for a card that had similar and or better raster performance than the 3080 was a nobrainer for me at least, or well until i discover i get no more gaming drivers support for a card i bought 2.5 years ago 🙃",pcmasterrace,2025-11-01 00:55:42,1
AMD,nm8udmh,Nvidias latest drivers literally still support Maxwell which first released in 2014 with the 750Ti  They are gonna cut off support soon to only 2000 series and up from 2018,pcmasterrace,2025-10-30 19:23:34,168
AMD,nm9l3e9,"Ya like am I just supposed to buy a new card because my 6700xt is 4 years old?!  What the fuck is ""maintenance mode""??   I've been an AMD exclusive PC gamer since 2019 (3x GPUs and 2x CPUs) but if they're gonna fuck my card for no reason I might have to consider Nvidia for my next upgrade.",pcmasterrace,2025-10-30 21:35:02,70
AMD,nm8s6hs,"They're still supporting pascal, but that's ending pretty soon.",pcmasterrace,2025-10-30 19:12:59,25
AMD,nm93i3i,"Does feel early considering the hd 7970 was supported from 2012 to 2022, while never highlighted for new games after like 2016, the updates seemed to add access… to me this points to a radical shift in gpu design coming up, trying to support as few structures as possible",pcmasterrace,2025-10-30 20:07:59,18
AMD,nm8szkg,It doesn't matter too much tbh. I've used drivers from years ago and have no problems. It only matters if there is a bug.,pcmasterrace,2025-10-30 19:16:50,9
AMD,nmbiijy,"not really. 5000 has no RT at all, and 6000 was there first gen so its half baked. The cards will still work but they are quickly falling behind with modern standards,  also people need to learn to read. they are still getting drivers, just not game specific ones. they are both RDNA cards so they will still benefit from the drivers being released.",pcmasterrace,2025-10-31 04:41:23,3
AMD,nma4n2p,"Way too soon, the 980ti I bought in 2015 is only now losing support.",pcmasterrace,2025-10-30 23:24:27,1
AMD,nmdj6gi,"Yes, very early, as the 6750 xt was released 3 years ago",pcmasterrace,2025-10-31 14:23:25,1
AMD,nmizzl9,"Yes and amd stopped updating vega Polaris GPUs 2 years ago (Rx 500 series) which released in 2017,  So they had updates for a bit more than 5 years, Rx 5000 series released in 2019 so they had 6 years of updates, makes sense for both but I'm upset about 6000 series as they're still a popular choice for new builds AND only released in mid 2021, just over 4 years of support feels too low",pcmasterrace,2025-11-01 13:04:16,1
AMD,nm8g36y,"Yeah, I edited my comment to include the link. Some people can't be bothered to do even 30 seconds of double-checking, or just got offended AMD is dropping support for GPUs so fast.   The 7000 series can be good, but I'd be a bit hesitant about those as well. They don't have AI cores, which could give AMD an excuse to drop game support for them in a couple years.",pcmasterrace,2025-10-30 18:16:05,44
AMD,nm99rpm,"It's not getting new game support for two games that DO NOT have RT, Battlefield 6 and Vampire: The Masquerade – Bloodlines 2.   I'm the biggest critic of AMD's approach to RT and half-hearted ""enabling"" it on RDNA 2, but it can at least run it (you can get a good 60fps+ in Doom Dark Ages with some upscaling and lowered settings, even on the lowest RX 6600, for instance). It also supports other DX12 Ultimate features like Mesh Shaders. It's gotten game support for lots of ray tracing games up until now. There's absolutely no reason to drop support for RDNA 2.",pcmasterrace,2025-10-30 20:38:17,12
AMD,nm8cm5x,"From the driver announcement directly: ""New Game Support and Expanded Vulkan Extensions Support is available to Radeon RX 7000 and 9000 series graphics products.""  https://www.techpowerup.com/download/amd-radeon-graphics-drivers/",pcmasterrace,2025-10-30 17:59:43,38
AMD,nm9gsei,"Not just current gen consoles, there's handhelds being manufactured and sold *right now* with RDNA 2 iGPUs, that will presumably lose game-ready support.   The RDNA 2 based PS5 Pro is even getting FSR 4 somehow made to run on the shader cores, while RX 7000 series doesn't get it despite having better AI capability.",pcmasterrace,2025-10-30 21:12:55,3
AMD,nmd7eqc,"Last gaming machine i had was a hp laptop from 2015, i stopped playing games on it before covid pandemic, so I've a huge backlog of games that i want to play, everything from the last decade. So I thought I'll keep my 6700xt for a year to play the older titles and get an 9070xt.    I don't do anything else with my pc other than gaming, some documentation work maybe, so amd made perfect sense, given the price to performance advantage over Nvidia's current gen.    But shit like this will affect my decision.   I was enjoying my 6700xt, performs great at the older titles i play. Amd pulling the plug isn't gonna ruin the gpu but it sure as hell isn't confidence inspiring.",pcmasterrace,2025-10-31 13:21:15,1
AMD,nm8wknj,It was supposed to be a game changer for VR.,pcmasterrace,2025-10-30 19:34:13,73
AMD,nm8v2ot,I'm trying to get all my shit working from a single port. I've tried muxers and other tools and nothing has worked well.   Needless to say I'm bummed.,pcmasterrace,2025-10-30 19:26:56,21
AMD,nm8zoc1,"It's sick for laptops, but that specific use case I feel is less crucial on a desktop card",pcmasterrace,2025-10-30 19:49:19,7
AMD,nm9607e,Are you certain that this is actually possible? This seems like such an edge case.,pcmasterrace,2025-10-30 20:20:04,26
AMD,nmbb11z,"If the warranty laws in your country actually work the way you think it does, then almost every budget Android phone could be returned to the store because driver and OS support ends within a few years. Somehow I doubt that’s the case.   Warranties in most of the world simply mean the product has to continue functioning. There is no requirement for a manufacturer to continue improving the product after selling it to you.",pcmasterrace,2025-10-31 03:42:41,2
AMD,nm8ls19,"its a non-issue for most people but I'd still be pissed if Nvidia released an update that disabled the VL port that I paid for on my 2080Ti for seemingly no reason. this is not a good precedent if its permanent even if it doesn't affect you or most people.  If one day AMD goes ""study shows most people dont use more than two monitors, so we're releasing an update that disables all but 1 of your HDMI and 1 of your Display Ports"" would you still be fine with that since you're not personally affected? ""most people dont use 2< monitors anyways""",pcmasterrace,2025-10-30 18:42:44,131
AMD,nm8oyod,"Those <1% are still customers that are going to be left eating dust, it's still a bad move despite you not being affected.",pcmasterrace,2025-10-30 18:57:36,36
AMD,nm8j06k,The more I know.  What are they for then?,pcmasterrace,2025-10-30 18:29:43,1
AMD,nm8pwb8,I mean they probably put some thought into this but I dont get why. Feels like it would just drive people to nvidia for free,pcmasterrace,2025-10-30 19:02:01,28
AMD,nm96gm5,They don't care they axed all 7000 users from future upscaling updates and theyre still selling the cards... at a time when upscaling is needed now more than ever. I knew my dumbass should of went with nvidia.,pcmasterrace,2025-10-30 20:22:16,3
AMD,nmbqz7l,"Im part of amd Vanguard and they are getting backlash and shit on for this decision even there but its already done, they just repeat this like broken record  “KMD2 based RDNA devices are on maintenance (25.10), legacy is still GCN (23.19)”",pcmasterrace,2025-10-31 05:59:28,1
AMD,nmczq0v,Yeah... I'm pretty pissed. Not sure they AMD thinks I'd ever consider buying another Radeon card after this.,pcmasterrace,2025-10-31 12:37:23,1
AMD,nmep6ld,"99 percent of people don't really care if their GPU gets game specific optimizations until a game actually breaks without it, so it likely won't change much at all",pcmasterrace,2025-10-31 17:50:26,1
AMD,nm91zui,I just got mine this past Wednesday. Smh,pcmasterrace,2025-10-30 20:00:36,30
AMD,nm9ogba,"Mines 3 years old now and the fans are starting to get a little noisy, i'm running more compute workloads than gaming loads now so i looked into getting a waterblock for it but no dice! The gigabyte aorus model doesn't have a compatible waterblock because it's a special board design!",pcmasterrace,2025-10-30 21:52:33,9
AMD,nma6bqp,"It’s not game support it’s feature support. Nvidia 20 series still gets latest DLSS support.   AMD slept while Nvidia laid out the ground for AI, upscaling and RT. Some of us saw this coming long ago because you can’t flip a switch and catch up YEARS of development, not in chip technology. AMD was too busy getting pats on the back from their “Raster  and native resolution only bros” meanwhile Nvidia saw the dead end road and the performance demand that software was going to require years ago. We’re seeing that now. GPUs hitting a wall and games requiring more and more raw power that isn’t there and can’t easily be made to be there.   AMD is just now getting off the ground with RDNA4/5, but that means they need to ditch prior gens because guess what, they can’t support what AMD is doing. They just have to cut their losses and stop trying to throw resources at developing new features for prior gen cards. Nvidia doesn’t have that problem because their prior gen cards support some of the feature they are putting out since the arch was being laid out from 20 series forward.   It was always worth spending $50-$100 more for those features and support. Always. Some of us have said that for years.",pcmasterrace,2025-10-30 23:33:57,21
AMD,nm8w5mc,"Yeah looks like it given Nvidia only just dropped 10 series support for cards first released in 2017, the extra $50-100 buys you real value on longevity it seems.  AMD own goal on market share, just as they we’re getting more support from gamers",pcmasterrace,2025-10-30 19:32:11,16
AMD,nm8zi5t,Wonder how the fine wine folks are going to handle this,pcmasterrace,2025-10-30 19:48:29,5
AMD,nm9074j,"BF6 wouldnt run at all on my driver from last year, it was the only one not causing stutters on my 6700xt. (Common issue)  So not getting updates for new games is going to mean I get to play my backlog!",pcmasterrace,2025-10-30 19:51:52,3
AMD,nmd1iz1,That is awesome to hear since in the past 4 days I have been trying every possible fix on Earth with no avail.,pcmasterrace,2025-10-31 12:48:09,1
AMD,nmbj3s4,I’ll be keeping my 7800XT for as long as she keeps chugging along.,pcmasterrace,2025-10-31 04:46:23,3
AMD,nmewb5a,"Some new games needs an updated driver to work correctly, think crashes, glitches, stuttering and other fun stuff to troubleshoot",pcmasterrace,2025-10-31 18:26:54,2
AMD,nmc1nxl,Nvidia pretty much has a vested interest in keeping their old GPUs alive just to keep as many people on Nvidia/CUDA as possible.,pcmasterrace,2025-10-31 07:49:45,5
AMD,nmbdq3z,"Funnily enough, my parent's PC banck in the time had a Radeon HD 4350 (yeah, back in the 2000s) and the card was supported for around 7 yeras before support was dropped...and those cards were as entry level as you can go.  Seeing high end cards in the RX 6000 series going out of support after half that time is so wild.",pcmasterrace,2025-10-31 04:02:42,2
AMD,nmw0dws,">Apparently AMD have walked back on their decision now  they haven't really, they basically reworded their last statement but the message hasn't changed...",pcmasterrace,2025-11-03 15:01:00,1
AMD,nm8rofc,Nice!,pcmasterrace,2025-10-30 19:10:34,2
AMD,nm8rlx2,"It's literally losing driver support for new releases, how does that make it a great card? There is no sugarcoating it, AMD made a bad decision on this one.",pcmasterrace,2025-10-30 19:10:14,5
AMD,nmd7pxt,RX6800 was around half the performance of RTX3080 with DLSS. It never compete with NVIDIA to begin with.  AMD only back to this market with FSR4 aka performant FP8 matrix FMA starting with RDNA4 RX9070 series.  Nobody wants to play games with blurry and aliasing native resolution today. DLSS/FSR4 is a must have feature.  RX9060XT runs way faster than 6800XT at similar image quality thanks to its FSR4 support. The int8 back port of FSR4 on 6800XT is basically showing you how large the performance gap is.  AMD’s marketing team was trying very hard to let people believe native resolution is the king and even sabotaging gaming releases by not allowing DLSS support in their sponsored titles. That didn’t change the fact that AI based TAAU solutions are an extremely useful feature today and AMD finally have to allocate some hardware resources for it and breaking older card compatibility.,pcmasterrace,2025-10-31 13:23:00,2
AMD,nmc55op,"Nice ninja edit. This is what you originally wrote:   > I have not, i specifically wrote this to piss people like you off",pcmasterrace,2025-10-31 08:26:17,1
AMD,nmhfztb,DLSS is just a part of raster performance. Just like how MSAA was part of raster performance when forward render was mainstream.  You don’t need to care about DLSS but should factor it in when comparing raster performance.,pcmasterrace,2025-11-01 04:00:25,1
AMD,nm8usl1,"Yeah, and like I totally get the criticism and defence here. Nvidia was probably not doing a huge amount of actual optimisation or fixing for these old GPUs but they still got the occasional fix and for some reason AMMD just doesn’t want to commit to that which seems odd to me.",pcmasterrace,2025-10-30 19:25:35,89
AMD,nmatdor,"The 750ti I used for 7 years, sold and replaced with an rx6700, will (theoretically) outlive it. Lovely.  https://preview.redd.it/bj2s3ryzrcyf1.png?width=720&format=png&auto=webp&s=2ca86fbd880c65730f6e8b69c6fb8d7ab8625bdf",pcmasterrace,2025-10-31 01:49:13,57
AMD,nmarw8b,They dropped Maxwell and Pascal recently   RIP my 1060 your days are numbered,pcmasterrace,2025-10-31 01:40:24,1
AMD,nmb4jpo,"Hilarious given the 2000 series can play many titles on ultra, the performance growth since 2000 has been pretty negligible",pcmasterrace,2025-10-31 02:57:29,1
AMD,nmd22xd,They did kill support for 700 series on laptops tho.,pcmasterrace,2025-10-31 12:51:20,1
AMD,nmb79mm,Yeah this is dog act from AMD. I have a 6800,pcmasterrace,2025-10-31 03:15:40,23
AMD,nmbhush,"Here's my thought, after 4 years hasn't AMD gotten as much performance as possible out of their architecture? I've never been on top of updates and I don't think I have ever had a problem playing a game. My current setup is 5600X on Asus B350-i Strix with a 6700XT that I managed to snag at MSRP a few years ago. To be fair the newest game i played on it was Elden Ring.",pcmasterrace,2025-10-31 04:35:51,0
AMD,nm943zq,"I would assume that maybe they’re planning on doing something with Redstone and focusing all of their work on this generation but maybe also the last generation since they got so much criticism for that. If the rumoured FSR 4 for RDNA 3 thing happens to pan out it does make sense that they would do this type of thing and focus their work on the generations that actually have the machine learning hardware to use a modern upscaler, but it’s still just a big shame.",pcmasterrace,2025-10-30 20:10:55,5
AMD,nmgrh2i,"They're still doing bugfixes and optimizing, just not worling with game devs pre-release on optimizing.",pcmasterrace,2025-11-01 01:03:19,1
AMD,nmbx0ij,"Okay, but aren’t there still some games coming out without an RT requirement? Like battlefield six doesn’t have an RT suite and is purely raster. I’m sure these players on 5000 and 6000 series cards really I would like to have bespoke optimisations for their hardware.   Yes they may be getting drivers but games specific drivers does help quite a bit, or at least small bug fixes for new games or even old games that have some sort of a small problem that the developers aren’t fixing but you can fix on a driver level. Especially if you’re using graphics cards that are in a very much clear minority compared to Nvidia.",pcmasterrace,2025-10-31 07:00:56,2
AMD,nm8i5xy,"thanks for the heads up. mind if i pick your brain? whats your thoughts on the reason why they'd disable the USB C port on the 7000 series? i have a 7900xtx, i never used the port, but now i have FOMO lol",pcmasterrace,2025-10-30 18:25:49,4
AMD,nmd256s,Oh I wouldn't dare touch the 7000 or 8000 series at this point. They have been talking about the roadmap to merging datacenter and gaming into one platform. So essentially they are going to be supporting an all new platform of both stuck together for 8000 series (not sure about 7000). I mean if they can't fix drivers for their current series how the hell are the support to support a NEW platform?,pcmasterrace,2025-10-31 12:51:41,1
AMD,nma08hz,"Battlefield 6 plays really well on an RX 6700, it doesn't NEED any extra support to provide a good gaming experience.",pcmasterrace,2025-10-30 22:59:50,7
AMD,nm8g0ia,You can still play new games. They just won't have game specific optimizations in the drivers.,pcmasterrace,2025-10-30 18:15:43,32
AMD,nma4ju4,Can you ELI5 why it would’ve been?,pcmasterrace,2025-10-30 23:23:57,1
AMD,nmbwz9y,"It’d be nice if my Index didn’t need to be plugged in to the wall, too, but oh well. I guess it works well enough.",pcmasterrace,2025-10-31 07:00:35,1
AMD,nmfo0rr,"And it should have been! One cable that did it all. Even without VR, it’s a useful port to have. So much better than an annoying box like some of the older ones (HTC Vive) had.",pcmasterrace,2025-10-31 20:52:56,1
AMD,nm980le,"In my country, Norway? 100% sure.   The rest of the world, you may experience pushback for sure.      Its about what you can resonably expect when buying a product, i can still buy a 6800xt new in a normal shop in Norway, unless they spesifically state that its about to lose support meaning you can expect trouble playing new games then i dont see how they have a case.   In Norway im almost certain they would not get away with only stating it in the EULA for example.",pcmasterrace,2025-10-30 20:29:47,33
AMD,nmchgp3,"In my country the store that sells you the product is responsible for it working as intended for 5 years for both phones and gpu's. So yeah.  Continue functioning as expected when buying it, like working on games you would expect it to work on, including new titles.",pcmasterrace,2025-10-31 10:25:27,1
AMD,nm8ureo,"Actually I'm just guessing that 1% may arguable have right to file a return if within warranty time. In Europe I'm pretty sure some shops would comply with it; you bough it with a functioning USB-C port and it's no longer working? Refund.  I generally like AMD better than Nvidia but fuck removing functionalities and, in this case, very much fuck AMD.",pcmasterrace,2025-10-30 19:25:25,13
AMD,nm8jw2z,It's called a VirtualLink port.,pcmasterrace,2025-10-30 18:33:55,8
AMD,nm8nl82,"To expand a bit to what the other person said, they're virtual-link ports that were meant for VR headsets. The plan was for the headset to just need one cable connection for data transfer and power which is the usb c port on those Gpus. Unfortunately a lot of the headset manufacturers decided to use separate cables for everything combined that with VR not taking off nearly as much as companies thought, the port is being sunset by AMD now. I believe Nvidia stopped even adding them after the 20xx series so it's hasn't been a thing on team green for a while now too.",pcmasterrace,2025-10-30 18:51:10,8
AMD,nm9ai3h,"Driving me back to Nvidia, have a 6750 since 2022 and was just eyeballing 9070xt but not anymore.",pcmasterrace,2025-10-30 20:41:49,8
AMD,nmaesn3,"Lol. Personally won’t go nvidia cuz of the 12vhpwr thing. I probably wont get a tier of card that could burn, but I still wouldn’t go nvidia. It’d be for me to sleep better at night.",pcmasterrace,2025-10-31 00:23:08,2
AMD,nmcy31a,"I do not expect the big corpo to care one bit. Besides, they are trying to follow in the green machine’s footsteps and sell exclusively to data centers…  Just sad.",pcmasterrace,2025-10-31 12:27:12,1
AMD,nmf5273,"It will when it comes to the longevity of a product. If the latest optimized title is BF6, people will not look to buy these new (which you can do still) and especially not Used.   I’d also wager it does more reputation damage long term.",pcmasterrace,2025-10-31 19:11:52,1
AMD,nmb4ich,Return it ASAP,pcmasterrace,2025-10-31 02:57:14,4
AMD,nmcd4wo,I can wholeheartedly recommend just removing the fan shroud and putting regular 120mm case fans on it. I did it with some Arctic P12 Max and couldn't be happier. The card is so quiet now while before that it was the loudest part of the system,pcmasterrace,2025-10-31 09:46:41,2
AMD,nmapq6j,"Hearing people talk about AMD over these last 2 years has been maddening. Don't get me wrong, they make good products. But if you want longevity and halfway decent features, there is no choice. I've been right there saying it too with you.",pcmasterrace,2025-10-31 01:27:27,8
AMD,nmb2s1r,"Yeah, the “raster only, native only” people were insufferably smug and now they have to eat their own words.  It was obvious that physical limits on GPU tech were reaching their zenith. You can only improve things so far with native rendering, and process nodes are about as small as you can get. Graphics are already photo realistic so the easy gains are now longer there.  Now is the real time for raytracing and DLSS tech, and as you said Nvidia has been laying the proper groundwork since the 20 Series. The 30 Series saw a huge step forward, and the 40 and 50 Series have begun to perfect it, making games look and run more fantastic than ever.  I love my 4090, been using it for over 2 years and the technology it has is incredible. I don’t need to worry about it losing support next year because Nvidia actually plans ahead rather than playing tribal politics to score brownie points.  Honestly this debacle shows you why it’s important to properly plan out and develop these technologies with the future in mind. DLSS and RT are the future for graphics technology and Nvidia are miles ahead of the competition.",pcmasterrace,2025-10-31 02:46:16,8
AMD,nm94lzw,Is this the one where you get massive fame stuttering as the game loads new textures?,pcmasterrace,2025-10-30 20:13:20,1
AMD,nm8ywsu,Ooh yeah I misread the reply.   I'm aware if the INT8 option. Just a shame they've dropped RDNA2 like this lol. I do understand it lacks the hardware but it's still a punch to the balls - i could have a 3080 back then but decided AMD because everything else i had was AMD lol.,pcmasterrace,2025-10-30 19:45:35,1
AMD,nmel382,"Good plan, I will keep my RX 5700 XT for now since it works well and I'm not huge into gaming right now so I can do without RT.",pcmasterrace,2025-10-31 17:30:06,1
AMD,nmewlzy,Sounds like my experience now.,pcmasterrace,2025-10-31 18:28:27,1
AMD,nm9c17s,I think he means it *was* a great card up until this short sighted asinine decision.,pcmasterrace,2025-10-30 20:49:14,3
AMD,nmet1x1,I did not write that,pcmasterrace,2025-10-31 18:10:03,1
AMD,nmhic7m,"MSAA is not part of raster performance, what do you mean? It is just an anti-aliazing tool which has its own benefits and performance costs.  DLSS is AI upscale technology, saying it counts towards raster performance is just an oxymoron. It's literally not the original image, but an upscale version of a lower resolution version.  Also on a personal note i fucking hate that even as recently as dlss 4, any game still get blurry whenever there's any amount of movement, of course is it not as noticeable as when it first came out, but it still noticeable enough to annoy the hell out of me whenever I play on a friend's computer and they have it on by default.  If they ever fix blurriness on movement fine i'll use that technology, otherwise i will never take it into account when buying a card.",pcmasterrace,2025-11-01 04:20:13,1
AMD,nmcjid5,Yes. AMD also supported some GPUs really long. But now they make this weird choice. I expected it to be better with more coherent series starting with RX 5000.,pcmasterrace,2025-10-31 10:42:46,1
AMD,nmepkht,"> for some reason      Money. The reason is money. Not worth it for them to spend money to support the <1,000,000 people still rocking a 5000 series.",pcmasterrace,2025-10-31 17:52:22,1
AMD,nmb69ra,It depends on which part of the GPU stack youre looking.  In the past 7 years. The top end GPU has improved on average performance 326%. (5090 vs 2080ti)  If youre talking just RT with new much improved transformer denoiser and upscaling that hits 2000 series hard. the difference is well over 400% or 4x better.  There is a much more than negligible performance increase its just that not all parts of the GPU stack improved at the same rate.  A 2060 vs 5060 is only about 80% better and doesn't come with a massive vram boost like a 5090.  But at the same time 60 class cards have been getting cheaper each gen. Not even including inflation since the 20 series much cheaper including it.  A 2060 launched at 349$ which is 450$ today while a 5060 is 299$,pcmasterrace,2025-10-31 03:08:52,1
AMD,nmbj709,This is telling me to go nividia for my next gpu. So rediculous.,pcmasterrace,2025-10-31 04:47:11,21
AMD,nmbkt2y,"I'm not expecting more performance out of my 6700xt, I'm just expecting it to continue to work the same as it did before they stopped supporting it. Unless I'm misunderstanding what ""maintenance mode"" means because I can't find anything about it.",pcmasterrace,2025-10-31 05:01:16,7
AMD,nm974hs,"it is a real shame, just another thing to hurt their rep",pcmasterrace,2025-10-30 20:25:29,1
AMD,nm8kl3i,May I direct you to: [https://www.reddit.com/r/pcmasterrace/comments/1ok68yj/comment/nm8e32a/](https://www.reddit.com/r/pcmasterrace/comments/1ok68yj/comment/nm8e32a/),pcmasterrace,2025-10-30 18:37:11,5
AMD,nm8lziv,"I honestly have no idea. My initial thought was that there's a problem with it, especially since they directed users to a significantly older driver version (25.3.1 from March 6th 2025). Maybe they found something in newer drivers that could cause crashes or damage? But if that's the case, I would have expected at least a brief explanation in the note (i.e. ""Disabled USB-C power deliver on RX 7900 products, due to to the risk of damage to connected devices"" or something like that.)   I really hope it's not just a decision to disable the feature because they don't want to support it anymore. But unfortunately, that seems to be their motivation for a lot of decisions, lately, like dropping RX 5000 and 6000 game support. AMD has a history of this, they dropped game support for Vega APUs a while back while they were still being actively manufactured and sold. Nvidia, on the other hand, supports their GPUs for far longer. Their latest driver still supports GTX 900 series Maxwell cards, although that's expected to end this month along with GTX 10 series support, probably with one final driver update today or tomorrow.",pcmasterrace,2025-10-30 18:43:44,3
AMD,nm9zlg5,"""...I never used the port"" - that's the reason, right there! It was designed in the hope/expectation that VR would really take off. It didn't and they won't bother ccontinuing to support a feature that nobody uses.",pcmasterrace,2025-10-30 22:56:15,1
AMD,nmb09lt,"But don't you see this is a precedent for the future? It doesn't matter if it runs well rn. BF6 is already well optimized, one of the few games released this year that are. But most games have been horrendous recently. The 6000 series is now effectively obsolete if you want to play newer games.",pcmasterrace,2025-10-31 02:30:49,1
AMD,nm8i3zw,"I mean, they're not going to brick your card so ofc you can still use it to play new games. It just won't have driver support for those games.",pcmasterrace,2025-10-30 18:25:34,32
AMD,nm8n6wt,"This is what I was wondering. I’ve had the 6700xt for 2 years now I think, and was like oh shoot do I need to upgrade already 🙄",pcmasterrace,2025-10-30 18:49:21,3
AMD,nma6lyz,For me it was back when the Oculus Rift came out people had so many problems with the USB and power delivery.  The idea of having both video and power coming from the GPU seemed like the solution we have all been needing.,pcmasterrace,2025-10-30 23:35:32,8
AMD,nmcgwmh,"VR Headsets tend to require power delivery, data AND display all in one cable.  Old school headsets had a thick ass cable like the HTC Vive that usually had 3 cables in one and were expensive to replace and limited by power delivery.  A single cable would have been cheaper, but even the current cables are pricy, as they were with Valve Index which blackboxed the cables into one.  Instead Meta's Quest Series of headsets basically made the requirement for cables moot as the wireless display protocols caught up quickly. While not beating the performance and the clarify of wired play, many more folks preferred it as a convenience and wire-free play for room scale experiences at the cost of requiring headsets to be standalone and have a battery making them unnecessarily heavy.  Modern Ultralight headsets like the Bigscreen Beyond are still however cabled up, but still have ""blackboxes"" that combine everything into one cable, but due to this requirement the everything out of a usb-c  port feature kinda fell on the wayside.",pcmasterrace,2025-10-31 10:20:37,2
AMD,nm988hn,I’m also Norwegian. I really don’t se komplett accepting this as a reason for return. If they do I’m returning my 7900XT and buying a 9070XT instead.,pcmasterrace,2025-10-30 20:30:51,19
AMD,nmdmj06,"RDNA1 already stopped working years ago on new games that require RT and mesh shaders, and if you tried to make a warranty claim because a product that never supported certain features continues to never support them I bet you would be laughed out of the store.  RDNA2 will continue to function on new games for the foreseeable future. Game-ready drivers were never necessary to run new games and in the DX12/Vulkan era their impact is negligible.",pcmasterrace,2025-10-31 14:40:32,1
AMD,nma5og3,"To be fair, I think you can still use it on the 20 series which is different from what AMD is doing here.",pcmasterrace,2025-10-30 23:30:16,1
AMD,nm8wgab,"AFAIK VR works better with nvidia anyways, one person i know ended up selling their 9070xt and going back to nvidia because of complications with VR and simracing",pcmasterrace,2025-10-30 19:33:38,1
AMD,nmd65nw,"Eu tenho uma 6800xt, depois dessa noticia. vou ir para nvidia, nunca mais AMD. minha 6800 tem 4 anos.",pcmasterrace,2025-10-31 13:14:19,2
AMD,nmbg14y,"AMD has always had a VERY loud (but smaller) fan base. It's always been like that ever since I got into PCs back in 90s and it was no different when AMD acquired ATI in 2006. In fact, I'd say it's only gotten louder when it comes to Radeon GPUs, sadly they haven't had much to show for it besides just hating on Nvidia and yelling louder. All they did was enable AMD to become complacent.  Heck just look at this sub or any PC sub here lol (or any influenreviewers on youtube, where most people here get their info from). Still as delusional as ever. There's a difference between supporting a product/company while still being critical of their poor choices and decisions...don't think AMD Radeon division saw that much, even as their market share dwindled from near 50% at acquisition to measily 6% now.  One thing I've noticed is that say 8 out of 10 people buy Nvidia... most of the time the 8 people will install their Nvidia cards and move the F on while the 2 AMD buyers feel the need to get on Reddit and tell the world about how they are now Team Red and how much Nvidia sucks. That's bassically how it's been and why real world markets don't look like Reddit PC subs.",pcmasterrace,2025-10-31 04:20:44,6
AMD,nmbrkq6,The features supported by the rdna4 cards are more than halfway decent. FSR4 is great and the frame gen is fine if that's your thing.,pcmasterrace,2025-10-31 06:05:28,1
AMD,nmb9snd,"Well said.  People love shiting on Jensen and his jackets but the guy's been there since day 1 and when it comes to looking ahead and making the right calls, he's been spot on most every time. Yah he's still a corpo and runs Nvidia like a greedy corporation but you can't deny the fact that not only does he know what he's talking about (as an engineer) he knows what the F he's doing.  AMD can't even figure out how much to sell their cards for lol.",pcmasterrace,2025-10-31 03:33:39,7
AMD,nm954o1,"Yes, but it never goes away. Using an older driver doesnt create the issue though.",pcmasterrace,2025-10-30 20:15:50,1
AMD,nmexjws,"Yeah thats something that comes with pc gaming unfortunately, but without new drivers you may not find a fix, and only being able to really confirm after troubleshooting everything else, good times",pcmasterrace,2025-10-31 18:33:16,2
AMD,nmhpeij,DLSS is AI TAAU and it’s not actually upscaling a lower resolution image into a higher one.  It’s s accumulating pixel data from multiple lower resolution frames and combining them into your native resolution.  The reason this should be counted towards raster performance is simple — you can’t avoid TAA today as no TAA means heavy shimmering and aliasing. And TAA is way more blurrier with or without movements. DLSS is already pretty good in movement and even ultra performance mode is comparable or slightly better than native + TAA at 4k.  MSAA was part of the raster performance due to Xbox 360 have free in-RAM compute 4x MSAA and games artists was designing their game around this. Back in 360 era games start to look more aliasing. You don’t really want to play games without 4xMSAA and 16x AF.,pcmasterrace,2025-11-01 05:26:25,1
AMD,nmb9hcj,"The thing is, most people don't like the newer frame gen. Ray tracing performance improved tangibly but that's about it. I'm talking raw raster performance. I have a 4080 and I still play with RT off when able because the performance penalty is never worth it.  4k performance on the 2060 is only like 5% worse than the 5060. 1080p performance is a starker difference but isn't that much more significant. And comparing the 2080Ti, a 1000 card to a 5090, a card you can't even get for MSRP at 2k, is odd, when it's performance difference in raster is definitely not 326%.  Ultimately my point being that the 2080 Ti especially can play anything including Cyberpunk at ultra (at 1080-1440p) at over 100fps",pcmasterrace,2025-10-31 03:31:20,1
AMD,nm8krcm,thanks bro,pcmasterrace,2025-10-30 18:38:00,1
AMD,nm8ndwo,Probably because it was designed for a standard that never got off the ground. The whole reason it’s there is for powering VR headsets but I think only a handful actually can do everything over USBC and don’t require additional hookups or power.,pcmasterrace,2025-10-30 18:50:14,2
AMD,nm8mqnb,"I hope i dont get jumped on for this, but i dont think AMD's GPU department are in the best position right now.      Regarding the port. i did a little reading between the past and now. apparently it will still have some functionality but the charging function will be disabled. ill link the post i read and you can see for yourself and judge the veracity of the article?  https://videocardz.com/newz/amd-disables-usb-c-power-on-radeon-rx-7900-moves-rdna2-rdna1-gpus-to-sub-branch-in-latest-driver#:\~:text=Someone%20at%20AMD%20apparently%20uploaded,Polaris%20to%20the%20legacy%20branch.",pcmasterrace,2025-10-30 18:47:15,1
AMD,nma2psm,fair play,pcmasterrace,2025-10-30 23:13:43,1
AMD,nmcq6rn,Most of the recent games that have been poorly recieved because of performance issues have been bad on ALL GPUs. Monster Hunter Wilds is a good example. This is because they have been rushed to market rather than properly tested prior to release.,pcmasterrace,2025-10-31 11:34:53,2
AMD,nmbi6sq,"No, you don't have to. You don't ""need"" driver optimizations specifically for every game. The AMD driver will work fine more likely than not. I've gone years before without updating graphics drivers before and still could play what I wanted.",pcmasterrace,2025-10-31 04:38:38,4
AMD,nm9alok,"7900xt have not lost anything as of yet, so no case. But if AMD is not getting enough pushback to change their decision on this moving forward, its only time before both you and me lose support, if that happens within 5 years we have a case indeed.  >I really don’t se komplett accepting this as a reason for return.  I dont see how they cant.",pcmasterrace,2025-10-30 20:42:17,5
AMD,nmjl5e4,"You are kinda missing the point here, Xbox 360 games were built with MSAA in mind sure, but we are not talking about consoles, just because a game's recommended settings have MSAA in mind doesn't mean I have to use MSAA or count how well my GPU works with MSAA.  >DLSS is already pretty good in movement and even ultra performance mode is comparable or slightly better than native + TAA at 4k.  Also hardcore disagree, fine if you are using TAA yes the the image will still look rather blurry, the problem that I have with DLSS is that not only will the image look just as blurry, depending on what the AI decides, it will sometimes remove details from the native image to save on performance. And this is still a thing on DLSS 4 ultra quality.  All other anti-aliasing solutions are just interpolating pixels from the same resolution, having DLSS decide to take pixels from a lower resolution and upscale them is why I do not count it as raster performance.  As long as DLSS keeps affecting the native image quality in favor of performance, it will not count as raster performance. It's not the native image.",pcmasterrace,2025-11-01 15:09:54,1
AMD,nmbblsz,Its is 326%  Thats on the low end actually.   https://www.techpowerup.com/gpu-specs/geforce-rtx-2080-ti.c3305  Techpowerups performance data base uses 1080p for cards slower than the 3080.  And comparing 4k for a 2060 vs 5060 is rather dumb when both will be choked to death with vram constraints and perform the same. At 1080p the intended res the difference isn't massive still but its about 80% faster.  And cyberpunk did launch right after the 20 series launched in 2019? Or 2020. So it makes sense they'd kill that game without using RT.   using RT though. Even a 3090 at 4k doesn't give a good performance with dlss performance.   While a 5090 hits 4k 60 with rt ultra natively,pcmasterrace,2025-10-31 03:46:53,1
AMD,nm8l84z,"Speaking from personal experience (I have a 7900XT and a first gen oculus quest), I never used the port on my GPU, I used my case's front USB-C, and it worked fine for all of my PC VR shenanigans.",pcmasterrace,2025-10-30 18:40:10,1
AMD,nm8zroi,"Sure, but (a) it's still a feature that's being explicitly removed from the card with (as of yet) no explanation, and (b) USB-C power delivery is useful for many things besides VR, such as driving a small portable monitor.",pcmasterrace,2025-10-30 19:49:47,1
AMD,nm8zytz,"Yes, that's what the driver release notes said, it's just affecting the charging functionality. Still, no explanation given is not the way to handle this.",pcmasterrace,2025-10-30 19:50:45,1
AMD,nm9bbej,"Ah I thought you meant the usb c port support. If they enter legacy mode within the next 4 years it’s a win for us no doubt. That being said it’s such a dick move from AMD that id be pushed back to Nvidia, where 6+ years older gpus are still somewhat supported.",pcmasterrace,2025-10-30 20:45:47,8
AMD,nmkcq96,Did I missed something? Why you insist on AI taking pixels for A lower resolution?  It was AI taking pixels from A BUNCH OF lower resolution frames.  It’s usually up to 30 frames that were jittered to get you sub-pixel sample just like how MSAA/SSAA did.  30 frames of 720p jittered gives you way more that native 4k pixel samples.  Btw MSAA was not done at native resolution it was effectively 4x resolution at geometry edges.,pcmasterrace,2025-11-01 17:32:31,1
AMD,nm8lrb5,ah cool big non issue then really i guess? well for most people im sure.,pcmasterrace,2025-10-30 18:42:38,1
AMD,nm9pcmz,I mean I’m not saying removing it is a good thing. I’ve never used the one on my card but I’ve been told they weren’t particularly stable anyway. And also it seems like a pretty rare scenario that you’re running both a high end card like a 7900 XTX and also a portable monitor so if it’s the kind of thing that already isn’t working great I can see how they might just drop support rather than spend time and money keeping it up to date. Losing features is almost always bad but I also understand the idea of pulling support for something that only exists for a very niche section of users. The classic pre-proton “why isn’t this being released on Linux” problem of if you can fit everyone who’s doing the thing into a Best Western conference room it’s probably not cost effective to continue support and not supporting it while still keeping it active on the card is selling people a product that might not be able to do the things they assume it can do. It’s kind of a “no good options” situation.,pcmasterrace,2025-10-30 21:57:30,1
AMD,nm90id7,i agree with that sentiment,pcmasterrace,2025-10-30 19:53:23,1
AMD,nm9coky,"Yeah the usb c port is more of an edge case, i agree. That would be a weaker case, but losing the ability to play newer games, i really cant see how anyone could be refused a warranty case as long as its within the warranty and the consumer protection laws are on par with EU or better.  >That being said it’s such a dick move from AMD  Yeah, im actually fuming here.",pcmasterrace,2025-10-30 20:52:23,4
AMD,nmkg8v0,">Did I missed something? Why you insist on AI taking pixels for A lower resolution?  >It was AI taking pixels from A BUNCH OF lower resolution frames.  Ok, why do you keep correcting that as if it somehow makes it better?  As I said I do not care for neither MSAA or DLSS, I know what they do and how they look, and I do not like it.  Saying I should count them as rasterized performance, because ""you have to run anti-aliasing"" like there aren't other options available, will always be dumb to me.  You are not going to convince me to count DLSS performance as rasterized, as long as DLSS keeps being AI upscaling that changes details of the native image in favor of performance I will not care for it.",pcmasterrace,2025-11-01 17:50:34,1
AMD,nmkmgo6,DLSS AI does not change details more than without. It’s not like it’s generating or hallucinating any detail from lower resolution images.  This is basically MSAA but using AI to save performance. And good luck trying to play games without TAA today.  In most games it’s even forced on due to many graphics FX are sampled using TAA and will break without.  If this is something you cannot turn off then it is a part of raster performance.,pcmasterrace,2025-11-01 18:22:16,1
AMD,nmtvdmg,"looks good to me, but I would keep looking around for deals wherever I can, I would also try grab a crucial p3 over a 2.5 ssd, assuming the kingspec is not an m.2  you can also cut corners getting stuff used",pcmasterrace,2025-11-03 04:36:27,1
AMD,nm9yiz5,Do you have a thermal limit set for the CPU in BIOS? is it downclocking because there's an 80ºC limit on the CPU?,pcmasterrace,2025-10-30 22:50:17,1
AMD,nmxgv2d,What size?,pcmasterrace,2025-11-03 19:14:39,2
AMD,nmxjme6,ATX,pcmasterrace,2025-11-03 19:28:17,1
AMD,nmxmsup,"I mean wattage. Fully modular is better for cable management, is a bit better quality(B+ instead of B) and is 750/850w. Non modular go do to 550w.",pcmasterrace,2025-11-03 19:43:59,1
AMD,nmxooig,"My bad, I thought I added it in the title. It is the 750W version",pcmasterrace,2025-11-03 19:53:14,1
AMD,nmtgebw,this announcement has more words but says exactly the same thing as the previous announcement,AMD,2025-11-03 02:52:56,347
AMD,nmtefeb,AMD one of the only companies whose marketing dept work for the oppostions,AMD,2025-11-03 02:40:27,237
AMD,nmuhvgq,It really does seem like AMD's GPU division exists only to make Nvidia look like it doesn't have a monopoly in the consumer GPU market.,AMD,2025-11-03 08:05:16,35
AMD,nmt7mrd,Most likely because Redstone will only officially come to RDNA3 and RDNA4. official int8 fsr4 support is likely only coming to rdna3 as well.,AMD,2025-11-03 01:59:25,62
AMD,nmswoht,"AMD certainly has a knack for putting out news in the most horrible way possible...  Then needing to do damage control after the fact.  Here is a simple workflow that might help.  1. Write the announcement - do not release it to the public. 2. Privately give it to a few key tech reviewers (under NDA of course). 3. Take the reviewer's feedback. 4. Adjust the phrasing of the announcement. 5. Repeat steps 2, 3 and 4 until the messaging isn't damning. 6. Release the announcement to the public.  I would appreciate it if AMD were to pay me a $1000 consultant fee.  PM me for payment details.",AMD,2025-11-03 00:52:32,329
AMD,nmsysf2,"This ensures they won't be held accountable for not putting fsr4 whatever it's name was into 6000s when the time comes when everyone already had a taste and saw it is indeed possible, way to go amd... If you didn't do this and when the time comes said ""we tried but it is really not performing nicely below this and this GPU so we won't release it for those."" Instead they pull this ahitty move out of nowhere thinking people area dumb enough to not read through the notes.",AMD,2025-11-03 01:05:20,92
AMD,nmtdcg2,Yeah I don’t buy it; the original statements reflects their attitude on pre RDNA4 architectures.  They’re just walking it back to undo the bad press and hopefully wait until people just forget.,AMD,2025-11-03 02:33:42,54
AMD,nmur8oc,"I remember their support real good when they cut from the drivers the R9 390 I had in the middle of GPU crisis where literally NO GPUs (even ""entry level"") were available and R9 390 pulled 60+ FPS on mid/high settings in every game. I mean, nothing says more they care about every radeon gamer.",AMD,2025-11-03 09:45:01,8
AMD,nmt0wa7,"Remains to be seen.  I very much doubt RDNA 1 & 2 will get game optimizations day one for everything as RDNA 3 & 4 does.   Separate code paths sounds maybe good in theory. But what happens when there is a massive conflict in the code? There is no way they will keep back porting every single optimization. Then why separate the code in the first place?  I still think they will stick to the initial announcement, ""by market demand"" which means a few high profile games and that's it.",AMD,2025-11-03 01:18:17,56
AMD,nmt0vbw,Pull the other one AMD.,AMD,2025-11-03 01:18:07,11
AMD,nmswd0k,"* No zero day optimizations for RDNA2/1 * No further general optimizations common with RDNA4+ unless deemed necessary or required * Game optimizations on ""market demand"" basis  \> The difference is that these products now benefit from a dedicated, stable driver branch, one built on years of tuning and optimization.  Lol, lmao even, way to sell the ""you're second grade citizens now"" to all the users of those GPUs.",AMD,2025-11-03 00:50:38,78
AMD,nmu3qwf,"Same stuff, different words. What about continuous Vulkan support or WDDM updates (when MS releases new one)?",AMD,2025-11-03 05:46:45,10
AMD,nmsxvm5,Nothing significant changed from the original announcement.   They’re just rephrasing it to make people think it’s much less relevant than they originally thought.  Doesn’t change anything about the problem itself,AMD,2025-11-03 00:59:45,44
AMD,nmu8bo3,Few more and maybe we'll win something 🤣 AMD has probably the worst PR department in all tech industry 🤡,AMD,2025-11-03 06:29:32,4
AMD,nmubz6u,Except for when you bought a Vega card.. that thing got shit on by AMD in support.. and it still doesn't work properly,AMD,2025-11-03 07:05:02,5
AMD,nmupobh,Give us official support for int8 FSR4 and we'll call it even.,AMD,2025-11-03 09:28:21,7
AMD,nmto8q2,"It's weird that on one hand, the tech media is happy to scream as loudly as possible ""this is a company with a history of terrible communication of changes they plan to make"" but on the other say ""this was a nefarious attempt to stop supporting older GPUs that they've now backtracked on"". Pick a lane, it can't be both.  Given the continual form that they've demonstrated, I'm far more likely to believe that a department with Frank Azor at its head released half baked comms that have done nothing but confuse people about their decision making, rather than attempt to kill off earlier generations of tech.  Maybe a rose tinted perspective, but the degree to which people jumped out a window on this was kind of insane to me.",AMD,2025-11-03 03:45:27,13
AMD,nmttvu1,"It's just lip service. RDNA 1 and 2 are in maintenance mode at this point. They'll get the Polaris/Vega axe soon enough; that's how this goes.  >~~Whether you’re~~ gaming on ~~an RX 5000, RX 6000, or~~ the latest RX ~~9000, you’ll continue to~~ get the reliability, performance, and care you expect  ftfy AMD.  It continues to be astonishing just how bad AMD's messaging can be.",AMD,2025-11-03 04:25:09,18
AMD,nmtnwbo,You'd think they would have not done this based on the b350/450 era mistake,AMD,2025-11-03 03:43:07,9
AMD,nmv6lui,"Business and Marketing schools got another good example how to avoid F ups using AMD case example.    Real life always teaches you something, but for AMD its a pattern.",AMD,2025-11-03 12:04:25,3
AMD,nmwb5ao,I will buy my next GPU based on market needs.,AMD,2025-11-03 15:54:00,3
AMD,nmui0ye,"This doesn't sound as a 'revert' per se, which is quite terrible, because it tells me FSR4 will not be released on RDNA2 whatsoever, although it could technically run it, but on the other hand, how many times do people need to realize AMD simply isn't up to par with nVIDIA when it comes to driver support and forward architecture thinking?  Turing (2018) has dedicated RT/Tensor cores specifically designed for RT/DLSS, whereas RDNA3 (2022 - **4 YEARS LATER**) doesn't, and with that, I think I said it all.  In fact, I'm already sure RDNA 3 and even 4 will have the same faith, as UDNA/RDNA 5 will be a totally different architecture - which, by the way, will be literally a copy of Turing from a core perspective, which was released, once again, in 2018 -- 'Radiance Cores' are the ray-tracing cores, and 'Neural Arrays' are the tensor cores - obviously, much more powerful than Turing's, but you get the point - AMD is about 9 years behind nVIDIA in terms of architecture philosophy at this point.  So do not act surprised that 5700 XT didn't have more value compared to 2070S at 100$ more as some of people thought, there's a reason ""nVIDIA -50$"" doesn't work anymore, those 50$ will bite you in your ass next time and people have woken up.",AMD,2025-11-03 08:06:51,9
AMD,nmt5mfn,RX470 users looking at RX6600 users: First time?,AMD,2025-11-03 01:47:15,6
AMD,nmupjh1,"This is what their communications department should have done in the first place. Using chats/emails with tech media channels to inform the public/customers for the first time should always be the exception. And when eventually done, a confusing short paragraph about any changes affecting customers written by a supposed brand representative should never be an acceptable possibility to be posted.",AMD,2025-11-03 09:26:54,2
AMD,nmtrk9h,"The direct statement released by AMD is a step in the right direction, but the best damage control would be to officially release FSR4 for RDNA2 and RDNA3 since we already know its possible.",AMD,2025-11-03 04:08:15,5
AMD,nmuk7cb,...........yeah I can't wait to get my RTX 5060Ti.   I can't believe I'm going through the same thing with my RX 6600 that I did with my R9 380 ALREADY. They were selling 6000 series GPUs this year.,AMD,2025-11-03 08:29:38,4
AMD,nmu47ag,AMD's post: meaningless word salad. It changes nothing from the original announcement RDNA 1 & 2 are deprecated.,AMD,2025-11-03 05:50:52,4
AMD,nmu9dyi,"AMD : ""we're making sure we don't break rdna1&2 drivers when updating 3&4""   Reddit : ""That's it, I'm buying Nvidia""     (plot twist : most never bought any RDNA gpu anyway)",AMD,2025-11-03 06:39:53,5
AMD,nmuexg0,"People shouldn't care about these game optimizations that nobody (without internal access) can really quantify.  What we can quantify is Vulkan/D3D12 feature support. Features that do not require new hardware. And there, Vega and older have no received anything since they were moved to the ""maintenance"" branch. And the recent driver release only adds `VK_KHR_shader_untyped_pointers` for RDNA3+, despite this being a quality of life extension that all hardware can support.  This means either graphics programmers have to plan around the RNDA2 drivers for the next 3 years and not require anything new, or they only support RDNA3+. Both are bad, and this is only caused by AMD wanting to save money in the wrong place.",AMD,2025-11-03 07:34:54,3
AMD,nmuii88,"Oh great, another gaslighting corporate press release clearing up ""the confusion"" around them.  Whenever these pop up, I know the company got caught with their hand in the jar somehow, I don't even need to know the context.",AMD,2025-11-03 08:11:47,4
AMD,nmtppyj,"Nope, my next card will be nvidia.",AMD,2025-11-03 03:55:32,2
AMD,nmus7ay,Some of y'all are acting like it's the end of the world.,AMD,2025-11-03 09:54:53,1
AMD,nmtv81j,"AMD don't miss an opportunity to miss the opportunity, and never fails to fail. It's impressive.",AMD,2025-11-03 04:35:16,2
AMD,nmuh8is,AI slop more like AyyyI slop lol,AMD,2025-11-03 07:58:42,1
AMD,nmum3a7,Marketing department always makes PR team work harder.,AMD,2025-11-03 08:49:46,1
AMD,nmunnnd,"I think my gut feeling of going with a new 1440p screen over 4k to be able to run everything without any fancy tech was a smart move. just can't trust these guys in either camp.  In true AMD fashion just when they managed to get some mindshare and traction with RDNA4, they manage to destroy that and start below 0 with this idiotic decision Yeah AMD, if I had not bought my 9070 xt two weeks ago and but me buying now, it would be a 5070 Ti. This will hurt their bottom line big time.",AMD,2025-11-03 09:06:19,1
AMD,nmw4lq4,"Amd,  ""chatgpt people are mad because of what we said, can you make word salad and add gaslighting to further confuse people without giving into the demands of GPU support""",AMD,2025-11-03 15:22:15,1
AMD,nmwk629,"Speaking of continued support, which sounds like bullshit, whenever I attempt to update through adrenalin, as of few days ago it no longer offers any optional or recommended driver. A week ago I could still see new and recent drivers but then it suddenly started showing only version 23.x and next morning absolutely nothing, it says that my 2022 edition is up to date.  Whether I'm being given the middle finger for still using windows 10 or just using a graphics card that is too old, nothing about it supports their PR statement. Fuck both microsoft and this other clown company who couldn't release a bug free driver over the last three years. Coincidentally it all overlaps with the release of a driver which includes no mention of windows 10 in the patch notes.",AMD,2025-11-03 16:38:28,1
AMD,nmtdpgx,"I’ll take this with a grain of salt, remains to be seen if RDNA2 will receive the same optimizations as RDNA3 and 4.  I very much doubt it, in fact I believe that probably most drivers releases from now on will focus on RDNA3 and 4, leaving RDNA2 with just crumbs.  I’m planning an upgrade next year and after this I won’t be buying another AMD GPU because I don’t want or need to deal with this type of bullshit.  I will not be buying another newer AMD GPU just for them to try pull this move again after 3-5 years of the GPU lifespan.  Edit: typo",AMD,2025-11-03 02:35:56,1
AMD,nmtpe4v,"The thing that gets me about this entire fiasco is no one can actually articulate what they think they are losing that they would have gained of they put features in a giant bucket. Almost all new features coming out are ray tracing features. AMDs older cars lack many features to make a ton of these features viable. This was explicitly stated at the time of purchase. Many people bring up the fact Nvidia keeps posting their features...yes their rtx technology was always ahead of the curve. If that meant that much to you, you should have bought Nvidia in the first place.",AMD,2025-11-03 03:53:16,1
AMD,nmtz9u6,I'm confused.,AMD,2025-11-03 05:07:29,1
AMD,nmuw172,"So the newly released xbox roh ally, which uses rdna2, wont get proper support? Beyond insane...",AMD,2025-11-03 10:32:48,1
AMD,nmtrptm,"...I still don't know if they walked anything back or they just vomited word salad but didn't do anything.  In any case, this whole controversy isn't going to change my buying habits in any way. This is what happens when there are only 2 (relevant) players in the GPU market. As shitty as this is, I'm not going back to giving money to Nvidia when I upgrade my GPU in the future. (Unless they adopt a more pro-consumer stance, but I highly doubt that will happen any time soon.)   In the aftermath, either nothing changes, or AMD's GPU market share completely collapses, handing Nvidia a monopoly. Great...",AMD,2025-11-03 04:09:20,1
AMD,nmt3tt0,Cool so my smart access graphics laptop basically doesn’t function anymore because the rdna2 IGPU and my rdna 3 gpu don’t share the same driver,AMD,2025-11-03 01:36:19,-8
AMD,nmvhren,"known stable driver series, 25.10.  cone *on*",AMD,2025-11-03 13:18:44,0
AMD,nmty9ms,"Damage is already done, because your intentions were clear and it was only reverted because of the massive backlash. There was absolutely no need to end support for the 6000 series. Maybe you didn’t get away with it now, but later down the road you’ll try again with something else that is equally as scummy, I think in the year of 2025 everyone has been down this road with big corporation or another.",AMD,2025-11-03 04:59:08,-3
AMD,nmtwq75,Was thinking to build an all amd machine due to the sweetspot of price to performance.  But nah.  I'm done.  I can take a gut punch by going with intel. Amd's marketing is so scummy that i had to buy an intel machine just because they can't get their head out of their ass in the nomenclature and selling old chips with new names and the fucking nomenclature changes every quarter.   Why the fuck fo you want to fuck up what's.never broken  Now this bullshit with the gpus. Glad i went with intel+ nvidia on my previous machine. Eventhough i don't like both,AMD,2025-11-03 04:46:49,-7
AMD,nmunv6c,They just asked chatgpt to reformulate it.,AMD,2025-11-03 09:08:34,68
AMD,nmup7hp,There was no need for this one. Everything was clear before but as always some baboons can't read,AMD,2025-11-03 09:23:18,6
AMD,nmueuev,Every IT Department hates marketing/sales team because they dont know shit,AMD,2025-11-03 07:34:01,59
AMD,nmul3ms,Exactly! If only the marketing dept could spin it properly so you guys would actually feel good about losing driver support!  Who cares about GPU?! It's feelings that matter!   Do you listen AMD? Reddit wants you to invest in better feelings. Hire better marketing people now!!!,AMD,2025-11-03 08:39:16,9
AMD,nmujc5s,"There is no opposition. Lisa and Jensen are cousins, and the CEO of Moore Threads is the former Global VP of Nvidia. Why would they compete when working together is so much more profitable?",AMD,2025-11-03 08:20:31,-12
AMD,nmucbzm,"This is IMO also the most likely reason indeed.  The int8 leak was just that, unintended.    I would hazard a **wild guess** that Redstone will only supprt an int8 WMMA version of FSR4. Thus excluding RDNA2.",AMD,2025-11-03 07:08:37,30
AMD,nmth8wl,"Such an insult as a 6950 XT owner who has watched multiple videos from tech channels showing FSR4 INT8 running on RDNA2, and like faster than native, often being about the overhead of XeSS with better visual quality. Hell, people have gotten it working on the STEAM DECK, STEAM DECK, that is much weaker than Navi 21 flagship slicion that has a lot more brute power to push through.  And I'd still like it for native AA too, FSR4 and DLSS4 at native res are still a significant improvement over TAA, in fact that gap is why to many quality upscaling looks just as good as native... because the upscalers have a MUCH better temporal AA algorithm than standard TAA. Giving native resolution that algorithm returns the edge back to native, granted at a performance cost.",AMD,2025-11-03 02:58:28,33
AMD,nmwzr6k,"I am under the impression that the int8 version of fsr4 that was in the leak, is actually the AMD side of the code that went into Playstation's PSSR, that was in part developed on Sony's dime, and AMD may have some contractual obligations to not officially provide it in any form apart from directly to Sony.",AMD,2025-11-03 17:52:13,1
AMD,nmul22d,Why do you think redstone is coming for rdna3?,AMD,2025-11-03 08:38:47,-1
AMD,nmt15fg,this is absolutely how you get leaks lol,AMD,2025-11-03 01:19:50,116
AMD,nmt5b9n,I think you’d get some weird conflicts journalists don’t want from being turned into unpaid PR consultants lol.   Now the reviewers are being accused of being shills when they didn’t predict how something would be received and warn AMD about it etc.,AMD,2025-11-03 01:45:22,15
AMD,nmt8jzx,"Nah, even a company 1% of AMD's scale should have a proper marketing team that know their words. All AMD's announcements in past few years are like a 5 year old kid to say whatever he wants to say.",AMD,2025-11-03 02:04:56,11
AMD,nmtb84i,"That's just PR, the core of the announcement is what shoud matter.  AMD looks like they are walking back a decision at best, at worse, just rephrasing their decision to be a bit more obtuse and hope people forget.  Neither is a great look.  Edit: i just read the clarification, it says very little and doesn't guarantee RDNA1/2 will get the same day 1 support as RDNA3/4 GPUs",AMD,2025-11-03 02:20:52,12
AMD,nmvfbod,Or even just ask some stakeholders internally.,AMD,2025-11-03 13:03:41,3
AMD,nmt80km,tbh i think that was premeditated drama.  this way you gonna have five times more people actually reading these blog posts.  and it worked.... everybody is talking about this topic now and sharing a link to a per se very boring blog post....,AMD,2025-11-03 02:01:41,5
AMD,nmu4qwj,What if I tell you that they have this or similar workflow already and they still fuck up?,AMD,2025-11-03 05:55:57,2
AMD,nmt2rey,"It hurts even more because I bought my 6800 back in mid 2024 as an upgrade to my RX 580, that was back when only the 7000s series was new, and they were still coming out with new 6000s variants as late as 2023, and I bought my card brand new from the factory line (Yes they were still making brand new 6000s cards as late as July 2024.)  Anyways I was pretty hyped to see they were working on a 6000s series capable version of FS4 and got my hopes up, thought AMD was a pretty nice company still, I mean even Nvidia still supports their oldest 2000s cards with the latest DLSS version. But instead of supporting their products and keeping their supporters happy (Only 6% market share BTW) they put the 5000/6000s on maintenance mode LMAO  What a great company that will certainly get my money in the future.",AMD,2025-11-03 01:29:47,55
AMD,nmty33x,They haven't walked anything back. It's a whole bunch of noncommittal and gaslighting.,AMD,2025-11-03 04:57:39,40
AMD,nmtrj0t,Unfortunately I think you're right. We'll have to see how much attention they're actually giving to RDNA 1/2 drivers in the coming months.,AMD,2025-11-03 04:08:01,10
AMD,nmu4i4r,one line in the driver notes interpreted in the worst possible way is gospel and any amount of official clarification about support is fake  y'all are freaking out,AMD,2025-11-03 05:53:42,-9
AMD,nmt4ysn,"This also how they slowly get themselves out of releasing new features for RDNA 2, even if the hardware is compatible with the new feature.  It's still quite lame. Honestly, until the PS6 comes out, RDNA 2 and probably even 1 should continue to get improvements and features, as they arise. Because game developers will continue to be targeting RDNA 2 hardware, in the PS5 and Xbox.",AMD,2025-11-03 01:43:18,41
AMD,nmtossq,You already run things on separate code paths because not all gpu architectures work the same and drivers just make everything look like magic to you.,AMD,2025-11-03 03:49:14,14
AMD,nmt3ni9,"Separating the code makes maintaining and hotfixing easier.  If one of the drivers is broken, it's easier to narrow down the issue, and they can release an update for just the affected hardware.  Not sure about recently, but i know the driver for Avowed had a performance uplift for me on a 6800XT at the time, switched to 9070XTblaunch day, so anything beyond that I can't speak for.",AMD,2025-11-03 01:35:14,17
AMD,nmt04ne,">Our goal is simple: to give every Radeon gamer the best experience possible.  By separating the code paths, our engineers can move faster with new features for RDNA 3 and RDNA 4, while keeping RDNA 1 and RDNA 2 stable and optimized for current and future games.  this is what they wrote in the blog post, don't lie to people because they are one click away from finding this out and you making a clown out of yourself  splitting of drivers was needed, we don't need another catalyst era driver situation where you rolled the dice on what revision was stable for specific application you ran because AMD was supporting a stupendous amount of cards while rolling out new cards and new tech  and not to forget RDNA1 can't benefit from anything new while RDNA2 while it can benefit from FSR4 (this being limited support because only INT8), it can't really benefit from anything newer outside of ray reconstruction which will boost performance but won't fix the fact that ray accelerators suck and that time flies quickly",AMD,2025-11-03 01:13:36,14
AMD,nmt9u3d,The r300 from like 2002 is still in mainline mesa. I do believe you will have many years left.,AMD,2025-11-03 02:12:40,-1
AMD,nmsyxtd,"> The difference is that these products now benefit from a dedicated, stable driver branch, one built on years of tuning and optimization.  This was my main point all along. They already got much faster for free post-release but people seem to want to ignore this fact. Let AMD cook here.",AMD,2025-11-03 01:06:15,-8
AMD,nmtfjm1,"Yeah it's still dumping them to maintaintance mode and pretty much saying RDNA2 isn't getting FSR4 INT8 with PR spin trying to tell you that ACTUALLY this is GOOD for you!   Tf lol, up to this point AMD has been fine adding FSR4 to RDNA4 and just disabling features that can't run on older GPUs while keeping them on the main branch. Nvidia for years and years and years supported GTX 10 and 900 series on the main branch even though yeah they got no DLSS and tensor core features, yet Nvidia didn't split them off until literally last month, AND GTX 16 series, Turing but with the tensor cores ripped out is still supported in the main branch!  And as a 6950XT owner I usually use the latest drivers and haven't had problems, RX 9000 coming didn't suddenly make my drivers start to be less optimised, been as soild as always. In fact a few months ago a driver update actually fixed a major visual issue in Forza Horizon 3, a game from 2016 that's delisted. Whatever driver tweaking had been made fixed this old game's problems that it'd had since at least 2022 apparently. Guessing there can't be a repeat of that with the drivers split, lol.   The Radeon software got updated on the main branch this month and I saw none of it on my 6950 XT, come on, if updates and improvements to the very driver app for my GPU don't even get ported over, what can I actually expect AMD to do? Very little, just the bare minimum that technically counts to fulfil their statement which is probably what their intent is and has been, they just had to concede a millimetre to make it sound nicer.",AMD,2025-11-03 02:47:29,17
AMD,nmuiatt,"I still have PTSD from my Vega 64 days. No wonder that years later, things are still broken.",AMD,2025-11-03 08:09:38,2
AMD,nmu11xc,"It definitely can be both, I'm not sure why it makes you feel that way.",AMD,2025-11-03 05:22:35,3
AMD,nmu4fc7,incoherent analysis,AMD,2025-11-03 05:52:58,2
AMD,nmunr0l,there are different ways of doing the same thing.,AMD,2025-11-03 09:07:20,3
AMD,nmtstub,"If you read between the lines a little, the statement confirms that they're explicitly not going to do that.",AMD,2025-11-03 04:17:21,11
AMD,nmujmj5,"Went from 5700XT -> 6600XT -> 6900XT -> 7900XTX.  Until now i was going to upgrade to whatever GPU they launch, but i think i’m not going to.",AMD,2025-11-03 08:23:30,7
AMD,nmvmc2a,"6800XT user here. Previous GPU was Vega 56 which was discontinued in a similar fashion.  Reddit's reaction is correct. Especially considering there are still new RDNA2 discrete GPUs and products being sold.  As for AMD's statement, it's not the real reason. The real reason is just AMD trying to cut costs and also force people to upgrade.",AMD,2025-11-03 13:45:55,3
AMD,nmvn88y,"People want FSR4 support for 6000 series, at least.   There are unofficial solutions which have proven it's possible, and not that much additional work either  And also same day 1 game support as 7000 and 9000 series  Oh, and people also want to get newer Vulkan extensions for RDNA2 (at least) too, whenever it's possible.",AMD,2025-11-03 13:50:58,1
AMD,nmu0gzg,There's nothing confusing about it. RDNA1 and 2 era cards entered maintenance mode. AMD regurgitated a press release to counter the backlash.,AMD,2025-11-03 05:17:31,6
AMD,nmv3436,sorry where do you read this?,AMD,2025-11-03 11:36:58,2
AMD,nmvni2w,"Yeap, this includes APUs, AMD confirmed that to HUB IIRC",AMD,2025-11-03 13:52:29,1
AMD,nmtugfo,"For all intents and purposes, Nvidia is already a monopoly. You aren't doing shit with a less than 10% market share. That ship sailed a long time ago.",AMD,2025-11-03 04:29:27,8
AMD,nmthgtg,what,AMD,2025-11-03 02:59:54,5
AMD,nmuqch1,As someone who had the pleasure in the past to own such a laptop with 2 AMD/ATI(so old can't remember what it was but I am pretty sure it was called ATI then) GPUs you will have to stay on the older driver. Maybe disabling the older GPU and go full performance with the newer one and latest driver will work. And there is the very rare possibility that AMD manages to make 2 different branches work on the same PC. And for those that say it might work the simple fact of having to use the same file names in the same path disproves this as chances are newer files from the newer branch won't work with the old branch GPU ones and vice versa.  But your fears are real as it has happened again in the past.,AMD,2025-11-03 09:35:35,2
AMD,nmtfloi,Why wouldn’t it work? It will be separate drivers for each.  No different than having a AMD igpu with an Nvidia dGPU,AMD,2025-11-03 02:47:51,3
AMD,nmtg9uo,You do realise GPUs immediately don't stop working if they're not on the latest drivers right?,AMD,2025-11-03 02:52:09,2
AMD,nmu4o7p,Nothing was reverted.,AMD,2025-11-03 05:55:16,14
AMD,nmurpd9,They didn't verify if it was the dopey version of chatgpt either.,AMD,2025-11-03 09:49:47,20
AMD,nmtxyxu,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-03 04:56:42,-10
AMD,nmuwunn,Nah here you hate IT department or whoever made business decision.  They explicitly said new vulkan extensions are coming to RDNA3/RDNA4 and not RDNA1/2.   Basically if game requires new shader model/WDDM/DX12/Vulkan feature you can forget about it working unless thousands of people will scream for AMD to fix something.,AMD,2025-11-03 10:40:41,21
AMD,nmwpdsi,"I see it as less unintended, and more, AMD doesn't want to deal with the official work with RDNA2, so someone ""leaked it"" to give RDNA2 an option without them needing to fix any of the irregularities that might come from it. By officially supporting something, that puts AMD in the stance of also officially fixing its bugs.",AMD,2025-11-03 17:03:15,7
AMD,nmx34w4,yet it works very well on rdna2 at least cyberpunk testing with my weak ass rx6600 it's 10time better than fsr3,AMD,2025-11-03 18:08:11,2
AMD,nmv8m6e,"makes me want to just sell my 6950xt, that I got 2 years ago and get a 50xx. AMD is supoosed have their ""fine wine"" drivers and here we are dumping out grape juice. I also just realised there isnt any BF6 support for 6000 cards either man wtf",AMD,2025-11-03 12:19:05,8
AMD,nmvuvv2,">FSR4 and DLSS4 at native res are still a significant improvement over TAA, in fact that gap is why to many quality upscaling looks just as good as nativ  I can confirm TAA is absolute dogshit and I would rather have jaggies.  But DLSS quality can look better than native because it removes those jaggies without anywhere near as much blurriness as TAA.  I wasn't impressed with older FSR implementations, but what I've seen of FSR4 is pretty promising",AMD,2025-11-03 14:32:28,4
AMD,nmusolc,"Possibly because all that wrongly pushed to github fsr4 stuff was there for a reason and Rdna 3 gpus has those so called ai accel caps, while previous ones doesnt.  Do note that people compiled it managed to get int8 version working but there was also a wmma path that uses fp16, they didnt manage to make it work.  An optimized fp16 wmma fsr version would be more suitable to rdna 3.",AMD,2025-11-03 09:59:38,9
AMD,nmt273w,"Unconfirmed leaks are way less damaging than having a repeated, seemingly uncontrollable foot in mouth syndrome, aka Chronic Radeon Disease (CRD).",AMD,2025-11-03 01:26:18,139
AMD,nmt27b7,leaks are unavoidable. amds poorly worded draft annoucement leaking is less scary to consumers then amds fully published poorly worded annoucement.,AMD,2025-11-03 01:26:20,35
AMD,nmt3f4f,This is not technical details.  This is generic info intended for public consumption.,AMD,2025-11-03 01:33:48,-9
AMD,nn02l4x,Agreed.,AMD,2025-11-04 03:42:36,1
AMD,nmtr5c7,"It's a classic PR statement, i.e. presenting a noncommittal statement in a way that reads as much as possible that they're committing to something.",AMD,2025-11-03 04:05:20,17
AMD,nmu46re,I like how the one line in the release notes about the driver branches is gospel and yet everything afterward is walking back and/or lies.  It's just an insane way to parse anything.,AMD,2025-11-03 05:50:44,-1
AMD,nmut1jo,"lol no it doesn't work, their gaming GPU division is, by far, their weakest division in the company. They keep making these ""intentional PR failures for attention,"" but they're still at their lowest marketshare and revenue in years?  Nvidia straight up sent out broken gpus missing ROPs and unfinished drivers yet they now own over 90% of the discrete market. This was basically the best scenario for radeon to gain market or profit back and instead they are doing worse than they have ever been.  The idea that any attention is good attention is a complete lie.",AMD,2025-11-03 10:03:12,2
AMD,nmwb3f6,Laugh/cry.,AMD,2025-11-03 15:53:46,1
AMD,nmuj4yu,The xbox handheld and legion go 2 base literally just came out with RDNA 2 like a week ago for xbox and a month for legion. using a chip that came within the last few months.,AMD,2025-11-03 08:18:26,8
AMD,nmue0bp,"doubt ill be going amd for my next card again, no wonder they got no market share",AMD,2025-11-03 07:25:18,9
AMD,nmumgds,"the difference between your 6000 series and nvidia's 2000-series is that it has dedicated hardware for ml. not even 7000-series has it, but it does have notably more ai tops, so it doesn't matter as much    amd was late with ml upscaling and they have to make the cutoff for fsr4/redstone support SOMEWHERE   the performance cost/scaling for fsr4 on these graphics cards without dedicated ml hardware is not intuitive and could in some scenarios result in performance loss instead, which is obv really bad for upscaling. they could segment specific cards, but that would look bad to the average user and would likely not include your 6800 anyway   also they are still getting game drivers when relevant, just not day 1 or whatever",AMD,2025-11-03 08:53:32,1
AMD,nmtg52p,What country papi?,AMD,2025-11-03 02:51:18,-16
AMD,nmtylzt,Figured as much. Scummy as hell.,AMD,2025-11-03 05:02:00,11
AMD,nmtsgix,I'll be more worried about RDNA 3/4 drivers if anything considering UDNA looks to be the largest architectural overhaul since GCN. This sets a really bad precedent.,AMD,2025-11-03 04:14:37,14
AMD,nmu8svl,"""We’ve heard your feedback and want to clear up the confusion"" they say in the first line. You'd think if that was the intention they would write as clearly as possible such that there's no room for interpretation.",AMD,2025-11-03 06:34:08,10
AMD,nmu9y9w,"No, the message in the driver notes was a absolutely clear. And so far neither AMD nor clowns like you have been able to provide assurance to enthusiast class consumers.",AMD,2025-11-03 06:45:21,10
AMD,nmw45kk,"Businesses are indeed our best friends! Noncommittal statements from multi-billion dollar organizations are always to be interpreted in the most positive and charitable manner of course. No business has ever left their ""friends"" hanging.   /s",AMD,2025-11-03 15:20:01,2
AMD,nmtdwqa,"Oh yeah this is still how they weasel out of FSR4 INT8 for RDNA2 despite RDNA2 (not 1 tho) supporting INT8 and it literally working, and a performance cost not far from XeSS yet it looks better.   I mean RDNA1 I could see why because it has no DX12 Ultimate, it is an architecture doomed long term when it comes to any game without an older fallback for it which there's already a few of. RDNA2 however does support it, is still current in the consoles and ultimately should still be getting full support for anything that can still physically run on those GPUs, which FSR4 INT8 is one of.",AMD,2025-11-03 02:37:11,19
AMD,nmtzt8v,"While the PS5 and Xbox target RDNA2, the driver model and feature set isn't a perfect match for desktop RDNA2.",AMD,2025-11-03 05:11:58,3
AMD,nmtrsh4,On top of that the drivers regardless of how true have been the major sticking point on people who buy green every time.,AMD,2025-11-03 04:09:51,1
AMD,nmuc99r,"Interesting, but still since they are literally releasing the drivers as separate downloaded files surely there has to be some kind of additional separation.",AMD,2025-11-03 07:07:51,3
AMD,nmt7eh8,"Maintaining and hotfixing might be easier, sure. But what about new things such as new game optimizations? The more the code bases diverge the more work they need to do to back port it over to the old branch.  Consider the scenario where they implement a new optimization for some game in the new branch. And that class or package doesn't even exist in the old branch. Should they then implement the same thing again in the old branch? There is no way in the long term this is feasible.   Or if they find a bug in the new branch and most likely the same bug is in the old branch somewhere in the old code. But the fix is not applicable because the new branch is refactored. Then they have to implement the same thing again. It's just not realistic in my opinion.",AMD,2025-11-03 01:58:02,10
AMD,nmuwnlf,"You don't ""keep"" things optimized for ""future"" games by freezing the codebase and switching the driver branch to maintenance mode.  >and not to forget RDNA1 can't benefit from anything new while RDNA2 while it can benefit from FSR4 (this being limited support because only INT8), it can't really benefit from anything newer outside of ray reconstruction which will boost performance but won't fix the fact that ray accelerators suck and that time flies quickly  This is just nonsense. RT and upscaling are completely irrelevant to the conversation. Day 1 patches mostly focus on raster performance and initial game support to let them run. Both RT and upscaling are pretty much solved issues on both arches: no RT, analytical upscaling. Both arches are also similar enough to benefit from general driver optimizations (which aren't gonna come anymore).  AMD wasn't lying when RDNA came out, fine wine is no more.",AMD,2025-11-03 10:38:51,7
AMD,nmtcwsf,"Linux is entirely irrelevant for this conversation, as yes, it can have longer support for hardware (although not always), and no, people are not going to switch to it solely because day 1 game support has been dropped.",AMD,2025-11-03 02:31:02,16
AMD,nmt67va,"\> Let AMD cook here.  Cook what? RDNA1-2 are done cooking, RDNA3 I assume is on the chopping block next in the very near future.",AMD,2025-11-03 01:50:53,28
AMD,nmt1dse,That’s just the typical AMD for the state of the driver during release. The same excuse for decades at this point.,AMD,2025-11-03 01:21:17,10
AMD,nmz3bth,">RX 9000 coming didn't suddenly make my drivers start to be less optimised  I just wanna say I find this funny, because when 9000 came out there was all sorts of complaining that bugs and optimizations for older cards were getting neglected while 9000 was getting all the attention in its first few months of drivers.",AMD,2025-11-04 00:13:51,1
AMD,nmulv99,Still got it.. custom watercooled yet unstable AF,AMD,2025-11-03 08:47:26,2
AMD,nmw3dc3,"Radeon VII is the posterchild for this.  Got purchased en masse for shitcoin mining, then after it wasn't profitable for mining anymore, AMD dropped support for it's use in ML despite it still being a powerhouse for inference.",AMD,2025-11-03 15:16:07,2
AMD,nmuvvkt,"The closest analog we have to a situation like this from AMD is when they killed off support for Ryzen 5000 on 300 and 400 series boards. [The comms were clear that they were going to do it](https://www.techspot.com/news/85180-amd-axes-zen-3-support-400-series-motherboards.html), people revolted and [the comms were clear when they undid it.](https://www.techspot.com/article/2114-amd-b450-or-b550-motherboard/)  In this case, the comms were so ham fisted that it could legitimately have been argued that game updates were still included (whether you agree with that or not is up to you) and AMD sent out multiple new comms that took the line of ""you misunderstood what we were talking about"" as opposed to ""OK, we've listened"".  At the end of the day, if they wanted to kill these cards off they didn't need a trial balloon to do it as they have form for doing this already. If they wanted to capitulate and undo it, they could have just undid it as opposed to making up some BS about ""well you didn't understand what we meant"". All of their actions point to their comms being shitty as opposed to them deliberately wanting to kill off these updates, up to and including Frank Azor and his team being shown to be continually hopeless at communicating with people.",AMD,2025-11-03 10:31:12,7
AMD,nmtu2y0,"Yes that's probably true. Im just being optimistic they will release FSR4 as a sign of good faith. Again, extreme long shot but you never know.",AMD,2025-11-03 04:26:39,4
AMD,nmv9oc9,"I'm going to sell my 6950 not and get an nvidia card. A 3 year old GPU (only 2 years for me) going into into mainenance mode, and  then I also learned that the latest drivers only have BF6 support for 7000+ series! WTF man. Sick of it.",AMD,2025-11-03 12:26:37,2
AMD,nmum4qr,"Weird upgrades but whatever. By all means, do what you think you have to.   Not sure why anyone thinks nv would do things differently though. Do we actually think it's the same code for the older cards that don't have access to all the software features and the newer ones ?  Honestly I don't understand why they even talked about separating the code base in the first place. If it's still going to receive updates, users don't need to know how you're doing it.",AMD,2025-11-03 08:50:11,-4
AMD,nmwjart,"As for the second one, that should be expected to happen for extensions like mantiencesync2, but not most of the ray tracing.    As for FSR, I agree we should all riot if 6000 series gets nothing. I've been using fsr 4 on 6000 and it works wonderfully, no reason not to give some version to us. This is the only part of this whole thing that makes sense and I 100% agree on, but we don't even know if they are or are not going to give us this feature.",AMD,2025-11-03 16:34:14,1
AMD,nmv3vkh,"on amd site, rdna 2 is in maintenance mode",AMD,2025-11-03 11:43:10,1
AMD,nmuhyve,Tell that to Intel.   And watch this colossal AI stocks bubble detonate in nvidia's face.,AMD,2025-11-03 08:06:14,2
AMD,nmujmil,"Amd has a program for laptops called advantage Edition. These laptops have an AMD APU and GPU and special features to maximize this benefit. Like being able to use eco smart graphics in Adrenalin which shifts the graphics to the IGPU so you can game on the go or just use the IGPU for silent gaming. Since the 680m and the 7700s In the laptop use different drivers, it crashes Adrenaline and refuses to run till i force it to use the dgpu.",AMD,2025-11-03 08:23:30,5
AMD,nmuj3p5,"You can’t install different drivers for each, Adrenalin then crashes when you try to switch to the other depending on which driver package you installed. So features I enjoyed on my 680m or 7700s are no longer able to be used.",AMD,2025-11-03 08:18:03,2
AMD,nmuk3f0,"I know, but what if I want to game on my IGPU and then boot up some BF6 and play on my DGPU? Oh because my IGPU needs a different driver Adrenalin crashes and doesn’t run, and since Adrenalin handles smart access graphics which is AMDs mux switch software, I now have to manually switch between power modes, reboot adrenaline and then game when this was all handled by adrenaline with zero fuss on a product only 2 years old. Sure I can stay on 25.9.2 but I’m missing out on bf6 optimizations.",AMD,2025-11-03 08:28:28,4
AMD,nmtyfvu,"> It's like they took the previous mistakes, poured it into a lawyer, hit blend, and regurgitated it back up. >  >  >  > AMD, it still says ""Frak you for buying our product spend more on Nvidia next time""   There you go little automod I use different words to say the same thing.",AMD,2025-11-03 05:00:34,13
AMD,nmxwj03,> Basically if game requires new shader model/WDDM/DX12/Vulkan feature you can forget about it   \* if you use windows.,AMD,2025-11-03 20:30:54,2
AMD,nmzlap8,"I understand their position but I wish they would still try to release FS4 on RDNA2 and call it ""Community Edition"" or like a permanent ""Alpha/Beta"" version.   So it's there but they won't actively support it.  Full support would be nice but even with no support, it has been proven that it would still work in a majority of cases so it's still a net gain for RDNA2 users.",AMD,2025-11-04 01:58:59,1
AMD,nmw71qn,"This is my own completely subjective review but as far as I can tell FSR4 looks as good as FSR3 at 50% resolution as 3 did at quality, if not better.  I was very impressed by the improvments. I had looked at a lot of youtube videos that compared dlss and fsr3 and at least through youtube compression I could barely tell a different. But running on my own computer the difference between FSR3 and 4 is night and day.   If this is what DLSS has been like for a long while then I now understand why everyone was acting like it was so much better lmao.",AMD,2025-11-03 15:34:11,2
AMD,nmwo516,"Man you're setting yourself up for disappointment. Curb the enthusiasm friend or it might make you an AMD hater.  Edit: Fsr4 int 8 being leaked doesn't make redstone coming to rdna3 more likely, it's a nonsensical argument. Fsr4 is exclusively rdna4 by all official accounts so there is zero reason to believe redstone is coming for rdna3.  It's hopium.",AMD,2025-11-03 16:57:24,1
AMD,nmttvr9,"Now that you put it that way... pushing hype leaks through broken clocks like MLID is a much better marketing strategy than ""make some noise"" and ""poor Volta""...",AMD,2025-11-03 04:25:09,17
AMD,nmt2qvp,"consumers will see the leaks and get immediately confused; tech media won't care because they get multiple bites of the cherry   the solution is a better marketing/pr department, and one that other depts in amd respect and trust enough to listen to, but leaking things is not a viable comms strategy for a multi billion dollar listed entity",AMD,2025-11-03 01:29:42,8
AMD,nmt3k7e,you think people wouldn't leak the news of amd splitting driver support? cmon be serious,AMD,2025-11-03 01:34:41,7
AMD,nmu4nk2,"Not insane, nvidia has one single driver release for all their supported products. Yes, older GPUs may not benefit from all the new features in each driver release, but it's one driver release.   AMD is proposing separate driver releases for products they last refreshed as late as 2 years ago, with many APUs still using those GPUs.  there's zero guarantee they will be supported as well as RDNA 3/4 that is why people parse it differently because AMD is doing damage control and still sticking to their 2 driver strategy.",AMD,2025-11-03 05:55:06,7
AMD,nmw39p8,"> This was basically the best scenario for radeon to gain market or profit back and instead  Pretty sure there's some unwritten rule that RTG or Frank Azor or whoever has to be all ""hold my beer"" whenever Nvidia fumbles the ball. Because like clockwork AMD dunks on their own hoop and starts talking shit while doing it.",AMD,2025-11-03 15:15:36,2
AMD,nmuygdv,"There is more to it.  Thing is AMD said new vulkan extensions are not coming to RDNA 1 or RDNA2.  Meanwhile Nvidia: GTX 960 https://vulkan.gpuinfo.org/displayreport.php?id=43485#formats  RTX 2080ti https://vulkan.gpuinfo.org/displayreport.php?id=43236#formats  And they are identical in format support, vulkan version API support, and core vulkan support.  There is diffrence in extensions but most extensions are optional and obviously no one cares about raytracing really on GTX 960.   What will happen from onwards on, is AMD will NOT update vulkan extensions and likely vulkan/WDDM/DX12 versions/extensions for RDNA1 and RDNA2 unless there is business benefit for them to delegate few engineers to upgrading RDNA1/RDNA2 package to support missing stuff for particular title.  Business benefit = backlash outweighs milking customers moving to newer GPUs. What should tell you backlash per title/game support basis needs to be REALLY strong.  The real mega embarrasing stuff is that Nvidia maxwell is from 2014, and Nvidia pascal is from 2016, and those technically still have feature support (it is ending with end of R580 release driver branch) when RDNA2 doesn't and latest RDNA2 full GPU was released just 2 years ago and iGPUs are still produced today. Meanwhile Nvidia Turing is older than RDNA1 and it is super likely they Turing will enjoy feature support for 2-3 years more.",AMD,2025-11-03 10:55:40,12
AMD,nmufhsv,"Yeah, I was hoping they'd reassure us that this won't repeat with RDNA4, their most popular launch in recent history, when RDNA7/UDNA7/UDNA3 comes out.",AMD,2025-11-03 07:40:45,4
AMD,nmv5uji,"If anything, since they've split into RDNA1+RDNA2 and RDNA3+RDNA4, I'd have to imagine that us 6000 users are dust and getting legacy security type stuff when UDNA comes around, with the R3+R4 goes getting the legacy stuff then.  Which actually will make my 6000 about.. 8 years old at that point? That's not ludicris, and it will still work.   Also, AMD generally promises this shit, but when something actually completely breaks that go back and fix it. Obviously not a guarantee, but it has happened.",AMD,2025-11-03 11:58:36,3
AMD,nmvz4sw,You see: exciting new architectural overhaul  9070XT owners see: looming end of support for this GPU I just bought two months ago.,AMD,2025-11-03 14:54:38,4
AMD,nmud13s,exactly.,AMD,2025-11-03 07:15:31,2
AMD,nmucion,\> > classic case of FUD causes hysteria  \> get called a clown,AMD,2025-11-03 07:10:27,-3
AMD,nmu1r01,"The performance cost for FSR4 INT8 is around 1.5x as large as XeSS for RDNA2. On a sufficiently weak GPU, like a Steam Deck, enabling FSR4 INT8 can be a performance regression due to the long execution time.  As for why RDNA2 and RDNA1 are getting the same driver update time, it's probably due to RDNA2 not making a lot of changes under the hood. RDNA3 added support for BF16, dual-issue FP32, and increases L2/L1/L0 cache sizes",AMD,2025-11-03 05:28:42,7
AMD,nmwy809,"If code is never touched the only difference is how much mb on your computer it is taking up.      if I type ""if architecture\_level < rdna3 do bla or\_else do ha"" you will never run bla and it makes no difference for you, except the fact that this function is slower by having this if statement at all.",AMD,2025-11-03 17:45:07,1
AMD,nmtnreo,"there's no new or old branch, it's just 2 separate driver branches with specific optimisations for the architectures they support",AMD,2025-11-03 03:42:11,0
AMD,nmtgp05,"I run a  bunch of windows vms with amd v340s flashed to vega 56s, games even work on those fine. But I'm hoping vega 56 will stay mainline for a long time not just for games.",AMD,2025-11-03 02:54:52,2
AMD,nmtqbpl,"Considering UDNA seems to be a complete clean slate redesign, even RDNA4 isn't off the hook unless they're planning to pull a Vega with UDNA lol.",AMD,2025-11-03 03:59:42,13
AMD,nmt44xl,"Maybe for RDNA1, but RDNA 2 drivers were great throughout the life of the product, absolutely rock solid, never once had a crash that wasn't related to OC/UV.  I bought based on Launch performance, anything beyond that was a bonus, the 6800XT was good enough in its own right at launch to be worth the asking price.",AMD,2025-11-03 01:38:13,7
AMD,nmzmugx,"Funny indeed, granted you'll notice with every driver post, even with Nvidia the comments are usually reporting problems, not that everything works has it always has because why would someone lol.   Now if it was about existing issues being put on the shelf because new GPUs yeah okay, less attention on them but I didn't run into situations with those I guess. To me drivers were working good as always, and a few months ago Forza Horizon 3 even had a major visual issue fixed sooo yeah. To me been fine on newest drivers, even had a long time problem in an old game fixed.",AMD,2025-11-04 02:07:54,1
AMD,nmv9us6,"Yeah I was vaguely thinking about that one case earlier, thanks for providing the relevant articles documenting this.   I somehow missed those back in the days, it was a good read.  There was no doubt for me the game optimisations were canned for those generations, the phrasing was clear + "" security & bugfixes "" are usually terms I meet on my field that announce a path to EoL or/and maintenance mode.  I agree with you in the end, while you can see from my flair I'm absolutely damned by both of those moves from AMD we talked about.   ( That doesn't mean I wouldn't raise my voice on such news otherwise though )      Back to the earlier point, to me this also means their *crisis management* from their shitty announcement that surprised everyone was even worse, thus the "" you don't understand, you're confused "" part.   They didn't even wait a full 24 hour to provide this.      While we're mostly kept in the fog of war, I think it's fair to get how medias view both point cases as true : terrible communication on axing support and backtracking it on a worst way.",AMD,2025-11-03 12:27:53,1
AMD,nmvdi10,Im sure we will get FSR4 for RDNA3 not for RDNA2,AMD,2025-11-03 12:52:08,3
AMD,nmumnql,"Weird upgrades?   Try to play on 4K Oled with a 6600XT, on some games even 7900XTX is not enough when you crank up graphics settings.  To be honest i dont want my GPU to be EOL after 2 freaking years, even Nvidia is not stupid enough to do that(as much i hate them).",AMD,2025-11-03 08:55:41,2
AMD,nmui731,"> Tell that to Intel.  Intel is in decline, but they still hold a majority of the total CPU market. And that's with almost an entire decade of mismangement and stagnation at this point.  >  And watch this colossal AI stocks bubble detonate in nvidia's face.  Nvidia's dominant market share isn't just limited to AI accelerators, in case you haven't realized by now.",AMD,2025-11-03 08:08:35,2
AMD,nmu21by,"Lol yeah the automod takes any use of ""x you"" as a direct attack at the person you're replying to regardless of context. I've had comments removed for the same reason before.",AMD,2025-11-03 05:31:15,7
AMD,nmwd8hz,At the very least I'll give FSR1 the... it was better than DLSS 1.  The original DLSS implementation was extremely blurry and painful to look at.  Worse than the built in upscaler many games already had at the time.  But ever since 2.0 it's been a very nice to have.,AMD,2025-11-03 16:04:10,2
AMD,nmwsumj,"Im not expecting anything from AMD, i said they might have expected that due to reasons said above.  I know much to not trust AMD on such matters, i use fp8 FSR4 on Rdna3 here already ( Linux )",AMD,2025-11-03 17:19:39,3
AMD,nmtvrot,"I mean, comic book movies have been getting free pr from leaks for years. A rumor here, a photo there, let the internet do what it does, and save a bunch of money on marketing.",AMD,2025-11-03 04:39:25,10
AMD,nmw42gx,"Consumers... what the hell happened to ""customers"".",AMD,2025-11-03 15:19:36,1
AMD,nn03c67,I have no doubt it is entirely possible.,AMD,2025-11-04 03:47:24,1
AMD,nmu52jb,"AMD: ""we are going to support RDNA2""  y'all: ""nuh uh""",AMD,2025-11-03 05:58:58,-3
AMD,nmwxkq1,"A vulkan extension is an extension. AMD never said nothing about dropping core profile. Most extensions get merged to core unless they are random features like ray tracing, which is exactly the new extension stuff they would be referencing.   The RX 580 also supports as far as I'm aware every extension to run vulkan 1.4. 1.4 core features are just guarantees and getting rid of KHR status. All those ""extra bits you wouldn't expect to run on a 960"" are the actual important features. For example nothing that old supports mesh shaders, meaning you wouldn't be able to run games utilizing modern rendering pipelines with no alternatives anyway.   This entire comment is basically framed in a way to make it sound like Nvidia is doing something it's not and AMD is doing something far worse when they are essentially doing the same thing. The only one who is actually really far behind in this scenario is Intel, who have a horrible track record with everything involving this.",AMD,2025-11-03 17:42:06,3
AMD,nmv0rxt,"I can admit some of it is shit (like the vulkan thing you just said) but again, in terms of hardware vs. software, 6000-series is like pascal and 7000-series is like something in between pascal and and turing. with 9000-series, fsr4, and redstone amd is focusing on ml, upscaling, raytracing, ray regeneration, frame generation, etc., and naturally this means that 6000-series and below will be deprioritized. amd would have to do this sooner or later, and this is what people got when they bought a 6000-series graphics card that was only really meant for standard rasterization with no special features  amd is late on the ball with these features, so now they're in a lose/lose situation where people will damn them for not prioritizing 2 gens old hardware or damn them for falling behind nvidia's feature set (this has already been happening though, lol). considering the latter is why practically everyone prefers nvidia and would even pay an extra chunk of money to have, why would amd not focus on the feature set?  yes turing is much older, but it already has the hardware to support the features that are all the rage right now. nvidia planned ahead to allow them to support their hardware for so long, amd didn't and therefore has to make a choice that will upset people either way",AMD,2025-11-03 11:16:52,1
AMD,nmuj0vi,I'm not even looking that far. I'm talking about something as early as UDNA2 or even UDNA1. We already see the segmentation they're attempting between RDNA4 and RDNA3.,AMD,2025-11-03 08:17:14,3
AMD,nmw3l7c,"Vega owners: ""First time?""",AMD,2025-11-03 15:17:13,4
AMD,nmw71mi,"Eh I don't think it's actually a completely new design. It's prolly gonna be a RDNA arch rebalanced for better compute, so basically they're going back to GCN style.  RDNA isn't actually that different from GCN as far as I remember, it's just GCN but rebalanced for graphics. And GCN is still alive and kicking.",AMD,2025-11-03 15:34:10,1
AMD,nmun1vr,"Eol ? How is exactly the same updates as before the split ""eol"" ?  I said ""weird upgrades"" because you've almost been buying one GPU per year in average. Which seems kind of a lot.",AMD,2025-11-03 08:59:48,1
AMD,nmun7kl,"""I dont want my GPU to be EOL after 2 freaking years""  It was never the case, just some random online said that  Apparently 7000 series it might also receive some redstone stuff which was supposed to be an RDNA 4 exclusive",AMD,2025-11-03 09:01:28,0
AMD,nmw9b2j,"> Intel is in decline, but they still hold a majority of the total CPU market.  That wasn't the point being argued, AMD was at 10% market sales share and now is ~1:2 ([as of Aug 2025](https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago)). Being at 10% isn't a no-win position where ""that ship sailed a long time ago"".  And AMD is the very company being asked to do the same change of fortunes again, so they have that experience to build upon rather than just emulate.",AMD,2025-11-03 15:45:12,1
AMD,nmwt2ky,As i said it's nonsense logic.,AMD,2025-11-03 17:20:42,0
AMD,nmxr8t9,"Consumer is just another way to say ""end user""  Customers purchase, but doesn't always use (e.g. person A buys a gift for person B)  Consumers use, but don't always buy (e.g. person B received gift from person A)",AMD,2025-11-03 20:05:36,1
AMD,nmw59ti,"Doing the damage control for a multi-billion dollar corporation, for free. AMD PR guys are proud of you good sir.   GN did a video on this that's really easy to understand. May wanna watch it.",AMD,2025-11-03 15:25:31,2
AMD,nmu9lkh,You know you fucked up when straw man arguments are all you've got left.,AMD,2025-11-03 06:41:57,5
AMD,nmv1jlf,More like putting it on legacy support when cards are still being sold new.,AMD,2025-11-03 11:23:35,1
AMD,nmxpw8c,"Oh there are good example of something Nvidia does that AMD does not. Example if Vulkan video VP9 decode extension. It doesn't exist on RDNA2 products even if DXVA exposes on them VP9 decoding. So if for some reason you open program that do not use DXVA, but will rely on vulkan video decoding, your VP9 video will be slowly decoded by CPU, not fast and efficient by GPU.",AMD,2025-11-03 19:59:08,1
AMD,nmzsbsb,"Assuming the rumors are correct, it's all but confirmed at this point. Being a clean slate redesign I mean.",AMD,2025-11-04 02:39:58,1
AMD,nmzsxpq,"> And AMD is the very company being asked to do the same change of fortunes again, so they have that experience to build upon rather than just emulate.  Build upon what? That's exactly what I'm saying you don't get. AMD clawed their way back from a 10% market share... after years with a superior product and years of their competitor sitting on their laurels and stagnating for almost an entire decade, and *even then*, it's not enough to take the dominant position. The dynamics between AMD and Nvidia right now could not be any more different.",AMD,2025-11-04 02:43:27,1
AMD,nmv4o59,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-03 11:49:33,-2
AMD,nmwa5ul,"I don't need to listen to more hysteria, thanks.",AMD,2025-11-03 15:49:20,0
AMD,nmy2ih5,"That program can still hardware decode vp9 video, just not through vulkan video, for hardware reasons. This has nothing to do with anything happening right now, and has been a common criticism of AMD for years and a big reason why Nvidia was recommended over and over for dealing with video in general. AMD only got their shit together regarding this with RDNA3.",AMD,2025-11-03 20:59:26,2
AMD,nmyhiwt,A patent troll that has never produced a chip with hybrid bonding,AMD,2025-11-03 22:14:20,357
AMD,nmyibmr,So a patent troll who is not making any products sueing. Still I was under impression AMD invented that stuff.,AMD,2025-11-03 22:18:35,181
AMD,nmyiro8,"Isn't AMD using TSMCs hybrid bonding solution?  I get that this is patent trolling but im surprised they didn't go after TSMC, given they are a much bigger company",AMD,2025-11-03 22:20:58,124
AMD,nmymhah,"Most definitely a patent troll.  Their patents read like someone just generated a whole list of semiconductor technologies currently in use and modified them slightly (e.g. by adding ""3D stacked"" to the claim).  Worse yet is that a lot of their ""3D stacked"" patents were filed after AMD released their first 3D stacked CPU.  Hell, they even have recent patents for integrating voltage regulation in semiconductor chips (e.g. patent #11894345 filed November 2022) - something that Intel has been doing for at least a decade now.",AMD,2025-11-03 22:41:12,64
AMD,nmymotr,Here before we found out that Adeia CEO is Userbenchmark girlfriend,AMD,2025-11-03 22:42:19,70
AMD,nmys23v,The patent trolls always come for you when you are making money.  filed where?  U.S. District Court for the Western District of Texas  Only patent trolls file electronic tech patent suits here,AMD,2025-11-03 23:11:27,31
AMD,nmym6xj,"I just want to ask one thing,  Did Adeia produce CPU?",AMD,2025-11-03 22:39:37,34
AMD,nmyjtfa,For duck’s sake I thought we were done with patent trolls.,AMD,2025-11-03 22:26:37,14
AMD,nmypbxm,Hopefully they will lose and receive a heavy legal strike.,AMD,2025-11-03 22:56:39,14
AMD,nmyr8az,Patent trolls = Scum of the earth,AMD,2025-11-03 23:06:58,24
AMD,nmz0gj6,"I'm gonna patent the idea of a better world through evolving, innovating and cooperating.   So when the world becomes slightly better then today, I'm gonna sue some random country that is doing good for breaching my patent.  Probably gonna file it in the U.S. District Court for the Western District of Texas. :P",AMD,2025-11-03 23:57:25,7
AMD,nn05bcj,A dei a,AMD,2025-11-04 04:00:30,1
AMD,nmz3fvs,"Would love to see the EFF get involved, and make the patent troll cry (and give up patent rights once they loose in court).",AMD,2025-11-04 00:14:29,50
AMD,nmypegb,"i mean they probably did, saying that patent trolls “invent” things is a bit of a stretch",AMD,2025-11-03 22:57:02,96
AMD,nmyn48i,I could be wrong but I view a company like TSMC as untouchable. Going after them would likely upset several major governments and who knows how many multi billion/trillion dollar companies.   That be a losing proposition no matter what.,AMD,2025-11-03 22:44:40,86
AMD,nmys8li,TSMC is a company that is outside of US alot harder for them to reach.,AMD,2025-11-03 23:12:24,21
AMD,nmyv2hx,"That is not the same thing. TSMC is basically how to put the silicon on the chip. The patent they claim about is *the idea* of using several layers and how to connect them, and why. Less on the physical how and more on the theoretical how.   TSMC can patent the how, separately from the why.      You know. Like apple can patent the logo of an apple with a bite hole on it, and someone can patent how to grow an actual apple with a hole in it.    The patents date to about 2009-2014 (when some published and later when appointed). They were done by several researchers from cornell and hangzhou universities, who owned the patents via some company they started, and later sold all those patents to adeia (somewhere around 2023).  Also since TSMC runs by different laws than the US, it would make it much harder to go after.",AMD,2025-11-03 23:27:46,6
AMD,nmypg0n,Never.,AMD,2025-11-03 22:57:16,15
AMD,nmyzffi,real issue is system that allows it.,AMD,2025-11-03 23:51:40,17
AMD,nmza821,Bigger scum are the states that allow them to flourish.,AMD,2025-11-04 00:54:06,10
AMD,nmz8j7f,"I just filed a patent for the concept of requiring the intake of some kind of atmospheric gas for life. Yall owe me, pay up.",AMD,2025-11-04 00:44:14,7
AMD,nmznkdq,"Patent trolls regularly go after the biggest corps in the world, AMD is literally a megacorp too",AMD,2025-11-04 02:12:00,8
AMD,nmyoxqa,This got real pirate real fast,AMD,2025-11-03 22:54:31,14
AMD,nmz2ukd,Never ceases to be amazing the patent office granting vague ideas patents. Whole thing is so broken.,AMD,2025-11-04 00:11:06,9
AMD,nmzpqtj,Aren't design patents (which the apple logo would fall under if anything) a whole different animal anyway? Is the logo even patented? Or is it just a registered trademark and/or or copyrighted?,AMD,2025-11-04 02:24:41,3
AMD,nn04w9g,Interesting your mentioned apple since don't they also use the hybrid bonding techniques?,AMD,2025-11-04 03:57:44,1
AMD,nn04wv2,"Yeah but everyone plays that game, Sony patented a way to have attention focused ad displays, will it ever reach market probably not but it prevents others from doing the same or at the very least cutting Sony in on the profit.  If we heard another major company doing this it would be regular business shit, like Broadcom making it impossible to make networking chips without some licensing fees",AMD,2025-11-04 03:57:50,2
AMD,nn05k8q,I just patented the idea of moving around with one or more appendages.. you all better pay me for walking..,AMD,2025-11-04 04:02:07,2
AMD,nn00qze,Depends on how smart (and conscientious) the average patent clerk is. I wouldn't expect them to be Albert Einsteins.,AMD,2025-11-04 03:30:44,2
AMD,nn01y2r,"The Apple logo's IP is primarily protected by both trademark and copyright law.  Design patents are more for things like e.g. the precise shape, arrangement, colours, and look of the camera bump; including where the flashlight is positioned, and where the LIDAR is positioned.  Exactness matters a lot for design patents. For example, if you took the same iPhone 17 camera layout, but made the bump corners more squircle and less rounded; and placed it on the right-hand-side (instead of left); that's probably not enough to infringe a design patent; especially if you use different subtle but noticeably different colours for your materials.",AMD,2025-11-04 03:38:28,1
AMD,nn04os6,I mean it shouldn't take a genius to weed out the patent trolling or pre-existing concepts. Especially since the internet is a thing.,AMD,2025-11-04 03:56:20,3
AMD,nmostjt,Getting one of those at MSRP is a deal. I'm very happy with my Sapphire Pure that I got at UK MSRP.,AMD,2025-11-02 11:41:36,162
AMD,nmovec7,Finally but now look at RAM,AMD,2025-11-02 12:02:57,88
AMD,nmovcl6,Surprised they got to MSRP at all actually. One of the few products anywhere that has dropped in price. Can't even get that with eggs. A for effort.,AMD,2025-11-02 12:02:34,40
AMD,nmq0e7w,"And here, in a third world country, it costs between $850 and $1050 dollars 🤷🏻‍♂️",AMD,2025-11-02 16:04:06,15
AMD,nmotxmq,The asrock challenger 9070XT is at 599€ too in France at Powerlab.fr,AMD,2025-11-02 11:50:59,8
AMD,nmouauf,"lol right after i bought mine of course, thankfully i got the steel legend version only $30 more than what it is right now on newegg",AMD,2025-11-02 11:54:04,9
AMD,nmowsn0,It's been around 600 USD + tax in Norway for a few months now. Glad it's finally reached the US as well,AMD,2025-11-02 12:13:45,6
AMD,nmpbrdd,The only issue now is the money saved is going straight into RAM.,AMD,2025-11-02 13:53:19,4
AMD,nmpr4b9,"Introducing the new RX 10070XT, for $700!",AMD,2025-11-02 15:17:04,3
AMD,nmpusez,Just in time for them to pull driver updates.  /s,AMD,2025-11-02 15:35:50,7
AMD,nmow541,was 3 months ago at 580 euro in europe.,AMD,2025-11-02 12:08:44,5
AMD,nmp7glt,FIVE HUNDRED AND NINETY NINE US DOLLARS!!,AMD,2025-11-02 13:27:01,4
AMD,nmq7w0c,And just 8 months of day 1 driver support left!,AMD,2025-11-02 16:42:20,7
AMD,nmowopf,I bought mine in may for 650€ and with Finland's tax the final price for me was 820€.,AMD,2025-11-02 12:12:57,2
AMD,nmqe1m2,7900 xtx is 700 on eBay why not just get that,AMD,2025-11-02 17:12:39,2
AMD,nmpqig6,"That particular ASRock model might have some issues. I'd wait for Black Friday and get one of the other ones. On those Amazon days a month ago one of higher end PowerColor models was only $620, so there's probably something better coming along.",AMD,2025-11-02 15:13:56,1
AMD,nmq3trd,"I paid around 740 for mine, but I bought mine from Bestbuy.",AMD,2025-11-02 16:21:33,1
AMD,nmqc4ue,I huh guess I shoulda waited lmao /s,AMD,2025-11-02 17:03:19,1
AMD,nmqz6hz,"I paid 620 for an RX9070 OC a couple months ago. Coulda had an XT, lol. I returned that card and just using a 9060 for now.",AMD,2025-11-02 18:54:30,1
AMD,nmr4bgy,only 8 months... who would have thought,AMD,2025-11-02 19:18:31,1
AMD,nmrhvbb,Sapphire Pulse and Powercolor Reaper are now available at MRSP in the UK again as well.,AMD,2025-11-02 20:23:23,1
AMD,nms9vp4,With Asrock still buring MB's how do their GPU's fair?,AMD,2025-11-02 22:44:05,1
AMD,nmtvb72,still mad at newegg for launch day,AMD,2025-11-03 04:35:56,1
AMD,nmuv8gs,Still over $1000AU Way overpriced for such mediocre performance.,AMD,2025-11-03 10:24:53,1
AMD,nmp2kau,I got lucky and got mine at msrp at launch  but dam it was hard,AMD,2025-11-02 12:55:15,1
AMD,nmpll2h,Got mine for $600 eight months ago. Just had to wait in the cold for a couple hours.,AMD,2025-11-02 14:47:55,1
AMD,nmsnwko,Sweet. So it has a good 3 years of driver support left then.,AMD,2025-11-03 00:01:06,1
AMD,nmrai64,Sooooo does this mean the 5090 is coming down in price as well?,AMD,2025-11-02 19:48:24,0
AMD,nmow7wj,And how much was that? I got mine from Amazon.de at 629€.,AMD,2025-11-02 12:09:21,23
AMD,nmqcurz,*Puke,AMD,2025-11-02 17:06:49,-13
AMD,nmoy8er,Yeah I just priced out a build for a coworker Friday and it was $100 for 16GB and that was pretty much the cheapest stock I could find that was decent speed DDR5. I remember a little over a year ago that being like half the price,AMD,2025-11-02 12:24:39,33
AMD,nmprivx,"What happened? Looking at the trends, it seems like prices just skyrocketed over the last month. Stupid AI bubble, just pop already!",AMD,2025-11-02 15:19:07,5
AMD,nmsk2h0,umm just download it???,AMD,2025-11-02 23:40:03,1
AMD,nmrz2zr,Gonna be 10x worse if China invades Taiwan. People aren't ready and can't grasp what would happen.,AMD,2025-11-02 21:45:34,-3
AMD,nmqpy4r,5070 are cheaper and they are being bundled with games so Amd was finally forced to move,AMD,2025-11-02 18:11:08,12
AMD,nmqnt52,"True, would not have believed this to happen right after launch",AMD,2025-11-02 18:00:41,1
AMD,nmri31q,CPUs are edging down too,AMD,2025-11-02 20:24:22,1
AMD,nmtcfro,"I'm not because honestly this headline at least is misleading, the PowerColor Reaper reference spec 9070XTs have been $650 since late summer/early fall (Which is the rumored pre-rebate fiasco MSRP) and $600 for over a month now. Also iirc when they were $650 Microcenter specifically had a deal if you bought any CPU with the $650 PowerColor it got price cut $50 to $600.  Which also Best Buy was price matching Microcenter if you asked and iirc Newegg was coming close to or matching those prices too.",AMD,2025-11-03 02:28:11,1
AMD,nmqo7lv,"I feel you, South America moment.",AMD,2025-11-02 18:02:41,4
AMD,nmq275l,Third world country folks like me: This changes everything! /j,AMD,2025-11-02 16:13:15,2
AMD,nmupaxx,US price doesn't include tax,AMD,2025-11-03 09:24:20,1
AMD,nmouual,"Steel Legend a tiny bit better quality than the Challenger, so yeah, at least some use for the price up xD",AMD,2025-11-02 11:58:30,5
AMD,nmupe5k,"This but unironically. It is older than many RDNA 2 CPus and handhelds. The xbox handheld came out last week, the chip a few months ago, and it is out of support.",AMD,2025-11-03 09:25:17,4
AMD,nmu2ibi,Riiiiidge Racer! Remember that one?,AMD,2025-11-03 05:35:31,2
AMD,nmupgea,"It is older than many RDNA 2 CPus and handhelds. The xbox handheld came out last week, the chip a few months ago, and it is out of support.",AMD,2025-11-03 09:25:57,1
AMD,nmqzivg,Price would need to be equal for 7900XTX to be worth considering.,AMD,2025-11-02 18:56:06,1
AMD,nmquc4b,> That particular ASRock model might have some issues.  How so?,AMD,2025-11-02 18:31:51,2
AMD,nmowxrj,£559 during Prime Day.,AMD,2025-11-02 12:14:49,18
AMD,nmpo45w,599€ Spain.,AMD,2025-11-02 15:01:24,4
AMD,nmu98zk,there was a 8% cash back deal a few weeks back with galaxus. i got the sapphire pure for 607 euros that way. 629 is still a good price imo. a decent 5070ti is like 150-200 euros more in germany,AMD,2025-11-03 06:38:32,1
AMD,nmqkrej,"Yeah, no.",AMD,2025-11-02 17:45:50,1
AMD,nmp8ovk,"Newegg has 1x16gb ddr5 6000hz cl30 ram for $69. Ita the Viper Elite 5 model. Get it while it lasts, or get 2 for 32gb.",AMD,2025-11-02 13:34:44,11
AMD,nmsuyft,yeah RAM prices have almost doubled in the past two months,AMD,2025-11-03 00:42:11,2
AMD,nmq7bvd,"OpenAI announced about a month ago that they would be buying 40% of the entire global output of DRAM over the next year. It's already starting to cause supply shortages. So yes, AI is the cause again.",AMD,2025-11-02 16:39:28,32
AMD,nmsi9ku,If China invades Taiwan the last of your worries will be the price of RAM.,AMD,2025-11-02 23:30:04,14
AMD,nmsns0w,China's not going to invade Taiwan,AMD,2025-11-03 00:00:22,3
AMD,nmscsc3,Most DRAM isn't fabbed or packaged at TSMC though…,AMD,2025-11-02 23:00:00,2
AMD,nmw77dg,And here 19% TAX,AMD,2025-11-03 15:34:57,1
AMD,nmow82t,"Yeah, I’m not too upset about it since steel legend seems like a good card. right now it’s $619 and challenger is $599, I bought it on sale 2 weeks ago for $659, so actually I paid $40 more",AMD,2025-11-02 12:09:23,3
AMD,nmvjhvy,"I would say the XTX needs to be a bit cheaper to consider, since they are close in performance but 9070 takes less power to run and has a longer potential for driver updates",AMD,2025-11-03 13:29:11,0
AMD,nmt35e3,"I don't know exactly, but it was listed as a ""frequently returned item"" on Amazon. That warning shows up on a bunch of other incorrectly collated listings that have multiple graphics cards, but this listing was just the one card. So you know which one it meant. After the AM5 motherboard fiasco, I'm kind of hesitant to recommend ASRock any more, too. It might be fine, but it seems suspicious to me.",AMD,2025-11-03 01:32:09,1
AMD,nmrqydk,link?,AMD,2025-11-02 21:05:49,2
AMD,nmu3q0d,I kick myself for not getting two 16gb sticks a few months ago when they were mid 70s.,AMD,2025-11-03 05:46:31,1
AMD,nmrzb02,Gotta prop up that bubble! So when it does finally burst it's going to wipe out the entire global economy.,AMD,2025-11-02 21:46:42,6
AMD,nmutr8q,"its crazy that all they have to do is announce it when they literally do not have the money and their own projected losses are about six times their revenue, so the only way it would be even possible is if new investment money came in.",AMD,2025-11-03 10:10:19,2
AMD,nmsv52k,"china's playing the very long game, they can wait patiently while the west destroys itself over squabbling and stupid self-destructive behavior, until taiwan's begging to reunify  they aren't going to do something stupid like invade, their industries are already fairly intertwined as it is",AMD,2025-11-03 00:43:18,4
AMD,nmsdi4j,East Asia would essentially be a war zone.,AMD,2025-11-02 23:03:56,2
AMD,nmua6dm,"> After the AM5 motherboard fiasco, I'm kind of hesitant to recommend ASRock any more, too. It might be fine, but it seems suspicious to me.  My ASRock X870 Pro RS WiFi with the 3.10 BIOS is still going strong with a day 2 9800X3D. Has there even been any actual proof that is ASRock's fault and not just AMD? Last I read, the 9800X3D chips have been dying on other vendor boards as well. The ASRock boards seem more ""common"" because they're great value boards and were highly recommended since the 600 series.",AMD,2025-11-03 06:47:30,1
AMD,nmsrz5s,Cheapest right now (via pcpartpicker): Powercolor Reaper at 599.95 eurp + shipping [https://es.pcpartpicker.com/product/8ZJBD3/powercolor-reaper-radeon-rx-9070-xt-16-gb-video-card-rx9070xt-16g-a](https://es.pcpartpicker.com/product/8ZJBD3/powercolor-reaper-radeon-rx-9070-xt-16-gb-video-card-rx9070xt-16g-a),AMD,2025-11-03 00:24:33,2
AMD,nmviylz,"No, just the ones that depend on financial markets",AMD,2025-11-03 13:25:57,2
AMD,nmt6xzl,"yup, we're already speed running towards collapse and China just sitting and enjoying the schadenfreude",AMD,2025-11-03 01:55:18,1
AMD,nmvvk6v,"There's some correlation, but no causation. It could be exactly what you say. Either way, it's best to point out potential issues so anyone going into a purchase reads up on it and has the details. I made sure to check the SoC voltage in BIOS settings even though my MB is MSI.",AMD,2025-11-03 14:36:06,1
AMD,nmdsrmd,>Loyal Radeon Customers  https://i.redd.it/0tuz50cnrgyf1.gif,AMD,2025-10-31 15:11:52,78
AMD,nmcfsr6,They are cutting support for a product they are actively selling SECOND time in a row. First Vega now RDNA what they are thinking about. Why billion dollars worth company cant handle to make good software for every product they sold?,AMD,2025-10-31 10:10:51,491
AMD,nmc4fs5,"My god, the consoles use this tech, it should be at close to no cost to support it. This is beyond belief",AMD,2025-10-31 08:18:40,302
AMD,nmcjmo9,"Massive mistake on AMD’s part here. It reminds me of how Apple supports many of its older iPhones with major iOS updates for around seven years, and Android makers followed suit. The difference here is that I would have expected Nvidia to make a move like this first, while AMD tried to gain market share - but instead AMD has practically shot itself in the foot. I bought an RX 6800 brand-new about four years ago, and now it’s been relegated to “maintenance mode” rather than being treated as a full-supported card. That’s ridiculous. Nvidia might very well be back on the cards for my next GPU.",AMD,2025-10-31 10:43:45,132
AMD,nmcn2g3,Feels masochistic being a long term buyer of AMD products.,AMD,2025-10-31 11:11:11,171
AMD,nmcox59,"My 5500xt was released practically at the dawn of 2020, I got it that summer. Getting a 5 yo GPU in maintenance is not what I was expecting when I gave my money once again in AMD.  I am here since it was ATi with the AGP 9600... never before felt more abandoned.  Adding insult to injury I bought a laptop at 2023 with R3 5425u with vega graphics too, I barely have seen 2 driver updates from AMD for that (I use the amd drivers not the laptop maker ones).  What is beyond me is dropping support for a 4 years old gpu like the 6600 and even worst the RDNA 2 APUs that are even in just released handhelds...",AMD,2025-10-31 11:25:23,38
AMD,nmcp8o2,Would actually be interesting to see some stats on how much have day-1 optimizations helped older archs in the last year or two,AMD,2025-10-31 11:27:50,34
AMD,nme44b7,I was about to buy an MSI RX 6700 XT Gaming X. Thank you for saving my money.,AMD,2025-10-31 16:06:32,12
AMD,nmc6xh4,"Now remember everyone , resale value is already low of amd gpus but don't forget amd will take good driver support away from you just after 4-5 years even for gpus which still get sold new.  When you get a new gpu consider this.",AMD,2025-10-31 08:44:42,182
AMD,nme4yvq,I went into a Bestbuy this year that still had a RX580 in stock and new.,AMD,2025-10-31 16:10:40,9
AMD,nmcne2j,AMD just can't stop shooting themself in the foot.,AMD,2025-10-31 11:13:41,48
AMD,nmdnng1,This is BS my 6950xt is like 3.5 years old…,AMD,2025-10-31 14:46:14,16
AMD,nmg4w01,"I'll never understand it. These cards are only 5/6 years old. Nvidia supported the 1080ti till like, this year didn't they? 6800 XT are still extremely viable.",AMD,2025-10-31 22:33:47,9
AMD,nmefvnc,"As a 6800XT owner who has owned 5700xt/V56/r390/380 before, this seriously makes me consider Nvidia. I was thinking of buying 9070XT but cause they are moving on to UDNA makes me think in 4 years they will throw the driver support in the bin for that aswell.",AMD,2025-10-31 17:04:15,17
AMD,nmccnj1,Absolutely unacceptable and baffling decision.,AMD,2025-10-31 09:42:08,28
AMD,nmcausi,"“finewine” nonsense is hopefully not parroted anymore after this, it was never true to begin with.",AMD,2025-10-31 09:24:10,80
AMD,nmc781z,I was planning on waiting for CDNA to upgrade my RX 6600 XT but now i guess I'm getting a RTX 5070 TI.,AMD,2025-10-31 08:47:38,75
AMD,nmctvxq,rdna2/ps5 already legacy 🤣,AMD,2025-10-31 12:00:16,11
AMD,nmcxrb3,I really hope they go back on this. I'm personally on RDNA3 but all my buddies are still on RDNA2. AMD needs to continue building brand loyalty if they ever want to chip away at some of the market share.   Like why would someone who just got screwed by AMD turn around and buy another AMD GPU?,AMD,2025-10-31 12:25:10,16
AMD,nmcd2yj,"Remember everyone, never be a ""White Knight"" for anything especially Corpos. Just like what Geralt said...      ""Evil is evil... lesser, greater, middling. It's all the same. If I have to choose between one evil and another, then I prefer not to choose at all.""      Corpos will never care about you, NVIDIA, AMD, or Intel. They are all the same. Always priority who offers the best value within your range - that's it.",AMD,2025-10-31 09:46:11,49
AMD,nmcfs64,"""Into the trash, you all go.""    ......BUT IT WILL HAPPEN TO YOU!",AMD,2025-10-31 10:10:43,10
AMD,nmdrjkz,Driver support should last at least as long as a console generation,AMD,2025-10-31 15:05:45,4
AMD,nmd5sac,"I would love for AMD to show how much extra it would actually cost to ""support"" 6000 cards",AMD,2025-10-31 13:12:13,9
AMD,nmdduiy,This sucks because I can't afford a 9070 so I have to get a 9060XT which is barely even an upgrade from a 6800,AMD,2025-10-31 13:56:00,5
AMD,nmdiguq,Wait.. What? i bough 6700 nonXT 2 years ago new.  WTF,AMD,2025-10-31 14:19:47,4
AMD,nmchh3q,I'll not be buying AMD GPUs anymore after this.,AMD,2025-10-31 10:25:33,51
AMD,nmcrauj,![gif](giphy|ktcUyw6mBlMVa),AMD,2025-10-31 11:42:49,14
AMD,nmd8u0z,"This is so stupid. I can understand them dropping support ro RDNA 1 seeing as it doesnt support ray tracing or mesh shaders, but RDNA 2 does, and its the same hardware thats being used in the current gen Sony and Microsoft consoles.",AMD,2025-10-31 13:29:09,11
AMD,nmc6z73,"""""loyal customers"" lol",AMD,2025-10-31 08:45:12,22
AMD,nmcrlms,Funny how AMD drops support for half of it's 5000 people strong user base,AMD,2025-10-31 11:44:50,7
AMD,nmc61x7,"These cards will still work perfectly fine for many years, and most users will be blissfully unaware as long as the games work.  P. S. They are still being actively maintained and improved on the Linux side, by AMD engineers no less. They're even implementing ray tracing support on Linux.",AMD,2025-10-31 08:35:41,66
AMD,nmcnhhd,I was looking at 9070xt but now i can't compromise on the longevity also.  Not to mention other compromise like pytorch support.,AMD,2025-10-31 11:14:24,15
AMD,nmchm1r,"It is always hard, especially for us as fans and users (maybe for a long time), that AMD is not our friend and it is also not the poor underdog.   Yes, I am also very disappointed in them (again).",AMD,2025-10-31 10:26:45,7
AMD,nmdnlgy,I got lucky and upgraded my mid-2022 6700xt to a 7090xt last week. Sucks for the guy who picked up my 'old' card. I feel bad for him. I think this will be my last AMD. What a shitty decision. Shame on AMD.,AMD,2025-10-31 14:45:57,3
AMD,nme3fl1,Just bought my 6600 xt this year new. Oh well I guess.,AMD,2025-10-31 16:03:12,3
AMD,nmetfnq,WHAT  They did NOT just drop my 6700XT!!!!????  I haven’t even had mine for 3 years!,AMD,2025-10-31 18:11:59,3
AMD,nmdpb49,Unacceptable. Lets hope this decision will be reversed.,AMD,2025-10-31 14:54:33,5
AMD,nmgpc1h,"Except... it's game specific optimisation patches? They are still developing and updating the drivers, and frankly with the state of game optimisation at the moment, I think them not wanting to waste dev time on bringing the latest blurry TAA slop from 8 to 9 FPS is fine. Especially since a lot of RDNA/2 cards are under VRAM budget for the games that benefit from these kinds of patches anyway. Load of whining over nothing.",AMD,2025-11-01 00:49:15,4
AMD,nmde7xc,"I don't have an AMD GPU so I've no dog in this fight, but is it not a bit soon to be making this video?   ""We have received no response"" could they not have waited a few more days for a response before getting people riled up over what could potentially be a misunderstanding? The article is from YESTERDAY and the driver notes from 2 days ago.   I really like everything HWUB do but this could end up being a misunderstanding/mistake and they may be causing a furor over nothing, not to mention potentially falsely accusing AMD of something.   It's likely to be the case that AMD are being scumbags but surely a few more days couldn't have hurt to be certain before attacking them in a video.",AMD,2025-10-31 13:57:55,8
AMD,nmcnpqv,"told yall, amd not releasing fsr4 for rdna 3 or earlier is already a betrayal to customers, while nvidia already released dlss4 for all its rtx cards.",AMD,2025-10-31 11:16:11,12
AMD,nmd3fqg,I bought my RX 6950XT back in march 2023 because of AMD's reputation of customer goodwill. My last time buying AMD gpu was an R7 260 and after that I only bought Nvidia. I guess this is the last time I will be getting AMD gpus for the time being. I am already looking to switch to something that is still supported,AMD,2025-10-31 12:58:59,6
AMD,nmf5nkf,Press release has already come out stating everything in this video is malicious FUD.  IDK why people still listen to these guys when they do ragebait headlines.,AMD,2025-10-31 19:14:59,5
AMD,nmcv8s5,I mean who buys games on launch anymore anyway. I always wait for 50-75% off like a year later.   Drivers should be out by then... And bugs fixed.,AMD,2025-10-31 12:09:16,6
AMD,nmczimr,"RDNA 1 and RDNA 2, no AI not relevant.  Since RTX 20, yes AI yes relevant.  Nvidia started earlier with AI = more supported GPUs.  AMD started late with AI = less supported GPUs   No AI = wasting resources.   AI is where most of the resources will go, this is where the world is going. It is not AMD or Nvidia, it is where the world is going.   Which is why GTX GPUs like Maxwell and Pascal switching to maintenance mode just like AMD is doing with RDNA 1 and RDNA 2.   You can hate it or love it... I know now there is more hate of the reality, downvotes accepted.",AMD,2025-10-31 12:36:08,9
AMD,nmd4txt,"Absolutely boneheaded decision, I've 2 6000 cards, they're still great and I don't see any need to change them in the foreseeable  future, which is probably why AMD is now trying to sabotage them through this.",AMD,2025-10-31 13:06:55,2
AMD,nme26q1,I think I'm putting my funds for UDNA/RDN5 on maintenance mode.,AMD,2025-10-31 15:57:10,2
AMD,nmfoo26,"As someone with a 6800XT that's still going strong for me, and who is someone who doesn't update their drivers too frequently, this still saddens me. My brother has been eyeing up and new pc and I've been considering looking into one myself and the points made in this video do put me off.      I was already looking at the 9070xt vs the 5080 and while I know they're not directly comparable as AMD has already lost a little bit of favour from me with them admitting they want to stay ""mid-range"" with their flagship, this driver choice is making think team green again as my current card feels like it could go for another year or two easily.",AMD,2025-10-31 20:56:25,2
AMD,nmgietr,I won't defend this but I generally upgrade CPU every 5 years and gpu 3 or so. But not everyone is so lucky.,AMD,2025-11-01 00:02:21,2
AMD,nmhc5su,Dropping support for the 5000 line I get  But there are *new* 6700s being sold as of at least last year.,AMD,2025-11-01 03:29:43,2
AMD,nmhwhac,They have like 1% marketshare im sure the 12 peoplenot using 5700xt for mining will be upset.,AMD,2025-11-01 06:42:51,2
AMD,nmlcrlx,"Proud 6750xt owner here. Seriously thinking about switching over to the green side because of this. I was waiting for black friday ""deals"" to get myself a 9070xt but now i don't feel like supporting a company that just killed the second hand market value of my current card.",AMD,2025-11-01 20:41:28,2
AMD,nmlg9xy,"When I bought my RX6800 I was pretty much talked into it being future proofed by its 16gb of Vram. Well ,,what a load of BS.",AMD,2025-11-01 20:59:43,2
AMD,nmmxwdb,"Nvidia user since 1999 (TNT2), I was gonna switch to AMD after Nvidia's ram & pricing BS  not gonna switch anymore",AMD,2025-11-02 02:07:56,2
AMD,nmcabsk,"Of course, they did the same thing to the previous generation devices. It's got to be demoralizing being a software engineer for a company who makes you dismantle support for devices right after spending years dialing it in.",AMD,2025-10-31 09:18:58,5
AMD,nmcqtvi,I just upgraded from at 6800XT that I only had for 2 years. Glad I did.,AMD,2025-10-31 11:39:36,5
AMD,nmc6aj6,No more AMD GPU.,AMD,2025-10-31 08:38:11,12
AMD,nmdkeiw,"Glad I upgraded from 6750 XT but it sucks for 6000 series users.  I almost bought a 6950 XT too, glad I didn't now.  WTF AMD?! That's a crazy decision to make when they've managed to claw back some market share with the 9000 series and Nvidia have been messing up, this is a great way to null that...GG, AMD.",AMD,2025-10-31 14:29:39,3
AMD,nmf7fr3,Wonder if HUB and Daniel Owen will issue an update or retraction now that AMD has clarified that rdna2 will continue to get game optimizations. Drive by journalism.,AMD,2025-10-31 19:24:20,3
AMD,nmcmyqp,Here my old pc with gtx 970 which I bought in 2014. Got last game ready driver on oct this year. 11 years of support. While amd can't support gpu which are being sold at retail. And it's not the first time. Previous with vega igpu on cpu.,AMD,2025-10-31 11:10:23,3
AMD,nmctubq,OMG HWU?? What happened?,AMD,2025-10-31 11:59:59,1
AMD,nmcux1q,"There are no good companies. At least Linux exist, which gives these users a second wind for their hardware, but its still unacceptable.",AMD,2025-10-31 12:07:07,1
AMD,nmdgiky,so if you have newish cards like 7800 etc when do you start needing optimisation supporting game drivers I haven't really gone out of my way to update the drivers for gpu it just works so if they follow the same lifecycle in this range when will I need to worry about game optimization level of driver support?,AMD,2025-10-31 14:09:45,1
AMD,nmdjzli,This should answer the abundant 9070xt vs 7900xtx questions on this sub,AMD,2025-10-31 14:27:33,1
AMD,nmdqkh6,"IT'S A HALLOWEEN PRANK, PLZ LET IT BE A HALLOWEEN PRANK !! 😭",AMD,2025-10-31 15:00:51,1
AMD,nmdv8mg,"That's a little surprising considering the amount of products these gpu's are in but I want to point out at the risk of being downvoted to oblivion that a lot of those gpus suck as he pointed out. Yes, there's some great value GPUs in there, especially on the used market but I strongly feel we need newer tech at better prices, not old tech at lower prices. We're realistically not gonna get that if AMD has to continue trying to squeeze out performance from their less successful desktop GPUs. AMD needs to focus on ML and ray tracing to catch up to Nvidia otherwise you're gonna get no driver support because there will be no AMD GPU division. Ray tracing on these cards is ass. It's not future looking.",AMD,2025-10-31 15:23:55,1
AMD,nmdvnyj,"They also recently killed amdvlk on linux (the driver written by amd), so the only option now is radv (the driver written by the mesa project)",AMD,2025-10-31 15:25:57,1
AMD,nmdvsug,What if this is just ending day1 support and still you can expect support within the month or regular release cadence whatever it is? Still somewhat reasonable if issues are addressed?,AMD,2025-10-31 15:26:37,1
AMD,nmdwjpl,So they are removing support for rDNA2 but they are releasing Ryzen 100 with rDNA2? WTF,AMD,2025-10-31 15:30:08,1
AMD,nme13hv,Damn that's straight up evil and nonsensical,AMD,2025-10-31 15:51:57,1
AMD,nme1tyn,"Thank you amd. I only had 6750 xt for a year. I guess, i will buy nvidia next time",AMD,2025-10-31 15:55:27,1
AMD,nme3csa,As someone who was looking forward to getting the 9070 xt I may spring for a 5070 ti; they supported my 1060 for like 9 years with compatible drivers.,AMD,2025-10-31 16:02:49,1
AMD,nme45ep,I have removed AMD cards from my deal watchlist.  Too much gamble.,AMD,2025-10-31 16:06:41,1
AMD,nme6pf9,"I had hope for AMD and the long term benefit of having more vram and rasterization performance for the same cost.   This throws everything to the trash, i got a 6750xt two years ago because i would still be able to play upcomming games on High 1440p.   Used to recommended AMD GPUs to new builder friends, never doing it again if Nvidia has better long-term support anyways lol.   They need to realize its mouth to mouth recommendations that mantain what little market share they have.",AMD,2025-10-31 16:19:14,1
AMD,nme8j1p,So much for the fine wine huh,AMD,2025-10-31 16:28:18,1
AMD,nmeckkh,"It wouldn't be AMD, if it didn't try to shoot itself in both kneecaps...",AMD,2025-10-31 16:48:11,1
AMD,nmedqix,"been using AMD since the x1650 and only switched to get a 1080ti between then and now, with my 6900xt pretty much being executed by LIsa early. May as well keep an eye out for what Team Green has cooking for RTX 6000 before my 6900xt becomes a super expensive power hungry paperweight",AMD,2025-10-31 16:53:51,1
AMD,nmei97m,First things first folks: loyalty to a company doesn’t mean shit and being loyal just means you’re stooge and a mark.,AMD,2025-10-31 17:15:57,1
AMD,nmeivlx,"First time amd user here, got a rx6500m laptop and really happy with the performance but why is this fuss all about? Like if we compare it with nvidia, even older gpus like gtx1050 still receives update so doesn't amd provide long term like that?",AMD,2025-10-31 17:19:01,1
AMD,nmemapq,"My first GPU was an AMD 9700/9800 Pro. I may have gotten a few more after that but I never turned back after my first 1060. The driver support from AMD was horrendous. I tried a 7700XT this year and still had issues with software. And now they do this... Yeah, i'll stick with my overpriced Nvidia GPU's thanks.",AMD,2025-10-31 17:36:09,1
AMD,nmf4140,Got to be honest as 7900 XTX owner feels like as soon as AMD released the 9070 xt they abandoned us. Like 7900 XTX buyer were early adopters. Took AMD whole year to fix VR perf on 7900 XTX. AMD never gave clear answer on FSR4 support for 7900 XTX. My next gpu prob not going to be from AMD.,AMD,2025-10-31 19:06:27,1
AMD,nmfvz4l,"I built my current PC in 2018, and I've been thinking of upgrading in the near future then this happens. . . I refuse to buy NVidia and now I have to be cautious of AMD as well?",AMD,2025-10-31 21:38:19,1
AMD,nmg1kvi,This will prevent me from buying AMD in the future. That's a huge bummer because Nvidia is overpricing to the moon...,AMD,2025-10-31 22:12:25,1
AMD,nmgef66,"Its not like they are supporting new games anyway some new games have random driver timeouts like cyberpunk 2077 or the last of us part 2 was not playable without random driver timeouts on release just got fixed, thanks AMD but should be on day 1.  Or Hell Divers 2 that had issues at launch, or Dying light 2 that had RGB laser show bug outside the Radeon boost issues the game had for 1 and half year or so more or less.  As well as some unity based games having random driver timeouts on least RDNA3 due those games not handling the high clock speeds that well on stock.  They may be dropping support, but they never properly supported these gpu's to begin with and now RDNA3 we just waiting for games to be playable, by the time it gets fixed it wont matter anymore, i could not play wow for example during the entire Dragonflight expansion and wasted 200+ euros on a game and subscription that i could not play for almost entire expansion, so i am happy i can finally play it again.  Hopefully i can as well during midnight expansion which has its first beta on 11 november now.  The attitude AMD often has oh its just hardware issue 99% of the time its a driver issue and their vanguard program users unable to reproduce issues, simply due lack of information required to fix these issues, because sometimes its not easy to reproduce however for those having issues, avoiding the issue is as simple as simply not playing the game with the issue, as much that sucks.  And there is the Radeon tax you pay you only get to play what the vanguard program users like to play, you want to play that latest game that AMD has not enough users playing well tough luck and sometimes its also because NVIDIA simply makes the game unavaible until launch so AMD can only fix it as soon as its released, so NVIDIA wins by default because AMD cannot support the game to begin with at launch.     So yeah that AMD is just straight out saying they putting RDNA1/2 in Maintenaince mode not supporting the latest games really pisses me off, but its not like they doing that right now anyway even RDNA4 suffers from issues with games at launch.  But i will compliment AMD for finally working with game devs these days they been doing that for over a year or 2 now and probably longer, but they have to figure out how to do things better, because this is not acceptable.  I could litterally spend time making memes out of these driver issues and actual proof that these issues are actually real driver issues, but i do not like a burnout or work for free, which is why i rejected joining the vanguard program few times in the past, alto i have cooperated with some users figuring things out even testing things, but AMD should honestly do this them self.  But i am still avaible to do so eitherway if i am able to do that, but as some one hat has no job AMD could really do something about it and start hiring people for these type of things, so they can actually fix things rather then ignore these issues.",AMD,2025-10-31 23:35:56,1
AMD,nmgz40x,Get just enough market share just to s*** on them,AMD,2025-11-01 01:55:57,1
AMD,nmhs9zk,Not buying any Radeon GPU again any time soon,AMD,2025-11-01 05:56:22,1
AMD,nmhyh5k,unacceptable,AMD,2025-11-01 07:05:11,1
AMD,nmi895r,"Yeah Vega (last competitive card in my line of work.) and Radeon Pro Renderer debacle is prob the reason why AMD GPU's are something I will never consider them as a 3d artist/animator.  (Pro renderer was pretty much forcing every 3d developer to learn their standard vs the industry standard for a 5-10% performance boost. When the Vega 64/1080 ti where pretty competitive in Blender Cycles at the time where it depended on the work load (lots of textures to load? Vega wins with it's memory. A lot of calculations the 1080 ti would edge out in render time. Pretty much 2 years after they just dropped support and in that time NVIDIA has secured such a lead that the 9070 XT is around the performance of a 3060 and 3060 ti in rendering. 7900xtx without zluda is around 4060 tier, and 7900xtx with zluda was slightly below 4060 ti.)",AMD,2025-11-01 08:55:32,1
AMD,nmiik29,"You have to look at it from a profit standpoint. Only 10% of AMDs profit comes from the gaming sector. This includes profit from Xbox/Playstation business as well. Their desktop GPU share is in the single digits. There have been multiple years where this sector operated as a loss for their overall business. So honestly it really doesn’t matter how mad you get. Now that AI is starting to be a bigger part of overall profitability, your voice is getting smaller and smaller. Gaming is evolving into a service as it is. Hardware eventually won’t matter much. Only difference will probably end up being how you want to experience it. Like watching Netflix on your phone or a big screen. Everything will be a “gaming device”. Younger Gen Z and Gen Alpha are the tablet generation anyway. One of the reasons why the Switch and Switch 2 did so well. Your anger is about as effective as putting an angry note in a dusty comment box.",AMD,2025-11-01 10:44:12,1
AMD,nmki7hz,"I still had my trusty R9 390 during the great gpu shortage of 2021, you know, so they pulled support for it, during a GPU SHORTAGE. So i bought 2 used 6700xt's when prices stabilized for my brother and i. I guess AMD is out to get me, sorry guys. But guess who's never buying AMD gpu's again? If im gonna get clapped by big companies anyway, might as well get clapped for longer",AMD,2025-11-01 18:00:35,1
AMD,nmkmdap,"There's 0 excuse for this, AMD is no longer a business just struggling to avoid bankruptcy. It would cost them almost nothing to continue supporting these cards.",AMD,2025-11-01 18:21:48,1
AMD,nmkpem8,Booo. So sad. I had high hopes for AMD...   Oh well... idk what to say now.. From where do i buy my hardware from then?,AMD,2025-11-01 18:37:49,1
AMD,nmlbe13,🆘🥺,AMD,2025-11-01 20:34:10,1
AMD,nmmgcij,"tbh, dont care, last amd card ever bought is on the way out",AMD,2025-11-02 00:25:37,1
AMD,nmn9xia,manufactured hate that just proves people are way too happy to shit on AMD no matter the reason,AMD,2025-11-02 03:23:27,1
AMD,nmnlj6f,Is there a way to contact AMD and tell them of our displeasure?,AMD,2025-11-02 04:40:59,1
AMD,nmno4ed,ahhh time to go back to team GREEN at least they still give 1 fuck about gamers,AMD,2025-11-02 05:00:29,1
AMD,nmo362c,"I don't know what to think of AMD now, my 5700xt is still vary capable, got a 6800xt in another machine, its no slouch, imagine a 1080ti getting more support than a newer 6800xt.... What a crappy move AMD....",AMD,2025-11-02 07:24:01,1
AMD,nmp4i4o,"This is dumb, considering my RX 6800 XT is still very capable even undervolted: [https://www.3dmark.com/3dm/144649607](https://www.3dmark.com/3dm/144649607)",AMD,2025-11-02 13:08:05,1
AMD,nmrtho2,Yeah... glad I sold my 6900XT when I did to say the least.,AMD,2025-11-02 21:17:57,1
AMD,nmtko0l,Well...I got a 6700xt 2 years ago.. but what I've been seeing with some of the low spec old tech gaming YouTube content creators give me hope since a lot of games made after support ended work on them with playable frames.. were talking the HD 7000 series running 30+ fps on games released in the last 2-3 years some even in 1440p.   I guess I still have Linux as an option if things get too rough.,AMD,2025-11-03 03:21:07,1
AMD,nmtm1ro,"RDNA1 RDNA2 RDNA3 UDNA, AMD cant support all of them",AMD,2025-11-03 03:30:31,1
AMD,nmuidqp,Never been a better time to make the switch to Linux.,AMD,2025-11-03 08:10:29,1
AMD,nme8ldu,"It's embarrassing how many people don't understand, or don't want to understand. AMD hasn't dropped driver support for RX 5000/6000; there simply aren't any more specific game optimizations for RDNA 2/1. Therefore, your cards won't become unusable overnight. Driver support with bug fixes will continue. If a game doesn't work due to a bug, it will be fixed as before. Sometimes I question some people's common sense. Please don't just parrot what content creators say. Read, think, and understand for yourself.",AMD,2025-10-31 16:28:39,0
AMD,nmcmo2x,![gif](giphy|vyTnNTrs3wqQ0UIvwE|downsized),AMD,2025-10-31 11:08:05,1
AMD,nmdl5f6,"AMD got bigger and richer all these years, and radeon played a big part in that as well. Yet they are actually decreasing support instead of increasing it. Jeez.",AMD,2025-10-31 14:33:30,1
AMD,nmdq6yb,"Since I use Linux, I know I won't be affected. But it also means too that once nvidia fixes their driver support for Linux, I may make the switch.  It's rather telling that AMD cannot bother to keep proper support for a gpu line (RDNA2) when new APU's for handhelds are using them.  Maybe AMD will reverse this decision since it's causing a lot of uproar",AMD,2025-10-31 14:58:59,1
AMD,nme60dl,"After owning a bunch of Nvidia GPU’s, I was so happy that I chose my RX6800XT.  Such a great product. Just like all of 6000 series above 6600. They have so much life still in them…  Now I feel betrayed. And I was dumb enough to praise AMD cards to everyone I ever got a chance to talk about gpu’s to!  AMD screw me, I screw AMD.   I will choose Nvidia next. Bye.",AMD,2025-10-31 16:15:47,1
AMD,nmgj8mc,Fake news!,AMD,2025-11-01 00:07:55,1
AMD,nmdgm6k,"Good job AMD, you sure do LOVE shooting yourself in the foot.",AMD,2025-10-31 14:10:16,0
AMD,nmct8ji,VERY VERY BAD MOVE FROM AMD   SHAME,AMD,2025-10-31 11:55:58,0
AMD,nmd2ajf,Welp.  I might have to go green for my next GPU. What a bunch of bullshit. I've been AMD 100% for 20 years.  They just ended optimization support for a GPU I bought just three years ago?!  Seriously disappointed.,AMD,2025-10-31 12:52:31,0
AMD,nmcfc01,Talk about an overreaction geez. The clickbait thumbnail and caption isn’t helping anyone 🙄,AMD,2025-10-31 10:06:46,-7
AMD,nmce7ke,is sad hardware unboxed chosed the way of click-bait and disinformation AMD state they still maintaining it rdna1 and 2 but just not get the day one game patchs and again rdna 1 and 2 lack features that the new vulkan and new games use...,AMD,2025-10-31 09:56:39,-11
AMD,nmchj2g,"This approach is often referred to as **""cutting corners.""** You really can't compare AMD and Nvidia directly. If AMD truly wants to improve and be innovative, they kind of have to cut corners in some areas.  It certainly stinks for the people who own older GPUs, but I think most people won't notice a big difference. I had an RX 6950 XT myself and didn't update that card regularly. That doesn't mean other people didn't have problems, but speaking from my point of view, there isn't that big of an improvement from the Day 1 updates. At least, I didn't really notice it.  Ultimately, it's the developer's job to optimize a game. Everything on AMD's side that causes games to crash and not start will still be fixed. They'll just stop with the **Day 1 optimization** efforts.",AMD,2025-10-31 10:26:01,-10
AMD,nmda3y9,clickbait amd haters hardware unboxed never miss a chance to shit on AMD,AMD,2025-10-31 13:36:12,-3
AMD,nmdrmry,My current AMD Graphics Card will be the last one I will own from AMD after this disgusting move. Time to ditch team red,AMD,2025-10-31 15:06:11,0
AMD,nme792w,Rage baiting session is over.  [https://videocardz.com/newz/amd-clarifies-rdna1-and-rdna2-will-continue-receiving-game-optimizations-based-on-market-needs](https://videocardz.com/newz/amd-clarifies-rdna1-and-rdna2-will-continue-receiving-game-optimizations-based-on-market-needs),AMD,2025-10-31 16:21:57,0
AMD,nmd0n39,I was waiting for these guys to post some shitty headline youtube again for money grabbing  I have a 6950xt and I am absolutely fucking fine with amd decision.,AMD,2025-10-31 12:42:56,-5
AMD,nmd5jy9,I was thinking about buying a 9070 XT this month. I am glad that at least they announced this bullshit before the November discounts. Though I fucking hate that my next GPU will probably be an Nvidia now.,AMD,2025-10-31 13:10:57,-1
AMD,nmd37oa,Was thinking of getting a 9070 xt. No more. Will buy a 5070 Ti even if it costs more.,AMD,2025-10-31 12:57:42,-2
AMD,nmdp7cd,"This p.o.s. being clueless about tech, and openly admitting to be working by Nvidia's guidelines on what and how to say and test - that much we knew already. But you, guys, still listening to what that moron says - shame on you.",AMD,2025-10-31 14:54:03,-2
AMD,nmcdzrs,AMD is pure evil,AMD,2025-10-31 09:54:41,-11
AMD,nmdcare,"After purchasing an expensive limited edition 6900XT card 2 years ago, I can confidently say, I'm never again buying another GPU from team red.   Green and blue, the future is you!",AMD,2025-10-31 13:47:56,-1
AMD,nmddmmj,amd suck,AMD,2025-10-31 13:54:53,-1
AMD,nmdlkgq,"My enthusiasm for my 6900XT died last night.   Dr. Lisa Su, we need answers.",AMD,2025-10-31 14:35:40,-1
AMD,nmdmcom,"I am not an AMD fanboy, or have any allegiance but I am somewhat understanding of their decision.  Providing game optimization for 2+ year old, previous generation, GPU's requires a lot of work and expense.  AMD has to constantly be releasing new tech, at a ridiculous pace just to keep up.  Thus, cutting off support for ""old"" products is not unheard or, or unreasonable.  The RDNA 1 and 2 generations are nearly 5 years old.  While they did release new product within the last couple years, it was primarily for the Chinese market.  Furthermore, this announcement is likely being driven in part by game developers who are frustrated with the demand to release higher levels of game performance, and yet still capable of running on older GPU's.........Besides, is this the excuse we're all looking for to justify that next purchase?",AMD,2025-10-31 14:39:38,-1
AMD,nmdqgrq,"Great,another reason I have to keep buying Nvidia cards.",AMD,2025-10-31 15:00:20,-1
AMD,nmdwla7,"I will never buy an AMD graphics card again, goodbye!",AMD,2025-10-31 15:30:21,0
AMD,nmeiyii,![gif](giphy|KjafhxqcSbpci5Mbce),AMD,2025-10-31 17:19:26,0
AMD,nmibe1o,"Remember, this is the same AMD, who gave Open AI a warrant of 160 million shares with strings attached, so they can get Open AI to buy and optimize AMD Instinct GPUs for Open AI's AI models. If Open AI succeeds, buys six gigawatts of AMD GPUs, and AMD stock goes the moon ($800/share), then AMD sold \~10% of their company for pennies on the dollar to Open AI, who will sell AMD stock to fund the AMD GPU purchases. AMD had no problems sacrificing their own shareholders, so they have no problem kicking gamers to the curb while they chase after the AI hype train.  [https://www.amd.com/en/newsroom/press-releases/2025-10-6-amd-and-openai-announce-strategic-partnership-to-d.html](https://www.amd.com/en/newsroom/press-releases/2025-10-6-amd-and-openai-announce-strategic-partnership-to-d.html)  Jensen Huang was surprised how AMD's deal was giving away their \~10% of their stock to Open AI.   [https://www.youtube.com/watch?v=cP8pfCJTw4Q](https://www.youtube.com/watch?v=cP8pfCJTw4Q)",AMD,2025-11-01 09:30:08,0
AMD,nmpfqmv,"And here i am, my long ordered 9950x3d arriving tomorrow. How long will it survive?!",AMD,2025-11-02 14:16:00,0
AMD,nmcjyw7,The clickbait circus continues.,AMD,2025-10-31 10:46:30,-10
AMD,nmf15xb,Thats why i sold RX6800 and bought 3080ti four month ago   \+$100 was totaly worth it   2.5x faster in Topaz Gigapixel and no lag   \+reflex +DLSS +better RT,AMD,2025-10-31 18:51:47,-1
AMD,nmcaw2r,This is borderline yellow journalism.,AMD,2025-10-31 09:24:31,-18
AMD,nmc8o01,"""Loyal""  Techtuber admitting they're nothing but lowly mutts. AMD customers are smart, handsome, loyal. Nvidia customers are ignorant and lazy.",AMD,2025-10-31 09:02:04,-26
AMD,nmc9bs1,seriously thought there were a lot more nvidia users than amd and today the internet is exploding about it lol.,AMD,2025-10-31 09:08:46,-8
AMD,nmdx1bx,AMD GPUs were always trash. Their CPUs... Amazing!,AMD,2025-10-31 15:32:30,-1
AMD,nmfgmkj,I'm not watching this slop. Give me the TLDR.,AMD,2025-10-31 20:13:13,-1
AMD,nmcluut,Cutting Vega was reasonable as vega56/64 were 6 years old at that point. The problem is the CPU refreshes with Vega.,AMD,2025-10-31 11:01:41,157
AMD,nmdkfn9,my rx580 still gets driver updates here and there (currently in 25.9.1 iirc). not getting mainline driver updates but I won't say it's neglected.  where's the problem here? (haven't watched the video),AMD,2025-10-31 14:29:49,10
AMD,nmnpskq,Every peanut counts when talking about profits,AMD,2025-11-02 05:14:01,1
AMD,nmcl0ld,"Lowering priority isn't bad, but I do think this might be a bit warly for the 6000 series. 1 or 2 more years woild have been fine. They relesed 2019, so this time we only got 6 years of support.",AMD,2025-10-31 10:54:55,-12
AMD,nmeh7wb,"They are not cutting support, just day one optimizations for games. Same thing nVidia does with it's Game Ready drivers - pay attention to what cards actually benefit from them.  Still, it's lame to include the RX6x00 in that category, when it's not much different from the RX7x00.",AMD,2025-10-31 17:10:51,0
AMD,nmc4xjj,Presumably Sony sorts the PS5 side themselves and Valve are using open source stuff that AMD don't have a part in.   Sucks for the Windows handhelds that are going to be affected. I have a legion Go but I will not be buying the Go2.,AMD,2025-10-31 08:23:55,114
AMD,nmc4yzf,Consoles uses their own drivers and software stack,AMD,2025-10-31 08:24:20,28
AMD,nmc4sux,Consoles don’t use Windows drivers,AMD,2025-10-31 08:22:33,61
AMD,nmc4yb9,That's not how Support works. Support means you have to test each individual configuration and make sure they work...,AMD,2025-10-31 08:24:08,9
AMD,nmdpe2u,what does the minimum support you'd expect look like? what specifically should it include in your opinion?,AMD,2025-10-31 14:54:57,1
AMD,nmcp3u9,"Just highlights how little these old archs benefit from any specific optimizations nowadays, there's just not that much performance on the table, especially since most companies use the same engines",AMD,2025-10-31 11:26:49,1
AMD,nmc58pm,There's no loss here. Day 1 optimisations are snake oil. You still get the full latest drivers. Games still run fine. Ignore the marketing. Or if you can afford full price premium games on day 1 use a later card.,AMD,2025-10-31 08:27:12,-9
AMD,nmd4464,I haven't purchased a Nvidia GPU in 20 years. Now Im really considering it.,AMD,2025-10-31 13:02:50,57
AMD,nmdc6l6,NVIDIA is still supporting the RTX 2000 series with the latest version of DLSS SR. AMD couldn’t even be bothered to provide official support for one generation back. It’s even more insulting considering there is an int8 version of FSR 4 that’s nearly fully functional and AMD hasn’t even mentioned it. Would never buy another AMD card based on that alone.,AMD,2025-10-31 13:47:20,24
AMD,nmdlrht,Ikr? When my 6900XT dies I may have to go back to Nvidia. Such BS.,AMD,2025-10-31 14:36:39,12
AMD,nmhyvet,Same. I switched from 1070 to 6800 2 years ago. This has me doubting if I should stay with AMD for my next upgrade.,AMD,2025-11-01 07:09:41,3
AMD,nmivfg8,Yep. Treating a 6800 like it’s an obsolete yesteryear card is so ridiculous. The whole lineup is perfectly capable in 2025 on brand new games.,AMD,2025-11-01 12:33:25,2
AMD,nmln1a2,Yeah maintenance mode and lack of FSR4 support means I'm no longer a loyal AMD customer.,AMD,2025-11-01 21:36:50,1
AMD,nmds96e,Went through this with the VII (what a lemon of a card)... Which is how I ended up with Ampere and then Ada with Nvidia.,AMD,2025-10-31 15:09:18,1
AMD,nmh0mx0,Just replaced my 6650XT with a 5080. Couldn’t be happier,AMD,2025-11-01 02:06:37,1
AMD,nmdtah2,"Lifetime AMD gpu user, never had an issue with drivers getting out of date due to lack of support.",AMD,2025-10-31 15:14:28,30
AMD,nmdsyh8,"What’s the alternative? Gamble with nvidia’s 12vhp connector?   Companies need to be more consumer friendly. But we’re stuck between a rock and a hard place in the GPU realm, there really are no good options.",AMD,2025-10-31 15:12:48,23
AMD,nmir2fl,ya i haven't owned an nvidia video card in 15 years... that might change now...,AMD,2025-11-01 12:00:07,2
AMD,nmdksx1,"still rocking a rx580 and Vega integrated graphics without issues.  honest question really: why all the fuss??  EDIT: thanks for all the answers. there's definitely reasons for the fuss, especially if you play recent games. 6000 series are pretty powerful cards that are still very capable. I had a 6800xt until recently and it held like a champ.",AMD,2025-10-31 14:31:42,-5
AMD,nmf9ozl,https://i.redd.it/yywuqfcd2iyf1.gif,AMD,2025-10-31 19:36:19,1
AMD,nmgrdq7,Literally how I feel.,AMD,2025-11-01 01:02:41,1
AMD,nmdv884,AMD cards are only worth it if you:  1. Replace every two gens (~4 years).  2. Budget is low to mid range.  3. Only use it for playing video games.  CS grad school taught me to never buy an AMD card again.,AMD,2025-10-31 15:23:52,-3
AMD,nmem8ld,"Lol asking the real questions, people may very well be blowing steam for no reason. Maybe some placebo updates would satisfy them.",AMD,2025-10-31 17:35:51,14
AMD,nmivouf,It’s moreso stability than fps in my experience. And it’s usually only a big deal on the cutting edge tech AAA games. But I’ve had game release drivers take a game from unplayable to playable before.,AMD,2025-11-01 12:35:18,4
AMD,nmddbw3,"Probably very little. Never bothered updating drivers unless there is some big change (which is rarely) or I get to a point ""maybe its time"".",AMD,2025-10-31 13:53:21,11
AMD,nmita7c,"The only one I can think of is Dying Light: The Beast. Solid \~30% FPS increase with the drivers. Aside from that, most of the time it's 0-5% improvement.",AMD,2025-11-01 12:17:25,1
AMD,nmcd6m1,"As a Linux enjoyer, I may just feast on the cheap unsupported GPUs. Every cloud....",AMD,2025-10-31 09:47:07,88
AMD,nmcqfs0,Is this actually affecting performance?,AMD,2025-10-31 11:36:44,3
AMD,nmcdm6c,"I’ve bought and sold many gpu on both AMD and nvidia, there’s been no noticeable difference between the two in terms of value - though AMD buyers seem to be more informed.  Depreciation was the same.",AMD,2025-10-31 09:51:13,4
AMD,nmd7r2y,So am I gonna regret 9060xt ?? I already have it 😭,AMD,2025-10-31 13:23:10,1
AMD,nmhyndt,3-5 years - latest rdna 2 gpus were released 3 years ago (gre editions),AMD,2025-11-01 07:07:09,1
AMD,nmc87uh,Just consider also that NVidia does exactly the same thing just silently.  Every gaming GPU company ever has stopped optimizing for older architectures when its 2 or 3 gens old.,AMD,2025-10-31 08:57:32,-19
AMD,nmivv3v,Same. And I’m sure you also know that there’s not a game on planet earth that it can’t run.,AMD,2025-11-01 12:36:32,1
AMD,nmixqkt,"My previous card before my 6950xt was a GTX 1080, the timing is a little hilarious. Fine wine my ass, I was recently really coming around on AMD cards due to their value on the used market.",AMD,2025-11-01 12:49:30,2
AMD,nmezna3,"Since the 9070XT and 5070TI are usually around $100 of each other it probably makes sense to pay the Nvidia tax unfortunately.  If nothing else, the reputation of short driver support is going to tank the resale value of AMD gpus so you're not saving anything in the long run when you upgrade down the line.",AMD,2025-10-31 18:44:01,6
AMD,nmcm4w2,The reality of finewine was just that GCN was remotely forward looking compared to the trash that was Kepler.,AMD,2025-10-31 11:03:55,39
AMD,nmdf76l,Finewine was always and will always be an excuse for poor non-optimal drivers released at the start.,AMD,2025-10-31 14:03:01,16
AMD,nmcnaav,You're gonna take away AMD fan's last sound bite??,AMD,2025-10-31 11:12:52,20
AMD,nmee0z9,"It was never ""finewine"". The state of drivers years after release was supposed to be release day experience, AMD just didn't care enough to do it properly and invest enough money into proper driver development. They kept doing it for *years*. The worse part is, CPUs were also affected to a degree. AM4 X570 early adopter here, it took them half a year to iron out most bugs. One of the bugs caused black screen, if you used any PCIE card aside of GPU.",AMD,2025-10-31 16:55:16,7
AMD,nmcppa2,"RDNA2 is already outperforming comparable Ampere, outside of RT, and I doubt that will change",AMD,2025-10-31 11:31:17,6
AMD,nmcgp9w,"same, I was thinking of upgrading my 6700xt to 9070xt this blackfriday, guess Im getting nvidia instead",AMD,2025-10-31 10:18:51,28
AMD,nmcbmue,"Same, my next GPU will definitely either be  Nvidia or Intel since AMD clearly doesn't respect its customers.",AMD,2025-10-31 09:32:04,-2
AMD,nmdmx39,Yeah I don't care about an extra 100 bucks in this case fuck this crap.,AMD,2025-10-31 14:42:32,-1
AMD,nmd7oir,I agree with you but the point of that Witcher story was about how he was wrong btw.,AMD,2025-10-31 13:22:47,22
AMD,nmcpwlp,"Nah, there is still bigger and smaller evil, despite what a writer says through their characters",AMD,2025-10-31 11:32:48,6
AMD,nmiwzkz,0.0000001% their recent AI deals,AMD,2025-11-01 12:44:25,3
AMD,nmctw75,"Don't worry, no one is buying them right now",AMD,2025-10-31 12:00:19,27
AMD,nmdhne2,if radeon decided they're sticking with iGPUs we're cooked,AMD,2025-10-31 14:15:33,2
AMD,nmcr4ho,Yeah -pouts- I'm gonna go back to the company that deprecated PhysX and melty connectors and overpriced garbage!!! -slams foot on ground-   Yeah okay 👌,AMD,2025-10-31 11:41:37,-10
AMD,nmczyik,Same.,AMD,2025-10-31 12:38:48,-1
AMD,nmc86jf,>They are still being actively maintained and improved on the Linux side   The <1% of the market that uses desktop Linux for gaming can rejoice!,AMD,2025-10-31 08:57:10,54
AMD,nmcaolp,Noone cares about linux,AMD,2025-10-31 09:22:28,-19
AMD,nmeocnw,"There is no misunderstanding, it comes straight from AMD.  **From AMD:**    “RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security fixes and bug corrections. To focus on optimizing and delivering new and improved technologies for the latest GPUs, **AMD Software Adrenalin Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode.** ***Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.""***",AMD,2025-10-31 17:46:22,5
AMD,nmde971,![gif](giphy|jihwEDnsFoaXWDTiKc)  Yep... buy a REAL GPU. DLSS4 is the water we have been searching for in the RDNA 3 desert.,AMD,2025-10-31 13:58:06,3
AMD,nmipzd7,link to press release?,AMD,2025-11-01 11:51:16,3
AMD,nmqsxp2,Source?,AMD,2025-11-02 18:25:14,1
AMD,nmdc47r,"Never really bothered with day 1 drivers. Cant recall a game every having problems without the newest drivers. I remember Baldurs Gate 3 recommending newest drivers but I think it worked fine with the old one.   Currently on 25.8,1 with my 6900 XT and havent experienced any problems with BF6. Runs perfectly fine.",AMD,2025-10-31 13:46:59,1
AMD,nmdhu1l,Honestly I feel this is the sad truth. Spoken as a 6800xt owner.,AMD,2025-10-31 14:16:29,4
AMD,nmddjo4,Maxwell and Pascal are 11 and 9 years old respectively not 6 and 5 years.     Kinda a big difference,AMD,2025-10-31 13:54:27,6
AMD,nmdgws4,"GTX 900 and 1000 series wasn't ""AI"" but it was still supported for 9-11 years, they are still recieving similar updates to what AMD are doing now ""maintenance mode"" basically security updates for another 3 years.  2000 series released in 2019 still is getting newer versions of DLSS .",AMD,2025-10-31 14:11:46,2
AMD,nmsh81h,![gif](giphy|H5GxpSpvUSBbAu3Gdh|downsized),AMD,2025-11-02 23:24:26,1
AMD,nmiy2f2,They are honestly good cards on the used market. It’s one of the decent performance options out there for $100ish USD used.,AMD,2025-11-01 12:51:42,1
AMD,nmddrg4,Why? Will probably work problem free for many more years. Still use my old Vega 56 in our HTPC. My biggest problem is that it doesnt support 4K120.,AMD,2025-10-31 13:55:34,4
AMD,nmc7xx1,"Nvidia does exactly the same thing, they just don't tell you.  The whole industry has done this exact same thing since gaming GPU's have existed.   Of course you stop optimalisation work for older GPU's eventually. You don't have unlimited resources as a company. well... nvidia does now, just not for gaming GPU's.",AMD,2025-10-31 08:54:45,-10
AMD,nmc71xw,Sure,AMD,2025-10-31 08:45:57,1
AMD,nmdyegi,im not defending AMD here     but are you really read Patch Notes ? because even Nvidia released this driver there is NO GAME optimization for older GPU     its same security patch only,AMD,2025-10-31 15:39:05,3
AMD,nmd61ok,"So Nvidia just calling it support driver means you get day 1 optimization?  Fun fact, you dont 😂",AMD,2025-10-31 13:13:41,5
AMD,nmcq79f,"AMD isn't cutting support, just day-1 optimizations",AMD,2025-10-31 11:35:00,3
AMD,nmhgy4a,Is that you Cp0?,AMD,2025-11-01 04:08:19,2
AMD,nmegoy6,"Everyone understands, you're the embarrassing one. 3 years old cards will no longer receive game ready drivers.",AMD,2025-10-31 17:08:14,5
AMD,nmcfv6b,Where did AMD say that? Please share the link.,AMD,2025-10-31 10:11:27,9
AMD,nmeoynh,"In the age of instant access to info people are still unable to spend 5 seconds to get their facts straight.   **From AMD:**  “RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security fixes and bug corrections. To focus on optimizing and delivering new and improved technologies for the latest GPUs, **AMD Software Adrenalin Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.""**",AMD,2025-10-31 17:49:21,2
AMD,nmcfrlg,They haven't even said that there will be no game optimization patches. All they said is that they will be focusing on RDNA3/4 optimizations.,AMD,2025-10-31 10:10:34,-2
AMD,nmckh79,">You really can't compare AMD and Nvidia directly. If AMD truly wants to improve and be innovative, they kind of have to cut corners in some areas.  Why would a corporation like AMD, who had enough cashflow in order to acquire Xilinx (Along with a number of other companies) only 2 years ago need to cut corners in this manner?",AMD,2025-10-31 10:50:38,12
AMD,nmep8eg,"**From AMD's mouths:**  “RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security fixes and bug corrections. To focus on optimizing and delivering new and improved technologies for the latest GPUs, **AMD Software Adrenalin Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.""**",AMD,2025-10-31 17:50:41,3
AMD,nmdu273,How they're hating on AMD? How is it a clickbait?,AMD,2025-10-31 15:18:13,5
AMD,nmeqaoc,"They walked back their official statement on not supporting it after the backlash. Not a good sign. And they still indicate that these GPU's will be in maintance mode, so not fully suppported. Nvidia is still providing updates to its over 6 year old 16 series GPU's.",AMD,2025-10-31 17:55:59,1
AMD,nmdazag,">""I have a 6950xt and I am absolutely fucking fine with amd decision.""  Thats like saying your fine with eating shit just because its served from AMD.   Why would you be fine with this lmao.",AMD,2025-10-31 13:40:55,2
AMD,nmuvjiy,Are you that stupid ? What does 9070xt have to do with any of this ?,AMD,2025-11-03 10:27:55,1
AMD,nmcipym,"Im looking for AMD to officially port FSR 4 to RDNA3.  With this topic and RDNA2 losing some support, not sure it will get FSR4.",AMD,2025-10-31 10:36:21,8
AMD,nme2t0r,"Wow pos, really? Getting a little emotional there mr loyal radeon customer.",AMD,2025-10-31 16:00:08,4
AMD,nmdyej7,"So you say HUB are faveroable to Nvidia and not AMD, that their testing isn't objective or correct ? Let say it's true (I'm not saying it is, but let assume). AMD said themselves:  *""Future driver updates with targeted game optimizations will focus on RDNA-3 and RDNA-4 GPUs.""*  meaning don't expect optimization in the Future for RDNA 2 and 1, might come it might not. When companies says they focus on something else it means they will put the minimal effort if any at all to support other things. This doesn't bode well for those cards, and quite honestly disappointing as well.",AMD,2025-10-31 15:39:05,3
AMD,nmuvp07,AHAHAHHAHAHAHHAHHAHHAHAHAHAHHAHAHAHAHHAHAHAHA the future of burned cards and degrading CPUs,AMD,2025-11-03 10:29:26,1
AMD,nme5pfk,Game developers don't give a fuck since they still have to support the 4 teraflop RDNA2 Xbox Series S   If anything this would only increase the number of bug reports and issues they have to deal with,AMD,2025-10-31 16:14:17,1
AMD,nmeprp0,LOL what? So why is Nvidia still providing game driver updates for the 16 series that is over 6 years old? thats 4 generations ago... So your point is completely invalid. AMD is being lazy.,AMD,2025-10-31 17:53:21,1
AMD,nmuvvuv,3 days,AMD,2025-11-03 10:31:16,1
AMD,nmcn5q9,"amd can solve this in 10 minutes, guarantee at least another 5 years of dayone support for rdna 1/2 and guarantee 10 years of support for the 9000. It's so easy to solve.",AMD,2025-10-31 11:11:53,6
AMD,nmd1zh8,"Certainly. If this exact situation was happening with nvidia, there would be tens of AMD unboxed comments, but lame AMD customers fail to write even one Nvidia unboxed post over multiple years. Calling them loyal is insult to people like you, none of them are ready to die for AMD like you gladly would for Nvidia.",AMD,2025-10-31 12:50:47,2
AMD,nmd7z7l,Dont forget the Radeon VII was like 4 years old when they did that,AMD,2025-10-31 13:24:25,44
AMD,nmcye18,6 isn't even that old. nvidia only just recently stopped support for the 10 series which was 9 years ago.,AMD,2025-10-31 12:29:06,140
AMD,nmcp0i1,yeah wtf I got a laptop with vega 6 that sits on a 2023 APU and it is already in maintenance...,AMD,2025-10-31 11:26:06,47
AMD,nmr8oqh,uhhh... bullshit.   They stopped updating the cards and fixing problems while they were still shipping to retailers. See Radeon VII.,AMD,2025-11-02 19:39:36,1
AMD,nmdlrqu,8 years old. First release in 2017.,AMD,2025-10-31 14:36:41,1
AMD,nmdr6jm,"You get security updates but they are effectively cutting off new game support. Battlefield 6 Optimizations are only in the 7000 and 9000 series drivers, there isn’t anything for 5000 and 6000.  I understand the game runs well on 6000 but it’s literally laughable that the min and recommended GPUs for BF6 don’t have game support from AMD when AMD and retailers were still selling very new 6000 series cards this year.   I’m not gonna lie, I usually upgrade my GPU every 5 years and I’ll prob build a new PC soonish so even if AMD’s standard is now 5 years for Radeon, I won’t be affected but people keeping their cards for a decade will be.  I understand that from AMD’s perspective it’s hard because all of these GPU arch’s a a bit different but it’s just a rough move going forward. Like RDNA5/6 is also supposed to be a diff arch, UDNA is what I think they are calling it. So that prob means 9000 series owners will lose support also in 5/6 years.",AMD,2025-10-31 15:03:55,21
AMD,nmrzgqs,">rx580 still gets driver updates  >I won't say it's neglected  Not really, though. This is something I came across when trying dxvk on a bit older laptop. It needs the `VK_KHR_maintenance5` Vulkan support level for newer releases, which the hardware supports. That all works fine under Linux drivers, but AMD gave up on Windows feature support, so it doesn't work with Windows at all.",AMD,2025-11-02 21:47:30,3
AMD,nmsbuq5,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-02 22:54:51,1
AMD,nmcpplx,You should count that time starting with end of production not from the start. I can easily find vega powered new laptops.,AMD,2025-10-31 11:31:22,25
AMD,nmcp3lm,"Correction: [The 6000 series was released November 18, **2020.**](https://www.polygon.com/2020/10/28/21538114/amd-rx-6800-6900-xt-gpu-release-date-price/) It's technically not even 5 years old yet. Their announcement/reveal was almost 5 years to the day (Oct 28, 2020) of this Adrenalin release. (Oct 29, 2025.)  Hell. I bought a _new_ 6800 in November 2024.  Thing was only 1 generation old at the time. It hasn't even been a year yet and it's still a great card.  EDIT: I suppose this also means one should expect having purchased a _new_ 7000 series card this year (before the 9000 series was available) might only see full support until what? 2027? 2 years?",AMD,2025-10-31 11:26:45,19
AMD,nmcs6ej,"6600 was released at 2021. 5500XT was released a month before 2020.   Who cares when the first of them released? If you keep releasing products on that GPU, you keep supporting them, I don't say to reset the clock, but 6600 is 4 years old at this point...",AMD,2025-10-31 11:48:47,12
AMD,nmcoiao,"> They relesed 2019, so this time we only got 6 years of support.   RDNA 2 was initially released in late 2020.",AMD,2025-10-31 11:22:16,14
AMD,nmcu5j2,"Try less than 5, but nice try.",AMD,2025-10-31 12:02:03,1
AMD,nmcs4va,"For what it's worth, the Open Source side AMD very much do have a hand in it. They're the original contributor of, and continue to be by far the largest ongoing contributor to the Linux amdgpu module.  Valve also contributes quite a lot though.",AMD,2025-10-31 11:48:29,30
AMD,nmcq50d,On the driver page: AMD Software: Adrenalin Edition does not include support for handheld gaming devices.  Users should check with the OEM for device specific drivers.   We're users typically using these drivers on windows handhelds?,AMD,2025-10-31 11:34:32,27
AMD,nmcjz4u,">Valve are using open source stuff      Not just using. They write it and maintain it.      But, to be fair, Valve is also known from abandoning things they no longer care about, like Team Fortress 2.",AMD,2025-10-31 10:46:34,5
AMD,nmc6g3m,not handhelds... AMD still release new products with RDNA2 which makes this even more ridiculous.,AMD,2025-10-31 08:39:47,18
AMD,nmelpc1,"At least up to par with the competition, that is, 7 years? A new rdna2  card was released around at this time in 2024",AMD,2025-10-31 17:33:10,1
AMD,nmc6c4r,There were quite a lots of serious problem with drivers for 6000 recently. Many games had game breaking bugs (Last of us part 2). Using AMD card is not like using my previous Nvidia card and this change doesn't convince me its about to change tbh.,AMD,2025-10-31 08:38:38,9
AMD,nmc5de2,but I like snake oil  no seriously calm down people,AMD,2025-10-31 08:28:35,5
AMD,nmduq3q,"This is the real problem.  In a vacuum, maybe it's just a bad look and AMD could get away with it.  But trailing a distant 2nd in the market, supposedly trying to gain market share, AMD absolutely cannot burn customer goodwill like this.  If Nvidia didn't use those cursed power connectors, I'd consider them too.  I feel dirty even saying that, but this is what happens when a company shows such disregard for its existing customers.",AMD,2025-10-31 15:21:26,21
AMD,nme5bfu,"Funny, I just recently sold my evga 1080ti Hybrid FTW after being nvidia supporter for the last 20 years and went Hellhound spectral white 9070 XT.  I feel like I betrayed everyone now, myself included...",AMD,2025-10-31 16:12:22,6
AMD,nmi8qot,"Lucky the last competitive GPU for a 3d artist was Vega 64, since then buying top of the line AMD has the same render speeds as XX60... Heck the 9070xt renders in blender as fast as a 3060 so it's even a generation behind now.",AMD,2025-11-01 09:00:51,1
AMD,nmn0b8z,"I was gonna buy AMD for the first time in my life, at 41... not anymore",AMD,2025-11-02 02:22:54,1
AMD,nmnv7o9,"Same here. I have a 6900xt. Polaris before that. Last nvidia GPU I bought new was a 7800GS.   Imagine, 1080ti still getting updates and my 6900xt is ‘legacy’  Poor form",AMD,2025-11-02 06:03:13,1
AMD,nmel250,"~~Ehh, my nvidia 3090 doesn't have access to the latest DLSS models and it can't use Nvidia frame Gen, and I got it in 2021 which wasn't too long ago~~ My bad, it has access to DLSS 4 but not frame gen, which is what I needed for my 3090 for certain games",AMD,2025-10-31 17:29:58,1
AMD,nmft2up,"As we can see here, it's not only masochism, but also Stockholm syndrome.",AMD,2025-10-31 21:21:26,28
AMD,nmkhfk3,"I had my trusty R9 390 during the great gpu shortage of 2021, you know, so they pulled support for it, during a GPU SHORTAGE. So i bought 2 used 6700xt's when prices stabilized for my brother and i 2 years ago. If you're gonna get clapped by big companies anyway, might as well get clapped for longer",AMD,2025-11-01 17:56:37,2
AMD,nmfauv0,...just other driver issues.,AMD,2025-10-31 19:42:35,6
AMD,nmdxmi8,AMD is supposed to be the alternative.,AMD,2025-10-31 15:35:22,16
AMD,nmga8zk,The vast majority of nvidia GPU owners will never have the 12vhp issue. Every earlier AMD owner is getting fucked by this.,AMD,2025-10-31 23:08:52,9
AMD,nmenl56,"The connector is fine, problem is overblown.",AMD,2025-10-31 17:42:39,-2
AMD,nmexumm,"Tbf the 12vhp is mostly only an issue for the RTX 5090, the lower power cards it's not much of one.",AMD,2025-10-31 18:34:48,-1
AMD,nmlrvk4,lmfao I can’t believe people have been convinced that power cable issue is anything more than a tiny risk,AMD,2025-11-01 22:03:48,0
AMD,nmdw3d3,"I'll bite.      Losing Day 1 driver support will mean poorly optimised at launch titles like Borderlands 4 will either run badly or potentially not at all when released, and opens the question as to when exactly they will be backported. While you can argue this isn't really AMD's (or nVidia) job to fix it helps develop a sense of existing customer support for medium age product (the oldest RDNA2 card is less than 5 years old, the newest less only 2) which can in turn cultivate sustained brand loyalty.   AMD was widely praised for using 16GB of VRAM in the 6000 series specifically because it would likely allow the cards to age well as VRAM became a larger consideration, by cutting driver support to maintenance mode they've now done they exact opposite.      The bigger issue is that this may hint that RDNA2 cards will not be receiving official FSR4 support, which would be a massive blow to customers with those cards because FSR4 is a significant improvement on previous iterations of the technology and we have proof, albeit unofficial, that RDNA2 cards can support FSR4 natively and gain significant benefit from it.",AMD,2025-10-31 15:27:58,13
AMD,nmdntsx,"Because people are fucking stupid, and act as if their cards suddenly just blew up. Reality is, AMD cards run games fine without day 1 drivers or anything. RX 580 of yours can even run software ray tracing games like Silent Hill f.",AMD,2025-10-31 14:47:07,-6
AMD,nmdvecx,Agreed. This is more headline catching than actual impact on PC gamers. A lot of these cards are mid at best as he pointed out.,AMD,2025-10-31 15:24:41,0
AMD,nmdt2re,I would get occasional crashes in some games with my Vega 64. Otherwise not that big of a deal.  But why wouldn't you get a card with better long term driver and software support if you can? I was on a Vega 64 till two weeks ago but finally jumped ship to an Nvidia 5060ti 16gb for $400.,AMD,2025-10-31 15:13:24,-1
AMD,nmgbwfu,"Funny CS grad taught me the opposite, don’t let one company corner the entire market.",AMD,2025-10-31 23:19:35,13
AMD,nmeabd2,"If your GPU is few generations old you're more likely to fuck shit up by updating drivers than improve anything.  People who think that there are actually any optimalizations for old architectures besides checking the checkbox that driver is compatible with said GPU are delusional. Might help by accident if some improvements for new generation also happened to help the old one, lol.  The Battlefield 6 compatible driver caused the Adrenaline to recognize the game as Elder Scrolls Online, fucking lol. Previously I was on 25.8.1 and it was fine.   That's literally the only change between 25.8.1 and 25.10.2 i've noticed and due to that Elder Scroll bullshit I actually regret updating. Should've stayed on old driver.  To be fair I don't know if anything changed performance wise since I didn't actually do any benchmarks since I can't be bothered.",AMD,2025-10-31 16:37:10,11
AMD,nmci3c3,"Windows users: Ugh, this GPU is going to be totally useless in 5 years. Thanks for nothing AMD  Linux users: Holy shit TWO CAKES!!!!  this post was paid for by the RADV gang",AMD,2025-10-31 10:30:58,73
AMD,nmcn2fz,"First Win10 now AMD, i guess it's a sign from heavens that i should finally ascend",AMD,2025-10-31 11:11:11,23
AMD,nmcl88x,"I'm new to Linux, aren't amd drivers kernel based, won't the support end all the same?  Also dafuq is it with the downvotes? A genuine question gets downvoted",AMD,2025-10-31 10:56:38,1
AMD,nmcrldt,"Likely yes , and also the day 1 fixes like glitches textures and shaders won't get fixed day 1 maybe in a few months if at all.",AMD,2025-10-31 11:44:48,20
AMD,nmd01ny,I'm seeing a higher price on 3080s 10GB around here than 6800XTs. It's not a huge difference but it's there. The 3080 also lacks VRAM so currently it's worth a lot less imo.,AMD,2025-10-31 12:39:21,5
AMD,nmdfyqn,"I bought a 7800XT for only 480CAD used the other day. A 4070S would have been at least another couple hundred bucks (although I doubt that’s are any for sale), and a 3090 would have been even more. I’ll be able to game on this thing for <100 bucks/year after I flip it for a 9070XT in a year or two; not sure who’s buying Nvidia at those prices lol.",AMD,2025-10-31 14:06:56,1
AMD,nmdakpe,Nvidia GPU's always held their value. Not surprisingly when even a Nvidia card from 2018 has better features than AMD cards 6 years newer...,AMD,2025-10-31 13:38:45,-2
AMD,nmd9b8v,"That's up to you , it launched in 2025 , by this standard and showed  it won't get good driver updates from 2029 or 2030 on anymore.",AMD,2025-10-31 13:31:49,3
AMD,nmknfbm,"Who knows at this point. Part of me doesn't see rdna4 getting cut off like the older architectures due to them finally settling on adding the AI functionality for upscaling and RT, but it's also AMD which keeps shooting itself in the knee",AMD,2025-11-01 18:27:17,1
AMD,nmknkty,There's an rdna2 gpu in the ASUS Xbox handheld that was released last month,AMD,2025-11-01 18:28:05,1
AMD,nmcf8c2,I’m sorry this is cope. The RTX 20 Series from 2018 has all of the latest features it can actually support architecturally (other than FG) as well as game ready optimisations,AMD,2025-10-31 10:05:51,26
AMD,nmcany1,they added new feature to 20 series last year like rtx video and all the latest dlss 4.0 upscaling features are all supported on these cards from 2018. It is just false.,AMD,2025-10-31 09:22:17,40
AMD,nmca8ex,"That’s not the point, Nvidia’s Maxwell GPUs from 2014 still get day-one Game Ready drivers included in the same package as the latest Nvidia GPUs, while AMD cards that are only four years old and still being sold new have already lost that support.",AMD,2025-10-31 09:18:02,17
AMD,nmc8fhc,Nvidia supports stuff way longer. And their drivers are higher quality even now in their state they usually don't have texture or shader issues which need a day 1 fix.,AMD,2025-10-31 08:59:41,7
AMD,nmcucn0,> Just consider also that NVidia does exactly the same thing just silently.  Yeah I'm gonna need some receipts here chief.,AMD,2025-10-31 12:03:22,1
AMD,nmkrqv4,I’ve got mine undervolted and over clocked and it’s faster than a 9070 non xt.,AMD,2025-11-01 18:50:06,1
AMD,nmj6z6z,I had a 1080ti before I upgraded to my 6900xt as well.,AMD,2025-11-01 13:48:49,1
AMD,nmf8u8e,well here the difference is more like 250-300€,AMD,2025-10-31 19:31:44,5
AMD,nmd35hi,"Some games need driver updates. I keep my drivers updated, so it's never an issue for me. But my partner doesn't do that, and had multiple issues last few years where a new game didn't work properly. And where ""did you remember to update the video driver"" was the fix.",AMD,2025-10-31 12:57:21,5
AMD,nmiuzn3,"Bit dramatic. There have been several games recently where NVIDIA didn't provide day one drivers for even **their latest** 5000-series graphics cards. On top of that, the 5000-series has been plagued by driver issues causing stuttering and black screen crashes for several months while the RX 9000-series has been rock solid.  They're still getting drivers. You may miss out on 0-5% performance on a game's launch (with emphasis on 0%), on 5 year old hardware. If this causes you to go NVIDIA, which has even more driver-related issues at the moment, you're ridiculous.",AMD,2025-11-01 12:30:12,2
AMD,nmcr4g6,"Me too, already put my 7600 for sale to get a 4060 ti instead",AMD,2025-10-31 11:41:37,-2
AMD,nmczjg4,Nvidia hardly respects gamers either. They would throw us all under the bus to sell more AI chips to enterprise tomorrow if it was as simply as flicking a switch.  Currently the state of the GPU market if you want to at least try to do the right thing is picking the least worst option whoever that is for you. None of them are loyal to us.,AMD,2025-10-31 12:36:16,26
AMD,nmd2weh,"Evil is still evil.   I just bought a 9070XT knowing what AMD did and I don't care, it's the best upgrade I could get for the amount I paid.  I paid 730$ for it, a 5070 Ti here costs around 900$ for the base models.   No way in hell is virtually same performance worth 160$ more. DLSS4 is tempting, but not for 160$.",AMD,2025-10-31 12:55:55,2
AMD,nmctid1,"I lost count of games I have preordered, crashing due to documented AMD driver issues. FSR4 not available for RDNA3 made me sell my XTX and finally switch to Nvidia.  And I enjoyed my XTX, despite the insane heat and power draw required to match a 4080S in performance. While sucking at RT, No Upscaling, No Reflex, broken DSC implementation and the list goes on. Adrenaline would crash on startup on fresh windows installs for months, until they \*finally\* fixed it. Their overlay crashing Helldivers entirely, games not detected, settings not applying on reboot.  R9 270, RX 480, 6950 XT, 7900 XTX. Not even one of these cards did not not have issues. And not like Nivida, oh the grass won't display well in that one game. No, 23.10.1, 23.10.2 and even 23.11.1 broke Hardware Acceleration completely. Edge, Steam and Discord in foreground would freeze the entire PC for seconds.  And I kept it all clean, updated, Windows, BIOSes, you name it. Meanwhile the 5070 Ti I got as a replacement has been no less than flawless. I work 6 days on 7, I have no time to waste on troubleshooting due to their incompetence, not even once a week.",AMD,2025-10-31 11:57:46,14
AMD,nmdgvih,"You forgot the fact that they paper launched their cards up until the 5060 and had the gall to allow Asus to make a US$10000 solid gold 5090. Honestly if anyone were to flaunt that card in front of me they’d be in for a fucking black eye. I don’t even want the card, but the thought of it existing enrages me.  Also I have completely dumped windows for Linux now (mostly pertaining to how I cannot control windows update when it comes to drivers, it seems to ignore everything I throw at it short of a group policy, even now I can’t do that because Microsoft disabled my pro key over the petty reason that they no longer allow upgrades from windows 7, 8 and 10 to 11. Never mind that the key has been used to upgrade to 10 and later 11 before and therefore should remain valid. And guess what? AMD cards are not only flawless but works better with Linux (RT support for Vega and Polaris by repurposing the compute cores? Yes please!)",AMD,2025-10-31 14:11:35,4
AMD,nmdn70a,Physx is legacy tech.,AMD,2025-10-31 14:43:56,0
AMD,nmcbnh3,"I find that kind of argument always a bit weird          If you look at the Linux market share on  steam you will see a higher percentage than for a GPU like 9070 XT, RTX 5090 (0.29% last time I checked) and a lot more yet talking about those isn't a problem",AMD,2025-10-31 09:32:14,29
AMD,nmd62hz,"I see more people moving to Linux distros like nobara and bazzite, with Microsofts force-feeding windows 11 shenanigans the past months.   Personally, the only thing I didn't get working yet is steamVR. Other than that I have no need for windows, at all.",AMD,2025-10-31 13:13:49,4
AMD,nmdnbwg,"Linux market share, according to Valve, is around 2.7%. Macs are about 1.8%.",AMD,2025-10-31 14:44:37,6
AMD,nmcbdc0,About to be in that % hence why I bought AMD cards recently for that exact reason. Will maintain copies of Windows for when needed.,AMD,2025-10-31 09:29:26,7
AMD,nmf5tq5,"They didn't reverse anything, it was right in the release documentation.  HUB lied and made rage bait headlines for revenue.  This isn't the first time this has happened.",AMD,2025-10-31 19:15:52,1
AMD,nmeqs00,"Yes I saw that, it probably does means that they wont get game specific drivers but it doesn't *explicitly* say that. It says they will focus on RDNA 3 and 4, but that doesn't mean they wont get them at all, it just might be later. That's why I'm saying it might've been better to wait and get **clarification** before dropping a video that will rile people up without knowing for a certainty.",AMD,2025-10-31 17:58:25,5
AMD,nmddxrm,Day 1 drivers are mostly marketing but they do occasionally fix bugs in the game that you will encounter without them.   It’s not a big deal though.,AMD,2025-10-31 13:56:27,3
AMD,nmdidcg,"And still, RDNA 2 will still get updated driver but probably not as frequently.   I think that once UDNA/RDNA5 comes, people will mostly forget about RDNA2 regardless.",AMD,2025-10-31 14:19:17,3
AMD,nmddy6h,"It doesn't matter. It is much easier, from now on.  Does a GPU have AI capabilities? Yes = full support, No = half support.   (I am talking about FP8 or better. Not just INT8)",AMD,2025-10-31 13:56:31,-2
AMD,nmdh4zq,That global decision is from now on.   AMD and Nvidia doing it basically at the same time.,AMD,2025-10-31 14:12:57,2
AMD,nmdju4n,GTX900 1000 so call support are just bug fix,AMD,2025-10-31 14:26:47,1
AMD,nmc9fqw,"Nvidia doesn't do this for their 3000 series, I think the 900s had 9 years of support or something like that",AMD,2025-10-31 09:09:53,28
AMD,nmcc9nw,Agreed. And they’re even worse.   I have a motherboard with an Nvidia chipset that can never work on windows 10. Installing win10 causes the board to try to install two separate GeForce drivers side by side until the registry becomes corrupted.,AMD,2025-10-31 09:38:25,0
AMD,nmd1hcr,It's in maintaince mode. So no day 1 or day 1000 games optimisation. Please learn the meaning of maintaince. Only security and bug fix.   So stop spreading misinformation,AMD,2025-10-31 12:47:54,-3
AMD,nmfhpzi,Not saying anyone is right or wrong here but when was the last time someone absolutely needed the game ready driver for a game to work at all.   Battlefield 6 just dropped and you can play it with almost any driver in the last 12 months and still get the expected performance based on benchmarks. The scope of dropping these optimizations for older cards doesn't seem as outrageous as it would be on Nvidia where games frequently have severe bugs or don't work without the Day One driver update (EX: Final Fantasy 16 on Nvidia).  I do forsee a problem though where if RDNA1 and 2 do not receive updates on a regular basis they might get gated out of launching a game with astonishingly lazy driver detection (like Warzone).,AMD,2025-10-31 20:19:05,2
AMD,nmg6q25,You can wipe away your tears now. xD The matter is settled.,AMD,2025-10-31 22:45:49,-1
AMD,nmdicz2,"From [tomshardware;](https://www.tomshardware.com/pc-components/gpu-drivers/new-amd-driver-snubs-radeon-rx-5000-6000-gpus-with-latest-updates-also-disables-usb-c-functionality-on-rx-7900-series)  >AMD has confirmed that its latest driver update involves sunsetting its [Radeon](https://www.tomshardware.com/tag/radeon) RX 5000 Series and 6000 Series graphics cards, placing them in maintenance mode to allow delivery of new tech for its more recent offerings. ""In order to focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 places Radeon RX 5000 series and RX 6000 series graphics cards (RDNA 1 and RDNA 2) in maintenance mode,"" the company confirmed to *Tom's Hardware* in a statement. **While the company says that its RDNA 1 and RDNA 2 graphics cards will continue to receive critical security updates and bug fixes**, new features, like the latest Battlefield 6 update, are reserved for the Radeon RX 7000 and RX 9000 series in the latest [AMD Software: Adrenalin Edition 25.10.2](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html).  I undestand this as ""new tech"" is not supported by 5000 and 6000 hardware (ai and shit) anyway. Driver updates are still coming but maybe not as frequent.  I dont think this is as bad as people make it out to be.",AMD,2025-10-31 14:19:13,3
AMD,nmcgox0,They did not say that will be no game optimizations they say no day one just that even polaris card that got to maintanance mode got game optimizatios time to time nof often but still get them.,AMD,2025-10-31 10:18:46,3
AMD,nmd5gfg,"The thing is we dont know, it sure has to do something with working on current projects to minimize costs etc. Not defending them in any regards but when it comes to big corpos you shouldnt be emotional. It always comes down to make more money which is not good for us (consumers).",AMD,2025-10-31 13:10:24,-1
AMD,nmfwq3t,"They didn't walk back anything.  This is what happens when you listen to techtubers and randoms on the internet.  until you all understand this below there is no point.  Maintenance mode implies ""No new features"" not ""No support"".",AMD,2025-10-31 21:42:41,0
AMD,nmgqmnz,"Because they're still releasing driver updates, just not giving them day-1 performance patches (which might I add, is almost always a failure of optimisation on the developers part). They will still get driver updates.",AMD,2025-11-01 00:57:45,1
AMD,nmuwwci,Use your precious brain to understand why someone would hesitate buying an AMD GPU whether it is affected or not by this decision. And next time think twice before calling someone stupid before asking dumbass questions.,AMD,2025-11-03 10:41:08,1
AMD,nme3c2u,"I'm on 2080 Ti, and using FSR 4 Native AA. Somehow this youtuber didn't mention that DLSS 4 is completely broken in DLAA mode.",AMD,2025-10-31 16:02:43,0
AMD,nme320d,So what? People still play latest games on GCN. You don't need latest drivers for games to work.,AMD,2025-10-31 16:01:22,1
AMD,nmf7v5f,"That's wrong haha. AMD fans would love to die for a multibillion dollar corp. They have to. No one else would be loyal, much less to bargain bin hardware. Takes a special kind of cultist mentality. This idiot just let the mask slip. ""Loyal"" lmao.",AMD,2025-10-31 19:26:34,1
AMD,nmr8v9e,"no it wasn't, They stopped fixing the drivers / bios while they were still new. Including major issues like screen going black and requiring a reboot.",AMD,2025-11-02 19:40:29,2
AMD,nmqfn5o,tbf they sold like 7 of those,AMD,2025-11-02 17:20:35,1
AMD,nmcz9ot,6-8 years is pretty standard for most GPUs in terms of mainline driver support. It's really only Kepler/GCN1/Maxwell that it got much longer.,AMD,2025-10-31 12:34:35,47
AMD,nmejlah,"I can still run games on my vega 64!  Oddly, everything i play still works",AMD,2025-10-31 17:22:34,0
AMD,nmfcboj,The same thing happened to me with a mini PC years ago.... I had it for about a year then it went to maintenance and the drivers sucked ever after.,AMD,2025-10-31 19:50:25,8
AMD,nmdm4ak,"That is your laptop vendor.  APU's are generally supported thru the laptop vendor, per their contracts  which also generally updates go thru windows.",AMD,2025-10-31 14:38:27,-2
AMD,nmdqz3j,"The VII wasn't. The APUs they launched weren't. The laptops being marketed with the APUs weren't.  Edit: Let's also not pretend that the support up until they ""ended support"" was good.",AMD,2025-10-31 15:02:53,1
AMD,nmdt25z,"so you're effectively losing a bit of performance in new games. what else?  edit: so yeah it's pretty shitty behavior by AMD, and it's not only losing performance in new games. thanks all for the answers, I wasn't aware of how bad this is.",AMD,2025-10-31 15:13:19,-1
AMD,nmd9gov,Don't forget the refresh 6x50 cards either. They only came out in 2022.,AMD,2025-10-31 13:32:39,8
AMD,nmcx54u,"RX 6750 GRE was released just 2 years ago, October 18th 2023.  RDNA 2 iGPUs products have released THIS year, Ryzen 5 7600x3D released August 2024, and the Ryzen 5 7400 released in September of 2025. Both have RDNA 2 iGPUs.  Ryzen mobile 7840HX and 7940HX both released in January 2024, both also have RDNA2 based GPUs.  You should be judging these decisions based on the LAST product release in the generation, not the first. And the last RDNA 2 products were released within the last 12 months, and many others within the last 24 months. Sad this is what AMD is doing to the few loyal customers who do buy their GPUs.",AMD,2025-10-31 12:21:17,9
AMD,nmdqtim,"Seems like I got my dates mixed up. Well, I said 6 years is a bit early. That makes is of course reallly bad.",AMD,2025-10-31 15:02:06,2
AMD,nmcti7j,Yep up until last week my boy was still gaming strong on a r9 390x with community drivers,AMD,2025-10-31 11:57:44,5
AMD,nmdkweo,They weren't involved in vulkan driver though.,AMD,2025-10-31 14:32:13,1
AMD,nmcrniy,"Check with OEM.....  Feels like a ""eat a bag of dicks"" comment from AMD lol. But yeah the vendor usually provides the drivers for handhelds, at least from my experience.   Presumably the OEM gets drivers from AMD and then customises for their devices so if AMD aren't going to do any updates we might as well get told to get fucked.   Advanced Marketing Disasters, lol.",AMD,2025-10-31 11:45:12,24
AMD,nmef6nl,"Yea no OEM is providing the level of support that AMD was themselves. As a follower of the PC handhelds since GPD's first launch in 2016, watching AMD move into this scene was like a drunk bull in a china shop.  OEMs do not want to write drivers for your hardware, they make little profit as is per machine, and long term support is not in their best interest. You will get a few drivers throughout the year if you are lucky, but their support is not going to be on the level of what AMD could provide. When the 780m came out from AMD, i was an early adopter in the form of the GPD Win Max 2 and many of the smaller OEMs had to battle AMD to start maintaining their drivers for their product as they did try and rug pull everyone with that ""OEM maintains drivers"" for a product none of them had architectural knowledge of. Over a few short months AMD took over maintaining them which was good, and its laughable their GPU side even considered releasing a product like that considering the demographics they were aiming for.",AMD,2025-10-31 17:00:51,1
AMD,nmcncoq,TF2 is almost 20 years old. It's hardly an apt comparison.,AMD,2025-10-31 11:13:22,31
AMD,nmdroe6,Why is everyone so hung up on TF2? Like name another game that isn't a subscription MMO or live-service/esport that had half as long of support. And it will still get vulnerability and compat updates like all their games get.,AMD,2025-10-31 15:06:25,8
AMD,nmd6mhe,Valve's driver developers are still pushing improvements for first-gen GCN hardware. There is nothing to worry about when it comes to RDNA support in the open source drivers.,AMD,2025-10-31 13:16:54,1
AMD,nmc7chq,Wouldn't be first time.  They pulled the exact same thing with Vega iGPUs,AMD,2025-10-31 08:48:51,11
AMD,nmfa0js,On the driver page: AMD Software: Adrenalin Edition does not include support for handheld gaming devices.  Users should check with the OEM for device specific drivers.,AMD,2025-10-31 19:38:03,1
AMD,nmj9h41,handhelds shouldn't be on windows,AMD,2025-11-01 14:03:42,1
AMD,nmc7v22,They are still fixing bugs.,AMD,2025-10-31 08:53:58,10
AMD,nmfouw1,"An RTX3090 has access to all the newest DLSS upscaling and ray reconstruction models, with the newest presets J and K working all the way back to RTX2000. You're right that framegen is locked away though.",AMD,2025-10-31 20:57:26,5
AMD,nmfto7e,"I just don't play brand new games that much, maybe thats the thing.",AMD,2025-10-31 21:24:53,0
AMD,nmftgd9,"That's why it's so frustrating, because they keep shooting themselves on the foot with dumb decisions over and over again.  It's almost as if there's someone from Nvidia infiltrated there making sure AMD never succeeds. That's how comical it is.",AMD,2025-10-31 21:23:36,14
AMD,nmf0hql,"The overwhelmingly large majority of 5090 don't have this issue. Spectacular when it happens but extremely rare.  Not a fan of the 12vhpwr connector nor NVidia's power design for the 5090, but it's extremely rare that they fail.  And indeed it doesn't happen on any of their lower powered cards at all. People just love being sensationalist.",AMD,2025-10-31 18:48:22,2
AMD,nme3t42,"hehehe, I wasn't baiting  I can see the issue with new releases. we're talking of losing bit of performance in new AAA games.  but people here are reacting as if their cards will suddenly implode into the nothingness, or if existing optimizations will stop working.  exaggerations aside, it's pretty shitty from AMD",AMD,2025-10-31 16:05:01,1
AMD,nme697e,Crashing or not playing would be a bug and continue to get addressed as it was said. I think people are reading too much into this.  Its mostly about minor perf improvements and feature support which the older cards cant do or at least not do well like AI.,AMD,2025-10-31 16:17:01,0
AMD,nmdtuxh,>But why wouldn't you get a card with better driver and software support if you can?  again: I'm still using a rx580 without issues. not playing new AAA stuff tho.,AMD,2025-10-31 15:17:14,1
AMD,nmcz96s,"If you're unsure about apps or features you might not have on Linux, I suggest Dual Booting.   I did that a week ago with Fedora, been on Fedora Gnome for a week now and it's been so enjoyable as an OS experience, the community is incredible and there are extensions for any feature you can possibly think of.   Just use whatever Linux Distro you go for as you would use Windows and then you'll slowly but surely find out what you need and possibly some alternatives for what you need",AMD,2025-10-31 12:34:30,7
AMD,nmco2bm,"Nope, we use a different driver stack that is open source through Mesa. Mesa actually is packaged for both AMD and Intel Arc. And we will maintain support for a looooong time. The AMDGPU driver supports every amd GPU to this day starting with I think the old 7000 series (like 7970).",AMD,2025-10-31 11:18:53,14
AMD,nmczaco,we don't know if any bug fixes will be day-1 or not,AMD,2025-10-31 12:34:42,2
AMD,nmdmb7w,"You’ve stumbled across sellers who were either looking to offload quickly or were uninformed.  I’ve bought a 7900XTX for 900 and sold it for 750 14 months later.  Our 4090 was 1500$ and sold for $1200. Before that 3080 lost about 250$ in 5 months.    I mean if you find buyers willing to pay top dollar for a 4070S in 2025 when the 9070XT and 5070TI exist, then you’re lucky, or they are dumb.",AMD,2025-10-31 14:39:25,1
AMD,nmdcre5,"Well, that’s certainly some conjecture.",AMD,2025-10-31 13:50:23,1
AMD,nmdwyb9,That's really sad. Even the resell value is gonna be low .. for outdated driver gpus.,AMD,2025-10-31 15:32:05,2
AMD,nmtwlkv,"I was talking about desktop, tho ur right",AMD,2025-11-03 04:45:49,1
AMD,nmcblp1,"And a lot of other features they are no longer adding to the 20 and 30 series.  They'll add it if it works out of the box with no extra effort on their part, like upscaling, while they wont add it if it takes extra effort, like reflex 2.  same as AMD.",AMD,2025-10-31 09:31:45,-19
AMD,nmcbayy,Having them included in the package and nvidia actually doing any optimalisations for those old GPU's are 2 different things.,AMD,2025-10-31 09:28:44,4
AMD,nmc8n4q,">Nvidia supports stuff way longer.  Does it? You see a lot of performance optimalisations on 20 and 16 series cards in their release notes? no, you don't. as they have also stopped actively optimizing for those older architectures.",AMD,2025-10-31 09:01:50,3
AMD,nmg2sd7,$6000mxn here in Mexico.,AMD,2025-10-31 22:20:06,3
AMD,nmhtuet,Depending on your area the price difference definitely affects the equation.  If a 9070xt is 700 euros and a 5070ti is 1000 euros then that's definitely a no brainer to go with the 9070xt.  Even if you only get 4-5 years of full driver support that's still a better deal.   In the US where the price difference is only $75 to $100 it makes more sense to go with the 5070ti.,AMD,2025-11-01 06:13:29,1
AMD,nmein23,bugs will still get fixed according to AMD,AMD,2025-10-31 17:17:51,1
AMD,nmdn16w,Nvidia doesnt drop support after 5 years or less,AMD,2025-10-31 14:43:07,4
AMD,nmdcanh,"Yeah after I dropped $1,000 on a 7900XTX over a 4080S I find out no more FSR updates after 1 year of owning the card I was done with AMD. None of this surprises me.  If your spending over $400 on a GPU just get a Nvidia card. AMD cards don't last long.",AMD,2025-10-31 13:47:55,1
AMD,nmdh576,"Sounds like you have an AMD issue, maybe don't buy their stuff then. If you want to Nvidia suffer, suffer away.",AMD,2025-10-31 14:12:59,0
AMD,nmdihuo,Well I don't give a damn about Linux cause I am not a power user and it's not user friendly and I don't want to waste my time with it.  Your argument is like saying a slow car and a fast car drive the same in a parking lot. You need Linux to enjoy AMD? The hell.,AMD,2025-10-31 14:19:55,0
AMD,nmdnfze,Ohh so it's fine one way but not another. Do you own Nvidia stock? Lol yall hilarious!,AMD,2025-10-31 14:45:10,1
AMD,nmdnjfv,"Down vote away, it's only making my point!",AMD,2025-10-31 14:45:40,1
AMD,nmcco0g,Its because of devices like steam deck,AMD,2025-10-31 09:42:16,-11
AMD,nmpcc5j,WiVRn and Monado have fixed the SteamVR problem by not using it to begin with (it replaces the SteamVR runtime)  More info here if you're interested https://lvra.gitlab.io/,AMD,2025-11-02 13:56:42,1
AMD,nmdr3mq,"Good luck! I still dual boot as well, for the very rare times I need it. So far, it has only been to update the firmware on a game controller. Otherwise, I've been on KUbuntu for about 2 years, and it has overall been a great experience. I think I have spent more time just installing Windows updates that one time that I booted into it than fixing any issues on Linux.",AMD,2025-10-31 15:03:30,2
AMD,nmj9ixo,This is the Internet. Get rage baited and be enraged so that content creators can make some bucks. Stop being so rational.,AMD,2025-11-01 14:04:00,1
AMD,nmdhah1,"It does matter, to say it doesnt is just pure amd fanboyism copium.",AMD,2025-10-31 14:13:43,4
AMD,nmcn6sd,900 series for 11 year. I have 970 which I bought in 2014. And got last game ready drive this month,AMD,2025-10-31 11:12:07,10
AMD,nmdghqu,2000 Series also is still supported and recieved DLSS4. 2000 series released in 2019 yet still recieving newer features other than frame generation.,AMD,2025-10-31 14:09:38,4
AMD,nmca5nm,"Just like the 5 and 6000 series cards will also still be supported, despite the clickbait headlines claiming something different. AMD is NOT dropping support for the 5 and 6000 series.   They'll get basically the same level of support the 20 and 30 series get from nvidia: bug fixes, and new features only if they work out of the box with no extra effort on their part.  (9 and 10 series (and titan V) support are ending at the end of the month btw.)",AMD,2025-10-31 09:17:15,3
AMD,nmc9nun,"By Nvidia's standard of ""supported"", you can consider Polaris and Vega cards still supported rn",AMD,2025-10-31 09:12:12,0
AMD,nmdcf5k,290x still game fine,AMD,2025-10-31 13:48:35,1
AMD,nmch55g,"Still got an XFX nForce 790i mobo in service, been running W10 on it for years.",AMD,2025-10-31 10:22:40,1
AMD,nmdtq6l,"It's beyond new tech, it's also optimizations for new titles, a lot of works is baked into drivers so new releases will work the best on hardware. If you will play new title that will come out let say a year from now - you might get bad performance, stuttering or other issues. Or let say if you play game as a service (fortnite, warzone, war thunder etc.) and let say two years down the line they have big overhaul to the engine, a game you played and enjoyed with no issue with your current GPU might not work well all of a sudden, even though it still has the capability to run the game well.",AMD,2025-10-31 15:16:35,1
AMD,nmd8ud9,"We do know, because the whole premise is nonsense. AMD doesn't have cashflow issues, they don't need to cut corners.  What AMD need more than anything else is the contrary, consumer goodwill alongside solid execution to justify themselves in the face of nVidia's dominance. This move sends a clear signal that it's not worth purchasing AMD GPUs, and everyone should simply purchase nVidia going forward.",AMD,2025-10-31 13:29:12,3
AMD,nmg3cun,"yes they did.... They literally said in the ""Important information"" section of the AMD Software 25.10.2 release ""  * *New Game Support* and Expanded Vulkan Extensions Support is available to ***Radeon RX 7000 and 9000 series graphics products.***  After the backlash from that official statement they ""clarified"" their statement and only added that RDNA 1 and 2 would be receiving game optimizations based what is *“required by market needs”*, what ever that means. The cards will still be in ""Maintenance mode"".   This is AMD trying to save face. I have 3 AMD motherboards in my room, have never built an Intel system. My first GPU was the ATI 9700/9800 pro. Been an AMD fan boy my whole life. Software/drivers are the exact reason I switch to Nvidia many years ago. Still looks like they are fumbling this aspect of their GPU business.",AMD,2025-10-31 22:23:46,2
AMD,nmux0he,Hey stupid....amd backtracked and will still release driver optimizations :p.....stupid,AMD,2025-11-03 10:42:14,1
AMD,nme98mi,"So stop optimizing driver altogther if it means nothing. Optimization does matter, and I would say even more on older card so you can get to something like 60 fps.",AMD,2025-10-31 16:31:49,1
AMD,nmfsoeh,"Great to see you feeling positive emotions. A bit unfortunate that you replied before reading my post, but don't stress it's all about small steps forward.",AMD,2025-10-31 21:19:08,1
AMD,nms9pp3,"Since I'm affected, I'm not a fan of what they did.   in fact at some point, they broke the openGL drivers so I have to use a very specific radeon pro driver (22q2).",AMD,2025-11-02 22:43:09,2
AMD,nmd22dv,RDNA2 isn't remotely that old.,AMD,2025-10-31 12:51:15,91
AMD,nmdzaao,"AMD doesn't support its products 6-8 years. Rdna2 is only 5 years old.   So like, that ruins your whole point.",AMD,2025-10-31 15:43:17,16
AMD,nmdsky9,"You really have no clue , just guessing , it was 6 years and that's not old",AMD,2025-10-31 15:10:57,1
AMD,nmdwfjs,"where do you think the laptop vendors get their drivers my friend? AMD where they also get the chips, so yeah AMD ends support HP can't do shit.",AMD,2025-10-31 15:29:36,14
AMD,nmdwd5s,"If you notice what I responded to, I wasn't talking about the Vega II.. we where talking about the Vega 56/64.    But if you read thru the comments there are others talking about the Vega II.. how about you go join their discussion.   Also, what support for what card, up till they ended support are you talking about?  If it's the VEGA 56/64 driver support had very few problems over all.  You just had to turn off fast start up to avoid windows BS from screwing stuff up.  Most problems people had where user error.  Of course we are in a generation where people want to blame everything else. Heck my Vega 64 is still running strong in my granddaughter's computer.  She's happily playing sims 4 and other games with no issues.",AMD,2025-10-31 15:29:17,1
AMD,nmdvz64,"Also means any bugs or visuals in the game due to AMD’s existing drivers.   But like I said, there isn’t much else you lose. The old GPUs work but new games may be hit or miss. I think people are just angry that this is a product AMD sold as recently as 2 years ago with refreshes and that it is now effectively just dead for optimizations and game bug fixes.",AMD,2025-10-31 15:27:26,6
AMD,nme271x,"It's more than that. Example: I have a Vega 56, so I am on the same maintenance driver branch as your RX580. I could not play the BF6 beta - the client flagged my GPU as having an old driver version. Now, I don't know if that changes for the retail release - BF6 system requirements list RDNA 1 GPUs as being compatible, but now they're only going to get maintenance releases and they won't be on the current driver branch either. My point though is that ""optimizations"" is understating what is happening for being able to play new games at all",AMD,2025-10-31 15:57:12,4
AMD,nme30b3,"Theres a lot of problems. For one, if there's a game where it refuses to start on older drivers (which is more common than you think), it means you're effectively SOL on playing that game. This is especially bad on RDNA2 iGPUs like the 680m (and apparently even the 780m is affected by this which means new game support is going to be unoptimized as shit on Windows handhelds) which can be touchy at playing some of the newest games without the latest drivers.",AMD,2025-10-31 16:01:08,1
AMD,nmfn63q,"> so you're effectively losing a bit of performance in new games. what else?  Imagine you're buying a new GPU by observing the FPS benchmarks for *Game #1*. And then the sequel called *Game #2* comes out but your GPU performs weaker than it should, not because it lacks raw power but because AMD or Nvidia simply did not prep the driver to fully utilize your GPU.  If we're talking about an older card, I kind of get it. If we're talking about a 2-4 year old card like the RX6000 series, I think that's a huge blunder.",AMD,2025-10-31 20:48:20,1
AMD,nmmzyqt,"or game doesn't even start without newer driver, happens from time to time  then you're shit out of luck, can happen after a game update also  plus now this extra performance really starts to matter  and resell value of your card will not improve with this news either",AMD,2025-11-02 02:20:45,1
AMD,nmo9xwx,not only that it could make your screen do weird things because of lack of proper driver support crashes etc,AMD,2025-11-02 08:36:05,1
AMD,nmcyzbx,"yeah I feel this, sitting here on my Vega 6 on the R3 5425u laptop I got in 2023... teh APU itself released a year prior, and barely see any update through the years.",AMD,2025-10-31 12:32:48,2
AMD,nmcutfm,Imagine a multi-billion company relying on community to make drivers for themselves. This is beyond pathetic.,AMD,2025-10-31 12:06:27,-10
AMD,nme5104,"Hey now, bags of Dicks to eat are pretty good where I come from.... https://www.reddit.com/r/SeattleWA/comments/mu83zl/i_love_how_in_seattle_telling_someone_to_go_eat_a/",AMD,2025-10-31 16:10:57,1
AMD,nmjb98o,"For Z line its true but there are handhelds having laptop chips. Even this week announced rebrand of Zen2 and Zen3 laptop cpus (Ryzen 10, Ryzen 100...) is expected to use RDNA2 APUs.  In Nvidia world use of RDNA2 in current gen product would mean RDNA2 support for next 3 years (at worst). But you never now what does it mean in AMDs world.",AMD,2025-11-01 14:14:06,1
AMD,nmgjhn4,"I mean, that's like buying a Ferrari, being told you won't have access to the 6th gear after 2 years, and being okay with that because you don't drive that much anyway...",AMD,2025-11-01 00:09:36,13
AMD,nmlnisn,"Well, the ceo of Amd and the ceo of Nvidia are relatives …",AMD,2025-11-01 21:39:29,3
AMD,nmei7qi,There is some doomposting but at the core of this there is the very reasonable (IMO) position that a series which is less than 5 years old and still performs in today's gaming environment alongside AMD's latest offerings (a 6800XT outperforms the 9060XT) should not be seeing its support level cut back this soon. It's just plainly anti-consumer.,AMD,2025-10-31 17:15:45,2
AMD,nmdxtfe,"I mean if you're not playing new AAA stuff, or trying to use new tech, GPU longevity discussion clearly doesn't apply to you right?",AMD,2025-10-31 15:36:16,2
AMD,nmd1gyw,"To add, we can enjoy people who happen to care-a-lot come around and [submit fixes for 13 year old hardware](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/37885).",AMD,2025-10-31 12:47:50,5
AMD,nmdcmuq,"Nice, thanks",AMD,2025-10-31 13:49:43,1
AMD,nmdrxsa,4080S selling used for $950... 7900XTX selling used for $650... Both had a $999 msrp.  Who held better value ? clearly the Nvidia GPU. This is a well known fact Nvidia holds better value. Nvidia features are superior.,AMD,2025-10-31 15:07:43,1
AMD,nmcbw1q,"You likely mean Mfg and stuff , those are just technically impossible on those.  Fsr4 as example runs on rdna2 just fine.  Heck fsr4 even runs on nvidia cards.",AMD,2025-10-31 09:34:39,7
AMD,nmcjg3e,They still have access to DLSS4 which matters more than frame gen. They still can use FSR frame gen or lossless scaling if they need frame gen.,AMD,2025-10-31 10:42:16,2
AMD,nmccgi8,"For what it's worth, Nvidia was doing real driver optimizations for Pascal for a **long** time (see Alan Wake 2), far longer than AMD could have been bothered for GCN 3, for example (don't get me started on my experience with my Fury X). Mainstream RDNA 2 support being dropped when there's **a lot** of laptops being sold right now with RDNA 2 is ridiculous. And no, they aren't going to get regular support from this point, it'll *primarily* be security updates and maybe some specific bug fixes like what Polaris and Vega got for a little bit.",AMD,2025-10-31 09:40:17,19
AMD,nmc9hs5,Sure but this is like stopping support for the 3000s,AMD,2025-10-31 09:10:26,28
AMD,nmcbyjx,"Can you actually provide a source for this? If true, HUB would be straight up lying in this content piece, as they explicitly state that Nvidia still provides day 1 game ready optimizations for 16xx and 20xx cards in their latest drivers and that those cards are still fully supported by Nvidia, and that Nvidia supports their cards much longer than AMD at this point in time.",AMD,2025-10-31 09:35:21,5
AMD,nmdi4ft,"Suffer what, exactly? My experience has never been better than with Nvidia, so what? Am I supposed to suffer from smoothness and crash-free gaming? Will do!",AMD,2025-10-31 14:18:00,-2
AMD,nmdp043,Your point being? Nvidia dropping a tech last used in like 2014 is equal to and dropping cards they were selling brand new this year? The xx50 lineup is like 4 years old at that,AMD,2025-10-31 14:53:02,1
AMD,nmcegam,"Ok, lets follow your idea then and look at the numbers.      Linux has a 2.68% market share. Out of those 28.04& uses SteamOS Holo / Steam deck, which means 0.75% market share is steam deck, or 1.93% use Linux as their OS to play games on their PC     Here is the market share for those GPUs      RTX 5070: 1.62%  RTX 5070 ti: 0.87%  RTX 5090: 0.29%  RX 6700 XT: 0.62%  RX 6750 XT: 0.34%  RX 580: 0.63%  RX 580 2048SP: 0.59%  RX 90xx: Not on the list   Lowest I could see on the GPU department is at 0.15% with Intel UHD Graphics 600, means something that isn't on the list will be even lower  That also means the market share of Linux Mint at 0.15% is higher than the market share of individual RX 9000 GPUs. I can't tell you high the total market share of RX 9000 is, but I think it gives great perspective      So should people now stop talking about RX 9000 or any of those GPUs?",AMD,2025-10-31 09:58:52,8
AMD,nmpt2ho,"Cool, I'll have to give it another go. Cheers.",AMD,2025-11-02 15:27:03,1
AMD,nmdhfqy,"This is a fact and reality, you just don't like that.  ""Today"" AMD and Nvidia decided to drop non AI GPUs to half support. This happens.",AMD,2025-10-31 14:14:28,1
AMD,nmctdvi,"Yeah I also had the 970 until 2 years ago and started hearing rumors about it a bit later, wasn't sure when it was going to happen exactely",AMD,2025-10-31 11:56:56,0
AMD,nmdnqdz,"'They'll get basically the same level of support the 20 and 30 series get from nvidia: bug fixes, and new features only if they work out of the box with no extra effort on their part."" Imaging making shit up",AMD,2025-10-31 14:46:39,1
AMD,nmclgho,"Mines an Asus NForce 980A SLi. I had two 650 Ti Boosts fitted on the board. Win10 would try to install some drivers for the TiBoosts, then detect that the 980A's on-board 8200 became undetected, then try to install another special driver for the 8200 only to find that the 650 Ti Boosts have become undetected, and then start trying to install both drivers over and over until the registry becomes corrupted...",AMD,2025-10-31 10:58:29,2
AMD,nmed0r2,I've never updated graphics driver to get more FPS. Did you?,AMD,2025-10-31 16:50:22,1
AMD,nmg5icu,I read your clever little post. 10/10 post,AMD,2025-10-31 22:37:50,1
AMD,nmseizx,Damn username checks out,AMD,2025-11-02 23:09:34,2
AMD,nmdiw96,it's almost 5 year now from an Arch stand point. It's not like 30 series getting any day 1 patch update at this point.,AMD,2025-10-31 14:21:58,-10
AMD,nmfv0hd,And that's also not considering that not all skus released at that time. Isn't the 6750xt like 3 years old?,AMD,2025-10-31 21:32:41,15
AMD,nmgjica,Meanwhile GCN1: January 2012 to June 2021. That's 9.5 years. Not unheard of.,AMD,2025-11-01 00:09:44,3
AMD,nmdzqsb,"> If you notice what I responded to, I wasn't talking about the Vega II.. we where talking about the Vega 56/64. But if you read thru the comments there are others talking about the Vega II.. how about you go join their discussion.  There isn't some magical division in support. It's not like all the later Vega offshoots saw good or long support they were basically retired and neglected all the same at the same time they stopped giving a shit about the 56/64. It's the same topic, not a different one just because you arbitrarily want to ignore half of ""vega"".  >Also, what support for what card, up till they ended support are you talking about? If it's the VEGA 56/64 driver support had very few problems over all. You just had to turn off fast start up to avoid windows BS from screwing stuff up. Most problems people had where user error. Of course we are in a generation where people want to blame everything else. Heck my Vega 64 is still running strong in my granddaughter's computer. She's happily playing sims 4 and other games with no issues.  There were a shitload of API support issues back when I had the VII. Broken lighting and shaders in some games wasn't uncommon. Driver regressions happened at times resulting in me figuring out how to revert to older drivers on a game by game basis. And then past a point they couldn't even be bothered fixing wattman issues for it.   Yeah I still have a Vega56 in a family members computer it ""works"" (but not entirely without sometimes having to disable things or change configs... browsers in particular can be headaches but I won't pin that entirely on AMD), but all the people pretending AMDs support has been good and running to their defense on this is a fucking laugh a minute.",AMD,2025-10-31 15:45:31,2
AMD,nmf72bc,yeah reading other posts it's actually pretty shitty. especially if you play newest games (why wouldn't you),AMD,2025-10-31 19:22:22,1
AMD,nmfvg5p,"> If we're talking about a 2-4 year old card like the RX6000  Except it's 5yo card  Still get your point, tho",AMD,2025-10-31 21:35:14,1
AMD,nmpazi8,"thanks for all the answers I wasn't aware how bad it is if you try playing new games. my RX 580 is still alive and kicking in the family PC but that's mainly indie games, Roblox, Switch emulation...  definitely shitty behavior by AMD right here",AMD,2025-11-02 13:48:41,1
AMD,nmcywgj,Which part of they're the biggest single contributor to the 'community' drivers was lost on you?  There's plenty to legitimately criticize about this latest announcement without attacking them about the stuff they're doing that is actually good.,AMD,2025-10-31 12:32:18,15
AMD,nmd43q8,The community that uses these drivers would rather have it this way.,AMD,2025-10-31 13:02:46,6
AMD,nmgc9hq,Lol the open source community would love it if they major corps released their drivers. That would be godsend for them.,AMD,2025-10-31 23:21:55,4
AMD,nmdeq3f,">Imagine a multi-trillion dollar company making it impossible for their community to continue supporting products that they already paid for and own  Sorry, do you like this better?",AMD,2025-10-31 14:00:32,6
AMD,nmio4ca,Mmmmmm..... bag of dicks  ![gif](giphy|Zk9mW5OmXTz9e),AMD,2025-11-01 11:35:28,1
AMD,nmi3pbg,"No its more like being called a masochist while games have just been running fine for me, so I don't get what the fuzz is about. If anything, nvidia users are masochists on Linux. What I'm trying to say is that its not quite that black and white or drastic, not for all users.",AMD,2025-11-01 08:04:24,0
AMD,nmebaru,"I want my GPU to work for long, but yeah I see the point",AMD,2025-10-31 16:42:00,2
AMD,nmer13i,Can you provide large scale stats to back this up?,AMD,2025-10-31 17:59:41,1
AMD,nmckyx0,">Fsr4 as example runs on rdna2 just fine.  A specially made, unreleased, int8 version of FSR4, not regular FSR4.",AMD,2025-10-31 10:54:33,3
AMD,nmdqchd,so what stops you from running fsr4 on rdna2 or nvidia then?,AMD,2025-10-31 14:59:44,1
AMD,nmcdr3d,Why is MFG technically impossible?,AMD,2025-10-31 09:52:29,1
AMD,nmclca0,"> And no, they aren't going to get regular support from this point, it'll *primarily* be security updates and maybe some specific bug fixes like what Polaris and Vega got for a little bit.  Where are you getting this bullshit from?  They'll be included in all the regular driver updates, and will get the same support older GPU's have always gotten at that point in their lives.",AMD,2025-10-31 10:57:32,-1
AMD,nmc9nt8,"It's not. it's not even close.  They are ONLY stopping actively optimizing for those older architectures.  I know a lot of clickbait headline and fake meme's have tried to create the impression that AMD is dropping support of the 5 and 6000 series, but they aren't.  edit: And as both companies start working on drivers for their next generation the 30 series will get the same treatment as the 20 and 16 series.",AMD,2025-10-31 09:12:11,-9
AMD,nmdipt5,"Good for you. There are lots of people like you, who get defective cards and suffer unnecessarily and cast blame instead of RMA. Be you, insufferable, it can't be a defective card, it has to be AMD, because reasons. Try everything under the sun except the obvious, can't be a defective card, must be AMD.   Be you.",AMD,2025-10-31 14:21:03,2
AMD,nmdiw64,"It's funner to pout and complain, than to RMA.",AMD,2025-10-31 14:21:57,2
AMD,nmdjdyw,Yea. Or you just suck.,AMD,2025-10-31 14:24:30,1
AMD,nmdpyg8,"Being an Nvidia shill is easy, we get it.",AMD,2025-10-31 14:57:47,2
AMD,nmorb8b,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-11-02 11:28:04,1
AMD,nmdq09l,yes.,AMD,2025-10-31 14:58:03,2
AMD,nmcp8q1,"Since the Vista days, I never let windows auto install any driver.  Use to go in registry to turn off the auto install.  DDU does a good enough job of shutting that off now a days with a simple click.",AMD,2025-10-31 11:27:50,1
AMD,nmef59q,"I always keep my drivers updated, and yes I update my drivers to a game ready driver before playing the games. Sometimes on performance is not good on release and gets better with newer updates be it the game or the driver - it depends on the game honestly.",AMD,2025-10-31 17:00:40,1
AMD,nme4ngo,Who told you that?  The 30 series is very much getting day 1 patches.,AMD,2025-10-31 16:09:07,29
AMD,nmdjffu,"5 years was 2020 when people paid a lot money for GPUs during the pandemic shortages. Their point was RDNA 2 is still being sold in APUs and the last RDNA 2 card was refreshed in 2023. Those customers basically got 1-2 years of support or 0 years if you're unlucky to get one of those APUs in a laptop form factor.  And it's not like RDNA is a bad arch, it's perfectly capable of running modern games well.",AMD,2025-10-31 14:24:43,42
AMD,nmdzc7f,"20 series is getting day one patch updates, let alone 30.",AMD,2025-10-31 15:43:33,16
AMD,nme2hal,"30 series is getting day 1 patches, wtf are you talking about.",AMD,2025-10-31 15:58:34,25
AMD,nmmwsua,"\> It's not like 30 series getting any day 1 patch update at this point  yes it is, even the 20 series still gets regular performance and security updates, and probably for years still",AMD,2025-11-02 02:01:18,2
AMD,nmincre,"The 6950XT was released in may 2022. It’s *not quite* 3 1/2 years old, and it’s a flagship card.",AMD,2025-11-01 11:28:39,1
AMD,nmpd3by,"there's a good chance you never notice anything.. most things will probably work fine  but even so, I don't build a PC to be ""probably fine"" running most games  kinda sucks because Nvidia is also really screwing us over, but in different ways",AMD,2025-11-02 14:01:07,1
AMD,nmn0vw4,"you can have games not start without an updated driver, it happens  you've been lucky  the fact that you can still buy a card in some stores now, that is now immediately in ""maintenance mode"" is laughable",AMD,2025-11-02 02:26:30,1
AMD,nmcrnhf,Exactly.,AMD,2025-10-31 11:45:12,2
AMD,nmchdsc,Not impossible just not worth it. Whatever core it runs on is too weak on older rtx cards. So a 30 series would lose a decent amount of real frames 20 series would have worse fps overall.,AMD,2025-10-31 10:24:45,3
AMD,nmc9zwy,Dude do you work at AMD?,AMD,2025-10-31 09:15:38,17
AMD,nmca7dr,Day 1 patches also fixed texture and shader bugs.  Which rdna 1 and 2 won't get anymore.  Guess what.,AMD,2025-10-31 09:17:44,13
AMD,nmf30ke,Such an Nvidia shill I am. I'm such an Nvidia shill I have s rx6650xt I paid 150 euros more than it's worth during the pandemic and was planning to upgrade to 9000 series this christmas,AMD,2025-10-31 19:01:10,1
AMD,nmo0nfv,"And they are currently selling cpus from 4 or 5 years ago with a new name and they abandoned the gpu since they give us support and honestly since rdna 1 it looks good for games but for any program it lacks optimization for example topas video AI and image, da Vinci resolution and any other application that requires a lot of mathematical control or in this case inference,",AMD,2025-11-02 06:57:42,1
AMD,nmdkryu,It's not a bad arch but it does not build for MMA in the beginning which in hindsight is shortsighted,AMD,2025-10-31 14:31:34,-4
AMD,nmj08wh,that's correct. pretty shitty from AMD indeed.,AMD,2025-11-01 13:05:56,1
AMD,nmcuykq,Its worse. He's happily doing this for free.,AMD,2025-10-31 12:07:24,6
AMD,nmcb258,I work against lies.,AMD,2025-10-31 09:26:15,-2
AMD,nmcb2ws,They are still doing bugfixes.,AMD,2025-10-31 09:26:28,6
AMD,nmf37yb,Alright not an Nvidia shill... just a brat.,AMD,2025-10-31 19:02:14,1
AMD,nmdt6j8,"We've gone through this shit already on their past archs. Their ""bugfixes"" even on glaring shit for Vega was pretty nonexistent and barebones even before they put it officially on the back burner.   AMD deserves and has earned no special credit or faith here. None.",AMD,2025-10-31 15:13:55,2
AMD,nmcb6ar,It just won't do anything if it's like on amd cycle 3 or 6 months after game release that you finally can play the games maybe bug free.,AMD,2025-10-31 09:27:26,12
AMD,nm9jtbj,"Can someone explain what new game support actually is?  My understanding is that game engines are written according to API standards. That game shaders are calling pre-existing directx or vulkan functions, and that the implementation of those functions is already optimized in the graphics driver.  So what exactly do AMD do when they release a driver with new game support?",AMD,2025-10-30 21:28:25,22
AMD,nm6yluo,Changenotes and full discussion on this release already done:   [https://www.reddit.com/r/Amd/comments/1oj6zgl/amd\_software\_adrenalin\_edition\_25102\_release\_notes/](https://www.reddit.com/r/Amd/comments/1oj6zgl/amd_software_adrenalin_edition_25102_release_notes/),AMD,2025-10-30 13:58:47,12
AMD,nm72xl5,Deprecating and no longer providing new game optimization updates for rdna2? A 6900xt that cost over $1000 got 5 years of updates... Expensive AIB like 6900xt formula editions got 4 years of support.     What a joke. Why would anyone buy AMD cards going forward when they lose primary support and become legacy so quickly and still cost insane amounts?,AMD,2025-10-30 14:20:20,110
AMD,nm7suxb,RDNA 2 GPUs were released as recently as 2 years ago with the 6750 GRE 10 and 12gb. The 6X50 refresh gpus are at the most 3.5 years old. That's despicable as far as support time.,AMD,2025-10-30 16:24:52,66
AMD,nmb2mz0,My 6950xt is 3 years old! Ffs,AMD,2025-10-31 02:45:22,9
AMD,nm9yako,Makes BF6 crash with device hung drx,AMD,2025-10-30 22:48:55,7
AMD,nm9cp8v,Back to awful driver issues again,AMD,2025-10-30 20:52:28,6
AMD,nmabzdx,"Installed it today and every time I restart/turn on my pc I get a gpu driver crash message and kernel error in the logs….tried to reinstall, yet still persists.   Anyone else? PC has been perfectly stable up until this point. Weirdly games run fine, it’s just on startup I can hear/see the drivers crash and then when adrenaline loads up it tells me it crashed….quite strange",AMD,2025-10-31 00:06:27,3
AMD,nm9j22l,My fps is halved in Overwatch 2 on my RX 7600. Nice...,AMD,2025-10-30 21:24:34,2
AMD,nm7qz73,So rx 6600xt is being updated anymore?,AMD,2025-10-30 16:15:55,2
AMD,nmai1r8,"Lo peor de todo este driver no funciona en RX 6800XT no tiene optimizaciones, y encima te dice que no es compatible o sea no hay driver que se pueda instalar ! Y no veo que lo quieran arreglar ese instalador.",AMD,2025-10-31 00:42:28,1
AMD,nmbm3c6,So bugged tried twice with ddu and just had horrible performance and insane stuttering. Rolled back to 20.9.2 and instantly better. Please do more testing before releasing these.,AMD,2025-10-31 05:12:52,1
AMD,nmhi1he,Is it just me or are people getting less performance with these drivers? My 3Dmark runs are obviously lower.,AMD,2025-11-01 04:17:39,1
AMD,nmkn3ad,"When I installed these today, upon rebooting I was greeted with multiple ""loadlibrary failed with error 87: the parameter is incorrect"" messages, I do strip down the drivers to get rid of the bloat, i did the same with 25.9.2 and Its fine.",AMD,2025-11-01 18:25:32,1
AMD,nmaj3ma,Usually minor optimizations and glitch fixes like textures appearing / loading funky,AMD,2025-10-31 00:48:35,9
AMD,nm7aozx,"AMD is insane for thinking this is okay, I only bought my RX 6800 last year when it hit $340, we need Omega drivers to make a comeback.",AMD,2025-10-30 14:58:37,55
AMD,nma1mhc,It has not even been five years since they launched the 6900XT for $999 🤦‍♂️  Nvidia can support the ancient GTX 10XX series for some nine years with day one drivers but AMD can't even support the 60XX series for five years. Certainly helps drive the gpu business into the ground. When Lisa Su became CEO AMD had 25-30% of the dgpu market and these days  more like 5% if they are lucky.,AMD,2025-10-30 23:07:35,13
AMD,nm8fl9z,The download link is broken if you get to download from the release notes it will work I have 7900xtx and 6700xt build and I got both working flawlessly,AMD,2025-10-30 18:13:43,3
AMD,nm87yb6,"They’re not retiring RDNA 2; it seems they’re simply branching it differently. Since RDNA 2 lacks ML extensions like RDNA 3 and 4, that’s likely the reason.",AMD,2025-10-30 17:37:30,12
AMD,nm99vop,Amd saw that people prefer staying on old drivers.,AMD,2025-10-30 20:38:49,2
AMD,nmlcpvk,Cries in OC formula. It is what it is I suppose.,AMD,2025-11-01 20:41:13,1
AMD,nm8d6n7,why you say no update.....NAD was the only one that improved overtime....even rx580 wa usable for long time.....  and you can use FSR4 and frame gen with all gpu.....,AMD,2025-10-30 18:02:24,0
AMD,nm821nr,Not because of this but because AMD let's issues just continue like driver timeouts. I'll never ever buy another AMD card. Sucks I will pay more for nVidia but I've never had problems like I've had owning an AMD card.,AMD,2025-10-30 17:09:01,-6
AMD,nm822v8,Things like the steamdeck are RDNA 2 as well,AMD,2025-10-30 17:09:11,31
AMD,nmc0379,Same. Was perfect before this.,AMD,2025-10-31 07:33:09,2
AMD,nmckrom,"Overwatch 2 does shader compilation every time you launch the game for the first five minutes or so on Radeon cards. After that it should run fine. If it doesn't, check if it somehow switched to the much less performant DX12 API in the game.",AMD,2025-10-31 10:52:56,1
AMD,nm89f2t,"It's being updated but stuff like day 1 support for games would be delayed and they would focus on RDNA 3 AND 4 first, but it would be still be updated.",AMD,2025-10-30 17:44:38,6
AMD,nmavf1d,"Optimizations I can understand because there are architectural differences in the hardware from each vendor. There are cases where things take different amounts of time or resources depending on the hardware. And each vendor wants the best FPS number in the day one reviews for AAA games.  But how do glitches occur if the software is written according to the API standards, and how is it AMD's responsibility to fix them if the software is glitchy because it didn't follow the standards?  Or are game engines and gfx drivers much less [deterministic](https://wippler.dev/posts/determinism-in-software---the-good,-the-bad,-and-the-ugly) than I'm assuming?",AMD,2025-10-31 02:01:30,2
AMD,nm9cclu,lol same here. Bought an RX 6800 a year ago. Shame really. Looks like I’ll have to upgrade much quicker than I anticipated fs,AMD,2025-10-30 20:50:46,14
AMD,nmb0e5z,"Omega drivers, wow that takes me back. I had completely forgotten about it",AMD,2025-10-31 02:31:35,2
AMD,nm8984i,Branching GPU Architures is bad?? From a developer perspective that is normal.,AMD,2025-10-30 17:43:42,-14
AMD,nm93jdk,"Again, it no longer includes new game updates. That is retirement for a gpu. Sure, the drivers can be updated. Doesn't change the fact those drivers you just updated will not include new stuff for RDNA2",AMD,2025-10-30 20:08:09,4
AMD,nm8qbnm,"It's clearly stated  on the driver description of the latest drivers :   ""New Game Support and Expanded Vulkan Extensions Support is available to Radeon™ RX 7000 and 9000 series graphics products.""",AMD,2025-10-30 19:04:05,11
AMD,nmcqram,Ye but reading isn’t strongest skill of Redditors. Never been or never going to be. Or thinking either.,AMD,2025-10-31 11:39:04,1
AMD,nmordh3,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-11-02 11:28:38,1
AMD,nm87ew9,The only time I experienced a driver timeout was due to a problem with faulty RAM.,AMD,2025-10-30 17:34:52,9
AMD,nm9agr1,"Sounds like you have a bad card or something wrong in your system. Been running my 6800XT since release day, never had a driver timeout naturally at all. The only time I did was when I did an unstable overclock.",AMD,2025-10-30 20:41:38,2
AMD,nmam8b9,Do you have a ram issue?,AMD,2025-10-31 01:06:41,1
AMD,nm9gmo9,"Just to clarify, Linux drivers are a separate ecosystem from Windows. RDNA2 cards get feature updates and optimizations through AMDGPU/Radv/Mesa (although recently radv and mesa combined) independent of Windows ‘Game Ready’/maintenance mode. Even the Steam Deck will keep getting Vulkan/driver improvements. So ‘maintenance mode’ on Windows doesn’t mean Linux support is frozen, the open source stack evolves on a different schedule and often keeps older cards performant for years. i.e the HD 7000 (from 2012) series is still receiving game patches on linux.",AMD,2025-10-30 21:12:06,23
AMD,nmha5wp,"Mine is actually crashing my pc on wake(just monitor shutting off). I even reverted to the previous driver and I am still having issues, multiple DDU as well. Absolutely no problems prior.     Edit. After doing a factory reset via the adrenaline software for 25.9.2 (again after using DDU a few times) it seems to have been fixed.",AMD,2025-11-01 03:14:57,1
AMD,nmclsek,"Yeah, on my previous cpu (ryzen 5 3500x) I had like no fps in the first 5 minutes, but since I upgraded to ryzen 7 5700x, it takes no time to load the shaders and I can play with 300+ fps. Now after this driver update it's around 150, and doesn't improve, not even after 30 minutes. But I will check again later today.  To be honest I had a lot of issues with this gpu and I'm really fed up with it. Thinking about switching to nvidia.",AMD,2025-10-31 11:01:08,2
AMD,nmckn01,Games are built with the generic APIs and hardware agnosticism in mind. A hardware vendor like AMD can always look at its driver to see if it can improve its interaction with the game to get additional performance out of it or if the game's implementation is conflicting somehow with the driver then see if it can be improved. In the vast majority of cases this doesn't result in big improvements but there have been cases in the past where driver patches greatly improved the performance or reliability of a game.,AMD,2025-10-31 10:51:54,6
AMD,nmapz1x,"![gif](giphy|9rjKLsynBodhiIovdD)  me with my 3 year old rx 6800.  GD it didn't even reach 5 years and they are preparing to put it into long term storage?  Will not make the same mistake again, one bitten, twice shy.",AMD,2025-10-31 01:28:56,8
AMD,nmbwe5u,"With 16GB memory, it'll be great for existing games and mid-tier for new games for a while yet. Just looks like AMD have decided to give up all pretenses of porting new software driver features into RDNA(2).",AMD,2025-10-31 06:54:32,5
AMD,nm8pzvq,"Nvidia relased DLSS4 on the 2060, an almost 8 years old card, so yeah, branching an architecture (placing it outside new games optimizations) that is still on the market (APUs) is not just bad, is totally unacceptable.",AMD,2025-10-30 19:02:30,9
AMD,nmaxye7,What I've read is day 1 optimizations. It doesn't mean they won't receive it.,AMD,2025-10-31 02:16:50,4
AMD,nm9gt6z,"These extensions likely require hardware support, so this is no surprise.",AMD,2025-10-30 21:13:01,1
AMD,nm93orw,I have them too if I wouldn't manually offset the frequency. These cards boost way higher than they should which can result in crashes. My card is rated for 2.97Ghz and pushes up to 3.4Ghz which is unstable. Why AMD decides it's ok to push the frequency higher than what it actually should use as a maximum is highly questionable,AMD,2025-10-30 20:08:53,1
AMD,nm9k489,Switching to Nobara this weekend. Thank you for this info,AMD,2025-10-30 21:29:59,3
AMD,nmbw748,"""Updated support"" still depends on if AMD engineers want to touch (or get paid time to touch) RDNA2-limited branches inside those open source drivers. They don't magically feature-update themselves.",AMD,2025-10-31 06:52:33,-1
AMD,nmha8ph,Event/BSOD states amdkmdag.sys as the problem.,AMD,2025-11-01 03:15:29,1
AMD,nmcptr4,"Yeah, I understand how you feel. I've been playing Overwatch since it came out and it worked flawlessly on Nvidia cards. With Radeon, there is always the pointless shader compilation every fucking time the game is launched and I don't know why it's the case because it absolutely does not happen on Nvidia. My 6700 XT works fine in most games but then there are issues in some games that are AMD only and I miss the days of using an Nvidia cards where I just did not have to think about the GPU at all.",AMD,2025-10-31 11:32:14,1
AMD,nmaxufz,day 1 optimizations,AMD,2025-10-31 02:16:10,1
AMD,nm9i2hc,"I'm not talking about the extensions, ""New Game Support"" is the problem.",AMD,2025-10-30 21:19:28,15
AMD,nmjv9ch,"Sure, AMD funds part of the work, but Mesa and RADV aren’t AMD exclusive. Valve, Collabora, and others actively maintain cross architecture code, so RDNA2 still benefits from broader Vulkan and shader improvements. The open driver stack doesn’t stop evolving just because AMD moves on from Windows support.  Mesa and RADV are architecture agnostic in many areas. When new Vulkan extensions or optimizations land, they often improve all supported architectures (including RDNA2) unless there’s a hardware specific blocker. So even if AMD isn’t explicitly targeting RDNA2 you still get continued support and patches.",AMD,2025-11-01 16:03:10,1
AMD,nmddwvs,"Mistake or not, a line like that making it into driver release notes in the first place certainly doesn't reflect well on AMD's driver team.",AMD,2025-10-31 13:56:19,310
AMD,nmdb55k,"...keep going AMD, you can do it... roll it all back.",AMD,2025-10-31 13:41:47,250
AMD,nmdhkug,Damn if that happens my whole setup would get cooked. I run a portable type c monitor with my sff build,AMD,2025-10-31 14:15:11,29
AMD,nmddqfn,Is there anyone who installed 25.10.2 and verify that it's really turned off power from type-c before AMD correct statement?,AMD,2025-10-31 13:55:25,13
AMD,nmdbef2,"Interesting, now bring back *full* support to my TWO YEAR OLD rx6750xt you crooks.",AMD,2025-10-31 13:43:11,169
AMD,nmdovkf,"Now, can they also correct the other error.  ![gif](giphy|4NnTap3gOhhlik1YEw|downsized)",AMD,2025-10-31 14:52:24,19
AMD,nmdipu3,![gif](giphy|PMfeaU44ChYmRdWDrY),AMD,2025-10-31 14:21:03,10
AMD,nmfqpqi,Gotta love AMD for fumbling the dumbest of things sometimes. Ugh.,AMD,2025-10-31 21:07:53,5
AMD,nmeeotb,"That wasn't a mistake, they were testing the waters. Realized people are getting pissed off so came back and said it was just a ""typo"". Sure.",AMD,2025-10-31 16:58:29,2
AMD,nmic6a4,"In theory if i had usb type c on my GPU and they disabled power delivery it would still deliver power, because i have one of those PSU's that has a feature that gives any usb port power even if i flip the power switch, unfortunately that also means some bugs with some boards that do not handle this well, especially if they cheap out and connect wifi and bluetooth over bluetooth rather then pci-e lanes.",AMD,2025-11-01 09:38:37,1
AMD,nmn8hpo,"There was the pink artifacts still happening for rdna 3 when hardware acceleration's on, and a few months back the kingdom hearts issue that constantly crashes. I wont say amd's flawless lol",AMD,2025-11-02 03:14:23,1
AMD,nmpmnqz,"The answer for everyone is linux.  Open source drivers means community made support.  Look at Alan Wake 2 on Windows vs Linux for a Vega GPU. It actually runs, playable and no bullshit graphical artifacts or glitches compared to windows which makes it near unplayable.  You telling me open source is capable of doing this but AMD can't?  I guess Linux just sucks for those who like online slop games like bf6. Guess can't play those.",AMD,2025-11-02 14:53:38,1
AMD,nmwabfn,AMD standing there in their hotdog suit trying to find the guy who did this,AMD,2025-11-03 15:50:03,1
AMD,nmdul12,and the driver 25.10.2 dont work with RX 6800 XT fix the setup ! is not compatible !,AMD,2025-10-31 15:20:46,0
AMD,nme6kjs,Uh buddy AMD’s drivers don’t reflect well on AMD’s driver team.,AMD,2025-10-31 16:18:33,133
AMD,nme0lit,"Truly remarkable incompetence from AMD  Created a situation where tons of customers get angry (including myself) and now view AMD in a negative light, all because they didn’t check their fucking release notes AND the driver package itself (initial driver file had no driver for 6000 series but they updated it). I bought a 7900xtx reference specifically for the usb c port.  This is truly pathetic",AMD,2025-10-31 15:49:36,73
AMD,nme5igm,"They just ""clarified"" RDNA 1 & 2 GPUs will get day one game patches.   The only positive is that AMD is still a push over, I guess.",AMD,2025-10-31 16:13:21,17
AMD,nmdlehg,Can i ask what type c monitor you have?,AMD,2025-10-31 14:34:48,3
AMD,nmdoxum,"https://www.techpowerup.com/342468/usb-c-power-delivery-is-fine-on-rx-7900-xtx-we-tested-it-amd-confirms  >  We tested the feature to make sure it is supplying the port with sufficient power. Using a USB-C power meter, we recorded 27 W of power output from the AMD Radeon RX 7900 XTX. Impressively, the card can supply full power to the port even without a driver installed, as demonstrated in our test. Installing the latest driver doesn't alter this behavior, indicating that the original driver changelog was just incorrect.",AMD,2025-10-31 14:52:44,44
AMD,nme39vn,"I tried my VR headset yesterday, I takes power from the same USB-C port and it works.   Giving that the USB-C on the reference cards is VirtualLink for use with VR headsets it was a bit concerning that they would disable power delivery on it.",AMD,2025-10-31 16:02:26,7
AMD,nmw54ws,"I've commented on a few threads. People just like to be outraged. Everyone who was pissed could have tested it in 10 minutes, including rollback to the previous driver if needed.",AMD,2025-11-03 15:24:51,2
AMD,nmdptcz,This has been updated as the poster below mentions.  also most AIB cards don't have this USB type C port its mostly reference cards.   My XFX card doesn't have one.,AMD,2025-10-31 14:57:04,1
AMD,nmdfaeh,"It doesn't matter from now on.   It is much easier to understand where things are going.  Does your GPU have ~~FP8 or better(AI/ML) capability~~ ""AI Accelerators"" ?   Yes = Full support, No = half support.",AMD,2025-10-31 14:03:29,42
AMD,nme6zl8,They've gotta backpedal all this nonsense I've been hearing the past few days. Please tell me the driver team is all playing a sick prank on us.,AMD,2025-10-31 16:20:38,3
AMD,nmec1cr,"I have the same GPU, works great with open source driver under Linux.   JoIn Us",AMD,2025-10-31 16:45:35,2
AMD,nmdejr2,"They won't, I'd sell it if I were you it's still not too late.",AMD,2025-10-31 13:59:38,-8
AMD,nmfcyut,AMD has had the best drivers this year,AMD,2025-10-31 19:53:49,70
AMD,nmu413r,AMD's Radeon division don't reflect well on AMD.,AMD,2025-11-03 05:49:18,1
AMD,nme3mca,"Until they give us a few more years proper support, I'm still going to be very, very pissed off.  I really don't want to buy an Nvidia card, but I don't see another option.... how the hell do I know if I buy a 9070XT it won't get cut off in a year or two when AMD make another AI breakthrough?  Only way to send a message is with your wallet.",AMD,2025-10-31 16:04:06,27
AMD,nmp5ayl,I keep saying AMD's biggest problem (despite the claims by a certain benchmarking website that they have throw everything into it) is that they seem to do poor investment into marketing.    It's near constant marketing and PR cockups.,AMD,2025-11-02 13:13:15,1
AMD,nme5w65,"Lol I'm not sure if adjusting release notes for a computer GPU counts as ""pathetic"".",AMD,2025-10-31 16:15:13,2
AMD,nmdmexz,Some generic yodoit brand from Amazon. 60hz freesync and it was only 50 bucks. 6950xt reference runs perfectly except for some artifacting that isn't present while using dp or HDMI ( monitor is fine on my steamdeck),AMD,2025-10-31 14:39:58,3
AMD,nme1a40,"Techpower up is wrong about one thing - the usb c port does support data transfer despite then claiming it does not.  I tested it with an external ssd not long ago, 10gbps enclosure ran at full speed",AMD,2025-10-31 15:52:50,22
AMD,nme74j7,Glad to read,AMD,2025-10-31 16:21:19,5
AMD,nmw8j12,Indeed,AMD,2025-11-03 15:41:23,1
AMD,nmdg60e,Why did they even shut off the power? What's the point?,AMD,2025-10-31 14:07:58,1
AMD,nmdivs2,"Meanwhile Nvidia has supported cards way older for much longer, no excuses.",AMD,2025-10-31 14:21:54,23
AMD,nmiv08m,2000 series cards don’t have fp8 but are supported. Also the 7000 series can’t do fp8.,AMD,2025-11-01 12:30:19,1
AMD,nmdndut,Its too late now I sold my 6800XT in 2023. Trying to sell one it now even if these changes were not made in late 2025 going to 2026 there isn't much resale value.  I've been doing this a long time usually you sell one gen behind to get the best resale value.  So when RDNA 3 launched that was the time to sell RDNA 2 cards waiting until RDNA 4 came out is abit to late.,AMD,2025-10-31 14:44:53,3
AMD,nmdf3j0,Whos gonna buy it now that it no longer gets support?,AMD,2025-10-31 14:02:29,-3
AMD,nmg9920,"Yes, but that's been an extremely low bar considering how broken Nvidia's were (AMD has been seesawing between terrible and fine depending on generation, with RDNA 1 and 3 having major issues, and RDNA 2 and 4 generally being fine).  Your timing is also kinda bad seeing how 25.10.2 is causing hard crashes and reboots due to driver issues with 9070XTs when computers are left idle.",AMD,2025-10-31 23:02:22,27
AMD,nmibz0e,"Oof, that's saying something. Amd has notoriously bad drivers, and that never seems to change",AMD,2025-11-01 09:36:28,1
AMD,nmfny08,Irrelevant if they only last 3 years before entering maintenance mode...  Edit: Clearly this didn't land as the joke it was intended to be following the controversy earlier. I will take my leave.,AMD,2025-10-31 20:52:31,-6
AMD,nme4om1,"I mean things definitely aren’t ideal. NVIDIA’s drivers have been dog shit all year long as well. My 5080 still freaks out if there are more than 3 monitors connected, the last truly stable driver from them was 566.36, which was from december 2024 and doesn’t even work on 50 series cards.",AMD,2025-10-31 16:09:16,24
AMD,nme9p3i,Well i am for sure happy with my 9070xt. Does its work for less bucks than nvidia,AMD,2025-10-31 16:34:06,5
AMD,nmdzyvi,"I don’t think they did? The correction is pointing out that it wasn’t changed and the patch notes were just incorrect.   Maybe it was something that was supposed to go out but got pulled cause someone was like, this is pointless it’s a tiny portion of cards and the code isn’t doing anything bad but never got removed from patch notes. Happens sometimes",AMD,2025-10-31 15:46:36,4
AMD,nmdkn46,"I can't explain in a more simplified way.  Nvidia started with AI GPUs earlier, thus more AI GPUs left to support.  AMD started late with AI GPUs, thus only RDNA 3 or better, to support.  It is a global decision made by both Nvidia and AMD. It just happened recently, they decided to do it in 2025.   Nvidia did it in July 2025, AMD did it now.",AMD,2025-10-31 14:30:54,29
AMD,nmgi6oh,"There isn't any more ""full support"" left to do. These cards are fully optimized and operating near theoretical performance peaks.  For 2027 games the difference between ""full support"" and ""maintenance support"" will be 0 FPS.  For 2029 AAA games, RDNA2 GPUs aren't going to be able to reach acceptable frame rates regardless of full support or not, because they lack the required hardware for the level of raytracing and upscaling that the games will have as minimum requirements.  Whether or not this was done this year, next year, or in 2027 is just optics, it doesn't chance anything.   There hasn't even been much optimization done in the last year anyway, so they've basically been in defacto maintenance mode.",AMD,2025-11-01 00:00:50,8
AMD,nmdl9g8,"I'll play devil's advocate for a bit.  Nvidia is 13x bigger than AMD, and is almost exclusively focused on GPUs. Yes they do some CPU work (it's mostly ARM chips) and some networking (high GBe NICs), but their core is exclusively compute.  Meanwhile AMD is so much smaller, and their money is split between CPU and GPU divisions; with the CPU side being much healthier financially.  Not defending AMD's move here, but it's a miracle they're even competing at all - anywhere - with Nvidia, given the orders of magnitude less resources they have.   It *LITTERALLY* is David vs Goliath in terms of resources available to get shit done. Hope this at least puts things into context.   TL;DR: Which is why I don't think comparing to Nvidia is fair.",AMD,2025-10-31 14:34:05,19
AMD,nmft6nf,"Does nvidia specify which gpu are not on there priority list? The problem is AMD announced they are just not putting RDNA 1 AND 2 on there 1 day support anymore but they are still supporting the drivers, Did nvidia do that?  Nope it's a problem with AMD release note that broke it that way.",AMD,2025-10-31 21:22:03,2
AMD,nmf6td2,"No. FP4 support is for 5000 series only. They could say that they're not supporting 4000 series anymore for new features.  Well, no, they just aren't phrasing it that way because it would drive away customers.  Meanwhile, AMD casually drops that shit.",AMD,2025-10-31 19:21:03,2
AMD,nmivnaf,"Yeah, you are not the only one mentioned it, I will edit it.  What I actually meant is specific hardware = Tensor Cores = AI accelerators in general.   Point still stands, hardware is what matters for this decision, not the year of which a GPU was released.",AMD,2025-11-01 12:34:59,1
AMD,nmdrcit,"Thing is my plan was to continue using it for a long while, I knew support would decline *eventually…* but only after *two years* of owning a card that only released a year before I bought it? It’s inexcusable regardless of anything.  As I said in a different comment, there is always something new: Power, VRAM, Raytracing, and now AI.  It’s simply not an excuse.",AMD,2025-10-31 15:04:45,0
AMD,nmdfhy6,~~half/slower~~ basic support and bug fixes as always...,AMD,2025-10-31 14:04:32,12
AMD,nmdfr6f,"Yeah it's becoming harder and harder to sell with each passing second as the news go out.  Other than that maybe one of the 13 people that use linux might buy it lol, it'll be supported there",AMD,2025-10-31 14:05:50,-3
AMD,nmgwfk3,Oh is this a known issue now? I’ve been having this for a long time now,AMD,2025-11-01 01:37:42,7
AMD,nmjkeqb,Basically zero issues on RNDA3 here so...,AMD,2025-11-01 15:05:53,4
AMD,nmk5t7s,"Lmao no one on this subreddit will accept this even though it's been a thing since the dawn of time.   They both have had their issues. But many people, myself included seems to notice it A LOT more with AMD. I never had an issue on NVIDIAS 900,1000 series cards. As soon as I went to AMD for the 7000. Problems. Black myth was fucked for a week for me. Literally unplayable after a driver update that took a week to get fixed. TLOU2 performance was awful at launch and that took awhile to get fixed. DL2 took a month for like this weird flashing all over the place. DL the beast still crashes randomly.   I mean the only problems I ever had on team green was related to $$$.",AMD,2025-11-01 16:57:27,2
AMD,nmg07gr,"is it? If the drivers are good, games work, performance is good, is truly such a travesty?",AMD,2025-10-31 22:03:46,3
AMD,nmfrh4n,"C'mon, it's mostly 4-6.25 year old GPUs.  Only a handful of 3 year old ones, which would have the same level of high optimizations anyway.  There just isn't much more work to do here, there is basically zero value for game-specific optimization at this point. And if there is a significant issue, we will see a patch for it for at least the next 2 years.  The industry is undergoing a technology transition and RDNA2 is going to be left in the dust for next-gen games, but Nvidia's 20XX series was farther along on the required hardware.",AMD,2025-10-31 21:12:14,-3
AMD,nmev0lv,"NVIDIA drivers have certainly been pretty sub-par this year. I now and again get a black screen on one monitor (I believe it is related to a VRR bug) in a certain game, only for it to instantly turn back on.   Toggling HDR in Windows sometimes makes both monitors go black, only fix is a reboot.   And I'm on 40 series, and it didn't become an issue until the 50 series launched lol.",AMD,2025-10-31 18:20:12,8
AMD,nmefun3,"2025 release Ryzen Z2A = 0yrs driver support  The GTX 750ti is running October 2025 game ready drivers 11yrs later. At worst, the GTX 1080ti only gets 9yrs + 3yrs security updates. How is any of that equivalent to AMD screwing over R9 Fury X, Radeon VII, VEGA iGPU, and now RDNA 2 handheld owners with at best 1/2 the driver support of their main competition?",AMD,2025-10-31 17:04:06,3
AMD,nme95f0,">Does your GPU have FP8 or better(AI/ML) capability?   Yes = Full support, No = half support.  and  >It is a global decision made by both Nvidia and AMD. It just happened recently, they decided to do it in 2025.   Nvidia did it in July 2025, AMD did it now.    This has nothing to do with Nvidia and Nvidia made no such decision. They still support all RTX generation back to the 20 series which doesn't support FP8 at all. The ""AI specific"" formats introduced and supported are:  30 Series (Ampere) = BF16   40 Series (Ada) = FP8   50 Series (Blackwell) = FP4  Nvidia announced the ending of support for Pascal and Maxwell GPUs, but the first ""game Ready"" driver missing support for those is still not out.",AMD,2025-10-31 16:31:23,4
AMD,nmhz3wg,"Sure, I don't expect GPUs that are 1-2 gens older to get any real optimization, but the only problem is that some games have now started putting minimum driver version checkers into their launchers, e.g. BF6.  I fear this will become the new trend so GPUs with no active driver support, even if it's just a checkmark without real optimization, get shafted early.",AMD,2025-11-01 07:12:21,5
AMD,nmhnv7l,"Sure, but just feels like the PS5 has barely gotten off the ground and it is on its 5th year already. Very few games that stand out in comparison to the previous genration, no wow factor at all.  Kinda feels the same going from GCN to RDNA. Sure it is better, but it hasn’t enabled for new gameplay concepts that would not be possible on GCN.",AMD,2025-11-01 05:10:59,2
AMD,nmivxk4,"When new games come out, optimizations to drivers are often needed. Sure the card has “full support” for all released games. But new games will be a pain. It’s not uncommon to see 25%+ increases in performance when drivers are optimized for a new game. So no, 2027 games will not see a zero fps difference. Also rdna2 isn’t that old or unsupported. It’s RT capabilities aren’t that much worse than the 7000s and rdna 2&3 both use fsr3. 7000 series is better at matrix multiplication but the 6000 series isn’t bad. The 6900xt has 40 tflops of fp16 performance which is as much as modern nvidia cards. Overall the 6000 series is comparable to the rtx 2000 series which is supported.  Rdna1 had to go. Doesn’t even have mesh shaders.",AMD,2025-11-01 12:37:02,2
AMD,nmdvp2p,"It may be David vs Goliath, but from the customer's prespective, that doesn't really matter. AMD was the ""FineWine"" buy. Seems to no longer be the case, which makes AMD seem less apealing than they were before.",AMD,2025-10-31 15:26:05,9
AMD,nme292f,"This argument would maybe make sense if Intel, the complete newcomer to dGPUs, hasn't been regularly rolling out updates to Arc multiple times a month since 2022, and their iGPUs have been supported since forever. I think their older iGPU driver branch STILL gets updated.",AMD,2025-10-31 15:57:28,8
AMD,nmdrodi,"To counter though whether they want to or not they have to compete with what’s in the market. The issue is investment into the team.   Profits arent good? Why? It’s because they aren’t competing well against nvidia? Ok let’s make another product to compete and add more resources.   They shouldn’t be letting the team be that small due to profit margins. That would lead to a bad cycle of releases lacking support.  I will say the 9070 xt is awesome though and I have one, but the driver support definitely has room for improvement even on their newest card. If they want to stay in the market they need to grow not stagnate with same size team and stop showing to investors every 5 seconds, but that is wishful thinking for any company perhaps doh.",AMD,2025-10-31 15:06:24,4
AMD,nmdwt85,"Let me put it into customer context: there is no point in AMD doing GPUs, they just have zero benefit compared to Nvidia. AMD just seems almost glad for that to happen",AMD,2025-10-31 15:31:25,3
AMD,nme2xqs,">Meanwhile AMD is so much smaller, and their money is split between CPU and GPU divisions; with the CPU side being much healthier financially.  AMD has a market capitalisation of 419 *billion* USD. nVidia being bigger is no excuse.",AMD,2025-10-31 16:00:47,-2
AMD,nmevy7s,"AMD is smaller, but not as you make out. From a quick Google search, AMD stands at around 28,000 employees, NVIDIA 40,000.  Yes, I admit, NVIDIA probably have far more dedicated to their GPU division than AMD, but AMD is still a massive company.   You HAVE to compare AMD to NVIDIA because they are direct competitors. This isn't a fantasy world, they are competition. You can't let shit slide because the competitor is so much bigger. I wouldn't buy an AMD GPU as it stands because of many reasons, but this driver support stuff sure as hell convinces me I won't.",AMD,2025-10-31 18:25:01,0
AMD,nmjj8ed,"That’s true, I wasn’t trying to nitpick (although I admit I can be bad about it)   No doubt ai hardware is necessary and seems to be the dividing line. The 7000 series is cracked at fp16 performance and can do int8, honestly its WMMA hardware is grossly underused imo. While this is a gross simplification, the 6900xt, 7600xt, and rtx 5080 all have 40 tflops of fp16 performance. It really sucks rdna3 can’t do fp8 calculations. Considering rdna3 is the most powerful gen regarding fp16 performance, I’m hoping fsr4 is brute forced. I know there’s projects but I’m hoping for something official. I have a 4090 and 7900xtx, which I want to give more love. The rtx 2000/3000 series can’t do fp8 either, while still having solid matrix multiplication hardware, so the 7000 series should be able to stand the test of time as long as amd supports it well.",AMD,2025-11-01 14:59:26,1
AMD,nmdmgtn,"just export it to a 3rd world country, I will be happy with a cheap used graphics card,",AMD,2025-10-31 14:40:13,7
AMD,nmhcabl,"Hasn't been added to the list but it just started happening to me when I updated to 25.10.2, and I saw some posters on the AMD driver thread also complaining about it.  Rolling back to 25.9.2 fixed it for me.",AMD,2025-11-01 03:30:40,3
AMD,nmuv381,">since the dawn of time   >lists a few recent series   >forgets to list all the NVIDIA drivers that cooked GPUs, multiple times",AMD,2025-11-03 10:23:27,1
AMD,nmkcfkn,"Yeah, i mean I had to nerf my 7900 xtx just to play BF6 because the drivers for the cards suck ass. And the problem has been there since the BF6 beta.   The fix was simple enough though, just cap the cards legs and it worked fine ever since. Shouldn't be a thing in my opinion though.",AMD,2025-11-01 17:30:59,1
AMD,nmej0er,"> The GTX 750ti is running October 2025 game ready drivers 11yrs later.  Yeah, but how many fixes / optimisations for games are actually being commited into those drivers for that gen of cards?  Not defending AMD here but ""support"" is a really wooly term.",AMD,2025-10-31 17:19:41,8
AMD,nmgk0nc,"Just because it's a Game Ready Driver, doesn't mean that GPU is receiving a single line of code for any games.  Those cards have likely not received any significant optimization for like 7 years and probably zero optimization for 5 years and are likely just checked for bugs and security on modern OSes, aka ""Maintenance Mode"".  The only reason they're finally being deprecated now is that they won't be able to run the next waves of games because that will require tensor cores or equivalent.",AMD,2025-11-01 00:13:10,4
AMD,nmgdyv3,Lol you think a 750ti with only 2Gb of ram is getting day 1 game ready drivers for something like bl4 or bf6? Would you like a cantilever or suspension?,AMD,2025-10-31 23:32:58,3
AMD,nmf1jnx,"GTX 750TI longer support, because it is based on Maxwell(GTX 900 series).  And for example GTX 16 is a Turing based GPU just like RTX 20 but lacks Tensor cores(hence the naming GTX vs RTX), it falls right in between and was lucky to stay the course thanks to RTX20, sooner or later Turing full support will be dropped too(I bet GTX 16 will be sooner).  Nvidia announced in July 2025 that they will drop Maxwell and Pascal after October 2025.",AMD,2025-10-31 18:53:41,1
AMD,nmebb9z,"OK I didn't explain myself properly. I was talking about specific hardware in GPUs.   There is a reason they switched naming from GTX to RTX, for example Tensor cores. All actual ML features released for RTX20+.   The only ""weird bird"" is the GTX 16 which is based on Turing but lacks Tensor cores, I guess it was just lucky enough to stay the course, unintentional ""support"".  World is moving resources to AI is true, it is what it is, for good or bad.  Nvidia said that after October driver release, they will drop support, October ends today.",AMD,2025-10-31 16:42:04,6
AMD,nmdmzs3,Nah I am a firefighter.,AMD,2025-10-31 14:42:55,9
AMD,nmg9v4w,"Intel has already moved 14th gen iGPUs to legacy support, and those are only 2 years old and still sold in plenty of new systems.",AMD,2025-10-31 23:06:22,2
AMD,nmeyw3o,iirc intel is only now slowing the pace of old igpu drivers,AMD,2025-10-31 18:40:09,1
AMD,nmdtfdf,"Well potentially 9070XT will get more more/faster support, they are moving more resources from anything before RDNA 3 to only RDNA 3 and RDNA 4.",AMD,2025-10-31 15:15:08,5
AMD,nmdvsdw,This is actually a great idea.,AMD,2025-10-31 15:26:32,1
AMD,nmhck6a,"I’ve had it periodically occur since launch, mostly seemed to happen after I locked the system or put it to sleep, thought an install in place repair had tackled it but it does seem like it started again lately",AMD,2025-11-01 03:32:49,2
AMD,nmhzcgz,"I don't expect GPUs that are 1-2 gens older to get any real optimization either but the only problem is that some games have now started putting minimum driver version checkers into their launchers, e.g. BF6.  I fear this will become the new trend so GPUs with no active driver support, even if it's just a checkmark without real optimization, get shafted early.",AMD,2025-11-01 07:15:05,3
AMD,nmgkda0,"I'd bet there have been 0 optimizations applied for the GTX 750 Ti in the last 5 years.  And I mean, that thing is way below the minimum requirements for games that you'd need to write optimized code for anyway, so it's a bit ridiculous it's even listed as ""supported"".",AMD,2025-11-01 00:15:35,1
AMD,nmgkgh4,"Of course the 2GB card isn't going to be able to play BF6, but the GTX 980ti 6GB based on the same architecture and below game min specs can sure do 1080p60fps low settings with FSR-Q or 70-75fps with FSR-B. The main concern here is how CPU intensive it might be while paired with an equally old CPU.  https://youtu.be/wF0bG3LmGRA  .....  BL4 barely runs on an RTX 3060 12GB at 1080p. Day 1 patches for older/slower hardware won't change that fact that the game runs like ass.",AMD,2025-11-01 00:16:12,2
AMD,nmecz0q,">OK I didn't explain myself properly. I was talking about specific hardware in GPUs.  FP8 is a number format and not something you use to describe hardware components.  >There is a reason they switched naming from GTX to RTX, for example Tensor cores.  That doesn't matter for FP8, all RTX cards have Tensor cores, the difference is in the feature set (sparse matrices support for example) and format support (as per the list in my previous comment).  >Nvidia said that after October driver release, they will drop support, October ends today.  Yes, the older, ""pre-RTX"" cards is what they are dropping support for, specifically the Pascal (2016) and Maxwell (2014) range. It has nothing to do with FP8 format support. The Turing (2018) cards (RTX 20 Series) don't support any of the specific ""AI"" formats (BF16, FP8, FP4) and still have support.",AMD,2025-10-31 16:50:08,5
AMD,nmdqb5n,"There is *always* something new, first it was just raw power, then ray tracing, VRAM, and AI. That don’t excuse premature software depreciation.",AMD,2025-10-31 14:59:33,-4
AMD,nmhn3k6,"I mean, you aren't getting any meaningful game optimizations on a fucking iGPU of all things.",AMD,2025-11-01 05:03:33,1
AMD,nmga8ky,"No, Intel stopped supporting their old iGPUs a while ago. They just recently ended active support for 11th-14th gen iGPUs (1st gen Xe iGPUs), which are only 2-4 years old (and 14th gen is still very popular for laptops and desktops).",AMD,2025-10-31 23:08:48,2
AMD,nmdvnc0,Right but my point is also they should add more resources to the team which in theory could be utilized for driver support and try to work more with RDNA 2 vs tossing it to a separate release branch and also resources for better GPU support with new stuff as well. So fix the driver support team in general is what I mean.   Whether they like it or not this act looks bad compared to the competition. So we can’t be too fair to them with team size vs Nvidia. AMD should fix that problem not stick with it and just have a smaller team.,AMD,2025-10-31 15:25:52,5
AMD,nmhqb19,You're getting bugfixes and support for new vulkan+dx12 extensions though.,AMD,2025-11-01 05:35:40,1
AMD,nmhzw46,"Sure, but how many people are realistically affected by that to begin with? It's not like iGPUs are typically used for much more than simple tasks like displaying the desktop or web browsing. dGPUs or Strix Halo level APUs are a completely different ball game.",AMD,2025-11-01 07:21:17,1
AMD,nmi2iwq,"Laptop iGPUs get used for a ton of gaming (usually on potato settings or old/indie games, but engine devs still want to be able to rip out old code paths and be able to rely on modern features since eg. modern vulkan is now much simpler than vulkan 1.0 was, while also having new faster ways to do things).  Desktops and browsers are also moving to Vulkan+DX12 (Gnome's vulkan compositor, chrome and firefox are both shipping vulkan+dx12 backends and requiring them for full webgpu support, etc).  The issue is also when hardware support is ended early like this, then it pushes back the time when developers can start requiring new features to run software, and alternative code paths for hardware that lacks whatever required feature support aren't free to develop or maintain (and in some cases mean you essentially have 2 separate renderers, eg. bindless vs traditional/non-bindless).  This is one of the main reasons software developers are pissed about Intel getting rid of AVX512, because it pushes back the time when devs can start requiring it by ~5 years (meaning they now still have to wait 10+ years they until they can start making it mandatory, instead of in 5-6 years if Intel kept AVX512 support).",AMD,2025-11-01 07:51:10,1
AMD,nm0uuqn,̷I̷n̷c̷o̷m̷p̷a̷t̷i̷b̷l̷e̷ ̷w̷i̷t̷h̷ ̷R̷D̷N̷A̷2̷?̷ ̷I̷t̷ ̷t̷e̷l̷l̷s̷ ̷m̷e̷ ̷t̷h̷a̷t̷ ̷t̷h̷e̷ ̷A̷M̷D̷ ̷g̷r̷a̷p̷h̷i̷c̷s̷ ̷h̷a̷r̷d̷w̷a̷r̷e̷ ̷i̷s̷ ̷n̷o̷t̷ ̷s̷u̷p̷p̷o̷r̷t̷e̷d̷.̷   They have already fixed the driver.  It can be downloaded from here: [Drivers and Support for Processors and Graphics](https://www.amd.com/en/support/download/drivers.html),AMD,2025-10-29 15:29:40,110
AMD,nm0uckw,>Stutter may be observed while playing games with some VR headsets at 80Hz or 90Hz refresh rate on some AMD Radeon™ Graphics Products such as the Radeon™ RX 7000 series. Users experiencing this issue are recommended to change the refresh rate as a temporary workaround.   Fucking finally,AMD,2025-10-29 15:27:18,121
AMD,nm10pux,"Wait wait, they disabled the usb-c power for 7900 series? Does this break the psvr2 running from it? :O",AMD,2025-10-29 15:57:14,37
AMD,nm0thfk,VK_EXT_shader_float8 support ... litteraly Vulkan FSR4 support coming soon.,AMD,2025-10-29 15:23:16,68
AMD,nm0qmlh,Work graph wizardry support on 9000 series??? Nice,AMD,2025-10-29 15:09:29,49
AMD,nm1aug7,Updated link for RDNA 1 and 2  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-10-29 16:44:52,23
AMD,nm1z1mh,"I discovered the cause of a bug that has been plaguing me for years, I thought it was an issue with Windows, but I just recently did a fresh install and discovered it is actually because of ReLive.     The problem is that if you are recording, or have the Instant Replay  feature turned on, when you change the volume and that Windows volume slider appears, it makes the game drop fps/stutter  severely.     Here is a video showing it, look at the fps numbers and graph next to it: [https://streamable.com/ui0vqj](https://streamable.com/ui0vqj)  Relive disabled (sorry for the recording quality, had to do it with my phone): [https://streamable.com/4a7e0h](https://streamable.com/4a7e0h)  There are some games where there is not a fps drop, but it shows some erratic behavior. For instance, on Little Nightmares 2 Enhanced Edition, it causes the game to ignore vsync/refresh rate of the monitor. I have my monitor set to 120hz, and here is what happens on LN2:  [https://streamable.com/jv4f5p](https://streamable.com/jv4f5p)  I also tested with 2 other recording software, Windows Game DVR and Steam Recording, and the issue doesn't happen with those: [https://streamable.com/rsesak](https://streamable.com/rsesak)     u/AMD_Vik Can you please look into to this? My system: 5800X,  2x8GB@3200Mhz, RX 6750XT, Windows 11 up to date, nvme. Thank you in advance!",AMD,2025-10-29 18:37:40,35
AMD,nm6vvam,"Hey, The new Driver is terrible on Battlefield 6, i lost over 100fps, if someone has that problem as well, just clean the driver with DDU and install the ""25.9.2"" one, now the game feels like it was yesterday for me. Edit: 7800 XT here.",AMD,2025-10-30 13:45:00,10
AMD,nm21txt,First version since 25.4.1 that doesn't freeze my 7840HS/780M. Hardware acceleration still broken though.,AMD,2025-10-29 18:50:40,10
AMD,nm9rl1n,"Thank you AMD, you just lost a ""20yrs of loyalty customer"" by sunsetting proper RDNA 2 support. While the potential for game optimisations might have been exhausted, the future WDDM and Vulkan updates are important and should have continued for a couple more years instead of freezing on some random branch that has plenty of issues still that won't be properly addressed.",AMD,2025-10-30 22:10:16,8
AMD,nm0wxdk,"Error 182 on RDNA 2 cards, also, there's no data on .inf about RDNA1/2 cards.  Wrong .exe on the web maybe?",AMD,2025-10-29 15:39:27,31
AMD,nm3hz2e,Battlefield 6 now crashes with DX12 error,AMD,2025-10-29 23:06:06,9
AMD,nm5u53s,Half of my steam games refuse to launch since installing this update. I had to ddu and roll back to last month's version,AMD,2025-10-30 09:21:45,7
AMD,nm12jpc,> New Game Support and Expanded Vulkan Extensions Support is available to Radeon™ RX 7000 and 9000 series graphics products.  Are the RX 5000 and RX 6000 series moving toward a legacy release like GCN?,AMD,2025-10-29 16:05:47,33
AMD,nm0wew6,>USB-C power charging has been disabled for Radeon™ RX 7900 series graphics products.  Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.  don't tell me they fixed the VR issue just to kill the VirtualLink port. a step forward and 2 backwards.,AMD,2025-10-29 15:37:03,24
AMD,nm10e3b,"Stupid question for a PC noob: what exactly does it mean when a new Driver adds support for a new game? I’ve been playing Battlefield 6 for a few weeks now, but now my Drivers support it? What does that look like?",AMD,2025-10-29 15:55:43,11
AMD,nm2il1w,This driver fixes AMD GPU driver crashes in 3DMark CPU Profile.,AMD,2025-10-29 20:10:14,5
AMD,nm795d2,"On my 9070XT this driver was causing a crash and reboot if I locked my PC then left it alone for a while (20 mins or so) and then came back - the reboot would happen as I moved my mouse to wake my monitor.  If I locked it and immediately went back in it was fine, and games and things all seemed to be running perfectly.  Gone back to 25.9.2 for now",AMD,2025-10-30 14:51:07,6
AMD,nm1ktl4,"Still nothing about the pink artifacts in Chromium? It’s been four months since AMD’s Matt said they were investigating this issue. At least it was working fine on 25.4.1, but now I’m forced to update drivers because of Battlefield. I don’t want those pink squares, ffs! I can’t replace everything that uses Chromium because of AMD… At least say something about it! Is our GPU broken? A lot of people are complaining about it! Just say it’s broken so we can replace it, ffs.",AMD,2025-10-29 17:31:22,11
AMD,nm29hda,AFMF new frame blending option works really nice. Definitely improved smoothness using blended frames in emulated games.,AMD,2025-10-29 19:27:03,3
AMD,nm2pwds,"Installed these, now battlefield 6 shows up as Elder scrolls online in the Adrenalin game tab.. like wtf",AMD,2025-10-29 20:44:04,3
AMD,nm6rzma,>USB-C power charging has been disabled for Radeon™ RX 7900 series graphics products.  Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.  This feels like a uniquely shitty thing to do with no information about why it's happening. I specifically used my USB-C port to play PSVR 2 on PC without the PlayStation Adapter. Guess it's about time I grab the adapter just in case I need to update my drivers in the future.,AMD,2025-10-30 13:24:35,4
AMD,nm1cidk,Anyone who updated the drivers have noticed any difference in Battlefield 6 between the Preview Driver and this one?,AMD,2025-10-29 16:52:35,7
AMD,nm1colv,Noise suppression still broken since 25.9.2,AMD,2025-10-29 16:53:24,7
AMD,nm0tgh3,Where arc raiders support,AMD,2025-10-29 15:23:08,13
AMD,nm0xk9j,"I still have an issue with this release, maybe it's related to Windows 11 25H2 idk  I'm on Windows 11 25H2, 5700x and 7900XT  I can't record or use replay, of course they're enabled, I get the first notification ""recording"" or ""saving replay"" but it's actually not saving anything when I click on my hotkeys again.  Even starting a recording manually with ""Desktop Recording"" enabled does absolutely nothing, it's not saving anything. I see an .mp4 appearing in the folder, but as soon as I stop the recording the MP4 file simply disappears.  I have tons of free storage, so that's not the issue, I'm using the default Radeon relive folder too, I didn't change a thing, tried DDU too but nothing.  Anyone else having this problem?",AMD,2025-10-29 15:42:25,3
AMD,nm1h6zo,Is 25.10.2 newer than my current 25.10.30? lol im noob,AMD,2025-10-29 17:14:21,3
AMD,nm1qqck,"I downloaded it and tried to install it on an RX 6800XT, but it says the installer is not compatible.",AMD,2025-10-29 17:58:35,3
AMD,nm4ztn5,"For anyone encountering frame pacing issues on BF6, try turning off AMD Anti-Lag. Before I had terrible frame pacing doing literally anything, and every time I scoped in the FPS would drop to like 50 fps. Brand new 7800x3D system and fresh W11 debloated install, all drivers updated and all temps within normal spec. It was driving me insane since this was the only game with problems. Also future frame rendering helps even further, but test to see if you like the slight decrease in responsiveness.",AMD,2025-10-30 04:26:37,3
AMD,nm5gggk,"Man this update is a goat my game graphics improved by alot ,I am running on xtx 7900 and this is the best driver for me without any issues",AMD,2025-10-30 07:01:54,3
AMD,nm5wuem,"When choosing the Minimal Installation, it is installing Complete version with all AI crap.",AMD,2025-10-30 09:47:53,3
AMD,nm7ib94,"crashes my 6900XT in BF6, anybody else have similar issues?",AMD,2025-10-30 15:35:06,3
AMD,nm0s17f,NIce going to install after I get off my meeting.,AMD,2025-10-29 15:16:19,9
AMD,nm1yof4,"Looks like we got a new AFMF setting, Fast Motion Response. Can be set to Repeat Frame or Blended Frame. [https://i.imgur.com/BCEN27U.png](https://i.imgur.com/BCEN27U.png)",AMD,2025-10-29 18:35:57,5
AMD,nm0rm90,AMD please fix eyefinity pro utility!,AMD,2025-10-29 15:14:17,2
AMD,nm1vkpi,Am I the only one seeing corrupted artifacts in Vtm Bloodline 2?,AMD,2025-10-29 18:21:17,2
AMD,nm2sse4,"Wonder if this solves the driver crash, timeout problem started with 9.1.  That was the worst bug I encountered on an AMD Gpu. Crashed the whole system. Only way out was to hard reset the system.",AMD,2025-10-29 20:57:30,2
AMD,nm34owx,AMD Software Installer Detected AMD Graphics Hardware in Your System Configuration That Is Not Supported - installing on 6800XT and there is no other driver to install? Why?,AMD,2025-10-29 21:55:10,2
AMD,nm3gn31,Not working on my RX 6600XT getting the error 182,AMD,2025-10-29 22:58:42,2
AMD,nm3llap,"Error 182 on my 9900x B650E system. I repair-installed 25.9.1, and that worked fine, but it didn't help installing this new 25.10.2.",AMD,2025-10-29 23:25:28,2
AMD,nm3oyht,"I was able to get by error 182 by using the auto-detect installer from the AMD website... weird that that works compared to downloading the direct, and distinct driver for the newest version.  Haven't ran battlefield 6 yet, but will attempt to install next.  Update: Seems to work now! Will report more if that changes.",AMD,2025-10-29 23:43:44,2
AMD,nm45ytt,"If I have 9070xt, should I upgrade to this version?",AMD,2025-10-30 01:19:18,2
AMD,nm54zy5,"whatever this update was, it broke Hades 2 w/ enhanced sync on my 9060xt. VRR flicker and shaky, works with enhanced sync off but wow that was obnoxious to troubleshoot.",AMD,2025-10-30 05:10:12,2
AMD,nm57ovd,My external mouse doesn’t work when I install this driver on my gaming laptop 7900m gpu driver,AMD,2025-10-30 05:34:58,2
AMD,nm5fqzw,"For some reason, I can’t install the driver only version. No matter what option I choose, it always installs the full package. I tried using DDU, and when that didn’t help, I also tried the AMD Cleanup Tool, but nothing solved the problem.",AMD,2025-10-30 06:54:49,2
AMD,nm6a5yh,You have put in your post:  # Package Contents  * AMD Software: Adrenalin Edition 25.10.2 Driver Version [25.20.21.01](http://25.20.21.01) for Windows® 10 and Windows® 11 (Windows Driver Store Version 32.0.22021.1009)  On AMD's official page it says nothing about Windows 10.  Under Compatability it only says W11    From AMD's Download page:  ===================================   Systems with RDNA series graphics products:  * [AMD Software: Adrenalin Edition 25.10.2 Driver for Windows® 11 (64-bit)](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  <---W11 only   # Compatible Operating Systems  AMD Software: Adrenalin Edition 25.10.2 is designed to support the following Microsoft® Windows® platforms. Operating System support may vary depending on your specific AMD Radeon product.  * Windows 11 version 21H2 and later     AMD dropping W10 support???,AMD,2025-10-30 11:36:47,2
AMD,nm6ded5,This stupidly poor update caused issues with my windows install. stuck with attempting repairs till I can get into safe mode and try to fix this shit. Companies releasing absolutely abysmal version updates is a bane on my existence at this point.,AMD,2025-10-30 11:58:53,2
AMD,nm6xkjx,"Texture flickering or corruption may appear while playing Battlefield™ 6 with AMD Record and Stream on some AMD Graphics Products.  Finally I know what was causing my problems, disabling this fixed them. UI was constantly glitching with square artifacts.",AMD,2025-10-30 13:53:39,2
AMD,nm6zqf6,"Anyone with past AMD driver issues try the newest release yet? Particularly, RX 6700 XT owners. 24.12.1 is still the last stable driver for me.  Here are some posts with the issues I speak of. https://www.reddit.com/r/AMDHelp/comments/1khzck4/version_2551_update_rx6800_crashes_often/   https://www.reddit.com/r/Amd/comments/1k60kzh/amd_ryzen_chipset_driver_release_notes_70409545/   https://www.reddit.com/r/AMDHelp/comments/1k2a66p/blue_screen_with_the_new_2531/",AMD,2025-10-30 14:04:24,2
AMD,nm75qkv,google play games for pc no longer work with this driver. Impossible to start a game. Had to rollback my driver to 25.9.1,AMD,2025-10-30 14:34:20,2
AMD,nm7dich,"Ok, anyone know the official way to complain about the removal of the usb-c virtuallink port for no reason, killing psvr2 on one cable? I need to vent to them, hopefully enough do to reverse this stupid decision.",AMD,2025-10-30 15:12:18,2
AMD,nm7hgws,Is anyone's discord freezing with this 25.10.2 update? I had to disable hardware acceleration but now discord feels so laggy,AMD,2025-10-30 15:31:06,2
AMD,nm7xha9,Welp time to move to Linux since I cannot afford to move out of my RX 6700 XT. Good job AMD!,AMD,2025-10-30 16:47:00,3
AMD,nm81xun,"BF6 already feels smoother! Before I was still getting 120 fps ( locked I don't want or need the extra frames, can't hit a consistent 130 anyway) but it didn't FEEL like I was getting anything more than 100.  Now it actually feels and looks like 120 fps with the added bonus of reduced to almost non-existent stuttering and micro-freezing. I think I gained a few extra frames as well but still can't hit a solid 130 so I'll stay locked to 120. Frame timings have definitely been improved!  Super happy with the update so far, will continue to monitor and update you all!  Edit 1: Ran a game of Conquest on Mirak Valley and two rounds of RedSec, I could INSTANTLY feel and see the improved frame timing and smoothness overall. As stated above, also noticed little to no stuttering or micro-freezing that I was getting on 25.9.1. Will continue to update!  Edit 2: Had my first crash, tried to alt tab back into BF6 RedSec in the waiting room and my whole PC locked up ( could still hear my friends on discord though). Swapped over to borderless and haven't had any issues.",AMD,2025-10-30 17:08:31,2
AMD,nm8x280,can i use this version for 6700xt?,AMD,2025-10-30 19:36:35,2
AMD,nm994kd,Today I downloaded the driver again from the RX 6800 XT section; it's 912 MB and it says it's not compatible. It still doesn't work!Today,AMD,2025-10-30 20:35:10,2
AMD,nm9qcbh,Almost at the end of the year and cyberpunk pathtracing crashes still aren't fixed and we haven't seen fsr redstone launch. But hey at least they finally fixed the vr issues after like a year.   Somehow this is a multi-billion dollar company 🥴,AMD,2025-10-30 22:03:05,2
AMD,nm9u66l,"BULLSHIT DRIVER... everything feels like 60hz and supper stuttery... everything was smooth and fine before... good job AMD, not! :\[",AMD,2025-10-30 22:25:21,2
AMD,nmaxjtx,Thanks for the update that does the exact opposite! Yay!!,AMD,2025-10-31 02:14:23,2
AMD,nmih8mp,guys after this update cyberpunk 2077 refuse to open with fsr 4 and gta 5 doesn't recognize my graphics card and directx do you have this problem or solution,AMD,2025-11-01 10:30:56,2
AMD,nmtl4lw,FSR4 6xxx when?,AMD,2025-11-03 03:24:17,2
AMD,nm0tlnm,No Arc Raiders game ready drivers?,AMD,2025-10-29 15:23:49,5
AMD,nm1q9si,"Noise cancellation still broken, it seems. :(",AMD,2025-10-29 17:56:29,3
AMD,nm12cgv,"Gotta love AMD support for 6000 series. Error 182. Would love to try this on BG3 with Vulkan, but you know, AMD.",AMD,2025-10-29 16:04:50,4
AMD,nm26605,"Hello, why is my 6900xt not supported???",AMD,2025-10-29 19:11:07,2
AMD,nm2l40x,"New issue I have after this update, pc screen goes black randomly while pc is still on and wont go back. Google says this is signs of a driver update, so Im going back an update to see if this fixes it.  Edit: After downloading drivers 25.9.2 so far no more black screens, looks like the new update did cause that for me.",AMD,2025-10-29 20:21:55,2
AMD,nm5epnj,Is that mean that AMD is trying to give up on RDNA 2?,AMD,2025-10-30 06:44:22,2
AMD,nm1t90b,"Still no ROCM Windows support?  I thought that since there was a preview, there would be support in the next non-beta release.  Also the VR issue is still not fixed.",AMD,2025-10-29 18:10:17,2
AMD,nm10yun,They remembered,AMD,2025-10-29 15:58:22,1
AMD,nm17wvr,"We will have a V2 soon I think, unable to install on a 680M laptop.",AMD,2025-10-29 16:31:05,1
AMD,nm1cc7y,"Quem estiver com problema com rdna 1/2 baixem pelas notas de lançamento:  [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html)  ou direto por aqui:  [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  O arquivo lá na frente da página tá só ""rdna3"", pelas notas de lançamento está ""rdna3-combined"", funcionou aqui 6700XT.",AMD,2025-10-29 16:51:47,1
AMD,nm1e5on,"glad to see work graph support, they're likely a large part of the future of rendering",AMD,2025-10-29 17:00:11,1
AMD,nm1h5v8,"For anyone getting a hardware configuration issue OR error 182.  There is a bug on some of the download pages. Usually I go to my GPU page (e.g RX 5700) and download straight from there. It's downloading a broken version, different file name and smaller size.  Instead either click on ""Driver Details"" and the release notes - then download it from there  OR  [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html)  Download from there \^  Broken version just has RDNA3.exe at the end  Working version has RDNA3-Combined.exe  Worked for me.  EDIT:  Driver is garbage. Just gonna wait for the next version. Can't believe this is listed as WHQL Recommended...  I've crashed twice today on Battlefield 6 since installing it.  Haven't ever had a crash, even on the beta everything was fine. Reverting back to 25.9.2.",AMD,2025-10-29 17:14:12,1
AMD,nm1ltwe,">USB-C power charging has been disabled for Radeon™ RX 7900 series graphics products.  Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.  What does this mean, because i can still quick charge my phone with the GPU USB Port with the new driver on an 7900XT?",AMD,2025-10-29 17:36:04,1
AMD,nm1og7m,"I literally just installed 25.9.1 only to see a notification for this version afterwards lol. I do plan to start playing REDSEC with some buddies, but might see how it performs on 25.9.1 first as I don't usually jump on drivers/updates the moment they're released in case there are undisclosed bugs.",AMD,2025-10-29 17:48:13,1
AMD,nm2a4cg,Atualização AMD! Espero que em breve venha o FSR Redstone. 👏👏👏,AMD,2025-10-29 19:30:04,1
AMD,nm2dwf6,Any of you 7000 series folks updating or gonna wait a bit? The drivers always mess with BO6 for me for the first few weeks. Might be a BO6 thing but I play a few games daily and I'd hate to deal with crashes after 2 games,AMD,2025-10-29 19:48:23,1
AMD,nm2j02w,So I've had an AMD gpu for a couple months now. Is it normal that the update manager in Adrenaline does nothing? The last two updates I've had to get from the website because the software says I'm already on the latest.,AMD,2025-10-29 20:12:11,1
AMD,nm3afov,Howcome I cant see any updates in Adrenaline for my 7800XT ?,AMD,2025-10-29 22:25:09,1
AMD,nm3nxkt,"Cannot install on RX 5700 XT !!!  It says it cannot detect any hardware. I then tried updating through adrenaline app and it installed something and now it shows that i have 25.10.2 released 24.9.2005, so do i have latest drivers or not? wtf  Also, my Windows 11 just updated today and after update, my monitor started flickering like crazy until i removed all drivers with DDU and installed new one (two months old drivers, because i couldnt install these). That was before i have tried new 25.10.2 drivers.  So somebody here messed up something. How can windows update mess up with my drivers?",AMD,2025-10-29 23:38:09,1
AMD,nm3ykyt,hopefully one day i see improvements on dx11. sim racing titles like le mans ultimate and assetto corsa competizione noticeably performing worse than nvidia counterparts and valorant stuttering like crazy during shader compilation.,AMD,2025-10-30 00:37:27,1
AMD,nm42jxi,alt r for fortnite crashes the game instantly but the gpu driver doesnt.,AMD,2025-10-30 00:59:52,1
AMD,nm50x2h,"For RDNA2 owners who playing BF6, I am using modded Driver with two old dx12 .dll files from 23.9.1 not get any single crash, I got crashed with official drivers. Hope this help. FSR4 int8  doesnt work in this game.",AMD,2025-10-30 04:35:28,1
AMD,nm5ua9k,Starts to stutter and low fps on any game like it's losing focus. Alt tab works for a few seconds and back to low fps on this version,AMD,2025-10-30 09:23:10,1
AMD,nm5xt4c,"* Stutter may be observed While playing Baldur’s Gate 3 on some AMD Radeon Graphics™ Products such as the Radeon™ RX 9000 and 7000 series.  I was losing my mind over this a few days ago, BG3 on Vulkan was unplayable on my 9070 XT. Stuttered every few seconds and I couldn't find anything online about it lol... sooo glad it got fixed and I don't have to play on DX11",AMD,2025-10-30 09:56:51,1
AMD,nm6bexp,"9070xt hellhound w GTA V enhanced blokuje na 60fps , po przywróceniu sterownika 25.9.2 300fps",AMD,2025-10-30 11:45:35,1
AMD,nm6zvsk,I wonder how long my laptop ryzen 7 5825u will be supported for especially given how intel has discontinued major updates for their processor graphics as new as 2022 I’m concerned if AMD may follow suit,AMD,2025-10-30 14:05:09,1
AMD,nm7hujj,"Every time i upgrade drivers my CPU Temperature disappears from Adrenalin, i have no way to add it again until i do a fresh install of the drivers. Is it just me or its a common bug?",AMD,2025-10-30 15:32:54,1
AMD,nm80y57,Just got white screen with 29.10.2 on second day watching YouTube on 8745HS.  Likely going back to 25.9.1.,AMD,2025-10-30 17:03:37,1
AMD,nm8hj08,I have three problems with this new driver: 1. My desktop freezes. 2. Adrenaline opens on its own even when I don't want it to. 3. VR is detected as a second monitor.,AMD,2025-10-30 18:22:50,1
AMD,nm8lldt,"Amd installer updated automatically today for me. Do i need to set again to not auto update or does it remember my settings. Also is this normal, on previous drivers amd installer wasn’t auto updating itself (not the driver)?",AMD,2025-10-30 18:41:52,1
AMD,nm8myi0,Is it a recommended method to update the drivers directly through the Adrenalin app? Or is it always better to download the new drivers from their website (specifically from the release notes page)?,AMD,2025-10-30 18:48:16,1
AMD,nmabjq5,Driver broke external mouse support on my 7900m laptop.,AMD,2025-10-31 00:03:52,1
AMD,nmamhm0,"Anyone get an error with AMDRSServ.exe? I got an error when turning my computer on saying ""\[file path\]/CNext/AMDRSServ.exe cannot be found"". I did sfc /scannow (idk what this command is actually called) in command prompt and that seemed to fix the issue as the end message did state there were some corrupted files that were repaired. Restarted and no issue upon signing in. Hoping this isn't a temp fix because I thought my computer bricked out at first and my heart sank in my chest lol",AMD,2025-10-31 01:08:11,1
AMD,nmccr5p,Still crashing on Battlefield 6 like many others.,AMD,2025-10-31 09:43:06,1
AMD,nmcjq49,I've been having an issue for a few days where my wifi antenna will randomly disconnect every once in a while. Does anybody else have the same issue? Only recent driver updates are this one and an audio driver automatically installed by windows. Neither seem to make sense as the causes of this issue.,AMD,2025-10-31 10:44:31,1
AMD,nmeiooz,"On my 6800 XT in CS2 with the same settings, my FPS dropped from 315 to 291 on Vulkan and from 344 to 331 on DX. So, apart from switching the drivers to legacy, are we stuck with worse performance than before?",AMD,2025-10-31 17:18:05,1
AMD,nmjjwps,Fortnite refuses to launch because the easy-anti cheat says it's an unsigned driver,AMD,2025-11-01 15:03:09,1
AMD,nmjudl1,"do not install these, I wasn't able to play arc raiders with these installed and some DX12 games crashed whenever I tried to open the adrenalin overlay, I fixed it by rolling back to 25.9.2",AMD,2025-11-01 15:58:33,1
AMD,nml3ofb,"GTA V enhanced doesn't launch. Am I the only one with this problem? The erros says: Failed to initialize graphics device. Please reboot or reinstall latest DirectX runtime.  Would you recommend downloading an older driver version? If yes, what version?",AMD,2025-11-01 19:53:20,1
AMD,nmo6mrt,"господи, оно само установилось. теперь после каждой перезагрузки ошибка драйвера pci bus",AMD,2025-11-02 08:00:56,1
AMD,nmqe1nn,Has anyone else been getting random AMD driver timeout when booting up your machine? I get the error sometimes and it doesn't really mess with anything as far as I can tell but it's annoying seeing it pop-up because I don't know if it's a driver issue or if the new GPU I got has something wrong with it. I've reinstalled after using the AMDcleanuputility.exe but might try out DDU if the error persists. 9060 XT 16GB(PowerCooler),AMD,2025-11-02 17:12:39,1
AMD,nmr11mr,it seems that with this driver (im assuming its because of the driver because nothing else changed) my fps in valorant went from 900-1000 in the range to about 1100-1200. although performance does degrade after a while. shader compilation stutters are still present.,AMD,2025-11-02 19:03:14,1
AMD,nms8pln,"I can't play a game for more than a few minutes after installing this driver. Game crash non stop. I did a clean reinstall using DDU and still the same issue. I get different types of errors, all driver or GPU related. The one I get the most is that the GPU can't accept more commands or something similar.",AMD,2025-11-02 22:37:35,1
AMD,nn0ah6r,"u/AMD_Vik Hi,Vik. Just curious is there any reason behind not supporting [VK\_KHR\_video\_decode\_vp9](https://registry.khronos.org/vulkan/specs/latest/man/html/VK_KHR_video_decode_vp9.html) for rdna2 when it is available for dx11? I checked and saw rdna2 driver already have support for the dependency [`VK_KHR_video_decode_queue`](https://docs.vulkan.org/refpages/latest/refpages/source/VK_KHR_video_decode_queue.html) . Is there anything more needed to get vp9 decode support for rdna2 in vulkan?  Same goes for the other two extensions which has [VK\_KHR\_swapchain](https://docs.vulkan.org/refpages/latest/refpages/source/VK_KHR_swapchain.html) support already in rdna2.  * [VK\_KHR\_present\_mode\_fifo\_latest\_ready](https://registry.khronos.org/vulkan/specs/latest/man/html/VK_KHR_present_mode_fifo_latest_ready.html) * [VK\_EXT\_present\_mode\_fifo\_latest\_ready](https://registry.khronos.org/vulkan/specs/latest/man/html/VK_EXT_present_mode_fifo_latest_ready.html)",AMD,2025-11-04 04:36:33,1
AMD,nm1ht5n,Did I miss an Adrenalin 25.10.1 release?  I checked for updates recently and the last available release was 25.9.2.,AMD,2025-10-29 17:17:15,1
AMD,nm0tkzs,cve has a meaning and I think a few of them are directly related to the vram memory leak problem.,AMD,2025-10-29 15:23:44,1
AMD,nm1q85n,"They are unstable in that new bf6 battle royale, 1min and crash with some driver error, 25.10.1 complety stable",AMD,2025-10-29 17:56:17,1
AMD,nm2brgl,"Damn, look at all the security vulnerabilities.",AMD,2025-10-29 19:38:04,1
AMD,nm2kg7u,BF6 keeps timing out for me with these new drivers,AMD,2025-10-29 20:18:53,1
AMD,nm2o3b3,"Guys, it's worth updating from version 25.10.1 (I installed this version for bf6)? I have absolutely no issues with this driver, except for the fact that it is a beta driver",AMD,2025-10-29 20:35:39,1
AMD,nm38i90,Anyone else missing AMD Chat?,AMD,2025-10-29 22:14:59,1
AMD,nm39v7p,Radeon software doesn't even give a fuck to update itself and you can't manually check for update manually within it any longer. Also can no longer update to non WHQL releases. What a piss change to some useless garbage AMD updater that doesn't even do its job - just more bloatware.,AMD,2025-10-29 22:22:08,1
AMD,nm7f268,"Definitely the worst RDNA4 driver yet. Massive frame drops in some games (Days Gone, Battlefield) and driver crashes + restart as soon as you reactivate Windows + monitor after idling for a while.",AMD,2025-10-30 15:19:41,1
AMD,nm1gmi9,should i hype about this ?,AMD,2025-10-29 17:11:42,0
AMD,nm1yjnq,"Cool, another update and Cyberpunk is still broken.  Another update I'm not installing.",AMD,2025-10-29 18:35:20,0
AMD,nm4u6uw,Why people on rx6000 are so eager to use this version? I don't see any mentions to rdna 2 that makes me wanna update.,AMD,2025-10-30 03:44:48,0
AMD,nm1lvke,This driver literally makes Stalker 2 unplayable had to revert back to 25.9.1 it's running fine now,AMD,2025-10-29 17:36:17,0
AMD,nm2b1go,"This driver messed up the performance in Battlefield 6 on my 9070XT,so I had to go back to 25.9.2  Edit: it’s not the driver,it’s either the game or EA app servers",AMD,2025-10-29 19:34:32,0
AMD,nm2gn42,The driver only version installs radeon software minimal. Another disaster amd driver.,AMD,2025-10-29 20:01:12,-2
AMD,nm4rsyq,Wait this is finally the driver update for battlefield 6? A month old already lmao,AMD,2025-10-30 03:28:18,-1
AMD,nm1i3gi,No Arc Raiders? 😢,AMD,2025-10-29 17:18:37,0
AMD,nm38u1y,I think this driver fixed bf6 crash.,AMD,2025-10-29 22:16:41,0
AMD,nm2l3hy,"doesn't get installed on my RX 6650 XT, Good job AMD  giving me more reasons to go Nvidia",AMD,2025-10-29 20:21:51,-3
AMD,nmbwkxn,"Bonjour, je suis le seul qui a de gros problèmes de lag avec la nouvelle fonction ""Réponse aux mouvements rapides"" sur Bf6, les FPS sont bons mais l'image saccade comme si mon jeu tournait en 20 FPS ?",AMD,2025-10-31 06:56:28,-1
AMD,nm2domr,">**New Game Support**  >Battlefield™ 6 (DX12)  This why latest drivers and game support is useless, with this drivers Battlefield 6 is extremely unstable.  Rolling back to 25.8.1 and I get stable 60FPS with no crashes.",AMD,2025-10-29 19:47:21,-5
AMD,nm2j51b,"No FSR4 for RDNA2? Guess Nvidia's RTX 2000 series is still a beast, thanks to DLSS.",AMD,2025-10-29 20:12:49,-8
AMD,nm0ptbs,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-10-29 15:05:30,-2
AMD,nm0ytk6,[AMD.com](http://AMD.com) is down for me  Edited to say it's a me thing,AMD,2025-10-29 15:48:22,-2
AMD,nm0wjly,YEP same here on 6750XT  error code 182 ''not supported gpu'',AMD,2025-10-29 15:37:39,47
AMD,nm3i9oh,"You need to download it from the link provided in the release notes: [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  instead of the one provided on the driver download page: [https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6800m.html](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6800m.html)  because the link is: [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe)  If you are an RDNA 1/2 user like me, you need the EXE with ‘-combined’. I don't know how they could have made such a mistake.",AMD,2025-10-29 23:07:43,15
AMD,nm0vb4x,RDNA 1/2 is totally missing in the inf file,AMD,2025-10-29 15:31:48,24
AMD,nm171ta,Yep. Same on a 6800XT...,AMD,2025-10-29 16:27:00,7
AMD,nm1oc6q,"Literally just tried downloading the driver a minute ago for rx 6700 xt, and even forced refresh of the page, still Error 182 for me. File shows as: ""whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe""",AMD,2025-10-29 17:47:43,3
AMD,nm1h0jy,Same here ...,AMD,2025-10-29 17:13:31,2
AMD,nm1lgug,Up,AMD,2025-10-29 17:34:23,2
AMD,nm2qljz,Same with rx 6900 xt,AMD,2025-10-29 20:47:20,2
AMD,nm997m0,Nop. Dont work for RX 6800 XT Not compatible .,AMD,2025-10-30 20:35:34,2
AMD,nm1u7us,"Oh, good thing I saw this. I was about to run DDU.",AMD,2025-10-29 18:14:53,2
AMD,nm1bzvx,"Baixem pelas notas de lançamento:  [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html)  ou direto por aqui:  [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  O arquivo lá na frente da página tá só ""rdna3"", pelas notas de lançamento está ""rdna3-combined"", funcionou aqui 6700XT.",AMD,2025-10-29 16:50:12,1
AMD,nm57ffp,"yeah same here on a 6800 xt, downloaded it tried installing even updated with the evil new windows update to see if this was the issue but still to no avail, the combined one i did find but it's way more unstable than the previous driver even for the ""supported new games"" they crash and it's not worth the little extra smoothness since it just crashes the whole driver specially with battlefield 6.",AMD,2025-10-30 05:32:31,1
AMD,nm73plb,Driver 912 mb is the right version from rx 6800 xt ?,AMD,2025-10-30 14:24:15,1
AMD,nm8eiu8,"mnao meu pc depois desse drive ta todo buagdo nem os jogos quer abrir  , nao consigo nem ver a placa mais sumiu",AMD,2025-10-30 18:08:43,1
AMD,nm8j6c8,o meu ta dando erro 182 ate com os drives antigos,AMD,2025-10-30 18:30:32,1
AMD,nm1x2gm,SAME !!!!!!!!!!!,AMD,2025-10-29 18:28:18,1
AMD,nmc5c7b,Sad that they disabled PD on the USB-C port to make it happen though.,AMD,2025-10-31 08:28:14,1
AMD,nmrrsh4,im confused you posted a bug the driver has  but said finally? did they fix the bug or ?,AMD,2025-11-02 21:09:41,1
AMD,nm1m083,"I sadly don't have one to check, but my phone still quick charges with the new driver so.... maybe? Maybe not.   Maybe it means some version of USB-PD being disabled?",AMD,2025-10-29 17:36:53,9
AMD,nm4rxat,Was there ever any news about USB-C power not working properly or causing issues? Would have been nice if AMD explained why USB-C charging was removed and why such an old driver would be recommended.,AMD,2025-10-30 03:29:07,4
AMD,nm0z217,One can hope. I'm literally putting off Doom and Indiana Jones until I can run them with FSR 4,AMD,2025-10-29 15:49:29,27
AMD,nmrs9so,whats the benefit to that?,AMD,2025-11-02 21:11:59,1
AMD,nm3pkve,What does this mean?,AMD,2025-10-29 23:47:08,5
AMD,nmo3hr8,Sure it also works on RX 7000 series. Just a shame that prolonged crossgen will take this paradigm shift as hostage well into the 2030s worst case but this alongside ML and PT will define gaming in the 2030s.,AMD,2025-11-02 07:27:32,1
AMD,nm1eea5,"not working, i get:   [https://www.amd.com/de/resources/support-articles/faqs/Download-Incomplete.html](https://www.amd.com/de/resources/support-articles/faqs/Download-Incomplete.html)  loadet the original , and its still the old one, no support rnda1 rx5700  \-> The new Link now works, thanks <-",AMD,2025-10-29 17:01:17,5
AMD,nm1ehay,Works on my GA402RJ to update both 680M and 6700S.,AMD,2025-10-29 17:01:40,4
AMD,nm1eatj,that link is broken. Maybe try linking to the page and not directly to the download.  Videocardz has the correct mirror download for the combined driver.   [https://videocardz.com/driver/amd-radeon-software-adrenalin-25-10-2](https://videocardz.com/driver/amd-radeon-software-adrenalin-25-10-2),AMD,2025-10-29 17:00:51,3
AMD,nm1t5je,"Thanks! Love how the wrong one is linked to on the direct 6800 XT page, but this one works fine.",AMD,2025-10-29 18:09:50,2
AMD,nm223wq,"Thanks for the heads up. I'm wondering, would it be possible for you to capture this stutter in a GPU trace using something like [UI for ETW](https://github.com/randomascii/UIforETW)?  I have a rough guide outlined in a [much earlier post](https://www.reddit.com/r/Amd/comments/11bhohu/refresh_rate_mismatch_in_232x/ja7t7h1/)  If you can capture a trace, compress to .zip, and fire it over to me via https://send.vis.ee or a method of your choosing, we'll be able to investigate; as far as I'm aware, ReLive shouldn't be causing any issues with overlays like that.  Reach out if you need a hand with anything,  Cheers",AMD,2025-10-29 18:51:57,27
AMD,nm2edlf,"There are some old bugs that have been swept under the rug, such as the gradual performance degradation in F.E.A.R. — you can't revisit the classics.",AMD,2025-10-29 19:50:41,0
AMD,nmcikcf,I have the 7900xtx and have the same issue.  The game runs horribly on the new driver.  How do I do what you suggested?,AMD,2025-10-31 10:35:03,3
AMD,nmf4t98,"This also happens on other games, any full screen mode sends my 9070xt into 1600mhz core clock, so i get 50% the expected fps. Grear driver",AMD,2025-10-31 19:10:35,2
AMD,nmdg6kb,Something like that happened to me in Escape from duckov.  The game played fine then I installed the new drivers and now I got like 15-20 fps and im on a 7900xtx.,AMD,2025-10-31 14:08:02,1
AMD,nmezrb3,"Same, battlefield 6 crashes",AMD,2025-10-31 18:44:35,1
AMD,nm5z6z3,Can you explain what you mean by hardware acceleration is broken?,AMD,2025-10-30 10:09:19,2
AMD,nm2nxqs,"it's fixed , need to redownload",AMD,2025-10-29 20:34:55,7
AMD,nm5y22w,"Yes, I have the same situation. BF 6 crashes without an error. When I try to launch COD Bo6, it gives me an error and doesn't start. My hardware is a 7800x3d and a 7900 gre. I've tried everything, but I had to roll back to 25.10.1, where everything is stable and working.",AMD,2025-10-30 09:59:07,3
AMD,nmbdpt5,"Confirmed also crashed here within the first match I played after driver install with a Powercolor 7800 XT but with no error, plus occasional artifacts in the loadout menu before that. DDUing directly back to 25.9.1 that I was on before 100% fixed it.",AMD,2025-10-31 04:02:38,3
AMD,nmezl5l,Same and i am trying to find stable driver -rx5700xt,AMD,2025-10-31 18:43:43,1
AMD,nm68kjg,"Yes, yes, yes! Are we going back to the days of problematic drivers?",AMD,2025-10-30 11:25:03,4
AMD,nm177xi,That'd be unfortunate considering that the 6000 series is only a few years old.,AMD,2025-10-29 16:27:50,34
AMD,nm14022,it would go down as the shortest lived GPU in recent memories if so,AMD,2025-10-29 16:12:38,18
AMD,nm30705,It looks like most of the extensions and their descriptions seem to either support FSR4 (exclusive to rx 9000 and soon to be rx 7000) for Linux or support  Dense Geometry Format (exclusive to future GPUs and possibly rx 9000 series) for Linux. So it makes sense that rx 6000 and earlier doesn’t have this support.,AMD,2025-10-29 21:33:03,1
AMD,nm2y57t,they probably just don't have the hardware required for some of those extensions (av1 seems to be one of them that isn't available pre 7000),AMD,2025-10-29 21:23:03,1
AMD,nm2dhsj,No. Just AMD messing around with the drivers.,AMD,2025-10-29 19:46:26,-1
AMD,nm5di8s,"Eventually, yes",AMD,2025-10-30 06:32:07,0
AMD,nm10xuj,Does this break the psvr2 support through the port? I was using that for it and it was perfect :O,AMD,2025-10-29 15:58:15,3
AMD,nm119yg,"Nothing really, maybe some stability or crash fixes. Sometimes, it could mean better perf.",AMD,2025-10-29 15:59:48,31
AMD,nm16tfa,"Sometimes bug fixes and performance increases, usually nothing. A game doesn't need to be supported by a driver version to work.",AMD,2025-10-29 16:25:54,1
AMD,nm2obpc,For example on 25.9.1 driver I wasn't able to use FSR4 on Battlefield 6. For some reason... Less crashes and probably better performance are other perks,AMD,2025-10-29 20:36:45,1
AMD,nm111qi,Doesn’t mean anything. It’s just marketing. At this point Intel drivers are better than both Nvidia and AMD. They have to try since they’re the underdog.,AMD,2025-10-29 15:58:45,-12
AMD,nm7c4n0,Same GPU same issue. Going back to 25.9.2 fixed the issue,AMD,2025-10-30 15:05:38,4
AMD,nmocaxe,"Same issue! I went checking this thread to be sure I was not the only one. Weirdly enough, it only happens with my monitor (my TV is also connected). I should roll back too",AMD,2025-11-02 09:00:32,2
AMD,nmp2kid,"Same GPU, same issue. PC reboots every time I lock PC or screen shuts off due to inactivity.",AMD,2025-11-02 12:55:17,2
AMD,nmlytvn,I was wondering what the heck was happening.  I went and downloaded an earlier driver.,AMD,2025-11-01 22:44:25,1
AMD,nm1qwii,"Ah so that’s why I see them on Steam and Spotify. I think I haven’t seen them on Firefox yet, I thought my GPU was in the beginning stage of failing.",AMD,2025-10-29 17:59:22,7
AMD,nm2k0qw,"Clean install with drivers only fixes all of the problems I was having, sucks that I cannot use adrenaline software but everything runs much much smoother without it now.",AMD,2025-10-29 20:16:53,4
AMD,nm8qk8h,I also have this issue but it only happens sometimes in YT. Such a weird issue.,AMD,2025-10-30 19:05:13,1
AMD,nm9p2sd,"Every time I ever saw pixel corruption like that it was because of RAM instability. And unfortunately RAM is fickle and hardware vendors take shortcuts.  It's not uncommon for RAM to be perfectly stable at X voltage and Y temperate, and very unstable just a little bit outside of that. It's not uncommon for MB makers to supply more voltage than they claim, because it generally helps with stability which reduces the cost of their returns and warranties. And it's not uncommon for instability to appear in only one specific place when your intuition tells you it should show up elsewhere too.  So my advice is test your RAM in cold and warm scenarios. And try reducing the voltage.",AMD,2025-10-30 21:55:59,-1
AMD,nm5y280,Try disabling Hardware Acceleration,AMD,2025-10-30 09:59:09,-2
AMD,nm2qn1g,"not new driver problem, this happens randomly",AMD,2025-10-29 20:47:31,2
AMD,nm9ufot,"me2 and it's super bad performance like 60hz@40-60fps (display is about 160-170, before it was 180-220)",AMD,2025-10-30 22:26:50,2
AMD,nm8lu2v,mano a minha rx eu nao consigo nem instalar a antiga versao de tao bugado que ficou,AMD,2025-10-30 18:43:01,1
AMD,nmrrsf7,Yea I bought the reference 7900xt for the usbc port as my mobo has none. I’m sick of AMDs shit drivers and pulling features I paid for. But the other option is nvidia that charges 2x what AMD does. GPU market blows.,AMD,2025-11-02 21:09:40,1
AMD,nm5wkty,Big difference. Crashed 2 times in the first 5 hours of having 25.10.2. Haven't crashed on battlefield from beta up until installing that shit driver.,AMD,2025-10-30 09:45:22,5
AMD,nm1el9t,Haven't noticed any difference between the two drivers.,AMD,2025-10-29 17:02:11,4
AMD,nm1le0j,i think its smoother on bf6 zonewar but audio is mega delayed,AMD,2025-10-29 17:34:01,2
AMD,nm6vr89,IDK it feels like the game is running on a slower refresh rate but FPS says otherwise. Spawnselection is noticeably stuttering. 9070xt,AMD,2025-10-30 13:44:25,1
AMD,nms9ojm,"I can't play the game, not just Battlefield but any game. The games crashed after a few minutes playing with my RT 9070XT.",AMD,2025-11-02 22:42:58,1
AMD,nm2xp4p,My frames droped from 140 to 50 on 7090 XTX,AMD,2025-10-29 21:20:54,1
AMD,nm4zj0k,This update broke mine,AMD,2025-10-30 04:24:15,2
AMD,nm5i0vb,Hatte gehofft es läuft endlich wieder. Sehr schade...,AMD,2025-10-30 07:18:00,-1
AMD,nm1c9xy,"Hold ya horses now mate! Can you believe this guy? Bro wants to have up-to-date drivers with big game releases lol. They have just released support for a month old BF6, so...",AMD,2025-10-29 16:51:29,20
AMD,nm0yfxc,"Fr, this is crazy. I wouldn’t assume too much a performance difference with how well amd had aged but it would guarantee fsr 4 support…..",AMD,2025-10-29 15:46:35,2
AMD,nm4v8ko,"Arc raiders ran pretty decently on my 9070XT. I wasn’t watching my fps (which is a good thing imo) and I don’t have to go back into settings to fix things like except get FSR4 working. While yea I would like a day one driver, it’s not like the game was running bad. If I remember posted benchmarks it was even above a 5080 in perf.",AMD,2025-10-30 03:52:13,1
AMD,nm1159j,Yes I had it. You must set a location to Store the Capture. There is not always an Error about it,AMD,2025-10-29 15:59:12,3
AMD,nm11051,Been having this problem for almost a year now. I gave up tbh lol.,AMD,2025-10-29 15:58:32,1
AMD,nm2dmdp,"Windows update may have forced a driver, overwriting what you had. I had this happen and it caused a few issues.   Use DDU to remove and download 25.10.2, or you may be able to install overtop.",AMD,2025-10-29 19:47:02,7
AMD,nm1ko66,why you on pro drivers? and yes it is the latest driver,AMD,2025-10-29 17:30:41,4
AMD,nm5dext,"It's based on branch 25.20.x, so yes",AMD,2025-10-30 06:31:10,3
AMD,nmc6c4w,"Ugh, gross. I guess this one is a skip for me then.",AMD,2025-10-31 08:38:38,1
AMD,nm8q2x3,BF6 runs well for me on the RX 6650 XT,AMD,2025-10-30 19:02:55,1
AMD,nm295om,nah,AMD,2025-10-29 19:25:29,-6
AMD,nm5y490,Absolutely,AMD,2025-10-30 09:59:39,4
AMD,nm66qfj,Always use latest drivers.,AMD,2025-10-30 11:11:13,1
AMD,nmaeii2,"If you have some bug in games or something that the 25.10.2 solved then yes, if you're ok with the driver you're using... for what reason? Don't worry, I'm rocking mi 9060xt with the 25.9.1 driver, it's sooo good that I don't think I'm changing to another driver soon jaja.",AMD,2025-10-31 00:21:27,1
AMD,nmelyc8,"I have the exact same problem, but with Hades 1. It only runs smoothly for me when I turn on the performance overlay.      I have the exact same problem, but with Hades 1. when i turn Performance Overlay",AMD,2025-10-31 17:34:25,1
AMD,nmgkeak,"Thank you for sharing! I'm having the same problem on EA FC 25 with the 9070 XT, and disabling Enhanced Sync also fixed it.",AMD,2025-11-01 00:15:47,1
AMD,nmxy2ca,Same here,AMD,2025-11-03 20:38:19,1
AMD,nmsslbc,Driver still installs on W10. Not sure why they aren't mentioning this,AMD,2025-11-03 00:28:11,1
AMD,nm8mh5o,po mano minha rx sumiu depois da atulizaçao ja to a 2 dias tentando arrumar,AMD,2025-10-30 18:46:01,1
AMD,nmai432,"Aah, had been thinking it was an issue with a google update then realised I'd just updated my drivers and checked in here. Can confirm this issue too.",AMD,2025-10-31 00:42:52,2
AMD,nmag6j1,good luck gaming on linux,AMD,2025-10-31 00:31:25,1
AMD,nmf56rp,for me the exact opposite... 25.9.1 was way better than 25.10.2 or .1 (i tried both) in terms of performance and smoothness especially. It's like i am playing without FSR (it can't be activated anymore.. GOOD JOB AMD!) and still with 130-150fps (with FSR it's like 180-220fps) it feels like low fps  and low hz (60hz?)     AMD: FU :(,AMD,2025-10-31 19:12:31,2
AMD,nmfipmv,"Try download it again from this specific link: [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html#Downloads](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html#Downloads)   The downloaded file name must be ""whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe""   Mine was working perfectly fine after that!",AMD,2025-10-31 20:24:24,1
AMD,nm0xjsh,Man fsr 4 better work out of the box…..,AMD,2025-10-29 15:42:22,3
AMD,nm1587k,"Tbh most of the new vulkan extensions won't work on rdna2 except for the vp9 decode..and hopefully, fifo latest present mode",AMD,2025-10-29 16:18:26,3
AMD,nm29jxi,There's a different download link for older cards.,AMD,2025-10-29 19:27:23,5
AMD,nm3vwy4,I'm having similar issue in the game Ark survival ascended,AMD,2025-10-30 00:22:20,1
AMD,nm66mqd,Drivers are available for RDNA 1-2. They just uploaded files incorrectly.,AMD,2025-10-30 11:10:26,1
AMD,nm3f7zn,"Para que eu deveria atualizar? Estou na versão 25.9.1 com a 6800xt. Está estável e com boa performance. Não vi nenhuma melhoria direcionada as placas da família 6000. Se não está quebrado, não conserte!",AMD,2025-10-29 22:50:55,1
AMD,nmo3koj,not likely it's already confirmed. Just incredibly early days still and due to PS5/PS6 crossgen the tech will have at least another +5 years to mature ahead of mass adoption. But I'll look forward to seeing what AMD can demo. Tree demo from June was impressive.,AMD,2025-11-02 07:28:25,1
AMD,nm1i4vt,"This is working, however the previous one when searching for 6600 drivers and getting it from the page also had rdna3combined. Still didn't work. However 6600 drivers compared to this is the file size is almost double. 903mb from 6600 page and the one you linked 1.5gb.",AMD,2025-10-29 17:18:49,1
AMD,nm67cpz,"7900 XTX, always using latest drivers",AMD,2025-10-30 11:15:53,1
AMD,nm35v3c,"Yes, it’s normal, the panel on Adrenaline only shows updates after a week or so of the update release date on a good day",AMD,2025-10-29 22:01:06,2
AMD,nm66wrq,They messed up driver names. You'll find the proper ones somewhere in the comments.,AMD,2025-10-30 11:12:33,1
AMD,nm5cvp0,"The big DX11 revamp was in 2021 (2022?). DX11 runs excellent. In the games you mentioned, AMD might be slower due to other reasons, not DX11 overhead.",AMD,2025-10-30 06:25:51,2
AMD,nm71ign,wouldn't using that FSR4 mod on RDNA 2 get you banned in BF6 due to its anti cheat?,AMD,2025-10-30 14:13:18,1
AMD,nm829iq,"Whenever I boot up sometimes it works, sometimes it doesn't.",AMD,2025-10-30 17:10:05,1
AMD,nmu30dx,Get them from the site. Also look for DDU and how to use it and do that,AMD,2025-11-03 05:40:04,1
AMD,nmicf8m,> CS2  24.12.1 was best cs2 driver for me when i still had the 6700xt,AMD,2025-11-01 09:41:17,1
AMD,nmss7qd,Launches fine on 7900 XTX.,AMD,2025-11-03 00:25:59,1
AMD,nm4f1fk,25.10.1 was the Battlefield 6 Preview driver released earlier in the month. Had to manually install it from the AMD site.,AMD,2025-10-30 02:10:51,5
AMD,nm1mi75,"Some updates don't come in adrenalin software, you need to download from the website.",AMD,2025-10-29 17:39:14,3
AMD,nm1sneu,'25.10.1' isn't completely stable for everyone. Device hubga device removed CTDs been happening for weeks and .1 dropped on 10/10.,AMD,2025-10-29 18:07:27,1
AMD,nm2ovbc,nothing changed for vega/polaris path.,AMD,2025-10-29 20:39:17,3
AMD,nm2fa3s,what you mean?,AMD,2025-10-29 19:54:54,0
AMD,nm52egh,skip driver upgrade if everything is working fine.,AMD,2025-10-30 04:47:52,2
AMD,nm66zoj,"It's a new branch of drivers, so yes, absolutely.",AMD,2025-10-30 11:13:09,1
AMD,nm3r0mx,No this company is clown shoes,AMD,2025-10-29 23:55:11,1
AMD,nm4fw7f,Not the shader cache repopulating?,AMD,2025-10-30 02:15:48,3
AMD,nm21k2k,What issues did you have? Mine is running the same as before. The driver did require a restart too if you didn’t do that?,AMD,2025-10-29 18:49:24,5
AMD,nmxyb1p,Same here,AMD,2025-11-03 20:39:28,1
AMD,nm562z2,Game runs without problems without official support. Most of the time official support is not a requirement for your card to render something. I'm running bf6 with version 25.6.1 since beta without any issues on a card released in 2020.,AMD,2025-10-30 05:19:59,5
AMD,nm71uhr,??? the 25.10.1 optional driver was released on launch day for BF6,AMD,2025-10-30 14:14:58,4
AMD,nm2p1c1,"dont talk, just go",AMD,2025-10-29 20:40:04,2
AMD,nm11l7l,It works for me,AMD,2025-10-29 16:01:15,3
AMD,nm1az67,Up for me,AMD,2025-10-29 16:45:29,1
AMD,nm0wwyt,![gif](giphy|XD4qHZpkyUFfq)  oh god the quality control is so bad,AMD,2025-10-29 15:39:24,43
AMD,nm51c6u,"Same here, with RX 5600 XT, the issue is I uninstalled the drivers first. And I did endless tests :/ to solve the problem.",AMD,2025-10-30 04:38:59,3
AMD,nm1quol,Same here with RX 6800 XT AND WINDOWS 11 25H2,AMD,2025-10-29 17:59:08,4
AMD,nm1lv30,Same error :((,AMD,2025-10-29 17:36:13,2
AMD,nm8mq5n,Is it a recommended method to update the drivers directly through the Adrenalin app? Or is it always better to download the new drivers from their website (specifically from the release notes page)?,AMD,2025-10-30 18:47:11,1
AMD,nmc3hjj,Do I need to use DDU to remove version 25.10.2 that I downloaded from Adrenalin itself? Because I also got this incompatible graphics error on my 6600. I use Windows 10 and I saw that it seems that this version It's only compatible with Windows 11. Will I have to revert to version 25.9.1 then?,AMD,2025-10-31 08:08:36,1
AMD,nm1m3ov,Yes there is no inf for rx 6000 series,AMD,2025-10-29 17:37:20,8
AMD,nm17mz7,Thanks,AMD,2025-10-29 16:29:46,5
AMD,nm1si2h,"for me is ""whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe""",AMD,2025-10-29 18:06:46,5
AMD,nm27en8,"for me it says it is downloading rnda3, guess it will wait for tmrw",AMD,2025-10-29 19:17:04,3
AMD,nm707ql,"Just a quick question, the last driver that was stable for me with my RX 6700XT is 24.12.1, have the  25.??.? drivers been working for you?",AMD,2025-10-30 14:06:48,1
AMD,nm3dn8i,"I did run to it, was updating my nieces RX 6400 and was like wtf when I got the error. DDU didn't help so I tried the auto detect and it pushes me to the pro drivers so some reason. I simply assume it was some conflict do to running Win11 on an ""unsupported"" system so I ended up rolling back to 25.9.1 which installed issue free.  Good to know its just AMD having a Microsoft windows update moment but lucky I have to need to go back and update her drivers again. She plays basically just plays fortnite so yeah see ya in 2026....lol     \*oh and this was as of 4:30pm EST  \*\*Just checked Windows 10 on 7900 XT and its fine.",AMD,2025-10-29 22:42:22,1
AMD,nmc7xuz,This was also a problem on my 9070XT since launch. I don’t think those problems are related to,AMD,2025-10-31 08:54:45,2
AMD,nmswts8,yes its fixxed,AMD,2025-11-03 00:53:25,1
AMD,nmrryhg,wait you charge your phone from your gpu?,AMD,2025-11-02 21:10:29,1
AMD,nmrs6qd,They disabled it because for some reason that fixed VR for  a lot of people.,AMD,2025-11-02 21:11:34,1
AMD,nm5wdtt,Both those games run absolutely fine on the system you've got in your flair.,AMD,2025-10-30 09:43:33,5
AMD,nmbq2d0,"If Work Graphs are used in a game, GPU can issue its own work via onboard command processor, completely bypassing CPU and expensive API draw call commands. So yes, more performance.",AMD,2025-10-31 05:50:22,2
AMD,nmo3chy,We don't know yet as it's very early days but the people behind it at AMD called it a programmable shaders 2.0 moment. It'll be a huge deal for the nextgen consoles post crossgen.  Too many changes and implications to talk about it here but I recommend the real-time GPU tree generation paper from this summer if you want to get an idea of what's possible.,AMD,2025-11-02 07:25:57,1
AMD,nma1kip,More performance at the beginning but a new excuse for developers to not optimize their games,AMD,2025-10-30 23:07:16,0
AMD,nm3k5n5,"Holly crapt, 1.56 gb ?? So we install the 900 mb wrong driver ?",AMD,2025-10-29 23:17:51,1
AMD,nm2rr8w,"Here are the captures from the games I had installed: [https://send.vis.ee/download/9f799176b79cd224/#su81YterFXzSOqZQXVvBXg](https://send.vis.ee/download/9f799176b79cd224/#su81YterFXzSOqZQXVvBXg)   One is Hellblade Senua's Sacrifice and the other is for Little Nightmares 2. The game I showed above was The Legend of Heroes: Trails into Reverie. I didn't have it installed, but if you want that specific game I can download it again.   Tell me if you have any problems or need anything else.",AMD,2025-10-29 20:52:41,24
AMD,nmimh4a,"I'm so sorry to have to do this here, but Reddit has destroyed the old chat system so I can't message you anymore.  I've run into a really strange issue that's quite concerning. When using Relive, if I launch a game and record any gameplay using Relive, Adrenalin of course creates a folder for that game in the designated folder. For Star Wars Battlefront II (2017), it seemed to create a folder that has permissions issues, to put it lightly. I cannot delete the folder and I cannot play any media that's been recorded to it. If I copy or cut the media out into another folder, it plays fine.  Just recently the drive I have been using for Relive media has had issues, so I created a new folder for it on a seperate, newer NVME drive. I put the earlier issues down to just the drive failing and that Adrenalin wasn't at fault at all. I uninstalled Adrenalin, used DDU and did a completely fresh install. Only now, can you guess what has happened? Yep, the exact same thing. Media recorded in the Adrenalin-created Battlefront II folder does not play until cut or copied out of it, and despite my setting user permissions on the main base folder itself, I can't delete the damn thing. Last night it said I didn't have permission. Today, just trying now before writing this message, Windows is telling me the folder doesn't exist.  I'm worried about why this is happening yet again, on a seperate drive on a fresh install. It's only in Battlefront II that it happens, but for it to repeatedly happen across different hardware is absolutely something to notice.  Do you have any idea?",AMD,2025-11-01 11:20:44,1
AMD,nmoioo9,"Not sure if it's a ReLive thing, but I've always had all keyboard shortcuts off, all the overlay options off. I briefly tried using the performance overlay in a game today; ever since the overlay was turned on I've been getting very bad framerates; if the overlay is enabled framerate goes back to normal... all very strange.",AMD,2025-11-02 10:05:41,1
AMD,nm2ergc,Most reported bugs will be ignored. What's the point?,AMD,2025-10-29 19:52:28,-21
AMD,nm2gupr,"I've played through FEAR 1 + DLCs relatively recently. You need mods for acceptable perf in game with any platform, aside from that, I'm not aware of any gradual perf decline.",AMD,2025-10-29 20:02:11,8
AMD,nm54u4s,Any more details? FEAR 2 still runs at crazy high FPS for me.,AMD,2025-10-30 05:08:44,4
AMD,nm5d917,"All the FEAR games run fine.  FEAR 1 has its classic issues with HiD, that needs a mod to fix. Unrelated to GPU drivers.",AMD,2025-10-30 06:29:32,4
AMD,nmckcie,"Deinstall the Graphic Driver with a small Programm called ""DDU"" then restart the Pc and just download the driver from the link, it says for 7800 XT but works for yours as well -> [https://www.amd.com/de/support/downloads/previous-drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7800-xt.html](https://www.amd.com/de/support/downloads/previous-drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7800-xt.html)     DDU Link: [Display Driver Uninstaller (DDU) download version 18.1.3.7](https://www.guru3d.com/download/display-driver-uninstaller-download/)  EDIT: i use the the 25.9.2 one, it works like it was before :) enjoy.",AMD,2025-10-31 10:49:36,2
AMD,nmg3wgq,"Oh really? thats a good Info, glad i deinstalled that one and went back to the version before. i'm sure they will fix it, but when is the question :D",AMD,2025-10-31 22:27:19,1
AMD,nmb7txz,HW video encoding/decoding leads to driver timeout or even system crash. Had to disable hardware acceleration in browser to watch youtube etc. Applications such as Apple Music that use hardware acceleration but do not provide an option to disable it are thus unusable.,AMD,2025-10-31 03:19:34,1
AMD,nm2s6td,"I cant install on RDNA 2, the file contains RDNA 3 in the file name",AMD,2025-10-29 20:54:42,1
AMD,nm2rqko,No it's not.,AMD,2025-10-29 20:52:36,-2
AMD,nm1c6ju,"> whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe  They updated the link. Now RDNA1/2 and RDNA3/4 drivers are branched, which is not good news.",AMD,2025-10-29 16:51:03,33
AMD,nm92knj,This is literal ASS.  ![gif](giphy|IVhivwuUT16HH7NRdP|downsized),AMD,2025-10-30 20:03:25,1
AMD,nm1748g,I don't want to believe it either. But Team Blue ended standard support for Windows drivers on Iris Xe with similar wording last month. RDNA1/2 and its release date are similar.,AMD,2025-10-29 16:27:20,12
AMD,nm5dr6n,That's Intel Ice Lake still. Maybe Radeon 7.  Maybe ATI X800 series.,AMD,2025-10-30 06:34:37,1
AMD,nm19zu0,"seems so, im not sure what this statement means,  i need to test my HMD once i arrive home. but if they disable power delivery over the VirtualLink port then GG PSVR2. forcing you to use that dumb convert box they sell for Nvidia cards.",AMD,2025-10-29 16:40:53,6
AMD,nm1lruu,I mean if you experienced the bug that they just fixed in this driver then working “perfect” isn’t really all that perfect is it?  There’s been a lot of issues with virtual link which is probably why they gave up and magically fixed the vr issues at the same time.,AMD,2025-10-29 17:35:49,-2
AMD,nm14dai,Hopefully it fixes bf6 crashing that's a big issue rn,AMD,2025-10-29 16:14:23,8
AMD,nm1at75,Unless you use specific DX12/Vulkan extension that is introduced by particular driver version ;),AMD,2025-10-29 16:44:42,4
AMD,nm197km,Intel drivers are by no means better. They seem to be since they add performance in more titles (compared to Nvidia/AMD) but that's more due to them being terrible at first and Intel working to fix those issues.,AMD,2025-10-29 16:37:13,8
AMD,nm4nq2m,Does this actually help with the Chromium-based app artifacts?,AMD,2025-10-30 03:01:48,2
AMD,nm99hvy,Oof that sucks dude 😩,AMD,2025-10-30 20:36:58,1
AMD,nm36l7m,does your overlay still work?,AMD,2025-10-29 22:04:52,1
AMD,nm1flih,"Thanks! Unsure if I'll update my drivers or just wait for now, as the Preview Driver is working well for me.",AMD,2025-10-29 17:06:54,1
AMD,nm768ot,"The spawn selection screen is also stuttering for me, it lasted on the 1.1 update.  I've seen people mentioning that it's tied to FSR. If you turn it off the stutter goes away.",AMD,2025-10-30 14:36:50,1
AMD,nm1dx2d,Nvidia had the drivers ready for the tech test 2 that ran for only 3 days lol. Also AMD prioritizing support for a flop that is vampire bloodlines 2 over arc is icing on the cake,AMD,2025-10-29 16:59:06,4
AMD,nm1bfxd,Not necessarily but I feel like fsr4 support is already there? Drivers back in September already made games with true fsr3.1 automatically enable fsr4.,AMD,2025-10-29 16:47:38,5
AMD,nm12rz6,"But I didn't change it, I'm using the default one:   `C:\Users\%USER%\Videos\Radeon ReLive`",AMD,2025-10-29 16:06:52,1
AMD,nm12nys,"I'm pretty sure it worked recently, maybe updating to Windows 11 25H2 broke it but I'm not sure as I said. That's a bummer because I've always been telling my friends that ReLive is a pleasure to use when choosing a new GPU :|",AMD,2025-10-29 16:06:21,1
AMD,nm2fbl6,Can i just download 25.10.2 and be done with it? I'm scared to try doing ddu since im noob lol,AMD,2025-10-29 19:55:05,0
AMD,nm2f05e,What is pro drivers. Just built my pc this year. Should i just download this 25.10.2 drivers and be done with it?,AMD,2025-10-29 19:53:37,1
AMD,nm90yxn,"I don't know either, I also have 10.30 installed. It was updated last through Andrenalin as I recall.",AMD,2025-10-30 19:55:38,1
AMD,nm8uvvl,should i use DDU when installing a new version of Adrenalin?,AMD,2025-10-30 19:26:01,1
AMD,nm8uvkw,should i use DDU when installing a new version of Adrenalin?,AMD,2025-10-30 19:25:59,3
AMD,nmt5gjy,They edited their text and d/l's to include W10... seems like the most idiotic egregious edit ever?  Why would they post it like that initially is beyond me.,AMD,2025-11-03 01:46:16,1
AMD,nmzhv9w,"Conseguiu aí mano? Eu usei um pen drive com windows pra reparar, voltar pra versão antes do update também arruma",AMD,2025-11-04 01:39:12,1
AMD,nmam8yv,"heh, already did on CachyOS bud - not really an issue for me.",AMD,2025-10-31 01:06:48,1
AMD,nmf7xbs,That’s unfortunate :(,AMD,2025-10-31 19:26:54,1
AMD,nmgmy6o,"Thanks, I downloaded it again, but normally, not from the notes. It seems AMD modified the installer today; it works now. The file size is 912MB.",AMD,2025-11-01 00:33:13,2
AMD,nm0yd4z,It did during the server slam. No reason why that would be turned off on a driver level.,AMD,2025-10-29 15:46:12,9
AMD,nm10xau,BF6 has worked this whole time before official support,AMD,2025-10-29 15:58:11,3
AMD,nm17xgd,"Anything is welcome news. Hopefully it fixes it, it doesn't specifically say 6000 but, one can hope. Otherwise, dx11 forever until I get a better gpu and cpu.",AMD,2025-10-29 16:31:09,1
AMD,nm2b35c,Found it!,AMD,2025-10-29 19:34:46,4
AMD,nm75o0y,"No, they're moving it to a separate branch. That's the first step in making them legacy and no longer receiving new game optimization.",AMD,2025-10-30 14:33:59,1
AMD,nm4ow64,"Bom, eu gosto de manter sempre atualizado independente de ter problemas ou não, mas tudo bem em não querer atualizar também, vai de cada um, se não tiver problemas não precisa.",AMD,2025-10-30 03:09:14,1
AMD,nm3nqhh,"Also, past drivers for my 9900x have been in the 900 MB range, just like this uninstallable new 25.10.2. I'm not moving up to a 1.6 GB driver when they've clearly made a mistake.",AMD,2025-10-29 23:37:05,1
AMD,nma9vr1,This my first experience with bad AMD drivers too. I guess they aren't immune to that just like nvidia. Been an awful experience playing Arc Raiders with this version lol,AMD,2025-10-30 23:54:10,1
AMD,nm61mf7,"it just doesnt make sense. in basically all games with new apis they are neck and neck, but in games with older apis especially ones that are cpu heavy it seems to slow down. at least from all the research ive done it points to dx11 and its dependence on driver to be the cause. if you have a different thought id love to hear it.",AMD,2025-10-30 10:30:11,1
AMD,nm74scy,"I just swapped the fsr upscaler dll file, it work very well in Marvel rivals. I haven’t got banned in BF6 since I swapped the dll file. Do not use Optiscaler in online games, you should be fine.",AMD,2025-10-30 14:29:35,1
AMD,nm1nkwu,"I checked the AMD website earlier on Monday.  I did not see a 25.10.1 release.  The most recent release at the time was still 25.9 2, which is what I had installed.",AMD,2025-10-29 17:44:14,1
AMD,nm2hzud,"The last point on the ""Fixed Issues"" list:  CVE-2023-4969 (RDNA only), CVE-2024-21969 (RDNA only), CVE-2024-36323, CVE-2024-36325, CVE-2024-36333, CVE-2025-61964, CVE-2025-61965, CVE-2025-61966, CVE-2025-61967, CVE-2025-61968     These are CVEs for security vulnerabilities.",AMD,2025-10-29 20:07:31,5
AMD,nm3gl7e,\>What issues did you have?  Extreme stuttering basically moving around causes the whole game to drop massive frames all the time no matter what you do old driver was fully uninstalled and DDU'd and i did the double restart as well i even tried reinstalling which gave me the same issue 25.9.1 doesn't do this i will just wait till next driver,AMD,2025-10-29 22:58:24,0
AMD,nmf4osy,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-10-31 19:09:56,1
AMD,nm8frsz,"If you haven't found a fix yet, I found one for mine user @aker666 described it in this thread perfectly  go to your driver download page   click driver detail  click release notes  scroll down until you see:  Systems with RDNA series graphics products:      AMD Software: Adrenalin Edition 25.10.2 Driver for Windows® 11 (64-bit)  and click on the link    the file is larger than usual, I think it was 1.6 GB, but it did the job",AMD,2025-10-30 18:14:35,2
AMD,nm8f9ml,"mano minha rx 7600 sumiu do gerenciador de dispositivos , mais ela esta dando imagem mais nao da pra jogar acho que so formatando pra concertar",AMD,2025-10-30 18:12:12,1
AMD,nm8k2ae,"opa vc consegui arrumar mano , aminha rx aki sumiu so ta com a dedicada ta foda fazer ela funfa",AMD,2025-10-30 18:34:44,1
AMD,nma1e0y,"In my case, doing it through the application caused application errors (I suppose Windows 11 being Windows 11...) and driver timeouts. Until one day I discovered that if I downloaded the full installer, disconnected the laptop from the internet (airplane mode) and without a network cable, used the installer to do a clean install and restored the user settings... I've never had a driver timeout while gaming again.       If it works correctly for you when you do it from the application itself, I see no reason to stop doing so.",AMD,2025-10-30 23:06:17,1
AMD,nmc4myv,"I prefer to use AMD Cleanup Utility: [https://www.amd.com/en/resources/support-articles/faqs/GPU-601.html](https://www.amd.com/en/resources/support-articles/faqs/GPU-601.html)      I think that if you download the full installer and do a factory reset within the installer, you should be fine.",AMD,2025-10-31 08:20:46,1
AMD,nm1uhsa,Just saw your comment and tried downloading it again from (forced a refresh with CTRL F5):   [https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6700-xt.html](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6700-xt.html)   And clicked the on Win 11 download link... not combined :c,AMD,2025-10-29 18:16:10,2
AMD,nm2a1fz,"Apparently if you get it from a different link, it has the combined version that is twice as large download, but it seems to work. [IllustratorDeep7780](https://www.reddit.com/user/IllustratorDeep7780/) updated his comment again, and it now has the link: [AMD Software: Adrenalin Edition 25.10.2 Release Notes](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html)",AMD,2025-10-29 19:29:41,7
AMD,nmdxri2,"I upgraded the driver a few times this year (been on 25.# branch a lot) and I haven't had any issues with stability at all and I keep my computer on all the time, and play a variety of games, including VR games.  If you're just having general instability in all games, I would make sure your memory or CPU are stable, like try a memory/cpu stress test. I recommend running like prime95 stress test for at least an hour and it should come back with zero errors. If it comes back with any errors, that means any video driver issues you had, could have been caused by CPU/memory problems.  (Correction: I had one strange issue with Minecraft (Java) and Complementary Reimagined 5.5.1 shader, where \~5% of the time when I used a bed to sleep, I would get a driver timeout, but I assumed it was a bad combination of mods and the shader I was using. It was so strange... only when I went to sleep, never any time else.)",AMD,2025-10-31 15:36:02,1
AMD,nms7hw5,It has a usb-c port... I just tried it because i had my vr connected to it and it just kinda stayed that way. Long cable with fast charging on my desk? Why not.,AMD,2025-11-02 22:30:46,1
AMD,nm6yepv,Bean soup theory much?  I know what I'm lookig at and on my 4k OLED tv it's not it. Feel free to game however you like,AMD,2025-10-30 13:57:48,1
AMD,nm7chw4,"Yeah, esp. Indi with path tracing on. And FSR4 will not be enough to make it playable at 4k on 9070XT, the FSR Redstone is necessary.",AMD,2025-10-30 15:07:25,1
AMD,nmo338l,"You could say the same for virtually every single improvement in graphics programming. This TI-esque dev bad BS has to stop.     Also won't matter given how much better Work graphs is vs EI. You could say it just works. Much easier to code, less headaches, no black box bs etc...",AMD,2025-11-02 07:23:10,1
AMD,nm2s2bs,Captures received - appreciate you,AMD,2025-10-29 20:54:07,28
AMD,nmjml28,I do vaguely recall an issue from some time ago with ReLive's capability to write into user directories coming into conflict with a Windows feature like Controlled Folder Access (I could be mistaken about the specific OS feature name). I need to double check internal docs on this to catch myself up when i'm back at work next week,AMD,2025-11-01 15:17:36,2
AMD,nm2j5jx,"I find it amusing that you'd take the time to complain to me in such a generic manner without helping me help you in any way whatsoever.  The example provided elsewhere tells me you've never played FEAR on a modern system. Good on you for checking out a cult classic, read the guidance over on PCGW to get set up properly, then enjoy the game.  Get back to me with actual data and I'll see what I can do.",AMD,2025-10-29 20:12:54,17
AMD,nmd6xve,Thank you!!,AMD,2025-10-31 13:18:41,2
AMD,nm3ijhu,"You need to download it from the link provided in the release notes: [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  instead of the one provided on the driver download page: [https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6800m.html](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6800m.html)  because the link is: [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe)  If you are an RDNA 1/2 user like me, you need the EXE with ‘-combined’.",AMD,2025-10-29 23:09:10,8
AMD,nm1o3dw,"Is that similar to the Polaris/Vega drivers, which are updated very infrequently and do not receive new features?",AMD,2025-10-29 17:46:35,11
AMD,nm1duvg,"Maybe it's just due to the AI hardware? RDNA2 has no AI features, so maybe they just don't bundle the AI NPU Driver?",AMD,2025-10-29 16:58:49,4
AMD,nm3rx02,Best case: FSR 4 int8 is coming soon and they want to only provide the necessary version for everybody.,AMD,2025-10-30 00:00:11,1
AMD,nmdgvvy,RDNA1/2 lack of hardware to support many of the new feature in vulkan so I understand why.  They can emulate it but it's kind of pointless,AMD,2025-10-31 14:11:38,1
AMD,nm5dmt5,They're literally not branched.,AMD,2025-10-30 06:33:23,-3
AMD,nm54pvs,"I don't know what it means at this point, because even with this driver I am able to QC devices and my headsets all work.",AMD,2025-10-30 05:07:41,1
AMD,nm1pwah,What’s your error? Mine throws a device_hung error at random but BF6 is the only game that does it,AMD,2025-10-29 17:54:48,6
AMD,nm5wead,"I haven't crashed on Battlefield 6 ONCE even since the beta. Downloaded 25.10.2 (which took them way too long to release might I add) and it caused me not 1, but 2 crashes in 5 hours lol. Garbage driver by the looks of it.",AMD,2025-10-30 09:43:41,2
AMD,nm8b4o0,*stares in ice lake gpu support*,AMD,2025-10-30 17:52:47,1
AMD,nm4pgpr,"It was happening on everything. Discord full screen streaming, wowup when I used to use it, Opera GX, etc… all purple/pink artifacts gone once I got rid of the adrenaline software and did driver only install. Wish I could tell AMD somehow that it’s their software causing issues and that they need to rebuild it from the ground up to stop all of this nonsense but I doubt they will ever see my post :(",AMD,2025-10-30 03:12:53,2
AMD,nm9djag,"bota azar eu ia fazer live no tiktok , mais se pa vou ficar um tempo sem pc",AMD,2025-10-30 20:56:33,1
AMD,nm6zju9,I don't use the AMD overlay I'm a rivatuner + capframe X guy and that is working fine for me.,AMD,2025-10-30 14:03:29,1
AMD,nm87uq6,Oh thanks Imma try it,AMD,2025-10-30 17:37:01,1
AMD,nm2es1i,"NVidia also has BSOD left and right, game ready drivers are useless.  **25.8.1** runs both **Battlefield 6** and **Arc Raiders** at stable 60FPS, while this so called game ready drivers are extremely unstable on **Battlefield 6**.",AMD,2025-10-29 19:52:33,16
AMD,nm2iyq1,"If you're not comfortable using DDU then yes, just install 25.10.2 overtop. It should be fine.",AMD,2025-10-29 20:12:01,1
AMD,nm2g1mx,pro drivers are for work stations-work station gpus and stuff but they still work fine but if you just game on the pc download the standard drivers aka 25.10.2-25.9.1 etc. not the 25.10.30.,AMD,2025-10-29 19:58:26,1
AMD,nmbkcih,Only if you experience issues that could be caused by old driver remains,AMD,2025-10-31 04:57:11,1
AMD,nmatpzj,"No, thats not *usually* necessary",AMD,2025-10-31 01:51:17,-2
AMD,nm0yun1,I don’t think it did. Since apparently it was an older build of the game. I was playing at 4k and I’m pretty sure it was fsr 3 even though it said fsr 4 was on. The grass looked like crap even on fsr quality at med/high settings. Bf6 vegetation/shimmering looked more stable than arc raiders.  I also did a quick test and I toggle fsr 4 and both with and without fsr 4 it looked the same.,AMD,2025-10-29 15:48:31,1
AMD,nm86bk5,">That's the first step in making them legacy and no longer receiving new game optimization.   From the release notes, looks like RDNA2 and RDNA1 have **already** lost new game support. Whatever they had last month, that's it for new game-specific fixes and optimization.  >New Game Support and Expanded Vulkan Extensions Support is available to Radeon™ RX 7000 and 9000 series graphics products.",AMD,2025-10-30 17:29:32,2
AMD,nm62u46,"There are a lot of quirks present in drivers to make stuff performant.  The devs might be choosing a really poor rendering path for AMD for example and no workaround can boost performance.  If you're CPU bound in DX11, use DXVK or DXVK GPLAsync.  If it's an UE 4/5 game, run in DX12 instead of DX11 and you'll be less CPU bound once again.  If you want to further lessen CPU boundness, consider playing on Linux (in DX12 with VKD3D) if possible.",AMD,2025-10-30 10:40:18,1
AMD,nm757aj,interesting thanks for the update.,AMD,2025-10-30 14:31:39,1
AMD,nm1q0s4,"I know, sometimes they don't appear if you search specifically for your gpu, you need to search for the driver's name in google and download from amd website, the software will automatically install the correct driver for your gpu model. This happened with my 6700xt, if I search for it drivers in amd website, the latest one is 25.9.2     EDIT: they fixed right now, it's showing 25.10.2 in the website for my 6700xt",AMD,2025-10-29 17:55:21,2
AMD,nm5d89e,save up?!? polaris and vega are 8 yrs old. skip the saving .. just go buy something new and enjoy lol,AMD,2025-10-30 06:29:18,2
AMD,nm5eai7,">in short never buy amd no matter what, unless they sell the 7900 xtx for 5 bucks, i've had amd for many many years and every single day i regret it until i finally saved up enough to get a 4080  that you? lol!! 2 yrs ago u been saving for nvidia already... hope u can soon finish saving up.   also i thought you had a 1060 3gb... so where does all the 'had amd for many many years' talk come from?!  embarassing stuff.",AMD,2025-10-30 06:40:05,2
AMD,nm5g8am,hey yo. tell me as soon as you buy the 5080 btw.  cuz i wanna congratulate you...,AMD,2025-10-30 06:59:36,1
AMD,nm5becb,This might be the reason that web browser is enabled by default on this version. Did anybody else realise it as well? Also animations & effects is disabled for some reason.,AMD,2025-10-30 06:11:03,1
AMD,nmee3jb,to com windows 10 sera que tem problema,AMD,2025-10-31 16:55:36,1
AMD,nmeiijr,deu o erro 186 mano kkkk azar ta grande,AMD,2025-10-31 17:17:14,1
AMD,nm8qmhk,"Updated as Combined file for Rdna 1 and 2, download it. If it doesn't work, go back to version 25.9.1 and it will be installed this time.",AMD,2025-10-30 19:05:31,1
AMD,nmabvbn,this world is really conplicated,AMD,2025-10-31 00:05:46,1
AMD,nmc6poj,I didn't understand.,AMD,2025-10-31 08:42:33,1
AMD,nm1vqql,from here [AMD Software: Adrenalin Edition 25.10.2 Release Notes](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html),AMD,2025-10-29 18:22:04,5
AMD,nm2ki6f,"yep, i saw, tho i will wait for tmrw and see, not in a rush",AMD,2025-10-29 20:19:07,2
AMD,nm3pivf,Yea I did it now it open by it self with the 6700xt. Got laptop ai 300 fix hardware acceleration when watching YouTube and working flawless in my 7900xtx finally fix warzone lag in big map .,AMD,2025-10-29 23:46:50,1
AMD,nmf66a1,"RAM and CPU are fine, have done many tests.  It doesn't crash games but after games and the fault is caused but the AMD DLL or whatever.  Only happens with the newer drivers, 24.12.1 and earlier are just fine. Others with 6700xt have reported issues with newer drivers as well. Thanks for the suggestion.",AMD,2025-10-31 19:17:42,1
AMD,nma537i,"Appreciate you taking bug reports via reddit, that's awesome!",AMD,2025-10-30 23:26:57,6
AMD,nmk8uyz,"Thank you Vik, I'm sorry to have to bother you with this, but I wasn't anticipating getting anywhere by posting in the forum about it.",AMD,2025-11-01 17:12:42,2
AMD,nm2nphp,"All the bugs I've cataloged and reported over the past two years are still in exactly the same place. What do you want me to say?   F.E.A.R. is an old game. I tested it on my laptop, and it runs at 120-150 fps@1080P, but inevitably, the performance degrades within minutes until it reaches 50-40 fps. No, it's not thermal throttling, running at 11-15W and 60°C. It's a driver issue.",AMD,2025-10-29 20:33:51,-4
AMD,nm60czk,"Okay, thanks.",AMD,2025-10-30 10:19:35,1
AMD,nm1s3gw,"Basically, yes. When Vulkan 1.5 is released in the future, RDNA1/2 will not be updated on Windows and will remain at 1.4. You will have to switch to Linux to get it.",AMD,2025-10-29 18:04:53,18
AMD,nm1g9d8,"This is a bit of a stretch. Considering that the RX 6800 and 6900 series are equipped with 16GB of VRAM, they can handle many basic AI tasks even without an NPU.",AMD,2025-10-29 17:09:59,6
AMD,nm65niu,"Really hoping for official int8 fsr4 support on rdna 2, but I feel like it's not gonna happen sadly. They don't seem that interested in bringing new features to older cards.",AMD,2025-10-30 11:02:50,2
AMD,nm1s2re,Device hung and device removed are the most common ones. Hopefully both are resolved with this but I'm not banking on it as it's like on Dice not Amd to resolve it as it impacts nvidia gpus too,AMD,2025-10-29 18:04:47,6
AMD,nm5yqkx,Bf6 is so buggy it's not funny i guess it's a combination of the game and the driver,AMD,2025-10-30 10:05:16,1
AMD,nm6cngp,"I get like 3 crashes everytime I play BF6 for long periods of time, with hard restart. The AMD software literally never loads up or disappears after every crash. :(",AMD,2025-10-30 11:53:57,1
AMD,nm4sa8j,"Had a 5080 since launch, no BSOD. Quit the cope. Bf6 stable 60fps lmao is that what you spend $1000 on for just 60fps.",AMD,2025-10-30 03:31:32,-1
AMD,nm2k1eg,"It was just a placeholder for this message, there's my real username in there",AMD,2025-10-29 20:16:59,1
AMD,nm4zen8,"I had 25.10.30 installed, its just what my pc installed automatically 😅 seemed to work perfectly fine,    25.10.2 broke noise suppression for me",AMD,2025-10-30 04:23:18,1
AMD,nm16r6p,For me toggling FSR4 on made the game run very sluggish for some reason. Also don't think it was working properly as the visual quality wasn't looking better than FSR3,AMD,2025-10-29 16:25:37,2
AMD,nm641lq,"unfortunately none of these solutions will work because ive tried them. dxvk doesnt work properly (i tried multiple versions) in these games and either tanks performance or just has bugs which arent worth it. acc's dx12 implementation is basically non existant. linux unfortunately isnt an option for me.    idk its just really weird to me that in most games its nice and dandy, but in these its not. usually such a disparity would mean a driver issue, but idk anymore at this point.",AMD,2025-10-30 10:50:05,1
AMD,nm65neu,How did u find this comment?,AMD,2025-10-30 11:02:48,1
AMD,nm9d7zy,pena  que da erro 182 toda vez,AMD,2025-10-30 20:55:01,1
AMD,nmcbx60,"You don't need to use DDU when AMD has an official tool for that very purpose, in case you want to do so. If you do not want to use these tools, when you use the installer, an option to perform a factory reset will appear at the bottom, and if you select that, another option will appear to restore your current preferences.",AMD,2025-10-31 09:34:58,1
AMD,nm1yzu5,thank you!,AMD,2025-10-29 18:37:27,1
AMD,nm2oggv,"The point still stands that you'd rather take the time to antagonise me instead of getting your issues addressed.  You may not be familiar with my involvement here, but I generally put in the time, care and effort to drive legitimate, reported bugs to resolution.  I'm ready when you are, but If you'd rather not cooperate, then that's on you.  As for that being a driver issue, I'm not convinced. I've played the hell out of FEAR1 with no such behaviour, though this was on RDNA2 and separately on Cezanne.",AMD,2025-10-29 20:37:22,15
AMD,nm1lmz2,they are still missing ai cores which even the 7000 cards have (not many and they are weak but they are there),AMD,2025-10-29 17:35:10,5
AMD,nm2alc5,"Fixed device removed problem by changing from pax to nato and one primary weapon, as stupid as that sounds but it actually worked at least on 1070ti which I’m using now while waiting my RX9060XT 16gb from RMA.",AMD,2025-10-29 19:32:22,3
AMD,nm5o4ob,"Unfortunately it’s not fixed, hopefully EA can fix it soon. The CTD’s are pretty annoying.",AMD,2025-10-30 08:21:03,2
AMD,nm7qman,I've had this AMD card for like 5 years now. Honestly mate I just think the majority of problems are AMD related. I know sometimes it might only affect one game so people blame it on that game - but then NVIDIA users have no issue with that game. I've had a whole wide range of issues regarding AMD drivers over those years and I'm pretty sure I'm gonna hop back to the other side when I get a bit more money.,AMD,2025-10-30 16:14:15,2
AMD,nm5roa1,"I don't know what lying gets you \[[1](https://www.neowin.net/news/nvidia-drivers-confirmed-to-cause-bsod-on-windows-pcs-without-certain-cpu-instructions/)\]\[[2](https://www.nvidia.com/en-us/geforce/forums/geforce-graphics-cards/5/556316/bsod-with-the-latest-versions-of-the-nvidia-driver/)\]\[[3](https://www.tomshardware.com/pc-components/gpus/nvidia-confirms-it-is-investigating-rtx-50-series-bsod-and-black-screen-troubles-no-timeline-for-a-fix)\], is quite clear on everyone who is copping.  Plus I didn't spend any $1000, the RX 6950 XT cost me 600€ and I getting 60FPS at 4K max settings, thank you for your unwarranted concern.",AMD,2025-10-30 08:56:54,3
AMD,nm64us1,ACC is UE 4.26. You can launch it in DX12 by adding -dx12 as a launch command. Let me know if it works and if it becomes less CPU bound.,AMD,2025-10-30 10:56:32,1
AMD,nmcdwx9,"So if I go into Adrenalin and restore the factory settings, it will revert to the previous driver, which in my case was 25.9.1?",AMD,2025-10-31 09:53:58,1
AMD,nm3ez8o,"I wasn't criticizing you, but rather the effectiveness of the bug reporting process and the apparent lack of fixes over the years that I've observed. Here's the same Cezanne that you said you tested. In five minutes, without leaving the spot, the performance degrades from 120 fps to 58 fps.     All drivers updated to the latest available version.  https://i.redd.it/k443c4kkq4yf1.gif",AMD,2025-10-29 22:49:36,-3
AMD,nm1n135,This makes sense from a marketing perspective. But ending standard support in 5 years is still too short for dGPU.,AMD,2025-10-29 17:41:39,10
AMD,nmq8f45,"If you don't mind me asking, what problems did you have that made you RMA your 9060 XT 16gb? Asking because I just got the same GPU(powercooler) and have had a couple things going on with it but not sure if I should RMA or it's a driver thing. Thanks in advance.",AMD,2025-11-02 16:45:03,1
AMD,nm6cszp,"Thanks for the reply man, saved me logging in to only get more annoyed myself! Arc raiders for now it is!",AMD,2025-10-30 11:54:59,1
AMD,nm7rntz,If I go nvidia it's 5090 or 6090 only so yea too end onlys,AMD,2025-10-30 16:19:10,1
AMD,nmlnkpx,"i run into issues with a good chunk of modern games on my 6800. this patch gave me the device removed crash right away, with crashes as early as 5 min after starting",AMD,2025-11-01 21:39:46,1
AMD,nm6553e,i know... as i said i tried most of this stuff. it exists but it might as well not. the performance hit is even worse than just running dx11.,AMD,2025-10-30 10:58:46,1
AMD,nmcjxmq,"No, you do a clean install for 25.10.2. If you want to go back to 25.9.1 use DDU or the AMD tool and then install the 25.9.1.",AMD,2025-10-31 10:46:13,1
AMD,nm3gcdb,"are you making use of the recommended mods like large address aware? The last time I played through, I remember having to also use a reworked directInput module as the game would respond horribly to mouse input otherwise.  Looking at it now, EchoPatch seems like a recent development (or at least one only recently recommended), though seems to offer a comprehensive solution across many problem areas experienced when running on contemporary operating systems.  E: One other thing comes to mind, are you running this from Steam or GOG? I've only ever ran this through Steam (requiring the above mods). Looks like F.E.A.R. Platinum is in the GOG Preservation Program (thinking i should pick this edition up anyway).",AMD,2025-10-29 22:57:02,4
AMD,nm5r2x7,"He's not doubting that it occurs but there's known problems with FPS drops over time on modern systems that happens that happens regardless of vender so a driver can't fix it. Also assuming it's a driver issue without actually testing another vendor or at least older versions of drivers is a bit silly.  Oh and don't post screenshots as gif, it's disgusting (waste of bandwidth and hurts the eyes)...",AMD,2025-10-30 08:51:01,1
AMD,nm1q3pb,"They aren't ending support, but you should expect some features not arriving to RDNA 1 and RDNA 2. (I wouldn't expect RDNA 2 to have FSR 4 officially)",AMD,2025-10-29 17:55:43,3
AMD,nm65p3y,"If performance drops in DX12, then you're GPU bound and not CPU bound. Then your options remain upscaling and/or Frame Gen.",AMD,2025-10-30 11:03:10,1
AMD,nmep6ua,But why would I have to do a clean install of 25.10.2 if I'm already on that version? Wouldn't it be easier to just run DDU and switch to version 25.9.1?,AMD,2025-10-31 17:50:28,1
AMD,nm7rzv9,"It's the Platinum version, from GOG. By installing the echo patch, performance increases to 200fps+, and degradation takes much longer to occur, but drops still happen, going down to 30fps.",AMD,2025-10-30 16:20:46,0
AMD,nm5vjhd,This Reddit only allows posting images in .gif format.,AMD,2025-10-30 09:35:34,1
AMD,nm1r1wt,"This also applies to Polaris and Vega if you think they are still supported. They do get Vulkan 1.4 on Linux, but end up with Vulkan 1.3 on Windows. RDNA1/2 suffered the same fate.",AMD,2025-10-29 18:00:03,9
AMD,nm65vq1,"but how then 9070 xt and 5070 ti are basically equal performance in other gpu bound games, but in these even a 5070 does better....",AMD,2025-10-30 11:04:36,1
AMD,nminf4p,"Earlier you said you had an error during installation, or so I understood. Do whatever you want, if it works fine for you, don't do anything and that's it.",AMD,2025-11-01 11:29:14,1
AMD,nm852zq,"That's neat, I'll need to pick up that GoG version to check in with. I've only ever used the Steam version on win10 and 11.  The Cezanne system I have has been dutifully running Fedora for a bit, will need to see if I have a spare m.2 NVMe disk to throw windows on. Can you tell me your windows build and in-game gfx settings?  e: Interestingly, there's an old [GoG forum post](https://www.gog.com/forum/fear_series/fear_fps_fix) from 2017 detailing the exact same issue you've described, with a link to a fix.",AMD,2025-10-30 17:23:36,4
AMD,nm89sdg,Make sure you do the 4GB patch that Vik suggested. I'm playing through FEAR on my 7800XT and it runs perfectly after doing all the suggested pathes/tweaks from PCGamingWiki. https://www.pcgamingwiki.com/wiki/F.E.A.R.#4_GB_patch,AMD,2025-10-30 17:46:25,3
AMD,nm5zr58,"https://i.imgur.com/UxevhaL.png  Can't you upload to an external site? I don't use the app so they don't even give you the opportunity to upload an image to Reddit so I wouldn't know about those restrictions. That said, if you are posting a screenshot of a game, wouldn't you also be in browser?",AMD,2025-10-30 10:14:16,1
AMD,nm65z22,There are games and games. Plenty of games where AMD GPUs punch above equivalent tier Nvidia GPUs. It's not all 1:1.  7900 XTX matching 5080 in DOOM The Dark Ages that uses RT for example.,AMD,2025-10-30 11:05:20,1
AMD,nm6hhiv,"The slide also states a Zen 6 release in 2026 for the ""Venice"" Zen 6 EPYC.  And a planned release of openSIL Firmware for Medusa Zen 6 in the first half of 2027.",AMD,2025-10-30 12:24:58,37
AMD,nm6r2kl,"I'm happy about openCIL, I think transparency and collaborating with others is the key to reducing attack vectors. Alternatives to UEFI/BIOS like coreboot should benefit from this!",AMD,2025-10-30 13:19:39,16
AMD,nmkyo45,"Sweet, going to put aside 25 euros a month until the Zen 6 X3D chips come out.",AMD,2025-11-01 19:26:24,9
AMD,nm8wfgm,Zen 6 in 2027 = my 9800X3D will be king of the gaming hill for a good while yet,AMD,2025-10-30 19:33:31,23
AMD,nm9runb,Nada do redstone?,AMD,2025-10-30 22:11:49,1
AMD,nmultzc,"so 2027 jan ces for zen 6 products  wow, amd is slowing down",AMD,2025-11-03 08:47:04,1
AMD,nmv0lzo,chilling with 5800x3d until zen6 x3d. Moving to am5 only after it matures to 12 cores CCDs  😊,AMD,2025-11-03 11:15:24,1
AMD,nm6mddr,First half 2027 probably means real late 2027 for the 3D chips as well.,AMD,2025-10-30 12:53:46,24
AMD,nmlw1i7,Great way to look at it 👍,AMD,2025-11-01 22:27:54,5
AMD,nmay29f,9850x3d enters chat. lol,AMD,2025-10-31 02:17:29,11
AMD,nmb3zdn,"well, Intel is dead in the water so AMD is in no rush to release a half baked 3% performance increase.",AMD,2025-10-31 02:53:51,11
AMD,nm9hkg3,It doesn't say anything about the release date of Zen 6.,AMD,2025-10-30 21:16:54,10
AMD,nmdnau1,"I think with games i play ( not latest ) mmos and arpgs,il perfectly fine on my 5800x3d until 2028",AMD,2025-10-31 14:44:28,4
AMD,nm6z0xu,I don't think opensil firmware release date tells you much about the actual CPU release date.,AMD,2025-10-30 14:00:50,9
AMD,nmc4x5h,"That's just a 9800x3d that doesn't have one of the overclocking lockouts in place, one that a large fraction of motherboards can already bypass via asynchronous eclks.  Identical hardware and probably 0 change for enthusiasts; a touch better for people who don't overclock or who use the cheapest motherboards.",AMD,2025-10-31 08:23:48,8
AMD,nmh0qj5,Zen6% /s,AMD,2025-11-01 02:07:20,7
AMD,nm9qpud,Not with that attitude,AMD,2025-10-30 22:05:15,8
AMD,nm7iilb,"For Venice the plan according to the slides is to have an openSIL release within 3 months of the hardware release, and the stated goal of AMD is to shorten this time gradually until openSIL is released at the same time as the hardware, so I'd assume that for all Zen 6 hardware, the time between the hardware release and the openSIL release is at most 3 months.  While that doesn't really tell us anything about 3D chips, it would put the Medusa release sometime Q4 2026 to Q2 2027.",AMD,2025-10-30 15:36:04,9
AMD,nmg2n9c,Likely higher binned,AMD,2025-10-31 22:19:10,5
AMD,nmajsp4,"I know this is about the 7500X3D, but product timeline like this one makes me believe the earliest we could see the 9600X3D, or even 9500X3D, would be 2028—heck, 2030 even because AM5 can continue AM4's absurdity.",AMD,2025-10-31 00:52:39,48
AMD,nmavbd8,Affordable… 200 usd or more lol,AMD,2025-10-31 02:00:54,23
AMD,nmcq4l0,will amd kill the support of it suddenly?,AMD,2025-10-31 11:34:26,8
AMD,nmrx5dc,will there ever be a x3d apu?,AMD,2025-11-02 21:35:51,1
AMD,nme95os,AMD is so off my purchase list. I don't trust this company anymore.,AMD,2025-10-31 16:31:25,-17
AMD,nmata9g,"They're in about 2 year life cycles, so late 2027 for the 9600x3D is my guess.",AMD,2025-10-31 01:48:39,10
AMD,nmbf3xn,$200 with performance that potentially beating out top end intel part. Seems like a good deal to me..,AMD,2025-10-31 04:13:27,48
AMD,nmf30aw,It may take the year to come only,AMD,2025-10-31 19:01:07,4
AMD,nmhzixh,"They might kill support before release... (Kidding, I'm on a 7500f+9060xt 16 gb 1440p and love it(",AMD,2025-11-01 07:17:08,2
AMD,nmu4iz4,thankfully the cpu division is much more different than the gpu division.,AMD,2025-11-03 05:53:54,2
AMD,nms06qf,Maybe this could help: [https://www.reddit.com/r/pcmasterrace/comments/1aenoh3/ryzen\_x3d\_apus/](https://www.reddit.com/r/pcmasterrace/comments/1aenoh3/ryzen_x3d_apus/),AMD,2025-11-02 21:51:15,1
AMD,nmg056u,Their  cpus are particularly good especially now. I wouldn't trust Intel  with ewaste cores and degrading cpus. And now  removing hyperthreading. Even the fx series isnt bad  although intel was better at the time,AMD,2025-10-31 22:03:22,8
AMD,nmih5gf,They’ll have to struggle along without you I guess,AMD,2025-11-01 10:30:02,7
AMD,nmea72a,Any Specific Reason?,AMD,2025-10-31 16:36:35,5
AMD,nmkcfyi,easily beat by a 12700k for cheaper     a 7600x cant even beat a 12900k much less a 14900ks,AMD,2025-11-01 17:31:03,-15
AMD,nmcso7m,"Yes, some people live paycheck to paycheck, actually most people. Sometimes, that 50-100 they have in savings for that month gets drained by needing an extra gas trip to visit a hospital, or food got more expensive out of nowhere, or you needed a new couple outfits because your current ones are falling apart. Life isnt cut and dry and shit happens, while we like to tell people to save those savings can vanish in an instant.",AMD,2025-10-31 11:52:09,4
AMD,nmcqb0z,You can't afford empathy?,AMD,2025-10-31 11:35:45,5
AMD,nmjhmlo,"hes talking about the fact that they have just stopped video game optimization support for 5000 and 6000 cards, the latest of which was released 2 years ago.  this changes the expected timeline for an amd video card to receive game optimization support down to 25 percent of what it was. these cards are still being sold new. that means you can buy an amd card today and have no support on day one.   it changes the idea that amd video cards age like fine wine to the idea that amd cards age like milk. its an exceedingly low period of support.  and loyalists were buying these cards largely for that reason.   while im still going to buy amd processors, this was the reason that last night i did not buy a 9700xt in my price range.",AMD,2025-11-01 14:50:36,-3
AMD,nms5mpt,Not in games.,AMD,2025-11-02 22:20:27,5
AMD,nmjmnhz,They have since clarified that this is not the case. The drama of you people I swear,AMD,2025-11-01 15:17:58,4
AMD,nmwp0jm,A 7600x is faster than a 14900ks in games? Cmon man Amd wins 9/10 times but be real,AMD,2025-11-03 17:01:29,1
AMD,nmysluu,It was never the case but people don't know that maintenance != legacy.,AMD,2025-11-03 23:14:23,1
AMD,nmwp904,We're talking about the 7500X3D vs 12700K here.,AMD,2025-11-03 17:02:37,4
AMD,nmysg2f,"The core limit might be a problem, and if you run games and background stuff, the Intel CPU would probably be faster, but the X3D could achieve most of that for a fraction of the price.",AMD,2025-11-03 23:13:31,1
AMD,nm6xr94,I still need to check if this kills psvr2 on my 7900xt or not - kind of an ass move to kill one of the unique features of that card.,AMD,2025-10-30 13:54:34,348
AMD,nm7opsi,\>AMD disables USB-C power on Radeon RX 7900  Isn't this a breach of contract or 'not as advertised' in some markets?,AMD,2025-10-30 16:05:13,161
AMD,nm6zsdx,They're definitely not backporting fsr 4 to rdna 2 at this point,AMD,2025-10-30 14:04:40,157
AMD,nm7x1sl,The USB-C port is the reason I bought a reference 7900 XT! I have a mini-ITX case with a handle and the USB-C port lets me do a quick one cable + power connection on any of my home desks or televisions. I now need to check if any of my unpowered hubs still work.,AMD,2025-10-30 16:44:58,28
AMD,nm7ini7,Killing support for rdna2 this quick is criminal while nvidia is still supporting rtx 2000 series which is rdna1 counterpart... I am glad I got my 6800xt for barely 300$ from a gpu mining farm instead of buying at full price. Can't even imagine how I would feel,AMD,2025-10-30 15:36:43,121
AMD,nm81372,My 6950 doesn't feel old enough to be pushed out already.  Makes me seriously consider getting nvidia for my replacement with this type of crap.,AMD,2025-10-30 17:04:20,73
AMD,nm70wgl,"the USB part is so baffling when you realize that the PSVR2 has an advantage on the reference cards due to requiring only one cable.   and WORSE still. its a catch 20/20 situation, this driver fixes a grave VR issue present for almost a year. but at the same time kills VirtualLink support.",AMD,2025-10-30 14:10:13,90
AMD,nm881fk,"branching off RDNA2, a four year old gpu arch, while nvidia just got around to dropping pascal/maxwell, is...  not cool guys.",AMD,2025-10-30 17:37:56,78
AMD,nm74dst,"This is some bs, they need to reverse this decision, AMD is getting a little too greedy, pulling stunts even nVidia haven't done yet,. There's no reason not to add FSR 4 support for older cards, and no one, unless it's tied to something breaking, should be allowed to disable features after release.",AMD,2025-10-30 14:27:34,58
AMD,nm8jsj3,Man was gonna eventually upgrade to a 6000/7000 series card in the future from a 5700 xt. I fell like there’s no point now.,AMD,2025-10-30 18:33:27,18
AMD,nm9qsm7,GTX1080 got like 9 years of support. 6800xt got 5years? Sorry amd this is my last card. Now with leaked unofficial support FSR4 that amd just decided not to implement. Im out boys,AMD,2025-10-30 22:05:41,17
AMD,nm6wlj8,"Jesus christ.   GPUs like the 6800 XT are only 5 years old & being considered 'Legacy' & so thus getting degraded drivers. Meanwhile Turing GPUs from 2018 are still getting frequent full driver support (and so is the 11 year old GTX 980 for the time being). People wonder why people go for Nvidia for GPUs, but things like the official driver support are a very good reason why people go Nvidia, as they give frequent GPU drivers for older GPU series whilst AMD starts to kill them off after 5-6 years.",AMD,2025-10-30 13:48:45,101
AMD,nm6prbr,It's sad that amd often kills support or degrades the support faster than nvidia  I plan to upgrade my gpu soon ( my cpu upgrade is a 7800x3d already here ) but I will absolutely look more on the nvidia side.  6000 series isn't old yet and them already putting it on some weird 2nd branch isn't looking great.  I'd rather pay 50 or 100 more and don't have a badly supported gpu in a few years.,AMD,2025-10-30 13:12:34,124
AMD,nm8i2z4,I hope that the sub-branch only means no AI stuff and they will keep supporting the GPU for at least another 3 years I just bought a Rx 6750xt 2 years ago damn...,AMD,2025-10-30 18:25:27,13
AMD,nm8nam2,"im not so sure about getting a 9000 series now, like are you kidding? 5 years of support only? 20 series of rtx is 7 years old and theyre still getting support and new tech, absolutely pathetic",AMD,2025-10-30 18:49:49,22
AMD,nm7mtia,"Damn AMD doing everything they can to fuck over their own products as usual. I literally have a 7900xtx reference, definitely not buying AMD again now. Even NVIDIA still supports it on 20 series cards",AMD,2025-10-30 15:56:13,33
AMD,nm8xzdp,"okay, i just tested and my HMD works, so does my [mini USB-C/Display doohickey](https://www.shophagibis.com/products/usb-c-docking-station-mini-pc-monitor), so VirtualLink is still working, which confuses me even more, so either they didn't disabled power delivery or they meant a complete different thing.  u/TsukikoChan for now seems that they haven't left us in the dark.  https://i.redd.it/hv1qy5xuyayf1.gif",AMD,2025-10-30 19:41:03,11
AMD,nm8i305,"Yeah...I have an RX 6950XT, thankfully did not pay anywhere near full price for it. If they're actually gutting driver support for a five year old architecture, one that is powering the current generation of consoles no less? My next card will not be AMD.   I would've begrudgingly accepted it had they done this in 2027-28. Wouldn't have been happy, but could've lived with it. This is just an atrociously short time frame, especially given how in recent UE5 games the RX 6950XT hangs around the RTX 5070 tier without heavy ray tracing.",AMD,2025-10-30 18:25:27,19
AMD,nm8q8yv,"This is some mayor BS. My 6900xt is 3 years old and no longer supported. My next card will be Nvidia for sure. Thanks Amd, you just lost a customer.",AMD,2025-10-30 19:03:43,19
AMD,nm8lukg,"Very bad behaviour, now i'll think twice before buying a card that eventually will be dropped in a couple of years. Meanwile nvidia relased dlss4 even on 2018 hardware, at least this happened before i bought the 9070xt, i'd rather pay 100€ more for the competitor insted of buying a card that will eventually be abbandoned in less than 3 years.",AMD,2025-10-30 18:43:05,16
AMD,nm98neq,"It's pretty funny that nobody double-checked if the functionality was still there before complaining.  > We’d like to inform you that the release notes for AMD Software Adrenalin Edition 25.10 2 posted included misinformation that has since been corrected. There is no change to USB-C functionality on the RX 7900 series GPUs in the 25.10.2 driver. There was an incorrect line in the originally posted release notes that has been removed, and the release notes have been updated.  tl:dr: the USB-C functionality was not altered",AMD,2025-10-30 20:32:50,13
AMD,nm7j50m,y tho,AMD,2025-10-30 15:39:00,6
AMD,nm998ws,"I foresee the same thing happening in about two years, when they launch UDNA. Due to the complete change in architecture, it's possible they will stop supporting RDNA.",AMD,2025-10-30 20:35:45,6
AMD,nm9t7wz,I’m an old AMD fanboy but man it gets hard to stay one when they do this repeatedly of removing support and minimizing function and features of old cards over time. This seems to be a common theme of AMD.,AMD,2025-10-30 22:19:53,7
AMD,nm9ik6a,"Thank you for making my move from my 6800XT (that could have gone down as one of the greatest cards if you supported it with FSR4) towards Nvidia upgrade so easy.  I hope your expectations for people to upgrade every other generation with planned obsolescence to completely bite you back in the ass.   You will NEVER see my money again, that is certain.  P.S. Fire the clown that came up with this strategy. Good luck.",AMD,2025-10-30 21:22:00,10
AMD,nm9o5he,My RX 6900 XT is more powerful than 80% of RDNA 3 cards…why no support!?,AMD,2025-10-30 21:50:55,6
AMD,nm9ptne,AMD just lost sooo many customers over dropping 6000 series support this soon... Huge mistake.,AMD,2025-10-30 22:00:08,6
AMD,nm9r8il,That's really sad that RDNA2 has moved to maintenance mode 5 years after being released.  My RX6800 is still a 1440p beast,AMD,2025-10-30 22:08:15,4
AMD,nma30y7,What a disgrace! Dropping driver support on not even 5 year old GPUs! My next GPU won’t be AMD anymore.,AMD,2025-10-30 23:15:27,5
AMD,nm78pa9,Crap I was going to buy a 7900xt reference for my psvr2!!!,AMD,2025-10-30 14:48:56,9
AMD,nm8xinc,"I will upgrade my 6600 XT soon but this support drop makes me want to go for 5070 TI now, 9070 XT was for sure the choice, not anymore.",AMD,2025-10-30 19:38:48,9
AMD,nm9og3t,I just bought the RX 6800XT and it's already legacy? DAMN I'm going to throw it out the window,AMD,2025-10-30 21:52:31,3
AMD,nm9ummu,"RDNA2 being killed off so fast just sucks. I wanted to buy another RX 6800 or a 7900xt for my LLM + gaming rig, but i dont think i'll be doing that. Intel B50 is probably the move then.",AMD,2025-10-30 22:27:55,4
AMD,nma8y2g,RDNA2 user here to pile up this is absolute bullshit.,AMD,2025-10-30 23:48:46,3
AMD,nmaiaum,With my RX 6950 XT having only 3 years of support I guess my next GPU will be one from Nvidia,AMD,2025-10-31 00:43:56,4
AMD,nm8sn1g,"So... my very first AMD discrete card... is not supported after I have just 2 years after I bought it... just ""legacy drivers"" Next time I will buy either an intel card, or one of those chinese brands like Huawei, or even nVidia.  This is scummy and abusive.  Bad on you AMD, this are the last build and laptop I will ever buy from you.",AMD,2025-10-30 19:15:10,9
AMD,nm93mas,I have a portable monitor that is powered by the usb-c port. Does this mean it will no longer work?,AMD,2025-10-30 20:08:33,3
AMD,nm9nspv,"Wow, this feels like a slap in the face.  Knew it would happen eventually but not quite yet.",AMD,2025-10-30 21:49:01,3
AMD,nmbw6ss,">We’d like to inform you that the release notes for AMD Software Adrenalin Edition 25.10 2 posted included misinformation that has since been corrected. There is no change to USB-C functionality on the RX 7900 series GPUs in the 25.10.2 driver. There was an incorrect line in the originally posted release notes that has been removed, and the release notes have been updated. We apologize for any inconvenience.",AMD,2025-10-31 06:52:27,3
AMD,nm89rpf,"To be honest if I’ll need to go to a new card, I’ll most likely go with team green simply because I need to learn a hard lesson at this point about AMD.",AMD,2025-10-30 17:46:20,6
AMD,nm8v7me,"Yep, never again, buying an AMD GPU was my biggest mistake when buying my latest PC",AMD,2025-10-30 19:27:36,8
AMD,nm8mrpr,Wait so the drivers not working yesterday on RX 6000 series was not a bug? Wtf... I got my card like 2 years ago and it's already going to become obsolete??? xDDD Even my old rtx 2060 is getting updates still,AMD,2025-10-30 18:47:24,6
AMD,nm8uebp,"Whaaat, my rx 7900 doesn't have a usb, omg that feature would have been so nice to have... Too bad they disable it anyway",AMD,2025-10-30 19:23:40,2
AMD,nm97id3,"Joke's on them, mine doesn't have USB-C power.",AMD,2025-10-30 20:27:20,2
AMD,nm99y7k,"The new driver 25.10.2 For RX 6800 XT (Weight 912mb) dont work, say error, not compatible !",AMD,2025-10-30 20:39:09,2
AMD,nmb7pnm,"This is so sad, and frankly baffling given that the consoles all run RDNA2 so games are basically being built with it as the standard!  On top of that it was the last generation where Radeon was actually competitive (better by some metrics) than GeForce and they've decided that needs memoryholing?",AMD,2025-10-31 03:18:44,2
AMD,nm9dl1n,"AMD dropping support for their GPUs much faster than Nvidia is just one of the reasons that explain their tiny market share. So much for ""FineWine"".",AMD,2025-10-30 20:56:49,3
AMD,nm9o0ga,Why would you even need that,AMD,2025-10-30 21:50:10,1
AMD,nman9k7,I hate it when all my shit becomes obsolete. I have to do a complete new build now to modernize,AMD,2025-10-31 01:12:43,1
AMD,nmapopn,"The article looks biased written.   It basically says AMD bad, Nvidia good.   However, disabling a feature is not cool",AMD,2025-10-31 01:27:12,1
AMD,nmax782,Lol I wish every card had a usb c port and they go and disable it? 🤦‍♂️,AMD,2025-10-31 02:12:17,1
AMD,nmbhe9n,i need more intel arc gpus,AMD,2025-10-31 04:31:58,1
AMD,nmc20q6,I play a game (path of exile) that has been out for 12 years and still going strong with a 6950xt   I be fine.  I await next generation for a possible upgrade.   Been really happy with the card as far,AMD,2025-10-31 07:53:22,1
AMD,nmc3ma5,"What the hell, I am connecting my Arzopa 17"" monitor on this port.  NVidia and AMD are really competing for being the most anti-consumer corporation, that's a fucking disgrace.",AMD,2025-10-31 08:09:59,1
AMD,nmc7yan,**IT APPEARS THAT** u/nevadita  **HAS VERIFIED THAT PD & VirtualLink STILL WORK**  [See further below.](https://www.reddit.com/r/Amd/comments/1ojxjuq/comment/nm8xzdp/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2025-10-31 08:54:52,1
AMD,nmcmkrj,"Well, I guess it's team green for my next gpu which sucks cuz I like the price/performance, but I don't it's worth sacrificing support for extra 50 bucks or so. way to go amd.",AMD,2025-10-31 11:07:22,1
AMD,nmd6tye,"Hell nah, I bought an RX6800 on July 2024...  I guess it is time to fully switch to Linux.",AMD,2025-10-31 13:18:05,1
AMD,nmdbz4w,Buddy of mine was looking to get a 9060 XT based on my recommendation this week. I think I'm gonna upsell him to a 5060 TI instead.,AMD,2025-10-31 13:46:14,1
AMD,nmfth6q,I noticed my obs screen recording been lagging like crazy since this latest driver update on my 7900xt.. I reverted back to the 25.9.1 driver and its working fine again.  It was so noticeable and now fine.,AMD,2025-10-31 21:23:45,1
AMD,nmgcnj2,"As someone with a 6750, what will this actually mean? Like, what affect will I see on my games? Will new games just not run? Or will I see like 50% less frames? Or?",AMD,2025-10-31 23:24:27,1
AMD,nmlb21r,🆘 🥺,AMD,2025-11-01 20:32:26,1
AMD,nmlbe25,Interesting move. I bought 7900 becaus of USB-C so I could use my XR glasses with it.,AMD,2025-11-01 20:34:11,1
AMD,nmnqyk2,Usb-c power still works.   https://www.thefpsreview.com/2025/11/01/amd-puts-owners-of-older-rdna-1-and-rdna-2-gpus-on-a-rollercoaster-ride-with-statements-regarding-its-latest-driver-updates/,AMD,2025-11-02 05:23:58,1
AMD,nn017e8,Start a class action lawsuit.,AMD,2025-11-04 03:33:41,1
AMD,nm6y59k,"I’d love to hear an update on this, my thought went exactly to this use when I saw they killed the power delivery for the usb c port",AMD,2025-10-30 13:56:30,119
AMD,nm7gp7k,"Hopefully not, apparently the headset's power label designates 7.2 watts or less, and that's less than 1.5 amps for even just USB-A.  Plugable makes a USB-C hub (3 device ports + 1 host port) called the TBT4-HUB3C which would probably resolve any potential power discrepancy (provides ~20w for attached devices), albeit at a fluctuating cost of $100-200 depending on the year.",AMD,2025-10-30 15:27:27,14
AMD,nm9d56e,I hope not. I've got an AMD card specifically for VR.,AMD,2025-10-30 20:54:39,4
AMD,nm9scw4,Please keep us updated,AMD,2025-10-30 22:14:47,2
AMD,nm8vam9,"I wish monitors and desktop GPUs would embrace USB-C for video already.  It's not like it's a hypothetical new standard, it's been on every laptop for I think a decade now.",AMD,2025-10-30 19:27:59,2
AMD,nm8yihf,"Might be, but AMD in their patch notes clearly clarified which version of the drivers customers can stay on if they want to keep the  feature so thats probably enough legally, as I don't know if they are required to provide driver updates that keep all the features the same.",AMD,2025-10-30 19:43:39,59
AMD,nm907t8,"Could be purchase / sale under false Facts and a few other things atleast in germany , if you own one contact your customer protection agency to ask.",AMD,2025-10-30 19:51:58,6
AMD,nmhza50,I'd be interested to hear the EU express an opinion on that.,AMD,2025-11-01 07:14:20,2
AMD,nm9plq1,They're cutting driver support for the 6000 series cards too. So much for the good guy underdog. I guess F people who bought a 6800xt a couple years ago. Nvidia is still supporting cards almost 10 years out.   Nvidia it is from now on.,AMD,2025-10-30 21:58:55,47
AMD,nm73os1,Definitely not..,AMD,2025-10-30 14:24:08,36
AMD,nm7u3cv,"they already did and accidentally released it, I tried it on steam deck and it works, when static - even 347p upscaled to 800p looks almost like native(although somewhat softer) but the problem is it's too heavy for steam deck.  on desktop RDNA 2 GPUs it must be running fine I'd assume, all you need is fakenvapi+optiscaler+leaked fsr 4 int8 dlls, throw them into the dir with your dlls-supporting game and set optiscaler to FSR 4 and the game to dlss  if setting via ini file(not the insert button menu) then set to fsr31 and then set the fsr4override to true in fsr3 section",AMD,2025-10-30 16:30:48,13
AMD,nm8hr6k,Very stupid move imo,AMD,2025-10-30 18:23:54,5
AMD,nm77npf,We'll see. i still think they will.,AMD,2025-10-30 14:43:49,4
AMD,nmarhr3,Except they already did... The code is there already,AMD,2025-10-31 01:37:59,0
AMD,nmb6ncg,I was using a Dual DVI-D adapter to USB C when I first bought the card to connect an old 27” Korean LCD panel.  I bought the reference specifically for this reason. So while I don’t currently use USB C I was glad I had it for future needs.  This is very frustrating.,AMD,2025-10-31 03:11:26,1
AMD,nmc1mb7,"It sounds like it will still work if you don’t update the driver, but still though",AMD,2025-10-31 07:49:18,1
AMD,nm90thv,"Meanwhile their primary Linux drivers see work to fully support GCN 1.0/1.1, so an architecture from 2012.  How is such a contrast possible? I guess since it’s an open driver and it’s not AMD working on that xD",AMD,2025-10-30 19:54:54,32
AMD,nm99zer,6950xt came out in May 2022.. man what the fuck.,AMD,2025-10-30 20:39:19,33
AMD,nm9nhso,I just bought the RX 6800XT and it's already legacy? lol... :(,AMD,2025-10-30 21:47:26,15
AMD,nm9ppve,I'll buy green from now on. Have a 6800xt so I guess F me even though it's still pretty fast.,AMD,2025-10-30 21:59:34,14
AMD,nm8cg2c,This has been my biggest reservation about buying AMD based graphics cards. Historically they have been pretty quick about reducing or ending driver support. nVidia tends to support their cards for a ridiculously long time. My 8800GT and 980ti felt like they got drivers forever.,AMD,2025-10-30 17:58:55,22
AMD,nm8ubih,"This is one reason I stopped using AMD GPU's, after they dropped support for my HD 6950 only 5 years after release. I see they're doing it again with RNDA2. And now I'd say it's worse since GPU advancements are a lot slower, and a 5 years old GPU is more than capable to play new releases.",AMD,2025-10-30 19:23:17,23
AMD,nm8ipr9,"ye while iam happy with the amd driver , i will pay for my next gpu rather 100 more than to go amd again for the gpu.  this is WAY too fast.     i was eying with a 9070XT but i guess not anymore.",AMD,2025-10-30 18:28:23,13
AMD,nma623g,My 6900xt cost about $2300 NZD about 4 years ago. lol.,AMD,2025-10-30 23:32:24,1
AMD,nmhe29t,"Nvidia is still updating 750 ti from 2014. And AMD has the 6750 GRE from Oct 2023, and handhelds that are being sold and released literally rn. Like the xbox ally (non x) for 600$ and base model legion go 2 for 1100$.",AMD,2025-11-01 03:44:47,1
AMD,nm8uyvt,"Literally, most of RX 6000 cards are still go to in low income countries and are still capable.  Seriously my next GPU is more then likely NVIDIA or Intel, if I am gonna get ass fucked at least let it be lubed.",AMD,2025-10-30 19:26:25,33
AMD,nma89ul,"Got identical combo as you, 5800X3D and a 6950XT. Next GPU definitely won't be Radeon with this BS going on.",AMD,2025-10-30 23:44:56,3
AMD,nm9vrle,Being pushed to a sub branch does not mean loosing support. There may be genuine considerations based on how RDNA 3+ are designed which would benefit from the split in drivers.,AMD,2025-10-30 22:34:21,2
AMD,nm7mk31,Catch 22…,AMD,2025-10-30 15:55:00,70
AMD,nm9aeqx,What is the VR issue?,AMD,2025-10-30 20:41:21,1
AMD,nm8w2g0,"The dumbest thing is that AMD gives away that drivers for older cards go into maintenance mode, while nvidia has everything unified so people think game ready drivers improve performance for old cards.",AMD,2025-10-30 19:31:46,21
AMD,nm8keik,yep was my first amd gpu in like 15 years ( had before that also some amd and even ATI cards ) will be my last AMD gpu ill shell again rather 50-100 more out for nvidia and dont get shafted after 5 years.,AMD,2025-10-30 18:36:20,11
AMD,nm76v1j,FSR 4 is dependant on hardware in the newer cards. So yes there is a reason.,AMD,2025-10-30 14:39:56,-28
AMD,nm8w1f5,"Just wait for backlash and see how AMD responds, If they arent gonna fix this go for Nvidia.",AMD,2025-10-30 19:31:37,15
AMD,nm7jrvu,It’s a little ridiculous considering the 6800 XT is faster than most 4000 series cards like the 4070 but let’s put it  out to pasture. Still plenty of power.,AMD,2025-10-30 15:41:58,25
AMD,nm7tx5p,"Part of it is that, while weaker, Turing has most of the hardware features of the 30, 40, and 50 series cards. RDNA4 is the first AMD generation to be more or less on feature parity with Turing. AMD dragging their feet for so long on ML and RT acceleration is going to make RDNA 1-3 age rather poorly in terms of feature backporting.",AMD,2025-10-30 16:29:58,10
AMD,nm76m79,"I have a 980ti.    What driver support is that card actually for real getting, other than being ""included"" in the driver package? No bug fixes, no additional performance improvements or game support.    Frankly I don't understand why AMD doesn't just let their driver packages get big. It avoids silly self owns like this.    A 1gb download nowadays isn't really that big of a deal for plenty of people.",AMD,2025-10-30 14:38:43,23
AMD,nm72ugu,Degraded drivers??,AMD,2025-10-30 14:19:55,6
AMD,nm6yils,It really is just like with Apple and Android. Apple provides system updates for phones that are almost a decade old at this point,AMD,2025-10-30 13:58:20,1
AMD,nm7w74q,Degraded drivers? My 5700XT is still getting all the drivers. I've no complaints at all.,AMD,2025-10-30 16:40:54,0
AMD,nm88v0y,"Who said that the GTX 980 isn't considered legacy, it might be that Nvidia didn't have a public branch naming.  RDNA 1 and 2 are still supported, where did they say they won't support it?  It's just a different branch that is all.",AMD,2025-10-30 17:41:57,-2
AMD,nm6qvst,Did they even say why they did this? I don’t know why you’d disable it if it’s working.,AMD,2025-10-30 13:18:38,40
AMD,nm73xl5,Thats part of being the underdog multi billion dollar company can't afford to support there products for years like the competition.,AMD,2025-10-30 14:25:20,7
AMD,nm77iju,What makes you think so? It's actually opposite.,AMD,2025-10-30 14:43:08,2
AMD,nm84p4d,Dude... Almost 6 years since it was released. It's pretty old already lol.,AMD,2025-10-30 17:21:44,-13
AMD,nm9qfrv,Can you please tell me what does this means? I also bought a 6750xt less than 2 years ago. We won’t get new drivers? No support for new games?  This is unbelievable,AMD,2025-10-30 22:03:38,3
AMD,nm8vq3b,"I bought 9060 this year and it doesnt scare me, but for someone buying in two years or three years... damn... Well I bought becouse features measured to nvidia and price was more appealing to me by far, But I guess I better pay more fo longevity... the price now makes sense lol, nvidia still king",AMD,2025-10-30 19:30:04,6
AMD,nm8ze1z,"I've said that yesterday already. Power Delivery seems to work, they must have meant something else. But people just assume things and no one tests it. Altough most of the people complaining also claim to have an affected card. So i don#t get why they don't test it and instead ask and spread misinformation.",AMD,2025-10-30 19:47:56,8
AMD,nm9r753,"Thank you for checking ❤️ that's good to hear, we finally have a good drivet then with the refresh fix and psvr2 works. It's probably one of the PD profiles they got rid of.",AMD,2025-10-30 22:08:02,3
AMD,nmc7nqg,"So GN's ""AMD just needs to shut up"" statement rings well then... why are they continuing to fumble the bag here?",AMD,2025-10-31 08:51:57,2
AMD,nm8mcvh,"abandoned after less than 3 years from its exit of the discrete market and still on the market on APUs relased in 2025, so bad.",AMD,2025-10-30 18:45:28,10
AMD,nm9dnkc,"no shit I bought my 6950xt literally like 3 years ago to the date, i bought it early november 2022, and now its no longer supported, i know that its not going to be useless but what is that bullshit, I remember before buying this card I had a GTX1060 6gb and that was still receiving updates by the time i upgraded and i bought that in 2016, i think i might need to go back to nvidia after all or just drop gaming as a whole lol",AMD,2025-10-30 20:57:09,7
AMD,nm9qaz4,I got my 6800xt in 2023 definitely gonna consider a different option when the time comes,AMD,2025-10-30 22:02:52,2
AMD,nm8u3x1,I mean I understand not having fsr 4 on RDNA 2 or 1 but man the driver should be supported,AMD,2025-10-30 19:22:15,5
AMD,nmaxrvn,"Hm. Well, good to know it's all intact",AMD,2025-10-31 02:15:44,1
AMD,nmc3w5j,https://www.reddit.com/r/Amd/s/Wb07F0DOUV,AMD,2025-10-31 08:12:53,2
AMD,nm91gmx,same,AMD,2025-10-30 19:58:02,6
AMD,nmfuguh,"Same, me and husband are going for Intel or Nvidia.",AMD,2025-10-31 21:29:29,1
AMD,nm96qn9,"It works, just tested with a external USB-c screen I used with my steam deck and it works fine.  They probably meant another feature being axed. Or haven’t disable it for this version contrary to what the patch notes suggest, only them can clarify",AMD,2025-10-30 20:23:36,3
AMD,nmc4tfx,Check your current driver version compared to the ones mentioned in the article,AMD,2025-10-31 08:22:42,0
AMD,nmc3u4o,https://www.reddit.com/r/Amd/s/Wb07F0DOUV,AMD,2025-10-31 08:12:17,1
AMD,nmatj8j,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-10-31 01:50:09,1
AMD,nm7jwye,The thing I don’t understand is why would they even do this? It would take effort to actively remove a feature like that so there must be some motive behind it,AMD,2025-10-30 15:42:39,65
AMD,nm6yym4,"I won't be able to check it until next week - I haven't played vr in a while but was meaning to get back into it. Currently on 25.9.1, so i'll check if it works there, then i'll try 25.10.2 and see if it still works or not (jumping back if not). AMD really screwed over people with this release.",AMD,2025-10-30 14:00:31,23
AMD,nm7ikd4,Can't you just... throw in a usb-c power injector? Something like this. [https://www.coolgear.com/product/compact-95w-usb-usb-3-2-gen-2-type-c-pd-injector-dfp-w-10gbps-speeds-for-seamless-legacy-host-to-pd-device-connectivity](https://www.coolgear.com/product/compact-95w-usb-usb-3-2-gen-2-type-c-pd-injector-dfp-w-10gbps-speeds-for-seamless-legacy-host-to-pd-device-connectivity),AMD,2025-10-30 15:36:18,5
AMD,nm7vr1t,"That's a pretty insane price for a basic usb hub. You can usually get a branded one all singing and dancing for like £20, or a Chinese one for £5.",AMD,2025-10-30 16:38:45,12
AMD,nmbj9p5,Lmao who buys a Radeon for VR yikes,AMD,2025-10-31 04:47:50,-7
AMD,nma8bll,"https://www.reddit.com/r/Amd/comments/1ojxjuq/comment/nm8xzdp   https://www.reddit.com/r/Amd/comments/1ojxjuq/comment/nm8ze1z   Two peeps already tested other HMDs, I'll test my psvr2 when I'm able at the start of the week",AMD,2025-10-30 23:45:12,3
AMD,nmaksw2,"Eh, I don’t think someone could sell you a car with heated seats that they disabled through a software update and be legally okay by saying “just don’t update”. I’m certainly not a lawyer but that just seems litigation worthy.",AMD,2025-10-31 00:58:29,22
AMD,nmb9atw,Praying the Rdn.id modded driver dev will make some bypass for you guys 🙏,AMD,2025-10-31 03:30:01,4
AMD,nmil5x0,Mute point now as AMD have said it's not disabled.,AMD,2025-11-01 11:08:38,2
AMD,nmal7nb,"It’s actually crazy, AMD literally had the “fine wine” reputation but now their “wine” ages to 4 years and gets promptly “shattered”? What a shame, they never fail to shit the bed when Nvidia gives them the absolute lowest bar to do better than.",AMD,2025-10-31 01:00:48,23
AMD,nmamjje,"I think the funniest situation are those budget builder in China who got their 6750 GRE about 2 years ago.  I mean, yeah it is discounted GPU sure, but 2 years of driver support is still quite funny.",AMD,2025-10-31 01:08:29,8
AMD,nmawb38,I got my 6800 non-XT a couple years ago 😬,AMD,2025-10-31 02:06:57,2
AMD,nmbur8z,As long as Nvidia doesn’t have open source support they can lick my boot.,AMD,2025-10-31 06:37:49,1
AMD,nmhdk34,There's the 750 ti from 2014 still getting updates,AMD,2025-11-01 03:40:46,1
AMD,nm7xcju,"He means officially support, as in you have a toggle in the driver and it does all the replacing for you like NVIDIA does with its DLSS swapping.",AMD,2025-10-30 16:46:23,17
AMD,nm8vqny,"It's better than FSR 3.1 on my 6700 xt, albeit at a performance cost when comparing the same input resolution. FSR 4 performance is like FSR 3 quality in terms of fps for me, maybe a bit better FPS wise. Although image quality wise it's still a lot better than FSR 3 so I still prefer is.   The thing is that I dont want to swap DLL just to get the best experience for my GPU lol. I also dont want to risk trying it with games that have anticheat.",AMD,2025-10-30 19:30:09,3
AMD,nm90l9u,What makes you think they will. I hope you're right,AMD,2025-10-30 19:53:47,7
AMD,nm9dcat,Linux not effected by the driver downgrade then?,AMD,2025-10-30 20:55:36,8
AMD,nm9pabm,And so did the 6750 XT and 6650 XT (6x50 refresh). 3.5 years of total support is ***embarrassing*** as fuck.  [https://www.techpowerup.com/294707/amd-launches-radeon-rx-6950-xt-rx-6750-xt-and-rx-6650-xt-new-game-bundle](https://www.techpowerup.com/294707/amd-launches-radeon-rx-6950-xt-rx-6750-xt-and-rx-6650-xt-new-game-bundle)  And it's only 3.5 years ***if*** you bought it launch.,AMD,2025-10-30 21:57:09,19
AMD,nmb6mk3,"Apparently, it seems so. I might finally have to dual boot linux to actually get longer driver support. lol",AMD,2025-10-31 03:11:17,1
AMD,nm9rd79,"> I'll buy green from now on  It seems so.  A couple years back, I swapped my RTX 3070 for a 6800 XT and up until this point, I had zero regrets.  Been fully satisfied with the card.  Was eyeballing possibly upgrading to 9070 XT over the the holidays.  But following this news, I'm going to have to just begrudgingly pay the additional premium to swap back to NVIDIA, so at least I know I can get as many years as I want out of the card without them pulling the rug out from under me by EOL'ing driver updates for perfectly functioning cards.    AMD, why do you keep doing this to yourselves??",AMD,2025-10-30 22:09:00,10
AMD,nm9t6tr,funny thing is there's people saying they didn't expect this from amd lmao,AMD,2025-10-30 22:19:42,5
AMD,nm8t0e1,"Same man (about eyeing the 9070 xt), this 6700XT I bought a few years ago was my first AMD gpu but if support is dropped this quickly I may not go AMD for my next purchase. I finally thought there was hope for us with int8 fsr4 and now this happens, making me think it's not going to be an official release ever, which really sucks because some games will just ban you for swapping DLLs",AMD,2025-10-30 19:16:57,3
AMD,nmhjvg8,It would be so stupid for AMD if they update those APUs but not the actual desktop GPUs.,AMD,2025-11-01 04:33:54,1
AMD,nm97wfm,The RX 6750 GRE was released in 2023! The card barely lasted 2 years.,AMD,2025-10-30 20:29:13,15
AMD,nm8ntum,No they just see the catch really clearly lol,AMD,2025-10-30 18:52:18,16
AMD,nm8w6vi,"You can see nvidia version values too, lol.",AMD,2025-10-30 19:32:22,12
AMD,nm9p1jh,>so people think game ready drivers improve performance for old cards.  ***All*** games and ***all*** GPUs benefit from bugfixes. Nothing has changed in game development that suddenly makes older GPUs immune to new-game problems.  They just simply lose support over time.,AMD,2025-10-30 21:55:48,13
AMD,nm9tuyu,"It's not just performance, sometimes it's visual issues that get fixed with future driver updates. Dropping support could also mean dropping support for new future technology that could run on the cards otherwise.  Like Transformer DLSS upscaling on RTX 20 series. Which RTX 20 did get, despite being ~~6 years old.",AMD,2025-10-30 22:23:35,6
AMD,nm7pwj0,WTF have you been for 6+ months?,AMD,2025-10-30 16:10:51,17
AMD,nm77em6,"No, as FSR4(int8) leaked, and proved it works well on older architectures, not as well as ""proper"" FSR4, but way better than RDNA 3.1. So there's literally no reason, the cat is out of the bag already.",AMD,2025-10-30 14:42:35,35
AMD,nm9iqmu,"No, also shilling does not work, you should know how big companies operate.",AMD,2025-10-30 21:22:55,3
AMD,nmah0jo,Sad but true. Turing was way ahead of its time.,AMD,2025-10-31 00:36:21,3
AMD,nm8zde5,"Of course it's not getting any performance updates specific to your GPU, but the general fixes that are not architecture specific still apply to your GPU.",AMD,2025-10-30 19:47:51,10
AMD,nm84f19,> What driver support is that card actually for real getting  More support than my Vega based GPUs ever got for longer if nothing else.,AMD,2025-10-30 17:20:23,19
AMD,nmag5hj,"Radeon group is notorious for massive fumbles in PR, and ass backwards decisions like this. They somehow manage to clutch defeat from the jaws of victory every time. I have no clue why they would do this either, a 1.5GB driver package seems insignificant to me. I would say someone is getting fired over this, but at this point I don't even know it might even be intentional.",AMD,2025-10-31 00:31:15,4
AMD,nm7aimk,I think it means less strict standards for driver quality or reliability,AMD,2025-10-30 14:57:46,10
AMD,nm71i5i,"Apple guarantees 5 years of OS updates. Samsung and Google guarantee 7 years.   Apple usually gets to 7 as well, despite a lower guarantee. Just don't buy shitty Chinese androids.",AMD,2025-10-30 14:13:15,21
AMD,nm6zd71,"Yeah, the iPhone 6S received iOS 15.8.5 security update in September and that phone was released in 2015 while my OnePlus 8 Pro released in 2020 got its final update last year. It's actually insane.",AMD,2025-10-30 14:02:33,8
AMD,nm72m20,That's no longer the case. Most Android OEMs have caught up to the importance of long term software support.,AMD,2025-10-30 14:18:45,3
AMD,nm74e83,"I mean... Android now promise longer support than Apple (which started promising anything just because Android phones got the ""promise"").   iOS have tied system apps to system, which mean even small bugs in apps like phone dialer can be system breaking and they need full system update to be fixed. On Android there its much less likely for system app bug to cause such big system wide problem and if such bug exist simple app update is enough to get it fixed.",AMD,2025-10-30 14:27:37,-1
AMD,nm7soos,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-10-30 16:24:03,1
AMD,nm6tmvk,">AMD hasn’t explained why it made this change, but since no RX 9000 cards include a USB-C port, it may have been removed to streamline the already large driver package.  >For comparison, NVIDIA’s drivers are around 850 MB and still support GPUs back to the GTX 700 series (Maxwell). NVIDIA still continues to support its similar USB-C standard, VirtualLink, which is no longer added to cards after the RTX 20 series. The Virtual Link standard was effectively cancelled around 2020.   They didn't give a reason, but probably to streamline the drivers more.",AMD,2025-10-30 13:33:18,49
AMD,nm74dhf,That's just bullcrap honestly.,AMD,2025-10-30 14:27:32,41
AMD,nm7x3ch,"LOL, still pretending AMD is some underdog and can't support an architecture for two more years. This isn't 2016 anymore, they're swimming in money and RDNA development basically gets bankrolled by Valve, Sony and Microsoft. What are you even talking about?",AMD,2025-10-30 16:45:10,17
AMD,nm79hyt,"Yeah people always go ""AMD finewine!"" and then ignore if you actually have to hold onto a GPU for a really long time nvidia will give you *years* more support.  I still remember when my friend had to go out of his way to find the single beta crimson driver that would work on the HD6000 series that was required to make games *that came out in 2012* not crash.  And then when that died and he had to go back to his 5950 which didn't even have the crimson beta driver we just had to give up on all of those games until I bought a new GPU and could give him my gtx 670.  meanwhile nvidia kept support for GTX 400 cards for several years past that point.",AMD,2025-10-30 14:52:50,24
AMD,nm764ul,I fail to see why that's my problem,AMD,2025-10-30 14:36:18,7
AMD,nm8fz51,"6 years isnt old , specially if you consider the mid to top end gpus still running games to date at 1440p high to max settings at 90fps +     after your logic is and what you say is any gpu beyond 5 years is EOL and should be replaced if you bought AMD which is terrible.",AMD,2025-10-30 18:15:32,8
AMD,nm8auz8,"N22 and N23 launched in 2021. N32 didn't come out until near the end of 2022.  Even then, Navi 22 was discounted and sold far into 2023. Navi 23 (6600/6650/XT) was even sold far into 2024 in the sub 200 dollar market.",AMD,2025-10-30 17:51:30,5
AMD,nm8s130,"Steam Deck uses RDNA 2, the newly released ROG Xbox Ally uses RDNA 2.  And they are currently releasing rebrands of Zen2/3 APUs using RDNA 2. Dropping support for a product that is still seeing new releases really sucks for the customer.  https://www.reddit.com/r/Amd/comments/1oh9yia/amd_again_reshuffles_mobile_lineup_with_ryzen_10/",AMD,2025-10-30 19:12:16,6
AMD,nm9zc8y,Only security patch and maybe driver fix but no more optimization for newer games,AMD,2025-10-30 22:54:50,3
AMD,nm9rdcn,"I hadn't got around to checking myself, won't be near it until sunday/monday. I'm thankful you two checked and confirmed, thank you ❤️",AMD,2025-10-30 22:09:02,5
AMD,nm90bdb,Oh it gets worse for some people. The RX 6750 GRE launched in the October of 2023. A literal two year old dGPU relegated to security and bug fixes only.,AMD,2025-10-30 19:52:27,11
AMD,nmaeubf,"And it seems AMD removed option to use modded FSR4 in RDN2 in this latest driver. All this hype about AI cores, machine learning, Tops , Flops whatever... and 6900xt plays modded FRS4 perfectly (at 10% more fps cost tho but is is worth it). Now we are not going to get jack shit officially.",AMD,2025-10-31 00:23:24,6
AMD,nm9l1t0,"We should not ""understand"" the lack of FSR4 on RDNA2 cards though lol.  That was a clown move as well by AMD, and this new shit is just icing on the cake.  I was planning to buy 9070XT for my girlfriend, but hello to 5070Ti now, absolute donkeys.",AMD,2025-10-30 21:34:49,9
AMD,nm89dbs,"This is the same driver where they apparently fixed the framerate instability in VR. Maybe the problem only came up when was using USB PD and USB data on the same port?  So they just disabled the user from being able to do that.  That'd still be dumb though. Plus, a lot of ITX builds use that port to hook up portable monitors.",AMD,2025-10-30 17:44:24,40
AMD,nm7mb2l,"I’m not sure either, unless it was causing some sort of stability issue I truly can’t understand why you’d disable such a random feature.",AMD,2025-10-30 15:53:51,30
AMD,nm8cllm,Did you notice that AMD fixed the refresh rate bug in the patch notes. Haven't tested my Index yet but it was finally marked as fixed. I've been stuck on 24.12.1 until now.,AMD,2025-10-30 17:59:39,6
AMD,nm7m430,"The thing is this was an advertised feature they took away, and just doesn’t make sense. There are work arounds but that just shouldn’t be necessary.",AMD,2025-10-30 15:52:57,31
AMD,nm8e4i2,"It’s much more than a basic hub. It actually deals with the DisplayPort signals across the USB-C ports, whereas most hubs only use their single USB-C port for power passthrough.  In this particular case it means you could plug in up to a 20W USB-C display.  I actually use it as a ""docking station"", hooked up to an HDMI adapter and one of those ""branded"" USB-C adapter (3 USB-3 ports, a card reader, and I think a second HDMI port). Works just fine on my Macbook, iPad, Steam Deck, pretty much everything but my Switch because Nintendo's gonna Nintendo.",AMD,2025-10-30 18:06:50,9
AMD,nmbnhf4,Linux users,AMD,2025-10-31 05:25:39,3
AMD,nmbom2z,I've been using AMD cards across several VR headsets and never had a problem 🤷‍♀️,AMD,2025-10-31 05:36:19,2
AMD,nmao4f0,BMW attempted something similar if you weren’t aware. They were trying to lock heated seats among other features on a subscription based model. Last I heard they rolled it back due to customer backlash.,AMD,2025-10-31 01:17:49,3
AMD,nmbj6z4,"The ""fine wine"" reputation was more of a meme than anything else. Realistically, you do not want your chips to undergo a ""fine wine"" process in the first place. That only means that your launch drivers were absolutely crippling the shit out of your GPU's performance, which reflects on the driver team.",AMD,2025-10-31 04:47:11,7
AMD,nmhddwd,"No, it's handhelds that are being sold and released literally rn. Like the xbox ally (non x) for 600$ and base model legion go 2 for 1100$. I wonder if they'll get sued by microsoft?",AMD,2025-11-01 03:39:23,1
AMD,nm9lf9q,I'd be silly if they don't considering it already works on linux ootb,AMD,2025-10-30 21:36:44,0
AMD,nmark3t,Because it is already running and tested,AMD,2025-10-31 01:38:24,0
AMD,nmaf1op,"Nope. Linux uses their own AMD drivers, which are fully open and community maintained. Just works without you having to do *anything* and tends to perform better than the AMD Windows drivers... Or the old AMD-made linux drivers that AMD dropped support for because the Open ones were better in every way.  You do have a couple tradeoffs though... You don't get support for new things quite as immediately, and if you're on a slower moving distro, you might not get it for a while. Stuff like FSR 4 took a couple extra months to work on Linux, for instance. Also, because the HDMI forum is... Well, I don't have nice things to say about them, but they blocked HDMI 2.1 from working on Linux, so on linux AMD card HDMI ports are limited to HDMI 2.0 and don't get VRR support among other things, and AMD is legally not allowed to do anything to fix that.",AMD,2025-10-31 00:24:38,15
AMD,nmd43vw,"Yup, even Phoronix wrote a nice article about that a few hours ago: https://www.phoronix.com/news/AMD-Windows-RX-5000-6000-Game",AMD,2025-10-31 13:02:47,1
AMD,nmb9wqf,Rip my 6650XT,AMD,2025-10-31 03:34:29,4
AMD,nmhptpl,"They're rdna 2 so they won't. The lego2 top tier version and xbox ally are on rdna 3.5 but the others are on 2, as well as the rog ally. The xes are on rdna 3/3.5",AMD,2025-11-01 05:30:40,1
AMD,nm980sm,"Yeah, highly considering selling my 6750 XT for a 5070 Super when it comes out",AMD,2025-10-30 20:29:48,6
AMD,nm9zmvl,Hey sometimes you go through life and you don't realise the correct way to say a phrase until some smartarse points it out online. Just goes to show you can't take life for granite.,AMD,2025-10-30 22:56:28,6
AMD,nm7t5eb,Did something happen? Have I missed something?,AMD,2025-10-30 16:26:16,4
AMD,nm7khcw,Maybe it works if integrated in a game.   Maybe a new driver isn't needed then.,AMD,2025-10-30 15:45:19,2
AMD,nm86isz,"There's little reason to use it though. On linux, the emulated FP8 version is just as fast as the int8",AMD,2025-10-30 17:30:31,3
AMD,nm9jlw1,I am not shilling though? I literally use Nvidia GPU now.,AMD,2025-10-30 21:27:22,2
AMD,nm8w8i9,"Like they were good quality to begin with  I won't forget them breaking RDNA 2 cards two times this year in span of 3 months, loved getting BSOD when alt tabbing",AMD,2025-10-30 19:32:35,5
AMD,nm88dll,"nope, its a full branch",AMD,2025-10-30 17:39:35,3
AMD,nm7fhzq,And who quantifies that the poster is using an intel / Nvidia setup so he doesn't even have first hand experience running a RDNA 2 GPU.  AMD just split the driver branch there is zero official mention from them about RDNA 1 and 2 going into legacy status.,AMD,2025-10-30 15:21:46,5
AMD,nm7eztd,Samsung kills tablets in 8 months though,AMD,2025-10-30 15:19:22,5
AMD,nm751ic,The problem there is essentially Apple did it for years already so everyone knows their word is good. Samsung only recently changed their policies so word hasn’t spread. Google benefits from being early to the long support time party among android manufacturers.,AMD,2025-10-30 14:30:51,13
AMD,nm7gz5o,"yet all userland jailbreaks/exploits still work on 15.8.5, Safari is outdated too.",AMD,2025-10-30 15:28:46,5
AMD,nm74ozd,"iOS have tied system apps to system, which mean even small bugs in apps like phone dialer can be system breaking and they need full system update to be fixed. On Android there its much less likely for system app bug to cause such big system wide problem and if such bug exist simple app update is enough to get it fixed.",AMD,2025-10-30 14:29:07,7
AMD,nm6y4n8,What confuses me is why they recommend a driver from March if a user needs the feature?  The driver size is now 1.5gb?! It ballooned in size. 25.9.2 is less than 900mb.,AMD,2025-10-30 13:56:25,14
AMD,nm7fr24,In my world it's so that we get rdna4 lol,AMD,2025-10-30 15:22:57,2
AMD,nm7hmxt,"Yeah reality is ""bullcrap"" because some people don't want to believe in it.  Nice way of looking at the world.",AMD,2025-10-30 15:31:53,-13
AMD,nm8teqt,"Agreed, but it's not just that. I'm not paying over 1k every two years or so because AMD decide to EOL my fucking card.   On the other hand, they can't even guarantee a set of drivers every month for their latest hardware.",AMD,2025-10-30 19:18:53,4
AMD,nm7x5yl,"Majority of wine isn't suitable for aging, so yeah, I think it fits the description perfectly, heh.",AMD,2025-10-30 16:45:31,8
AMD,nm7m2ek,You’re talking about GPUs that’re literally 15 years old. Go a couple generations ahead to Tahiti and Hawaii and you’ll find that AMD did *much* more for those products than Nvidia has ever did for Kepler.,AMD,2025-10-30 15:52:44,-8
AMD,nm93lvj,It's 2 generations behind. It doesn't matter if the performance is still good for the current workloads.,AMD,2025-10-30 20:08:29,-4
AMD,nm9z83z,Rdna 2 don't have AI core and it run very slow with the int8 model,AMD,2025-10-30 22:54:12,-1
AMD,nm7wcmp,"If i were to hazard a guess, one of the CVEs is probably to do with secure comms or usb standards and amd probably thought it easier to take off the feature rather than get it working under a cve. Another hazarding guess, it probably doesn't follow a standard they said they are, they got caught, and just got rid of it rather than address it.",AMD,2025-10-30 16:41:39,41
AMD,nm80f9h,It does make sense. AMD doesn't want you to use their cards for too long. They want you to upgrade every gen.,AMD,2025-10-30 17:01:02,-15
AMD,nm8uggo,"Oh, I see. You need that connectivity for the psvr2 to function, so a cheap usbc hub might not do, though passing usbc through a switch and converting to hdmi and so on are quite different things. While usbc to hdmi adaptors are really cheap to get a proper expansion hub is very expensive. I don't have psvr2 but it's not clear to me that that's what required if it's just to add power to the usbc port.",AMD,2025-10-30 19:23:58,3
AMD,nm8vulc,at that point why not just buy the adapter for $60?,AMD,2025-10-30 19:30:41,2
AMD,nmaolsn,"No, that’s different. If I fully disclosed that I’ve locked a physical function behind a payment and someone still buys the product then they’re the idiot. That’d be like if AMD charged $50 to access the USB-C functionality in the first place, but this is the equivalent of BMW offering the heated seats initially and then stripping them via software later (effectively having sold a product with a function and then removed it after the sale).",AMD,2025-10-31 01:20:42,12
AMD,nmcn1k3,"Also fair, yeah. Nonetheless, I would take “fine wine” over whatever this new shift from AMD is though.",AMD,2025-10-31 11:11:00,1
AMD,nmeqnpr,"Its true that what fine wine really meant was that the drivers where not good enough to squeeze out all the performance of the hardware at launch.  But in practice, If I bough a card based on the price/performance ratio at launch, I'm still happy too see it improve over time.",AMD,2025-10-31 17:57:49,1
AMD,nmalgyd,Silly like dropping support for GPUs released under half a decade ago…I could see it at this point.,AMD,2025-10-31 01:02:17,2
AMD,nmbw31j,"Yeah, “AMD ages like fine wine” has honestly been more true on Linux than windows for a fair amount of time - but that’s just the nature of FOSS when actually talented, passionate people work on it.",AMD,2025-10-31 06:51:23,2
AMD,nmftdr3,"Same, got it in 2023 I think. And they're still selling them at 300 euros, absolutely absurd.",AMD,2025-10-31 21:23:11,1
AMD,nm7y13d,INT8 version 'leaked' and can be run on older AMD cards while providing much better visual fidelity with very little performance drop compared to FSR 3.,AMD,2025-10-30 16:49:38,17
AMD,nmddmo9,Now that's interesting. I wonder how it's emulating that. Is it getting converted to FP16 or BF16 and then doing the maths in that precision?,AMD,2025-10-31 13:54:53,1
AMD,nm88ggt,typically when you split the driver branch its because you're putting one of them into maitenance,AMD,2025-10-30 17:39:58,7
AMD,nm7epc4,I mean we know that android phones could run the newest android versions even with like 7-9 yo phones (rip custom roms 😭),AMD,2025-10-30 15:18:01,5
AMD,nm7pjdz,Very much the opposite. Apple used to degrade performance deliberately on only slightly older phones. Get sued for it.,AMD,2025-10-30 16:09:08,-4
AMD,nm72bm7,The RDNA 3/4 version of the driver is 900mb.  The combined version with RDNA 1 and 2 support is the 1.5GB one.,AMD,2025-10-30 14:17:19,22
AMD,nm7pdty,Adrenalin 25.10.2 (WHQL Recommended) is 902 MB.  Nvidia 581.57 is 896 MB.  These are very normal sizes for how many features they've added in recent years and how many thousands of games they support with custom code. They're really not growing that fast considering how fast flash storage has gotten cheap. It's like 50 MB/year or something?,AMD,2025-10-30 16:08:23,10
AMD,nm726vb,The main driver download includes both branches and their separate installers.,AMD,2025-10-30 14:16:39,7
AMD,nm7ufsb,"Pardon my ignorance,   Does the driver just run on the CPU?  Or does some of it also run on the GPU?",AMD,2025-10-30 16:32:26,2
AMD,nm8jlw8,iam honestly checking for a 5070 or smth now instead of a 9070,AMD,2025-10-30 18:32:34,8
AMD,nm8vtzy,"Yeah good luck most people won't buy AMD regardless for a good reason   My 6750XT has been filled with issues, my RX 550 died 2 years in",AMD,2025-10-30 19:30:36,2
AMD,nm7p3jb,Companies 1000x smaller can still support products longer.  Yes. It is bullcrap and not reality.,AMD,2025-10-30 16:07:01,22
AMD,nm9i9yk,"This is what the AMD Stans can't seem to understand. As someone who's used both Radeon and GeForce stuff, I can tell you... NVIDIA even when they remove support, they just get it right from the beginning. With GeForce you get 99% of the performance day 1. Rather than waiting months for the full performance of my card, NVIDIA gives it basically right away.  The whole FineWine argument makes no sense to me, these cards have a limited shelf life, if the first 6 months you're getting gimped performance, how is that a good thing? These cards are only relevant for maybe 24 months and their support is maybe 6 years of support if you're lucky with AMD and more like 8 years with NVIDIA. If I have to wait 4-6 months to get the full performance of my card, then any money you saved buying AMD is basically mute imo. The extra $50-100 is worth getting the full performance day 1 rather than waiting months for it. Especially if the issue is making the device completely unusable.   Then take into account black screen issues, game issues, multi-monitor power usage and inconsistent driver updates where like you said AMD may not ship a driver every month. I just never understood the FineWine thing... Now I will be fair and say NVIDIA's drivers honestly haven't been the best with Blackwell, but they've been just bad relative to how they were with Ampere, Pascal, Turing etc. They're still a lot better than AMD's worst driver moments the past couple of years and they're no where near Intel's driver problems for GFX. So NVIDIA is still the better choice in terms of driver support and I will give AMD credit, they held back RDNA4 to work on drivers more and it helped them a lot.  Either way, I'm tired of this duopoly and the GPU industry in general, too many people not willing to call out the BS from any of these companies and all of them are just screwing consumers. I honestly wished Intel was going to do better, but they've lagged for three generations now and they're looking like they'll abandon dGPU altogether.",AMD,2025-10-30 21:20:32,9
AMD,nmao5d3,I’ll gladly pay Nvidia $2000 for performance that laps everything else if it means I won’t need to worry about driver support being gone after a mere half decade. Nvidia JUST ended support for Pascal (as in they’re doing the same thing for a May 2016 GPU arch that AMD is now doing for a November 2020 arch).,AMD,2025-10-31 01:17:59,1
AMD,nm7of0k,"I'm also talking about stuff that happened almost 10 years ago.  But it is part of the reasons my friends and I have avoided AMD GPUs.  We had consistently had bad experiences with it, including driver related problems like this.  And here we are literally talking about AMD removing a feature from a product that released less than 3 years ago that nvidia is still supporting on products it released 7 years ago, and hasn't even included on anything newer than that.",AMD,2025-10-30 16:03:48,10
AMD,nm83zyx,"Vega/Radeon VII, RDNA1, and Vega APUs staring at you like Samuel L Jackson rn.",AMD,2025-10-30 17:18:23,5
AMD,nm93z9l,"2 gens doesn't mean stuff , you could say the 7000 series are also 1 gen behind the 9000 series yet in most cases it's only a 5-10% difference.  These gpus are only 5 years old don't defend a multi billion dollar company it's not your friend dude.",AMD,2025-10-30 20:10:17,6
AMD,nma1nox,Still better than fsr 3 by a lot. Fsr 4 peformance > fsr 3 quality,AMD,2025-10-30 23:07:47,6
AMD,nmauesz,Nah that has been disproven.,AMD,2025-10-31 01:55:26,2
AMD,nm8ltrp,> amd probably thought it easier to take off the feature rather than get it working under a cve  You're probably right. That's exactly what Intel did with undervolting (laptop) CPUs a while back. It made me pretty upset given I couldn't rollback my BIOS far enough to get around it.,AMD,2025-10-30 18:42:58,22
AMD,nm90s8q,"Hopefully it's all a moot point and it works for OP anyway, due to needing less than 10 watts.  You're right though, I'm pretty sure there's inline power enhancement adapters for USB-C which would likely cost way less.",AMD,2025-10-30 19:54:44,2
AMD,nm8zwuo,"Again, this is all on conjecture that's probably a moot point (7.2w should be easy even if PD is disabled), but probably because the adapter is hot garbage, at least so far as I've read/heard.  4K @ 120hz is a hard sell even under the best of circumstances, and being able to bypass any kind of active conversion in display/data pipes is definitely preferable.",AMD,2025-10-30 19:50:29,2
AMD,nmhdw0z,The youngest GPU is 6750 GRE from October 2023. But there's handhelds that are being sold and released literally rn. Like the xbox ally (non x) for 600$ and base model legion go 2 for 1100$.,AMD,2025-11-01 03:43:24,1
AMD,nmd3bf3,"This + when the company isn’t a sole maintainer, they can’t just go and remove some stuff when they feel like it, because that will be contested by others. So the least they can do is not invest anything specifically to improve old hardware, but that mantle will be picked up by others evidently (in this case it’s a Valve contractor)",AMD,2025-10-31 12:58:17,1
AMD,nmfu42r,I got mine for 250CAD and new ones are now 300+ CAD it’s crazy! It’s a great card and you can play 1440 if you turn down a few settings.,AMD,2025-10-31 21:27:26,2
AMD,nm811ev,"Interesting. Do you have some articles I could look at? I assume that FSR is powered by some kind of deep learning model and that this is a quantized version of that model.  I have not been using AMD for graphics for a little bit to be honest, so I don't really know what the latest developments are.",AMD,2025-10-30 17:04:04,2
AMD,nmdg2dk,"Yup, exactly. They did massive optimisations in RADV for FSR4 and it's FP8 on FP16 emulation",AMD,2025-10-31 14:07:26,2
AMD,nm8utbl,It was when battery got down to 80% of capacity. they made that into a function,AMD,2025-10-30 19:25:41,1
AMD,nm9jo5y,Good. Punish these clowns with our wallets. I am upgrading my current 6800XT with the Nvidia 6xxx line-up and I threw my plans for AMD 10XXX cards in the garbage.,AMD,2025-10-30 21:27:41,-2
AMD,nm9shkf,True words,AMD,2025-10-30 22:15:32,2
AMD,nm99uep,Again you are using performance to judge if it's old or not.  Nvidia gpus get full feature and driver optimization for 5-6 years on average. So amd is not doing anything crazy here.,AMD,2025-10-30 20:38:38,-2
AMD,nmabl63,Yep. I tried it for the first time on Outer Worlds 2 and it looks so much better I thought my brain was making shit up.,AMD,2025-10-31 00:04:06,3
AMD,nma2cvy,"Wait, that was the fix for plundervolt?!",AMD,2025-10-30 23:11:43,3
AMD,nm9a9q5,"It splits the signal, not conversion. Your GPU is doing the same thing over it's USB port.   They both maintain DP 1.4. One does it over USB-C DP alt and the other does it over a DP cable.   The VR2 adapter does what it should do, it's been reliable. The software was buggy in the beginning, but that wasn't exclusive to the adapter. What issues did you hear about?  The hub you linked requires thunderbolt host or USB4 for all the good features. It's an expensive path if you don't have a native port. Add in cards are pricey and didn't work well for me.  The [SUNIX UPA2015](https://www.sunix.com/en/product_detail.php?cid=1&kid=2&gid=11&pid=2146) is an option as well and cost ~$40",AMD,2025-10-30 20:40:41,3
AMD,nmfvowt,"Yep, the only reason to put drivers on the back burner for these is €€€€€ $$$$$.",AMD,2025-10-31 21:36:39,1
AMD,nm8ppc5,"i have tested it myself and its absolutely a gamechanger, performance is not great (fsr 3.1.5 quality = fsr4int 8 perfomance) but image quality? man not even a competition fsr4 performance looks better than both xess ultra quality and fsr 3.1.5 quality.",AMD,2025-10-30 19:01:05,3
AMD,nm89fa4,"It's not really anything that is special magic be it DLSS, XeSS, or FSR4. It runs better on cards that have dedicated units for the precision/calculations being run. It's still just basically math and for this type stuff usually lower precision.",AMD,2025-10-30 17:44:40,3
AMD,nm93eqq,r/steamdeckhq posted an article not too long ago. Here's the link:  [https://steamdeckhq.com/news/we-can-now-use-fsr-4-on-the-steam-deck-for-better-visuals-with-a-power-tradeoff/](https://steamdeckhq.com/news/we-can-now-use-fsr-4-on-the-steam-deck-for-better-visuals-with-a-power-tradeoff/),AMD,2025-10-30 20:07:32,1
AMD,nmdgbmr,This begs the question why AMD made an Int8 version when they could just do this instead and probably get better results.,AMD,2025-10-31 14:08:45,1
AMD,nm8y1ub,"> I wouldn't be surprised to see NVidia going the same route as AMD to cut costs now that the precedent has been set. I'd wager they're keeping a very close eye on consumer discord to see if they can follow suite.  I'm not sure what do you mean by this. AMD cutting driver support soon is nothing new. Ass stated, HD 6000 cards only had driver support for less than 5 years. That happened 10 years ago and nVidia hasn't followed suite.",AMD,2025-10-30 19:41:22,3
AMD,nm84m25,You are right.  I'm using an old as hell driver for my nvidia GPU because otherwise I run into issues.  And it probably only works for me because I'm too poor to play new games now.  If I ever get the chance to buy another GPU it probably won't be nvidia after this disaster and what they're doing with AI and their friendliness with fascism.  I just wish there were any good options.,AMD,2025-10-30 17:21:19,2
AMD,nm9cyoq,Again 5 years isn't old.,AMD,2025-10-30 20:53:46,3
AMD,nma72xl,"Yes, because if you ran the chips at a low enough power, it caused errors that could expose sensitive information.",AMD,2025-10-30 23:38:12,12
AMD,nm8aile,Yes I know how running AI models works. You don't need to explain. I am asking for something specifically on FSR4,AMD,2025-10-30 17:49:53,0
AMD,nmdk366,"From all the info optiscaler devs could gather, the int8 version is older than any known fp8. Probably just internal testing and development on earlier hw.  FSR team is located in poland and they rarely, if ever, have access to pre-release hardware. Then FSR is rewritten from research models to C for GPU hw",AMD,2025-10-31 14:28:03,2
AMD,nmamh9v,I’m surprised you so confidently said it *requires* hardware that RDNA 3/2 do not have when [this](https://www.techspot.com/article/3040-amd-fsr4-upscaling-older-radeon-gpu/) article (and many like it) blew up this very subreddit not even a month ago now. There are even articles about it running on the Steam Deck.,AMD,2025-10-31 01:08:08,3
AMD,nmbcchp,"I didn't see those articles until I made that comment. Technically what I said isn't wrong. This is a different variant than the official FSR4 as the official FSR4 is FP8. This variant is Int8 and apparently has less performance. It's a different quantization. So someone has had to convert it to make it work, which is some what clever but it's not exactly the same thing.",AMD,2025-10-31 03:52:19,1
AMD,nlq9s7a,I don't even know what is what anymore.   Maybe this is exactly what they want. Maybe they have such an overabundance of Zen 2 and 3 supply that they don't know what to do with them. Maybe this is their way to get rid of all that without them coming off as outdated.   It's purposefully confusing and misleading.   I don't like it.,AMD,2025-10-27 22:43:08,91
AMD,nlrfxkt,"to make this more clear:     Ryzen 7 170 = Ryzen 7 7735HS     Ryzen 7 160 = Ryzen 7 7735U     Ryzen 5 150 = Ryzen 5 7535HS     Ryzen 5 130 = Ryzen 5 7535U     Ryzen 3 110 = Ryzen 3 7335U     Ryzen 5 40 = Ryzen 5 7520U     Ryzen 3 30 = Ryzen 3 7320U     Athlon Gold 20 = Athlon Gold 7220U     Athlon Silver 10 = Athlon Silver 7120U     Specifications are 100% the same. So, in this case, AMD ""just"" changed the names of it's 2023's mobile portfolio. Which, as well, was already a rebranding of older CPUs.     Source: [3DCenter.org](https://www.3dcenter.org/news/news-des-2526-oktober-2025)",AMD,2025-10-28 02:39:49,48
AMD,nlq0gud,This is so brain dead. 😵,AMD,2025-10-27 21:50:59,123
AMD,nlq551d,"The Ryzen 30 is wild.  I get that AMD shuffling the names every year (7000 & 8000 series in 2023-2024 where the 3rd digit indicated the architecture, 7520U was Zen 2 from 2019 -> Ryzen AI Pro Max 395+ bullshit where there's 3 words that mean absolutely nothing -> Ryzen 30, Ryzen 40, Ryzen 170, Ryzen 110) is \*\*meant\*\* to confuse consumers, but...  Well...  Not really much more to say, is there?",AMD,2025-10-27 22:16:48,67
AMD,nlpx7fs,"AMD is a bunch of clowns, they just changed the stupid names to RYZEN AI, etc.  They're ridiculous.   They need to get rid of the amateurs who make these changes every 6 months, it's not possible.",AMD,2025-10-27 21:33:35,64
AMD,nlq7u2t,"Uhh... That third digit wasn't always there. Looking at handheld PCs with AMD chips, I see 5800U, 6800U and then 7840U.",AMD,2025-10-27 22:31:59,11
AMD,nlsi4sm,Must’ve let some of that Rebrandeon marketing team over to the Ryzen team.,AMD,2025-10-28 07:35:57,7
AMD,nlqlznf,"Someone there needs a nice slap. I still don't know the Ryzen AI variants until someone mentioned just yesterday that one of them, allegedly, has the GPU power of a 4070.",AMD,2025-10-27 23:51:00,11
AMD,nlqp599,Wtf. If it wasn't already confusing enough,AMD,2025-10-28 00:09:04,5
AMD,nlsd8qf,1. **Halo series:** AMD Ryzen AI Max 300 (Strix Halo) 2. **Premium series:** AMD Ryzen AI 9 300 (Strix Point) 3. **Advanced series:** AMD Ryzen AI 7/5 300 (Krackan Point) 4. **Mainstream series:** AMD Ryzen 200 (Hawk Point) 5. **Entry-level series:** AMD Ryzen 100 (Rembrandt) 6. **The PC should run:** AMD Ryzen 10 (Mendocino)  [ComputerBase.de explained it best.](https://www.computerbase.de/news/prozessoren/amd-folgt-intel-alte-ryzen-prozessoren-bekommen-nun-auch-neue-namen.94804/),AMD,2025-10-28 06:48:59,5
AMD,nlscx22,Sigh... the brainrot is getting worse.,AMD,2025-10-28 06:46:22,3
AMD,nlssigj,It does feel AMD is a technically strong company with a terrible marketing department.,AMD,2025-10-28 09:24:29,2
AMD,nlt330o,"I want to remind everyone that this naming bullshit is done by requests from laptop manufacturers, not on AMD's marketing whim. Case in point is that they don't engage it with desktop lineups.",AMD,2025-10-28 11:00:10,2
AMD,nltylpn,This is honestly disgusting... Reducing brand value.,AMD,2025-10-28 14:15:41,2
AMD,nls6k9y,What a shame with AMD rebranding scheme,AMD,2025-10-28 05:50:56,2
AMD,nls28xg,"When intel did that 8 years ago, everyone cursed them - included me.",AMD,2025-10-28 05:14:04,3
AMD,nlql2j7,both would be a novelty on the desktop,AMD,2025-10-27 23:45:49,1
AMD,nlt40xp,"well, they wanna move probably to xxx naming scheme  they have strix point with 300, like the hx 370  they have strix halo with 300, like hx 395  10 will be zen 2  100 will be zen 3  200 will be zen 4  300 zen 5  wonder what they will do with zen 6 - probably 400 - but they are kinda all over the place with the naming",AMD,2025-10-28 11:07:47,1
AMD,nlv7m2l,"So it's OEMs who request rebrandings in order to sell old APUs as new to non-tech savvy consumers? TBH it makes sense, as desktop SKUs, which are sold directly to consumers, have a far more consistent naming.",AMD,2025-10-28 17:54:35,1
AMD,nlvunqn,what are the chances 10000 series is ryzen ai desktop 100 or something,AMD,2025-10-28 19:47:31,1
AMD,nlwd2ef,whatever... better than whatever the fuck they were previously doing.  i guess they do this to prevent new old stock. much clearer what generation a chip is at least.  simple ask. stick with it. dont add bullshit. 2xx is Zen 4 3xx Zen 5 and so on. no jumping numbers or having two generations in one number. all i want is a sane product line up naming scheme that they stick to.,AMD,2025-10-28 21:17:07,1
AMD,nlx27md,"Wow, laptop sales are this bad that they’re rebranding every two years so we just buy whatever.",AMD,2025-10-28 23:30:36,1
AMD,nlxpnd8,Should be illegal to confuse consumers using rebrands. Should be extremely clear branding and models,AMD,2025-10-29 01:43:15,1
AMD,nlxv2qu,"Didn't expect there are some zen3+ leftover. mendocino chip should become  sempron  and rembrandt as athlon (for desktop).    Oh btw, where's the new zen3? /s",AMD,2025-10-29 02:14:12,1
AMD,nlz4tjx,"the naming scheme is god awful, as expected, and leaving aside the ""morality"" of selling 6 year old CPUs... Why do they do it?  Maybe they are reusing the """"old"""" TSMC 7""nm"" production, instead of manufacturing these CPUs on the latest node? I really don't get it.",AMD,2025-10-29 08:43:19,1
AMD,nlzkocu,"Throw those plans in the trash, change for the better for the love of silicon... There is no justification for this.",AMD,2025-10-29 11:08:04,1
AMD,nm7mp2k,are they still producing zen 2 and zen 3+ APUs or what?,AMD,2025-10-30 15:55:38,1
AMD,nmml6bo,putting zen2 product on the market in 2026 is just wow.,AMD,2025-11-02 00:53:57,1
AMD,nlse7kd,Igpu updated for have latest driver support. Vega is really on dead way,AMD,2025-10-28 06:57:00,1
AMD,nlrw8u2,this is largely driven by OEMs who want the cheapest chips while it not being obvious that it's previous generation silicon,AMD,2025-10-28 04:27:11,0
AMD,nlrsiau,"While different naming schemes can be hard to make quick evaluations and ads an extra step, all it means is consumers should be educated on the basics and not be afraid to pull up a chart of the performance metrics that matter most to them, benchmarks, battery, etc. They should already be evaluating the totality of a notebook so looking at real-world testing and scores in relevant things they want to have the laptop or mini or desktop etc do is valid regardless.   It's hard for consumers to navigate, but the change in numbering scheme is enough for consumers to be aware its meant as a low power option.",AMD,2025-10-28 03:59:50,0
AMD,nlw6lu8,AMD's Marketing team should be reshuffled ... out the door,AMD,2025-10-28 20:45:35,0
AMD,nlr2wbu,"Eh, I would say this new system is less confusing than the old naming scheme. 0x0 is Zen 2 1xx is Zen 3 2xx is Zen 4 3xx is Zen 5  First digit is gen, next two bigger number = better.  A bit simple, but not bad.  If they keep this system up (using 4xx for Zen 6 and 5xx for Zen 7) then I think most people will be fine with this scheme.  The problem is that there is no trust that AMD will continue this naming scheme, especially since their actual customers for these chips (the large laptop OEMs) seem to want confusing naming on stuff.",AMD,2025-10-28 01:25:03,19
AMD,nlzjrmh,Oh my... Dual-core processors in 2026? Not even smartphones use dual-cores.   Ryzen 5 40 should be Athlon if AMD had learned anything.,AMD,2025-10-29 11:00:50,5
AMD,nlrsb1z,shameless shit from AMD,AMD,2025-10-28 03:58:24,20
AMD,nlseg7p,It's like the person responsible for this has multiple personalities. One year it's clear and understandable and the other year they turn this shit.,AMD,2025-10-28 06:59:10,3
AMD,nlt2gw7,"How is it confusing? You have two laptops, one Ryzen 170 and the other Ryzen 370. People will assume that the 370 is newer/better.",AMD,2025-10-28 10:55:11,3
AMD,nlu9vx3,"I'd assume AMD will phase out the 4-digit numbering scheme, and for a few years, the 2/3-digit scheme will make some sense before they start introducing a new one.",AMD,2025-10-28 15:12:48,1
AMD,nlq1vse,"Honestly it works.   Im fairly tech savy (look im in a tech subreddit) but i couldnt, if my life depended on it, give you advice on amd laptop cpu skews.    And i think thats entirely their goal.",AMD,2025-10-27 21:58:37,33
AMD,nlsmc5v,"What's sad is confusing the customer seems to be the norm in tech. Try figuring out the naming convention of any laptop brand, you probably won't be able to. HP for example has hundreds of models that change by location, store, and just over time. Why do they do that? So you have no choice but to buy whatever they offer you.  It backfires with tech-savy consumers though. I bought a Ryzen system specifically because AMD had an understandable naming convention at the time and Intel didn't.",AMD,2025-10-28 08:19:43,2
AMD,nlqqcke,"Ah, you are missing something really important here - what the actual customers want.  The customer for laptop chips is not the end consumer like you or I, but large laptop OEMs .  And those companies want regular rebadged names for the cheaper older chips, as they believe newer names equals more sales.  Hard to blame AMD for making their direct customers happy.    Note how for stuff that has more direct consumer sales like desktop chips they don't do the rebadge game (granted the desktop chips do have some questionable naming decisions, but the same chips keep the same name), likewise for the professional stuff.",AMD,2025-10-28 00:15:59,0
AMD,nlumf3x,"There were SKUs that had a third and even a fourth digit in previous generations like the 5825U (mind you that is a Barcelo part, which was basically a small refresh of Cezanne). But AMD really popularized it and introduced it with the 7000 series to provide ""clarity"" as to what architecture something is. Why did they do that? Because in that same 5000 series lineup they had a mix of Zen2 and Zen3 parts. Like the 5300U which was Lucienne architecture, which was Zen2, but the 5800U was Zen3 Cezanne.  But it's just stupid because with the 6000 series, everything was Zen 3+, so whether you bought a 6800U or a 6600U it was all Zen3+. It didn't matter what you picked and tbh that's how they should have carried on, only make the new stuff the new generation naming.  In the 7000 series though they went back to the 5000 series naming sort of because they decided to mix old architectures again with the new stuff. For example, the 7735HS a Zen 3 chip and the 7745HX a Zen4 chip, that one third digit is supposed to notify you that you're buy Zen4, therefore it was as an indicator as to what Ryzen generation you were buying, but it just confused consumers even more because some people thought a 7640HS was worse than a 7735HS, but in single core the 7640HS beat the 7735HS and in multi-thread both were about even.  In the end, AMD made a shit solution to a shit solution, all of their own making and now they've come up with ANOTHER shit solution to try and ""fix it"".",AMD,2025-10-28 16:13:20,2
AMD,nlvvaou,tangentially related but what exactly is nvidia going to do in 4 years when they need to release a card called the rtx 9070 when the rx 9070 already exists,AMD,2025-10-28 19:50:42,1
AMD,nlqn6uz,"That would be the Ryzen Ai 9 395, which yes, has a  iGPU that is pretty close to a mobile RTX 4070.",AMD,2025-10-27 23:57:45,7
AMD,nlsq615,"Yeah, that was another own goal from AMD marketing. They were claiming 4070 performance (may even have been desktop 4070, but not sure anymore so I won't pushed that) then when it came out it turned out to be somewhere between mobile 4060 and mobile 4070. Plus the devices with that SKU are so expensive that you are better off buying an actual gaming laptop with a 4060 or 4070.",AMD,2025-10-28 09:00:18,1
AMD,nluddnq,"Mendocino should only exist on Chromebooks.    Kraken should be mainstream, and Strix-point premium. Let the old stuff die. But AMD doesn't want to evolve as a brand.",AMD,2025-10-28 15:29:50,2
AMD,nluhpg5,"AMD's been doing this sort of thing for years. But this is the worst variant of it. I mean the fact the 'Ryzen 7 170' even ""exists"" as a name is absolutely shameless. But even before that the 7735HS (which is the former name of the 170) was bad because many people thought it was Zen4 like the 7745HX was because of the 7000 series moniker and the fact it too is a Ryzen 7. Nobody will call out AMD really because they're the ""good guys"" and because they command market share in DIY now days.",AMD,2025-10-28 15:50:33,2
AMD,nlxgmb9,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-10-29 00:51:45,1
AMD,nltbrfa,There was info circulating about a year ago that AMD dropped Vega dGPU and iGPU driver support that turned up to be misinformation. Here are the list of the latest driver release:   Mendocino drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html)  Rembrandt drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html)  RX Vega 64: [https://www.amd.com/en/support/downloads/previous-drivers.html/graphics/radeon-rx/radeon-rx-vega-series/radeon-rx-vega-64.html](https://www.amd.com/en/support/downloads/previous-drivers.html/graphics/radeon-rx/radeon-rx-vega-series/radeon-rx-vega-64.html),AMD,2025-10-28 12:04:39,2
AMD,nls15lb,> it's previous generation silicon  Some of it is way worse than even that. There are _Zen 2_ chips (previous-previous-previous?) implicated here. That is some shamelessly re-badged garbage on AMD's part. These chips are barely even new enough to run Windows 11.,AMD,2025-10-28 05:05:30,7
AMD,nlshc0l,"No the real problem is they keep changing it every other generation. Maybe the new one makes more sense than the one before, but then again the one before that was even better. As a customer it is impossible for me to keep up, so the likelihood of me buying something expecting the latest CPUs and ending up with something from half a decade ago is disturbingly high. There is also no guarantee they won't change it again in a few years.",AMD,2025-10-28 07:27:33,34
AMD,nm681up,"For what it's worth, availability of the Athlon 7120U was **extremely** limited. I should know as I was trying to look for a laptop with that chip that didn't have 4GB of RAM with it, which effectively limited me to some really obscure HP slop box.   It doesn't make it excusable per se, but you'd have to stumble upon one of these completely at random or entirely intentionally to end up with one.",AMD,2025-10-30 11:21:10,1
AMD,nlts0kp,"The problem's that the moment these come onto the market isn't the moment every single older laptop gets wiped off the market, especially considering these are practically the same chips being re-released time and time again by AMD. Here's an example:  One customer looks at 5 budget laptops and their processors. #1 has a Ryzen 5 7520U, #2 has a Ryzen 3 7440U, #3 has a Ryzen 5 220, #4 has an older, discounted Ryzen 5 5600U, #5 has a Ryzen 5 40. Place your bets on the best one now.  Where do you even begin to compare these? Well, the Ryzen 5 7520U has a higher number than the Ryzen 3 7440U and the Ryzen 5 5600U, so it should be better, right? Wrong. Both the 7520U and 7440U are 4-cores, 8-threads, but the Ryzen 5 7520U has a 15W TDP, 4MB of L3 cache, is Zen 2-based (from 2019), and has a Radeon 610M iGPU. Meanwhile, the 7440U has a 28W TDP, 8MB of L3 cache, is Zen 4-based (3 years newer), and has a Radeon 740M iGPU. It's going to be an order of magnitude faster than the chip that's a higher number than it. As for the Ryzen 5 5600U? It's a 6-core, 12-thread Zen 3 chip with a 15W TDP, 16MB of L3 cache, with a much inferior Vega 7 iGPU.  Okay... so the Ryzen 7520U is a no-go, and the Ryzen 7440U and Ryzen 5600U are kind of up for debate if you have a dGPU in these laptops (though absolutely pick the 7440U if the iGPU is the only form of graphics onboard). Well what's up with the Ryzen 5 40? What generation does that fall under? If the third digit of the 7000 series was architecture, and the first digit of the 5000 series was architecture, is the Ryzen 5 40... Zen 4 based? Well no, you're not supposed to know that, you're supposed to just believe it's a new chip. Well we know it's a Zen 2 4-core 8-thread with 4MB of L3 cache from the article, but this potential customer doesn't. They'll find it out if they research it, but by this point they're probably getting sick of researching CPUs when the vast majority of people know barely anything more than an Intel i7 or Ryzen 7 being good, and an RTX graphics also being good. They most likely don't know anything about cores, architectures, TDP, or L3 cache, and probably just want a working laptop.  What about the, uh, Ryzen 5 220? That seems... probably a fair bit worse than the Ryzen 3 7440U and Ryzen 5 5600U going off the name and potentially being connected in naming scheme to the Ryzen 5 40, so it's worse, right? Nope. The Ryzen 5 220 is a 6-core, 12-thread, Zen 4-based, 28W chip with 16MB of L3 cache, and Radeon 740M graphics. A clear choice to pick that gets the best of both worlds with regards to the cores & cache of the older chip, and the newer architecture and iGPU of the newer chip.  Outside of this hypothetical scenario now, I want to add an honourable mention to compare to the Ryzen 5 220. I'd like to bring up the Ryzen AI 5 330, which is a Zen 5-based 4-core 8-thread 28W chip with 8MB of L3 cache, and Radeon 820M graphics. It's got less cores and less L3 cache than the Ryzen 5 220 and most likely will be worse in benchmarks [(and as a matter of fact, it is)](https://www.cpubenchmark.net/compare/6616vs6917/AMD-Ryzen-5-220-vs-AMD-Ryzen-AI-5-330), but despite this is a tier above (X20 v. X30) the 220, and very few will catch that the newer chip is worse than the older one. Even chips within this new naming scheme are inconsistent.  When these chips come out, the laptop market isn't going to have done away with the older chips for no good reason. [These chips are still out there](https://www.amazon.com/Lenovo-ThinkPad-Notebook-Keyboard-Professional/dp/B0FT3CR5VK), and customers looking for an AMD chip are gonna regret looking for one if they have to compare this kind of shit. This naming scheme's only a year old with the Ryzen AI 300 series, and the previous one (where the architecture was defined by the third digit) lasted two generations from 2023 to 2024. Pre-2023 laptops are perfectly usable for the average person and they're still going to be selling those laptops today, and god help anyone looking for an AMD laptop that has to deal with this naming BS. Intel's not much better with their 200V series but at least it looks like they're going to stay the course and keep using that naming scheme now that desktop and mobile are somewhat unified. AMD's just drunkenly driving off a cliff with their mobile naming scheme and landing on another cliff they'll drive off on in a year or two. Meanwhile, their desktop chips are very easy to understand in comparison. No random digit amount switching, no sneaky third digit signifying architectures, or any of that nonsense. The Ryzen 5 9600 is faster than the Ryzen 5 7600. Easy.",AMD,2025-10-28 13:40:55,20
AMD,nltue0h,"> How is it confusing?   I'm so confused as hell. There's also a 9000 series on laptops? Also, aren't the 300 series the ones with AI? But if they make 100 series, do the 100 series have AI too? But hey there's a 8945HS and that has AI. So does the 9000 series for laptop have AI too? What's the latest too, is it the 300 series or the 9000 series?  It's so disingenuous denying that their naming scheme isn't confusing.",AMD,2025-10-28 13:53:37,6
AMD,nlq775p,"I didn't say that I don't like rebranding, I said that it's crap to do a rebrand every 6 months, everyone gets confused, it's not clear. However, I like AMD but they are very bad at it.  Like their rx9000 gpu range to stick to the rtx5000, except that the next generation would be rx10000 and people will be lost again 😅",AMD,2025-10-27 22:28:26,9
AMD,nlqu1zc,By that logic everything nvidia does is extremely right.   That would be very unpopular on this sub I imagine.,AMD,2025-10-28 00:36:47,7
AMD,nlz49y4,Bold to assume they will still produce gaming cards for us mere peasants,AMD,2025-10-29 08:37:43,2
AMD,nlw0ep9,"Most customers dgaf about ai and want a cheap laptop for office, and the better overview while browsing the Internet.  Kraken point is actually the mainstream offer with NPU.",AMD,2025-10-28 20:15:44,0
AMD,nltmaai,"Yes you received some security update but you don't have AFMF for example. I know it , I have this laptop",AMD,2025-10-28 13:08:47,0
AMD,nlx4lgt,TBF why would they not change it again with how everyone bitched about it?,AMD,2025-10-28 23:44:00,1
AMD,nlui4by,"The fact you need to decode a CPU name and need basically a whole manual for it is utterly ridiculous and it's 100% anti-consumer (made to be that way IMO). AMD continues their shit branding meme and this is why they make no headway in Laptop market, consumers find their branding confusing and when they try to learn it they just quit and buy Intel instead to avoid the headache and minutes to hours or research to just understand what architecture and generation they're buying. Mind you Intel aren't exactly saints either by renaming stuff like the i5-10400, but at least with the Core Ultra 278V or 275HX you know you're getting Intel's latest architectures.",AMD,2025-10-28 15:52:31,9
AMD,nlqt3nb,"""Everyone gets confused""   That""s the whole idea, to confuse and mislead 😂",AMD,2025-10-28 00:31:41,4
AMD,nlrajen,What do you mean? Nvidia is doing the best thing for it's customers.,AMD,2025-10-28 02:08:49,1
AMD,nlzmt3v,"i have a great idea, instead of RTX, how about they focus on AI! now they can call their card the ATX 9070 ah shit nvm",AMD,2025-10-29 11:24:32,1
AMD,nlzkc4z,"Zen2/Zen3 + Vega-based laptops are easily found for under $500, so this already exists. And they are faster octa-cores and more efficient than Mendocino.",AMD,2025-10-29 11:05:24,2
AMD,nm85dbo,When did AMD advertise or promise AFMF for Vega?,AMD,2025-10-30 17:24:58,1
AMD,nlvxjs4,"Have u read the tech specs for these CPUs. They are new configs with old tech. So it absolutely makes sense to have new names for them.  My customers who are interested in the hardware are able to Google the difference, and the others need help anyway.",AMD,2025-10-28 20:01:48,1
AMD,nlvyjch,"PS: For 90%, it is done with the numbers 3, 5, 7 and 9.  It is for Intel and amd even the same meaning, which makes it actually really easy.",AMD,2025-10-28 20:06:40,0
AMD,nlzya54,The ASUS Astral ROG ATX 9090 Housefire Edition,AMD,2025-10-29 12:41:11,2
AMD,nm5mrkt,"Correct me if i'm wrong, but the new 100 series has ddr5 support and a rdna3 igpu. This would imo be a real good mix between cpu and Gpu performance for the entry class laptop.  They should definitely cost around 400 bucks. If they are above 500 bucks, it's a rip-off.",AMD,2025-10-30 08:06:54,1
AMD,nm8809s,"Indeed, I need to be more specific on my case. I have Asus G513AY with dgpu rx6800m (rdna2) and CPU 5900hx with igpu vega.   This laptop design is strange, because you have limitation due of igpu vega. If you want afmf from rdna2 you need to use only dgpu with a external monitor. It's why i like AMD rebrand zen 2 but with new igpu and not vega.",AMD,2025-10-30 17:37:46,1
AMD,nlxevge,"You can judge a CPU based on if it's a Ryzen 3, Ryzen 5, or Ryzen 7, but that's not really a good idea beyond a generalization. A Ryzen 7 1800X is much worse than a Ryzen 5 7600. An Intel i7-870 is much worse than an Intel i3-12100.  Going past the obvious, a Ryzen 9 9900X is worse for gaming than a Ryzen 7 9800X3D, and Ryzen 7 5700 is worse than a Ryzen 5 5600 (because the 5700 is a special chip - whereas usually chips like the 5600 are based off lower-clocked 5600Xs under the same architecture, the 5700 is based off the 5700G with PCIe 3.0 and much lower L3 cache. This continues with mobile hardware, from my comment above, with CPUs like the Ryzen 3 7440U and the Ryzen 5 7520U. One is two architectures newer, with double the L3 cache, double the TDP, with higher clocks, and a better iGPU. You'd think it to be the Ryzen 5 of the same ""generation"" (7000U) but it's not.  Desktop CPUs are easier (most of the time, there's still chips like the Ryzen 7 5700 though) but what I was saying up above is that AMD is making their laptop CPU naming schemes hard by switching them up every few generations, and it's almost certainly intentional to mislead consumers.",AMD,2025-10-29 00:41:58,3
AMD,nlzzgqu,still using the “definitely not just 12vhpwr” connector of course,AMD,2025-10-29 12:48:08,1
AMD,nm2n9pb,Hasn't that always been the case? Intel came up with NUC for mini pcs way back in 2013.,AMD,2025-10-29 20:31:49,19
AMD,nm3o5oa,Smart choice moving to AMD chips for these.,AMD,2025-10-29 23:39:22,10
AMD,nm2yvl8,It's an Intel name like ultra book,AMD,2025-10-29 21:26:38,3
AMD,nm54fbu,Minisforum sells an itx mobo with the 7945HX3D and 16x PCIE 5.0 slot for $500. Thats a really cool concept that I hope they duplicate with this chip. IMO its pretty ideal for an always on home server. I have an old home server with a 3950X and the 7945HX is about 30% faster yet consumes less than half the power. This will probably consume the same power and be 10% faster still.,AMD,2025-10-30 05:05:06,3
AMD,nm4s1td,Nice!    It is too bad Intel never had the chips to realize what most of us wanted out of a NUC.,AMD,2025-10-30 03:29:57,1
AMD,nm59th0,And of course it's limited to just the 5070 configuration while the Ultra 9 gets the 5070ti and 5080,AMD,2025-10-30 05:55:26,1
AMD,nm31z0l,"intel stopped producing nuc systems in 2023, and passed the rights for nuc to asus",AMD,2025-10-29 21:41:42,15
AMD,nm6bmdp,They use both intel and AMD for these. More intel models actually,AMD,2025-10-30 11:47:01,4
AMD,nmbs0hk,I have a gaming PC with that Board and it's pretty good for what it is. Supposedly they are set  to release a X870 Based Board with the 9955HX.,AMD,2025-10-31 06:09:52,3
AMD,nmilm0r,it does not have pcie 5.0,AMD,2025-11-01 11:12:48,0
AMD,nmbji16,An AMD chip?,AMD,2025-10-31 04:49:50,1
AMD,nmj4pbk,"Expandable Storage】 This BD790i X3D has DDR5 (SODIMM Slots x 2 | up to 5200 MT/s | up to 94GB) memory slot + 2x M.2 2280 SSD (PCIe5.0 x4) slot. It also comes with a PCle5.0x16 slot, maximize the potential of high-performance components, which is deal for various scenarios that require high-performance graphics.",AMD,2025-11-01 13:34:38,2
AMD,nmgk5ut,"Well AMD is doing a better job right now.   However I actually believe both manufactures have failed the APU market by not focusing more die space on things outside of the CPU core count.   The need for a bunch of relatively slow CPU cores is not what most of us wanted out of an APU.   8 simple fast CPU cores and a robust GPU would have been a better focus.   These days you need plenty of AI acceleration too.  Frankly the latest Strix APU's from AMD are a lot closer to meeting most users needs.   They still allocate a lot of space to CPU's but on the positive side the GPU is now robust and we have AI acceleration.  Of course Intels real failure is that they don't have a low power process which means they never had chips to put into NUC sized machines.   Even if they got the die allocation right, Intels chips would just run to hot.   So yeah AMD.   It would be even better if we had a high performance ARM based solution outside of Apple's.  Frankly Apple seems to know what to do with hardware these days (their software sucks but that is another manner) in a way that the other chip suppliers don't.   For example no hyper-threading but extremely fast CPU cores.   Then they add decent GPU's, AI acceleration and other special function units.",AMD,2025-11-01 00:14:10,1
AMD,nmwr19r,"I mean after number two, I would stop using that board..",AMD,2025-11-03 17:11:07,123
AMD,nmwjwat,At the same time?  ![gif](giphy|BTbo1iT1yEfOE),AMD,2025-11-03 16:37:09,35
AMD,nmwjo7i,First time that i'm glad that i got Gigabyte am5 board. Because ASRock was my second choice,AMD,2025-11-03 16:36:03,46
AMD,nmwq82q,Massmurderboard,AMD,2025-11-03 17:07:15,15
AMD,nmx3eso,What the hell has happened to ASRock quality control?  AM5 has been a fiasco for what used to be a fairly reliable manufacturer.,AMD,2025-11-03 18:09:31,18
AMD,nmx2r2v,"I've really liked my X570 Taichi, but after his fiasco, I don't think I will consider an ASRock board for probably 10 years. Luckily, I got a 5800X3D instead of upgrading to Zen 5, because there's a good chance I would have gone with ASRock again.  I feel badly for the people on these things. ASRock isn't fixing it properly, and the accountability of ""keep replacing it if it breaks"" is a bitch when you're talking about some people's only PCs and possibly primary forms of entertainment. Waiting weeks for an RMA swap, especially multiple times, isn't an acceptable solution when the problem is this severe.",AMD,2025-11-03 18:06:20,10
AMD,nmyb2qf,"I saw yesterday that Newegg has an ASRock B850 motherboard on sale for $40 off. Even if it were free, I wouldn't accept one and install a CPU on it.",AMD,2025-11-03 21:41:31,3
AMD,nmwioat,Shiiiit this is my exact mobo (except i have the wifi version)   Gonna keep a really big eye out for my 7800x3d,AMD,2025-11-03 16:31:09,13
AMD,nmy9w8k,Glad I went with an Asus b650 with my 9800x3d. Considered Asrock but they eat x3ds for breakfast,AMD,2025-11-03 21:35:43,2
AMD,nmwlmyw,Sell the board to Tech Jesus™. He might want to figure what the problem is.,AMD,2025-11-03 16:45:31,6
AMD,nmx0qrn,"On average, or for dinner?",AMD,2025-11-03 17:56:49,1
AMD,nmy0upw,Talk about platform longevity. Can always upgrade your CPUs.,AMD,2025-11-03 20:51:36,1
AMD,nmyp0hv,"Meanwhile me, with an Asrock A620M Pro RS Mobo and a 9700x     ***haha I'm in danger***",AMD,2025-11-03 22:54:56,1
AMD,nmyyg30,I have 9600x and B650 motherboard it's saddening to see happen to this brand they used to be so goated.,AMD,2025-11-03 23:46:13,1
AMD,nmz41e1,Never had an issue on Asrock motherboards. What a shame.,AMD,2025-11-04 00:17:58,1
AMD,nmzj2tu,Just another reminder to stay far away from asrock if you're on am5.,AMD,2025-11-04 01:46:15,1
AMD,nmxkbsc,"As someone who just built a PC but doesn't understand what exactly the motherboard does, can someone explain to me how these problems happen? I've seen reports of AsRock boards doing this previously as well to AMD chips.   I find buying motherboards to be the scariest part of the build because I know very little about MoBo vendors compared to the CPU (Intel / AMD), yet they produce the glue that holds the build together.",AMD,2025-11-03 19:31:46,1
AMD,nmxocn8,Asrock sure is on long kill streak,AMD,2025-11-03 19:51:37,1
AMD,nmxzki8,I’m so tired of seeing these asrock posts. Nobody cares. It’s a known issue.,AMD,2025-11-03 20:45:30,-6
AMD,nmwt5by,after number one I would stop using ASRock boards,AMD,2025-11-03 17:21:04,42
AMD,nmxb5hn,I wouldn't have used it for the first one after hearing the news on these Asrock boards,AMD,2025-11-03 18:46:56,3
AMD,nmxm368,"now hold on a minute, we can't be completely sure it was the motherboard. Better try again with CPUs 3,4,5 just to be extra sure   # /s",AMD,2025-11-03 19:40:28,2
AMD,nmwwsas,Yeah went with Gigabyte B850 WiFi 7. Rock solid board,AMD,2025-11-03 17:38:25,12
AMD,nmxutzo,asrock has never been a good brand imo lol,AMD,2025-11-03 20:22:47,-7
AMD,nmwplbp,"One of the reasons i went with the 7800x3d and b650m hdv m2, i did not want to experience cpu or board failures on the newer models, the cpu is also fine for years to come in 4K",AMD,2025-11-03 17:04:14,6
AMD,nmxwurt,"It's largely been the 9800X3Ds frying, so I'm hoping us 7800X3D people are safe. It is a more locked down processor, overclocking wise",AMD,2025-11-03 20:32:29,1
AMD,nmwjf9p,"so, not exact",AMD,2025-11-03 16:34:50,-6
AMD,nmwq2uq,and add the power supply as well and same Windows settings. I have a X870 Pro RS and a B850M-X at home that are working just fine with really good power supply.,AMD,2025-11-03 17:06:33,9
AMD,nmy9vt3,"The motherboard facilitates communication between all of the other components and also sends power to most of them. Because it sends power, it's possible for it to send too much and fry them. In this case, the motherboard software (the BIOS) on ASRock boards is occasionally sending too much power to the CPU, killing it. In general, it's a good idea to not buy too cheap of a motherboard if you want to be safe and avoid issues... though this here seems to be an issue with the software on this particular vendor's motherboards, not the quality of the boards, per se.",AMD,2025-11-03 21:35:40,6
AMD,nmxl912,ASRock boards used to be goated 💔,AMD,2025-11-03 19:36:22,19
AMD,nmx0vdz,"I continued to use the board on the second because I had missed my chance to return it. I would’ve had to buy a new one. The cpu was replaced under warranty.  Thankfully, no issues since putting my second one in. I did manage to update to latest bios as well.  Point is, if I already laid a good price for a motherboard it’s tough to go buy a second one.",AMD,2025-11-03 17:57:24,7
AMD,nmy7gtt,"Same. I've had that board for over 6 months with absolutely no issues. I paid about $40 more than I would've for an ASRock, but it was worth it so I wouldn't be constantly worrying about my CPU frying.",AMD,2025-11-03 21:23:39,2
AMD,nmwrr07,WiFi isn’t a big difference is it. The board will be exactly the same other than a WiFi card.,AMD,2025-11-03 17:14:30,12
AMD,nmyhadw,"They worked so hard to provide great feature sets on their motherboards for a good value. It's a shame they threw it away. I don't imagine they did this on purpose, and I can forgive a company for making legitimate mistakes, but it's hard to forgive deflection and denial of a legitimate issue. ASRock tried so hard to shift the blame and deny responsibility before sheepishly admitting this issue, and worse still the issue appears to still be there (though less common because of some mitigation patches).  Edit: This is coming from a person who is currently using an ASRock Steel Legend X670E with a 7950X3D",AMD,2025-11-03 22:13:06,13
AMD,nn0c2jr,My b550 velocita was the best board I've owned,AMD,2025-11-04 04:48:26,1
AMD,nmx8sbi,you can just get a refund for the board because it's an asrock am5 board that killed your CPU,AMD,2025-11-03 18:35:25,6
AMD,nmz13rx,"Checking in with another 670E Steel Legend/7950X3D.  Also hesitant about putting zen 6 in this board, but have loved it anecdotally.",AMD,2025-11-04 00:01:05,1
AMD,nmymg0i,"I have an ASUS board rn bc I couldn't find a decent ASRock AM4 mobo when I built re-built my PC 5 years ago, but I used to rock the ASRock Z97E-ITX/ac and I liked it a lot",AMD,2025-11-03 22:41:01,0
AMD,nmxdzee,Who provides the refund?  The store I ordered from will let you return a damaged item up to 3 months after purchase. After that it’s ASrock warranty or AMD. Unfortunately my first CPU died after 3 months.  Are you saying ASrock will refund my board? I thought they would just fix or replace it.,AMD,2025-11-03 19:00:30,5
AMD,nmyqa0c,"I'm also loving my current Steel Legend X670E. After this recent CPU burnout issue, mostly because of their handling of it, I would most definitely look to another brand if I had to buy a new motherboard tomorrow. Hopefully over the coming years ASRock can rebuild their reputation. It will be a long time before I can trust them again.",AMD,2025-11-03 23:01:46,2
AMD,nmxi0sv,"Assuming it’s still under warranty and you can prove it killed your cpu then you can work something out with ASrock, it’s similar to if a power supply fails and kills other pc components, the warranty should cover the failure of other parts that are directly caused by the warrantied component",AMD,2025-11-03 19:20:23,3
AMD,nmxpkwp,"It's easy to forget not every country has mandatory 2 year warranty from the store, not manufacturer. Sorry.",AMD,2025-11-03 19:57:37,2
AMD,nmxj9jt,Thank you! Hopefully it won’t fail again (been awhile) but I will take this approach next time.,AMD,2025-11-03 19:26:34,3
AMD,nmy27bf,Those consumer rights are sounding pretty nice right now!,AMD,2025-11-03 20:57:58,1
AMD,nlx2dxe,Are any of the OEMs making these with 2x8 pin power connectors?,AMD,2025-10-28 23:31:35,23
AMD,nlwuzrz,Btw it has about 30% slower bandwidth than the rx 7900 XT which are starting to have some age and ship with 20gb memory,AMD,2025-10-28 22:51:31,10
AMD,nlwhnbf,"Edit: I wrote something completely wrong.    I thought these were out of stock and they are not, at least at the time of editing this post.   I'm removing what I wrote earlier because of how incorrect it was.",AMD,2025-10-28 21:40:19,15
AMD,nm0o8lg,So who's going to buy one to play games on it?  haha,AMD,2025-10-29 14:57:45,2
AMD,nm0d6lg,Anyone know if there's a significant difference between the creator version and the xfx version? Is it by chance slimmer?,AMD,2025-10-29 14:02:24,1
AMD,nm3bt4h,I wanna see benchmarks.  Make sure the AI aspect of it doesn't detract from the gaming aspect.,AMD,2025-10-29 22:32:26,1
AMD,nlx96kt,"Wait, is this like a 7950 XTX, or like, a 7900 XTX XT?  What's the relative performance?",AMD,2025-10-29 00:09:53,-1
AMD,nlwkkwi,"I don't understand why choose this card instead of 7900 XTX, for the money. Even if you need those extra 8GB memory, but for almost double the price?",AMD,2025-10-28 21:55:21,-18
AMD,nlx93r4,Currently no.,AMD,2025-10-29 00:09:27,4
AMD,nlwwboy,It’s also not a gaming card,AMD,2025-10-28 22:58:36,15
AMD,nlyrndr,"I got 110-120 TPS on my RX 7900 XT using the  Qwen3-https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF in the Q4_K_S   Then I used lmstudio and put all of the model into the gpu using the ""offload to gpu"" fully",AMD,2025-10-29 06:26:20,3
AMD,nlwwz65,"You can literally order one on newegg right now. What are you talking about?  Item|Price  :----|:----  [ASRock Creator Radeon AI Pro R9700 R9700 CT 32GB 256\-bit GDDR6 PCI Express 5\.0 x16 Graphics Card](https://www\.newegg\.com/asrock\-challenger\-rx9060xt\-cl\-8go\-radeon\-rx\-9060\-xt\-8gb\-graphics\-card\-double\-fans/p/N82E16814930143?item=N82E16814930143&utm\_campaign=snc\-reddit\-\_\-sr\-\_\-14\-930\-143\-\_\-10282025&utm\_medium=social&utm\_source=reddit)|$1,299.99",AMD,2025-10-28 23:02:03,12
AMD,nm1beqj,"I am extremely tempted ngl. Was looking at a 5080 FE for being the fastest reasonable thing in 2 slots. Getting even close to its performance in raster with the 5090's VRAM capacity is enticing. Also, being a blower card means I can dig back out my designs for a custom ITX case where this card lays flat. This is close to the same price locally right now. About $150 more than the 5080.  It will be slightly slower than the 9070XT I'm sure, likely closer to the 9070 due to the lower clocks, but it's small and has 32GB of VRAM.",AMD,2025-10-29 16:47:28,2
AMD,nm6thek,I'm considering it for my fileserver. I could run comfyui on it but also use it as a headless game streaming rig? It's enticing.,AMD,2025-10-30 13:32:30,1
AMD,nlxte0q,9070XT with twice the memory.,AMD,2025-10-29 02:04:30,39
AMD,nlwvxyj,This isn’t a gaming card,AMD,2025-10-28 22:56:37,28
AMD,nlxyw6l,The RDNA3 in the same category was the W7900 which cost $3500-$4000 so the Pro 9700 is actually a significant price drop.,AMD,2025-10-29 02:36:22,8
AMD,nly24rl,AI models and video editing,AMD,2025-10-29 02:56:16,1
AMD,nlwxm69,Its 30% slower for AI models that fit within the 20gb limit on the 7900 XT,AMD,2025-10-28 23:05:30,13
AMD,nlwzvm2,I don't know what happened and why I thought it was out of stock because when I looked it genuinely was not purchasable.    I sincerely appreciate you sending me this link because this link did work.   I ordered 2. Let's see if they come.,AMD,2025-10-28 23:17:46,5
AMD,nm1z8ry,"Wouldn't it boost to the highest speed it can just like the 9070XT?  My 9070XT boosts well above the stated numbers from AMD...  If you do, please post some results for gaming!  I'd really like to see some numbers.",AMD,2025-10-29 18:38:36,1
AMD,nlyq265,"Ah - that actually sounds amazing ngl, defo going to be looking at reviews!",AMD,2025-10-29 06:10:06,0
AMD,nlytfyo,"What advantage does the AI Pro card have over the 7900XTX, cause you can do AI, video editing on it, too?",AMD,2025-10-29 06:44:44,2
AMD,nlyt907,"You can do same on 7900XTX, I already tried it.",AMD,2025-10-29 06:42:45,2
AMD,nm6n2uq,"It's not for regular desktop use like the 7900XT where you would use just one card. It's for workstations where you can fit several, or even datacenters.",AMD,2025-10-30 12:57:42,1
AMD,nlx1tui,Can you do a review? Either by video or text. Would be awesome.,AMD,2025-10-28 23:28:29,7
AMD,nm21v78,"It will, but bear in mind your 9070XT likely has a triple-slot cooler and a higher TDP than this card will. This is dual-slot, in a blower, and capped at 300W. 9070XTs regularly get near 340W. This will also likely get above its rated 2350mhz clock, and it's rated up to 2920mhz. But, Asrock rates their Taichi 9070XT at 2570mhz base and 3100mhz boost. That puts the 9070XT at a roughly 6% advantage if both are hitting their max clocks. Not much and I expect slight driver differences to exaggerate it a bit too. Based on my experience with the W7900DS (dual-slot N31 48GB) I expect this one to maintain around 280W under load and probably sit in the 2800-2900mhz range.",AMD,2025-10-29 18:50:50,2
AMD,nm1l4i3,"Don't get too carried away, they want you to pay AI fanatic money. Double the memory equals double the price you know.",AMD,2025-10-29 17:32:47,4
AMD,nm1xuwm,RDNA4 ML performance is significantly higher than RDNA3.,AMD,2025-10-29 18:32:04,3
AMD,nm1df7f,"It does have ECC VRAM, for one. Also more VRAM, 32GB vs 24GB. Newer architecture that is better at raytracing and machine learning. And expanded support presumably, like with any of the Pro parts. Unless you mean the W7900 version of the 7900XTX, which features 48GB VRAM. But that one is also twice the price at least.",AMD,2025-10-29 16:56:48,2
AMD,nlzrm86,"There are few key differences.  a) First, PCIe 5.0 interface rather than 4.0. So technically double the bandwidth if you DO need to send any data to the card during your wokloads.  b) 32GB VRAM vs 24GB IS a major difference for the use case. You can barely fit 16-17GB model on a 24GB VRAM card once you also count in the context size. But 32GB? Easy.  c) Blower style cooler and 2-slot height, AMD is generally speaking expecting most target users to be buying **two** of these. It's going to be hard with 7900XTX.   d) a bit lower power consumption.  Honestly it's not a bad value proposition at it's price. Yeah, you are paying double for the same specs as 9070XT, just twice the VRAM. But as far as AI premium goes it's genuinely pretty reasonable compared to most other options.",AMD,2025-10-29 11:58:46,1
AMD,nm766h1,Still slower no matter where you put it,AMD,2025-10-30 14:36:32,2
AMD,nm24u29,Given it's not far off the 5080 in price depending on where you are (+$135 over the FE near me) the price actually seems downright reasonable for a 32GB card. I'm mostly interested in this as a CFD card where Vram capacity is king. For $1300 this sure as hell beats the 5090's value and having twice the memory over a 5080 is very attractive. Biggest competitor in my book is either a pair of B50s or a B60 Dual.,AMD,2025-10-29 19:04:42,3
AMD,nm26j4s,"Yes, but it has lower memory bandwidth, 640GB/sec vs 960GB/sec, lower number of compute units.  In some areas it has advantage, in others it lacks...",AMD,2025-10-29 19:12:53,1
AMD,nm0c0d8,"You can use riser cables for 7900XTX if you cant put two next to each other. A really fast ssd reads at 14GB/sec, so not much advantage from PCIe 5.0. You dont load models that often, typically you load one and start using it. With 3 fans and better heatsink, gpu can use more power and boost clocks to give you most performance when you need it. Huge price increase for just 8GB extra, is not worth it. You could go with 48 or 64GB if you really need it, there is always going to be a model that you cant fit in vram.",AMD,2025-10-29 13:56:28,1
AMD,nm4o8f3,"My thoughts exactly, more V-ram for cheaper/same price = more better.  Now that real-time rendering in games is a debate between 140 fps or 130 fps (ignoring the dumpster-fire games that run at 30 fps wether you like it or not, despite looking like someone rubbed vaseline all over the camera, those failed tech demos don't count in my books) the V-ram has become the real selling point of the card. Not only does it give you more versatility in productivity scenarios, but it also makes games run more smoothly at the same time (and in some instances, makes the difference between a game even running or not at all).  Honestly would be nice if we could swap out our V-ram, or had unified ram and could use our M-board ram for the GPU as well.  Or better still, dedicate 2 ram slots to the CPU, and the other 2 ram slots (which are usually empty and useless since you lose performance running 4 sticks in most scenarios nowadays) are dedicated to the GPU as MOAR V-RAM.  That would actually be epic and massively increase the adaptability and versatility of each individual GPU, which would also increase competition too (meaning cheaper GPU's that are also significantly better).",AMD,2025-10-30 03:04:58,0
AMD,nm2cyhg,Each compute unit is significantly faster on RDNA4 and far more versatile for ML workloads. Memory bandwidth comparisons are meaningless as they’re different architectures and RDNA4 is more memory efficient than RDNA3.,AMD,2025-10-29 19:43:51,2
AMD,nm0egor,">A really fast ssd reads at 14GB/sec, so not much advantage from PCIe 5.0  It's 64GB/s vs 32GB/s (at x16 speeds). I am not saying you should stream data from your **SSD** into a GPU. More like RAM -> GPU. In some situations it will double your results. Admittedly for LLMs inference PCIe bandwidth is not THAT important (x4 4.0 so far seems to be sufficient) but it could make a difference in training.  >gpu can use more power and boost clocks to give you most performance when you need it  For LLMs it's primarily memory bandwidth that matters. Higher boost clocks mostly increase your electricity bill. It also means more heat and with a triple fan design bottom card just blows all the air back into your system.  >You can use riser cables for 7900XTX if you cant put two next to each other  You can if you have a massive case or are literally plugging video cards outside of it. On the other hand, consider a relatively cheap TRX50 board:  [https://www.gigabyte.com/Motherboard/TRX50-AERO-D-rev-10](https://www.gigabyte.com/Motherboard/TRX50-AERO-D-rev-10)  You can shove 2-3x R9700 without any fiddling.  >Huge price increase for just 8GB extra, is not worth it  If you are after 24GB cards then you shouldn't buy 7900XTX either to begin with. You go on ebay and hunt for 3090s. Arc Pro B60 is an option too if you find it at a reasonable price.  32GB is a different story. Right now this is the cheapest card offering this much. Whether you personally find it worth it or not is a different story but as far as workstation grade hardware goes... honestly it's not **that** much of a markup. Just look at how much you pay for a consumer grade 5090, let alone an RTX Blackwell.",AMD,2025-10-29 14:08:55,1
AMD,nm529gz,"I definitely agree on wishing vram was upgradable by the user. Perhaps in the future something like LPCAMM could provide that suitable interconnect. A 128-bit interface per module would mean that even the biggest modern busses only need to fit 4 of them.  As for unified memory, I don't think many people on these forums will like the road that takes us down. Like it or not, the future (at least near future) of that path is things like Lunar Lake, Apple Silicon, and Strix Halo. Those are all very impressive SoCs, but you may have to trade the ability to upgrade your GPU separately from the CPU to get your upgradable unified memory.  As for dedicating certain slots on the motherboard to each side, CPU and GPU, that gets you the worst of both worlds in some ways. Your memory is no longer unified, the full bandwidth is not directly accessible to either in most architectures, but you can upgrade it. If you are going to go down the route of integrating the CPU and GPU to use the same memory and connections, give both access to all of it in full bandwidth.",AMD,2025-10-30 04:46:42,2
AMD,nm2q8sx,"Isn't AI memory intensive? Doesnt bandwidth matter? You can have faster compute units, but they can be memory starved. Also 7900xtx has more compute units, this should make up for some of the speed gap.",AMD,2025-10-29 20:45:41,1
AMD,nm1a9hk,"I'm with you on this. At 32GB+ your options are slim right now. The B60 Dual is the only other thing I find particularly attractive that is somewhat near this, though that is 2x24GB per card rather than a single 32GB. The W7800 also offers 32GB on a 256-bit bus, but this card is substantially faster than that and currently cheaper.",AMD,2025-10-29 16:42:09,2
AMD,nm5j350,"How have I not hears of LPCAMM until now!?  Yeah, in an ideal world you would have M-board that has 3 separate chip sockets, one for your CPU, one for your unified RAM, and one for your GPU chip. It would probably look something like 3 CPU sockets.  Which would also make cooling very easy and scalable, something our current M-board layout didn't account for. They did account for development add-on cards however so it was a necessary step and an integral part of computer design history. However now that we've optimised general use computers to CPU, GPU, and RAM (ignoring storage, keeping that scalable with addon slots is definitely the way) I do definitely feel that new layout standard is due.  Of course that's an ideal world where CPU, GPU, and RAM manufacturers all agree on the same socket, we'd probably need some kind of modular M-board that allows the use of whatever Intel, Nvidia, AMD, etc.... GPU, CPU, and RAM sockets each company invents, otherwise you'd get stuck in a closed ecosystem where you're at the whim of the manufacturer of wether you're allowed to have more RAM or a more powerful GPU or a more efficient CPU or not.  Though it's nice to dream of a more optimal computer layout (of course my personal take is to have the CPU and GPU on opposite sides of the M-board, that way they each get their own separate airflow, and also the backside of each socket/slot can get some extra passive cooling which would help with the higher and higher clock speeds our chips are now reaching).  It is the worst of both worlds, however it is highly scalable and if the GPU had it's own dedicated memory controller then it could work as an alternative where you have double the memory, but each chip only sees half of that memory. I imagine in high-demand dynamic workloads where the CPU wants more RAM one second, then the GPU wants more ram the next, that having separated RAM for each chip could become a benefit and act as a safety margin. Though I do agree that it's probably not the best idea and would require just as much of a redesign as unified RAM would.  Unless you just put RAM stick slots on the GPU (probably a really good use case for LPCAMM as that would allow you to maintain the same backplate clearance standard current GPU's are built to) then independent scalable RAM would just be a direct upgrade to our current layout as-is.  Though I definitely want discrete chips on an M-board with unified memory (and probably some extra silicone chip, hell you could probably split the traditional rasterisation GPU chip and the RT/AI/Tensor/etc... chip into separate sockets) cause that would mean having 3-4 AIO's on your board with nothing else and that would just look too cool imho.",AMD,2025-10-30 07:29:00,1
AMD,nm2yoxs,"RDNA4 has 4x the INT8 performance without sparsity enabled, 8x with it, so the gap is enormous. You are obsessed with bandwidth and ignoring things that matter more like the implementation of Matrix cores.  Plus, RDNA4 has 2x the PCIe bandwidth.",AMD,2025-10-29 21:25:44,1
AMD,nm6nh8n,I'm fairly sure capacity outranks bandwidth in most cases.,AMD,2025-10-30 12:59:54,1
AMD,nm6oe0a,"Then you can easily go with those mac systems with 128GB unified memory and not use a gpu anymore. (they are quite good at running LLMs, but mem speed is around 200GBs/sec)",AMD,2025-10-30 13:05:00,1
AMD,nmykflk,Can some AMD staff please clarify what RDNA1 and 2 users AREN'T getting anymore in comparison to before?? That would be more concise than the vague statements published before.,AMD,2025-11-03 22:29:57,15
AMD,nmx9pn0,"Imagine cutting driver support for hardware that is still being sold, today, right now. What kind of business does that?",AMD,2025-11-03 18:39:58,127
AMD,nmwlter,I am fully with GN and HU on this. This notice provides no promises and AMD’s intention behind the driver notice could not have been more clear. I will need to see significant work towards continuing day 1 driver game support before I go anywhere near an AMD GPU again. Can’t risk it if this 7900XTX is going to suffer the same fate as the 6950XT just did.,AMD,2025-11-03 16:46:23,270
AMD,nmykcwf,AMD doing the weaponized incompetence again. Uff.,AMD,2025-11-03 22:29:32,10
AMD,nmxip35,I’ll tell AMD what I am as a 6950 owner - I’m pissed off!,AMD,2025-11-03 19:23:46,41
AMD,nmyhmfu,Corporations have lost it. They all think they’re Apple now. “You’re holding it wrong.”  Same mentality.,AMD,2025-11-03 22:14:51,11
AMD,nmx7ooj,"My wife has a 6800xt. if driver supports ends, i can already see something green on the horizon",AMD,2025-11-03 18:30:04,49
AMD,nmx8s7a,"AMD make the drivers on Windows open-source, pretty sure the community will fix your fuckups",AMD,2025-11-03 18:35:24,20
AMD,nmxs0d8,"I’m so frustrated by AMD. Right when I talk myself into switching from Nvidia for better Linux drivers, AMD pushes me right back into Jensen’s greedy arms.",AMD,2025-11-03 20:09:16,12
AMD,nmywz9k,Just bought a 5070 instead of 9070. Literally swayed me.,AMD,2025-11-03 23:38:12,4
AMD,nmy8ydr,Laughing while using and open source for gaming and I have for years now.  Often it seems gaming on Linux is actually smoother than with windows now.,AMD,2025-11-03 21:31:04,2
AMD,nmypbhk,"There still random driver timeouts with newer games sometimes they are acknowledged and take forever to get fixed, and sometimes they are ignored and ignored forever, its getting really annoying and the alternative is even more annoying especially if you have OCD and repeatly checking the 12vhpwr connector, being a PC gamer is very depressing last couple of years.  You can claim to do day 0 driver support for games but AMD is failing even on this simple task on some games and its getting annoying.",AMD,2025-11-03 22:56:35,2
AMD,nmytr9x,"If they want to resolve it a sensible decision Would-be is if they committed to to do the same level of day one game driver support for x years, (for 5000, 6000)  Then that might be enough to satisfy the situation?  Lacking that the only way to tell if they're not just ""It's totally not in maintenance guys"" is to actually have the drivers recombined.",AMD,2025-11-03 23:20:37,1
AMD,nmz8b8a,AMD does not have the market share to even be delusional about this. Corporations will corporations ig,AMD,2025-11-04 00:42:57,1
AMD,nmzanxd,"I get that there are segments to this video, but i'm 3 minutes into this and the only thing I know for sure is GN were paid to sell a case of some type....The next section is called what happened so hopefully it calms down a bit and starts making sense here. Can't imagine this is going to be a good video bringing in viewers who are not already GN faithful.",AMD,2025-11-04 00:56:39,1
AMD,nmzcek1,"Guys, you aren't supposed to actually *EAT* the FUD, it says so right on the packaging.",AMD,2025-11-04 01:06:50,1
AMD,nmzf7rv,"it's funny.  one moment people thrash about nvidia for the practice+pricing and now amd. anyone jumping to intel GPU?  FYI memory foundry are shifting more of their capacity to delivering HBM memory for data centre if you have not read it.. RAM cost is going up fast.  are we going to complain about Samsung and Hynix next for not supporting gamers?  amd like any company has finite development resource, Wendel praised the pace of RoCM 7.x delivery and the AI dev ecosystem chasing CUDA.  where do we think that came from?  i expect the entire industry will pivot and we will hear more of this sort announcement.  those openai deals in billions over next couple of years..   AI made gaming industry tiny.",AMD,2025-11-04 01:23:25,1
AMD,nmzfuh7,"I have two 6800 XTs, both of my kids PCs have 3060 Tis which I was thinking about replacing with something from team red. Not anymore. Way to push me back to team green for my PCs and keep my kids there.",AMD,2025-11-04 01:27:08,1
AMD,nn07qwc,"its weird when mobile market is start to using AMD again after very very long time intel dominated, AMD suddenly cut the drivers support 🤣 even though most laptop still using Vega Based igpu and RDNA2",AMD,2025-11-04 04:16:56,1
AMD,nmxsau2,I do not care about day 1 drivers.,AMD,2025-11-03 20:10:39,-2
AMD,nmx0li1,"Honestly, as I've commented previously, this is just a nothing burger. 5000 and 6000 series are already very mature technologies, there are very little things left on the table to optimize for. AMDs biggest mistake is that they should've kept quite on this, and just include them in the driver notes just like what nvidia is doing. My 2070 is rarely getting any game optimization nowadays anyway.    I'll likely continue buying amd recommending AMD cards in the future, unless Nvidia finally picks up their price performance and Vram game",AMD,2025-11-03 17:56:08,-26
AMD,nmxi5ce,I can’t listen to audio right now could someone tell me whats up? I got a 7900xtx,AMD,2025-11-03 19:21:02,0
AMD,nmyvcst,"Man, this shit would happen after I already planned on buying my buddy's 6900XT🥲",AMD,2025-11-03 23:29:20,0
AMD,nmyyj4s,Hey amd we been gassing you up since Ryzen you thought you were invincible huh?,AMD,2025-11-03 23:46:41,0
AMD,nmxtcfh,"Amd gpus and drivers issues, a tale as old as time. I will rather buy a weaker nvidia gpu for the same price instead of dealing with a company that cannot write a proper driver at gunpoint.",AMD,2025-11-03 20:15:37,-5
AMD,nmyi3on,Might want to read this PC World article. [AMD continues support for older Radeon GPUs with driver updates](https://www.pcworld.com/article/2959622/amd-older-rx5000-rx6000-cards-will-have-an-optimized-driver-path.html),AMD,2025-11-03 22:17:24,-4
AMD,nmwpfgq,"I've lost all respect for tech jesus cus he has turned into a rat shill, he has no problem ripping companies left and right for every little misstake the make while making a 3 hour long ""investigation"" video on nvidias 12VHPWR connector burning up, and comming to the conclusion to be a user related problem.",AMD,2025-11-03 17:03:29,-111
AMD,nmyy7mt,"The reason why they are vague is because the precise truth would send many more people into rage.  With the vague statements, AMD gets some whiteknights defending them at least in the meantime.",AMD,2025-11-03 23:44:56,9
AMD,nmynuoz,"So it sounds like they're saying RDNA 1/2 will no longer receive feature updates, and game optimizations, but will still receive support for bug fixes. Basically they're putting them in maintenance mode, but they aren't EOL.  Which is entirely reasonable. AMD has a history of doing this, but I really think RDNA 1/2 are pretty rock solid, and don't require further driver optimizations.",AMD,2025-11-03 22:48:41,-2
AMD,nmxze23,"The nice thing about these threads is that it exposes all the bots and shills so I can tag them and know I don't have to bother reading their comments in the future. It's kind of tragically funny to think of the type of person who's replying to your comment and present in these posts, jumping to the defence of a multi-billion dollar company with the most flimsy and pathetic excuse for why they don't care about anti-consumer behaviour.",AMD,2025-11-03 20:44:39,26
AMD,nmxwf54,they didn't.,AMD,2025-11-03 20:30:23,-36
AMD,nmxclqf,No one. That's why was easy to understand it was a fake news.,AMD,2025-11-03 18:53:56,-55
AMD,nmxqjh7,Dgpus I don’t know that they are still selling it.,AMD,2025-11-03 20:02:13,-16
AMD,nmxmdx4,"And this is after amd told us how RNDA is built to get new features or performance boost in the future even when it gets old, and driver’s compatibility will be long lasting.   I feel this is their way to try and force people to upgrade since the market is stagnating.   GPU prices go up and people increased their upgrade cycles. Amd this hurts amd more than nvidia.",AMD,2025-11-03 19:41:56,49
AMD,nmxnp67,"it also basically kills the used market.    Who am I to recommend a gpu to a budget gamer, that does not recieve diver support?",AMD,2025-11-03 19:48:24,24
AMD,nmxpm7o,"I have a 6950xt, bought it 2 years ago. The fact they even considered this, let alone announced, then pretended to unannounce this notice makes me want to sell the card tomorrow and go team green. I am definitely never buying an AMD card again with the potential for support to end 3 years into it's entire lifecycle... or 2 years! after I bought it.",AMD,2025-11-03 19:57:47,42
AMD,nmyxcsy,"I’m on the same boat, if they don't change their minds, it will be my last AMD GPU.",AMD,2025-11-03 23:40:16,3
AMD,nmzitxc,Yeah. Very glad I went with an nvidia gpu,AMD,2025-11-04 01:44:51,3
AMD,nmzupnj,Engineering's patch notes said what was happening in plain English before marketing got a chance to look at it and sanitize it. Anyone that thinks they weren't being 100% serious in the first statement is fooling themselves,AMD,2025-11-04 02:53:50,3
AMD,nmx3kc9,"i already burried my plans for a amd gpu , iam rebuilding atm 2 pcs entirely and iam eying now with a nvidia gpu.  Maybe a 5070 or 5070TI yes its more expensive , but heck resale value is like 2x as good AND they wont drop major support after 3-4\~ years.  Heck even 2000 series got DLSS4.",AMD,2025-11-03 18:10:15,23
AMD,nmxfj4i,"Same here my dude, I love my 7900XTX but I'm considering going back to Nvidia if this is the game they want to play.",AMD,2025-11-03 19:08:07,7
AMD,nmwu1fl,![gif](giphy|Us4Q3yIswCNq6YJdXP|downsized),AMD,2025-11-03 17:25:19,4
AMD,nmx21f5,I’m sorry but what date did the 6950xt just suffer?,AMD,2025-11-03 18:02:54,-4
AMD,nmxwd6i,"what fate? nothing changed, lol, they just shuffled the downloads.  you people continue to be so obtuse.",AMD,2025-11-03 20:30:07,-8
AMD,nmym9ee,Lol. Calm down buster brown haha,AMD,2025-11-03 22:40:01,-5
AMD,nn087o5,Incompetence? It's more like malice at this point. They know what they're doing.,AMD,2025-11-04 04:20:11,1
AMD,nmxpyii,"as a 6950 owner I will tell them that I am very soon going to be a idk, 5070ti owner. Fuck this.",AMD,2025-11-03 19:59:26,20
AMD,nmz07sm,"I’m beyond pissed by this, if I knew they would try to pull this off I never would’ve bought a RX 6750 XT in the first place.",AMD,2025-11-03 23:56:02,1
AMD,nmzufo4,"Apple's share of the mobile market was *quite a fair bit higher* than the ~8% AMD is sitting at now for dGPUs, so that's an even greater level of insanity.",AMD,2025-11-04 02:52:11,1
AMD,nmxgqgg,"AMD struggled building driver confidence amongst its users for the past ~10 years, and at least for me personally, I was just starting to take them as a serious option in the GPU space.  Aaand back to zero they go.",AMD,2025-11-03 19:14:01,31
AMD,nmx8llx,interesting bit is the open source drivers will still get support even AMD abandons them the community will still continue. i never did get any of the newer AMD GPUs after the HD7000 series. never did fall in my lap. 6800xt would be fun a one to play with on open source drivers.,AMD,2025-11-03 18:34:31,6
AMD,nmxdvm2,Unfortunately the green horizon is a scam.,AMD,2025-11-03 19:00:01,4
AMD,nn08mzc,"Yeah.  I bought my husband a new (not used, not refurbished) 6600xt this time last year.  It was and remains, even with this change, a huge upgrade over the 1050ti he had before.  It's a good fit for his needs.  Bumped him from a R5 1600 processor to a R7 5700x at the same time.  Still, 6600xt was released only 4 years ago.  And there are even newer releases than that on RDNA2.  Things releasing *right now!*  It's too soon to be putting RDNA2 onto a maintenance branch.  These companies are definitely not our friends, I'm pretty unhappy with both nVidia and AMD right now.    But this will factor into what recommendations and purchases I make for friends and family going forward.  It's one thing to risk getting burned on something I get for myself, but for others?  Nope.  I need to be able to trust that what I recommend to them will be supported for a reasonable length of time.",AMD,2025-11-04 04:23:12,1
AMD,nmx82rn,Driver support is not ending,AMD,2025-11-03 18:31:59,-8
AMD,nmxp7l1,AMD would probably still need to do the releases even if the community did the work because of driver signing on Windows. They wouldn't want the main driver to be something where people would need to bypass Windows controls to install.  And that means they'd probably still need to test it before a release as their name would be on it so it would still need their support.  It sounds like a good idea in theory but I see it being not that great or a mess in practice.,AMD,2025-11-03 19:55:50,11
AMD,nmxd0lb,"The monkey's paw curls. AMD now only supports DX12 and now expects the OpenCL, OpenGL, and Vulkan compatibility pack (Mesa fork) to be installed from the Microsoft store.",AMD,2025-11-03 18:55:54,12
AMD,nmybrx4,"Might happen actually, or at least partially. AMDVLK on linux has ceased development in favor of helping with RADV. AMDVLK is what they use for vulkan on their windows driver as well. So maybe they switch to the open source RADV for their windows driver too?",AMD,2025-11-03 21:44:58,1
AMD,nmxyue8,Linux will be non-issue since FOSS drivers are still rocking older card support.,AMD,2025-11-03 20:42:02,23
AMD,nmz2qc6,9070 is a bad buy now considering it may be in maintanance mode is a year /s,AMD,2025-11-04 00:10:26,5
AMD,nmyxpgg,"It is wording situation always. Nvidia just calls it game ready drivers and say its supported until x time or something and everyone is happy. Branching older gen is still done. For some reason people still think AMD mean end of life and their GPUs will stop working or get worse.  I am not taking AMDs side in the fucked up communications they do. But so many people clearly have taken shit so wrongly its absurd as well.  In reality they should have just shut up with confusion and called that ""RDNA 1 and 2 are going onto their long term fine wine gamest readiest driver support"". Then everyone would have been happy. The branching off and not being able to get full and good functionality of latest features is understandable. Trying to be a bit too upfront about it and using vague terms that only their own engineers know the full picture of is not something they should come out with.  Now instead the best thing they can do is get an engineer or someone and have a long full explanation of how everything works so people can understand shit and so we can have something to refer to when people are thinking the world is over situation. And no dont have any of the marketing and pr people give statements or interviews. Give full information from a direct source. This vagueness of accusations back and forth wont stop now until something like that is done. Everyone is making their own conclusions of things.",AMD,2025-11-03 23:42:10,3
AMD,nmzerew,"Well, as long as some rando online doesn't care none of us should!",AMD,2025-11-04 01:20:44,3
AMD,nmx2x8h,"> My 2070 is rarely getting any game optimization nowadays anyway.   Your 2070 released what like 7 years ago? And Nvidia isn't still putting out Turing products. It's not been actively sold in ages, and it still got some of DLSS4 functionality.   AMD is still releasing and actively selling RDNA2 products.",AMD,2025-11-03 18:07:09,33
AMD,nmx1ycf,"How is it even possible you think this was even a fine decision to begin with? You do realize rdna 2 was the competitor to geforce 30 series right? Cutting support this early for gpus they are still selling is an awful decision. It's bad for the consumers and it's bad for amd if they want to keep growing market share. People are gonna think twice from now from buying their gpus if they next they are on the chopping block this soon.   It's bad for the consumer but it's also bad for radeon, which is the exact point that gn and hub have been trying to say. This will only make people want to buy geforce more which is a bad thing. Nvidia is incredibly greedy and making them even more of a monopoly than they already are is a horrible thing...  > AMDs biggest mistake is that they should've kept quiet on this  What kind of corperate bootlicking comment is this? Are you an amd employee or something? I don't see how secretly cutting support on rdna 1/2 and not telling anyone when it happens is good for anyone. People will be even more mad when they find out.",AMD,2025-11-03 18:02:29,22
AMD,nmx5re7,The amount of posts on this in this sub and the Radeon sub is insane. It’s nearly as many as people asking what model of 9070 they should get,AMD,2025-11-03 18:20:46,2
AMD,nmx25u2,"the dumbest thing is over the past literally like 15 years, AMD doesnt' need day one drivers. I still remember witcher 3, AMD's like day 3 driver worked basically identically to the 3 months old driver, no bugs, no problems. Nvidia, pre game driver, buggy, day one driver, buggy, iirc it had flickering and lag, day 2, day 3, day 5, etc, it took like 1-2 weeks for Nvidia guys to have a driver that wasnt' buggy as shit.  I think maybe AMD needed a driver to make physx hairworks not destroy performance and that's just straight sabotage shit like nvidia ask them to massively overuse tessellation and ahead of time know to limit it in the driver and AMD don't know to limit it ahead of time.  I can't remember the last game that AMD needed a game specific driver to not be awful while Nvidia have that issue all the time. NVidia tend to break driver 'rules' a lot and need a lot more optimisation while AMD maybe leave performance on the table but tend to follow the rules and so most games work in my experience.",AMD,2025-11-03 18:03:30,5
AMD,nmx8v2v,"> there are very little things left on the table to optimize for  maybe , but theres still tons of things to fix.  and to improve.",AMD,2025-11-03 18:35:48,2
AMD,nmx8wr9,"i believe drivers get tweaks to better run newer games due tto the odd ways devs process the gfx from time to time. however the open source drivers will live on, even after AMD stops. open source will still get tweaks from the community.",AMD,2025-11-03 18:36:01,1
AMD,nmz98m3,Hopefully it you have any friends they know not to listen to you for advice,AMD,2025-11-04 00:48:22,1
AMD,nmxaiai,\>vram game  AMD's top dog has only 16GB of vram.,AMD,2025-11-03 18:43:49,0
AMD,nmx1kb8,But AMD sucks!  And booooo!!! Thumbsdownmeanface.  You must join the ragebait bandwagon.,AMD,2025-11-03 18:00:38,-12
AMD,nmyoixm,AMD are partially dropping driver support for RDNA 1/2.,AMD,2025-11-03 22:52:18,2
AMD,nmya7ri,maybe give us more information,AMD,2025-11-03 21:37:19,0
AMD,nmz5jhf,"Oh please dont, this is dumpster value now aftee this debacle.",AMD,2025-11-04 00:26:42,0
AMD,nmz34mr,Ryzen isnt really that good. People seem to forget about Zen5%. Its just thta Intel is really bad now but when Nova Lake lauches with Celestial at least it will give AMD a run for its money and at least then the corporate rot in AMD will finally realise their mistake.,AMD,2025-11-04 00:12:42,1
AMD,nmyyx19,"You might want to watch GamersNexus video this thread is referencing instead of you reposting regurgitated AMD statements that have already been dismantled by GamersNexus in said video.  I read the article you sent. The website has no comment section which means there's no pushback, and the article is written in the most generous, corporate-favoring manner the author could muster up for the occasion. Bunch of BS",AMD,2025-11-03 23:48:49,7
AMD,nmz8yua,AMD is not your friend,AMD,2025-11-04 00:46:45,0
AMD,nmwqb3e,"You haven't watched any of his other videos about Nvidia, and it shows",AMD,2025-11-03 17:07:38,54
AMD,nmwqjfk,"So he rips companies apart, and is a shill... At the same time. Got it.",AMD,2025-11-03 17:08:44,78
AMD,nmwsua0,Someone is not a shill for not coming to  the conclusion you wanted them to at the start of an investigation.,AMD,2025-11-03 17:19:37,30
AMD,nmx6lud,"I believe you are flat out wrong. He said user can contribute, but the main issue is the connector is flat out crap.",AMD,2025-11-03 18:24:51,7
AMD,nmxt4up,"Well that conclusion was valid because the investigation showed that the cables were not inserted all the way in, which is user error.  He then also pointed out that if the user error rate is so high then it becomes a design issue.",AMD,2025-11-03 20:14:38,3
AMD,nmxaj39,Your head has to be exhausted from all of those mental gymnastics.,AMD,2025-11-03 18:43:55,7
AMD,nmycs1x,"The more I read the opinions of tech nerds, the less I think we're an inherently smart group.",AMD,2025-11-03 21:49:54,1
AMD,nmyg1ao,"[You do realise that Sapphire, an AMD only board partner is also using this same connector on some of their cards.](https://videocardz.com/newz/second-sapphire-radeon-rx-9070-xt-suffers-burned-12v-2x6v-connector) What say you about that, this isn't just an NVIDIA thing anymore. Honestly, the connector is an issue and GN established that in his video. However, an issue presents itself most when user error is involved which is what that first 12VHPWR video was about. GN is not a shill and calling out AMD here is a good thing. It's not like GN hasn't also rightly criticised NVIDIA too, he has hence why he's blacklisted by NVIDIA.",AMD,2025-11-03 22:06:27,1
AMD,nmztkmi,"Given the general sentiment of comments over here, where white knights are perhaps overrepresented if anything... I think the difference is negligible despite what armchair theorizing and common sense might suggest. AMD has certainly burned a lot of goodwill in the past couple of days.",AMD,2025-11-04 02:47:10,1
AMD,nmypxyc,"AMD still sells new parts with RDNA2 graphics. Getting full driver support for more than just 5 years would've been nice. AMD used to provide that full support for longer too.  So no, I don't think that's reasonable at all. It's also unreasonable that AMD doesn't release the INT8 version of FSR4, i.e. as ""FSR4 lite"" or an experimental feature for RDNA2. That FSR version has production-level quality in most games - the effort and investment is wasted.",AMD,2025-11-03 22:59:58,14
AMD,nmyykwz,"> Which is entirely reasonable  How in the hell is it reasonable for AMD to drop support for graphics cards released in 2020 (with some new models as recently as October 2023), while the late-2018 released Turing architecture Nvidia RTX20 GPUs just got DLSS4 Transformer model this year and those cards continue to receive fresh new drivers addressing various issues as they arise?",AMD,2025-11-03 23:46:58,2
AMD,nmy1why,"Yeah that's what I've said in the other thread. Boggles my mind. They should be demanding better quality of software support for the amount of money the GFX manufacturers charge these days, not defending their shady practices.",AMD,2025-11-03 20:56:33,9
AMD,nmzovdn,"Lmao I know right, especially if you hop over to the Radeon subreddit you can find soooo many trying to defend AMD on this it's insane.  Nothing short of putting these GPUs back onto the main driver branch is a full walkback, and I think even then doing it to just RDNA2 and keeping RDNA1 separate would make people happy enough as DX12 Ultimate still makes sense as a clear boundary between full support and more limited, though existing support.   But ughh, the best AMD has done so far is tell HUB that RX 5000/6000 will get game optimisations at the same time as the main branch, it doesn't say every game gets optimised but what they will optimise will be at the same time... at least for now until AMD decide they can get away with discontinuing it.  What sucks though is still no INT8 FSR4 on RDNA2, none of the new Vulkan instructions that the latest drivers proudly put near the top to then sneaky make a note below says for 7000 and 9000 series only. And of course the driver app as of now on the RDNA2 branch didn't get the tweaks the main branch did this update that ancient gameplays showed in his video.... yeah, yayyy.",AMD,2025-11-04 02:19:37,1
AMD,nmy376a,"They absolutely did, and still are.",AMD,2025-11-03 21:02:47,20
AMD,nmye3x3,"In China there's recently new RDNA2 cards like the [RX 6600 LE](https://videocardz.com/newz/another-radeon-rx-6600-le-graphics-card-has-been-released-in-china) and the [6750 GRE](https://videocardz.com/newz/amd-rx-6600-stock-runs-dry-in-china-the-company-shifts-focus-to-radeon-rx-6750-gre-10gb). I know 2024 is last year, but I don't think customers would like ""new"" products to lose driver support and be put into the maintenance branch after a year or so.",AMD,2025-11-03 21:56:31,6
AMD,nmxuxql,"It depends on the market. Here in argentina you can still find new [rx6600](https://compragamer.com/producto/Placa_de_Video_Asrock_Radeon_RX_6600_8GB_GDDR6_Challenger_D_12947?cate=62&sort=lower_price), [rx6800xt](https://compragamer.com/producto/Placa_de_Video_Asrock_Radeon_RX_6800_XT_16GB_GDDR6_Phantom_Gaming_D_OC_12696?cate=62&sort=lower_price), [rx6400](https://fullh4rd.com.ar/prod/23298/video-radeon-rx-6400-4gb-sapphire-pulse-lr)",AMD,2025-11-03 20:23:17,10
AMD,nmyf4l1,"dGPUs are still being sold in certain places, to my understanding at least Navi 23 (RX 6600 - 6650 XT) is still manufactured today according to TPU, but there's also cards like the 6750 GRE series that was launched two years ago in 2023. More importantly there's chips like the Ryzen Z2A, Z2 Go that were released \*this year\* with RDNA 2 iGPUs.",AMD,2025-11-03 22:01:41,1
AMD,nmyy6io,When did they say these things.  Not saying you're wrong but I don't remember that and I was paying attention during the lead up to the XTX or at least I thought I was.,AMD,2025-11-03 23:44:46,4
AMD,nmza49a,I'm not so sure this wasn't the intent behind the whole thing.,AMD,2025-11-04 00:53:29,3
AMD,nmznxf7,"still runs fine on linux, im sure.",AMD,2025-11-04 02:14:09,-1
AMD,nmygkne,"This. Granted much of my driver support on Linux comes from the opensource maintainers of RADV and MESA (with a healthy helping from Valve), but some comes from AMD involvement and losing that will undoubtedly skew the project toward newer GPUs.",AMD,2025-11-03 22:09:22,12
AMD,nmyz2qj,"I feel the same, I bought an RX 6750 XT in 2023 and now 2 years later with all this mess I wish I didn’t have bought it.  I’m planning an upgrade next year, but I don’t intend to buy another AMD GPU, I’m not going to risk losing support after 3-5 years of my GPU lifetime again.",AMD,2025-11-03 23:49:41,1
AMD,nmxq6ud,"To be fair, when you bought it Rdna 2 was already 3 years old.",AMD,2025-11-03 20:00:31,-18
AMD,nmyweob,go ahead and sell your playstation then while you're at it.,AMD,2025-11-03 23:35:06,-2
AMD,nmxg5wy,In other news 2000 was so ahead of it's time they are basically the same arch. It is easier for them to get better performance when it comes to dlss.,AMD,2025-11-03 19:11:12,-17
AMD,nmxw1pk,not cool,AMD,2025-11-03 20:28:37,-4
AMD,nmx8r00,"6000 series got already put on maintenance tree of drivers.     So for amd its 3-5\~ years , for nvidia 7-9\~ years but with new features like DLSS4.",AMD,2025-11-03 18:35:15,34
AMD,nmyvwmb,"You simply either can't comprehend or refuse to acknowledge that ""maintenance branch"" exists for a reason.  This isn't AMD doing RDNA2 users a favor, dude. This is AMD saying ""we are no longer committed to supporting your GPU with our software updates"", and anything you get from now on are just scraps.  If it means nothing like you say then they wouldn't announce it. The fact they had to announce it (for legal reasons) should be a huge neon sign that points out why this does in fact significantly change things for RDNA2 users going forward.  A few months from now a game could come out and AMD might not optimize it or fix some critical visual issues on RDNA2's maintenance driver branch. RDNA2 users no matter how much they cry will be told ""well we told you, maintenance branch baby"", then a few months later maybe AMD finally gets around to fixing the issue but that won't be a reason to celebrate.  Why shouldn't RDNA2 users be upset about AMD telling them ""yeah yeah whatever, buy a newer card fool""?",AMD,2025-11-03 23:32:21,3
AMD,nmyhqft,"pretty much this , the time they get the driver ""kinda"" fine ( i mean theres still many oudated or broken features like FRTC is simply horribly outdated with how it works even amd vik said it )  they pull this stunt and ruin all the goodwill they built up.",AMD,2025-11-03 22:15:27,9
AMD,nmxh698,"I love open source, GCN1 from 2012 still being supported with Vulkan 1.3 via RADV and DX12 via VKD3D, even ray tracing is emulated on HD7000 series",AMD,2025-11-03 19:16:11,8
AMD,nmyhrty,Bit prizier but way better support.,AMD,2025-11-03 22:15:39,8
AMD,nmxclu7,"Many people consider ""maintenance mode"" and ""driver support ending"" to be very similar.",AMD,2025-11-03 18:53:57,17
AMD,nmyfab6,"Then driver support for the GTX 900 and GTX 10 series might as well not be ending either. They're still getting security updates for 3 more years, just no more game-ready drivers.",AMD,2025-11-03 22:02:31,1
AMD,nmybcls,In fact they've shut down their proprietary linux vulkan drivers to supposedly put all their linux effort into helping the community open source vulkan driver.,AMD,2025-11-03 21:42:53,10
AMD,nmz4ly6,Ikr?,AMD,2025-11-04 00:21:15,1
AMD,nmzwabx,"Nvidia's ""Game Ready Drivers"" still brings the latest features to older GPUs, contingent on hardware support for said features.. Turing has access to DLSS 4 transformer models. It's not on a maintenance branch like you're trying to pass it off as.",AMD,2025-11-04 03:03:10,0
AMD,nn05mwd,It's just not that big of a deal.,AMD,2025-11-04 04:02:36,1
AMD,nmx4y1w,"I'm just using 2070 as an example cuz that is the card I have. My friend has 3060 and 4070. They aren't seeing a magical 10% on driver improvement either.   Nvidia is also actively seeking GTX 1650 laptops, so your point being?",AMD,2025-11-03 18:16:53,-4
AMD,nmx6ww7,"I'm personally a software engineer, and honestly I 100% get the reason AMD is doing so, and I can tell you Nvidia is doing the same thing but they kept it quiet.   AMD's ""fine wine"" was because back then the driver was shit on release, they have to take years for it to extract maximum performance. Now the driver already gets like 95% performance on release, there are very little thing to further optimize for. There will not be any more manic driver that finds 10% more performance on your 6700XT for instance.   Separating the driver out means less chance of the driver breaking older cards. Putting the newer architecture on the ""bleeding edge"" branch while putting the older cards on ""stable"" branch is absolutely the right thing to do.   In fact this should be the selling point for older RDNA2 cards. If you just want a good solid and stable cards that just plug in and forget it, RDNA2 is the card to get now",AMD,2025-11-03 18:26:21,2
AMD,nmx3w9t,"We just going to gloss over the times AMD's had flickering lighting in some games, broken API support, etc. over the years?   Like yeah <any> cards with proper API support more or less can do most titles without a ""game ready driver""... until they can't because something new is leveraged or because of a bug or whatever.   Funny though how every time AMD drops the ball somewhere it's ""Nvidia's fault"". Must be Nvidia that made them lag on API support too! Nvidia that made them handle the Radeon VII how they did as well.",AMD,2025-11-03 18:11:50,9
AMD,nmzf809,took nvidia a week to release fixed drivers for Fallout 4  I don't remember anyone complaining about it,AMD,2025-11-04 01:23:27,1
AMD,nmxag4s,"Hence maintenance mode, bug fixes and minor improvements",AMD,2025-11-03 18:43:31,4
AMD,nmxbf6x,"Actually, 288GB if you can afford the MI355X   VRAM numbers along doesnt tell anything without the price. You get more VRAM for a lower price",AMD,2025-11-03 18:48:14,2
AMD,nmx40il,"issue is , nvidia gives 7\~ year old products new features , amd buries them at 3-4 years WHILE actively selling them still.",AMD,2025-11-03 18:12:23,8
AMD,nmypdet,Thats extremely disappointing my gpu isnt even old i swear you cant trust any companies in hardware anymoreZ,AMD,2025-11-03 22:56:53,1
AMD,nmz290r,Amazing the pushback from AMD fanboys have been great. The amount of copium going in this community is just sad. The days of competition is over its just Nvidia foe GPUs and AMD for CPU and soon Intel will hopefully destroy this scum of a company in CPUs as well. Tired of their anti consumer BS.,AMD,2025-11-04 00:07:41,1
AMD,nmz29wb,"It refers to [Tom’s Hardware](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-its-clarifications-on-controversial-rdna-1-and-2-driver-note-company-will-continue-game-optimization-support-after-all), they have a comment section. I try not depend heavily on information from places like YouTube that depend on manufactured anger for clicks.",AMD,2025-11-04 00:07:49,1
AMD,nmxilo3,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-11-03 19:23:18,0
AMD,nmyskco,Shilling for the rats.,AMD,2025-11-03 23:14:10,1
AMD,nmwr0ft,An Nvidia shill to be precise. He acts as the good guy fighting for the people why while puting all reponsability on burning GPUs on the user,AMD,2025-11-03 17:11:00,-66
AMD,nmwvbid,[https://www.youtube.com/watch?v=eFYR1yn7Ivs&t=536s](https://www.youtube.com/watch?v=eFYR1yn7Ivs&t=536s),AMD,2025-11-03 17:31:23,-21
AMD,nmytctw,"I was there Frodo.  I was there the day Steve tried to reproduce the burn down and couldn't unless he barely plugged it in.  He did utter, in the black tongue, ""user error"" and those words echoed from post to post across the internet.  And Jensen looked on and laughed.",AMD,2025-11-03 23:18:25,1
AMD,nmximrb,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-11-03 19:23:27,1
AMD,nmwv4my,"[https://www.youtube.com/watch?v=NgJgoIWP9fA&t=5s](https://www.youtube.com/watch?v=NgJgoIWP9fA&t=5s) this dude repairs gpus for a living and one can easily tell its not a end user mistake that the connector are melting, there is a video on his channel where he pulls a BUCKET full of melted connectors from repairs he has done.",AMD,2025-11-03 17:30:28,-7
AMD,nmyujht,"Which turned out to be incorrect as plug after plug as burned across different AIBs, cable vendors, even AMD cards.  It was never user error.  His video failed to reproduce the problem, but that didn't mean the problem didn't exist without being partly plugged in, it only meant he didn't reproduce the problem in his sample size...of one card.  He is a significant part of why Nvidia is still using the cursed plug because he distracted people with ""user error"".",AMD,2025-11-03 23:24:52,1
AMD,nmxho8j,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-11-03 19:18:39,1
AMD,nmyqv4z,"Hey, whatever you think. This is what AMD does, and has done in the past. They don't have the time, money, manpower or market share to keep releasing optimized game drivers for their old GPUs.  If you want longer driver support, look at getting an Nvidia GPU next time.",AMD,2025-11-03 23:04:59,-4
AMD,nmzxrj4,"AMD is a small struggling indie company. Please understand. Also please ignore the 421 billion dollar market cap, it's all Nvidia propaganda.  No really, I don't get how people are making excuses for a $421 billion company.",AMD,2025-11-04 03:12:04,1
AMD,nmzve0f,"You know, I've been seeing a number of white knights trying to imply that those complaining about it don't actually own AMD GPUs, but I wonder if these white knights actually do own an AMD GPU. You'd think actual owners would be the ones who are pissed, while the ones unaffected by any of this would naturally have the composure to defend such bullshit.",AMD,2025-11-04 02:57:50,1
AMD,nmyhebj,In germany too theres tons and tons of 6000 series availaible NEW.,AMD,2025-11-03 22:13:41,6
AMD,nmxzx29,he meant rdna2 cards are still in production  TLDR; you can find a rdna2 card manufactured in 2025,AMD,2025-11-03 20:47:09,1
AMD,nmzahyf,which is so stupid because why would i buy a amd card new when i know i cant sell it for anything in 3 years,AMD,2025-11-04 00:55:41,5
AMD,nmyalms,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-03 21:39:12,0
AMD,nmzp7n0,"lol, I bought a 6800XT and it is currently sitting in a drawer and I'm still using my Geforce 1070, I wanted to upgrade my egpu which I use with a Legion Go but AMD software/driver support has compatibility issues with their own products. Garbage GPU division in my opinion.",AMD,2025-11-04 02:21:35,1
AMD,nmxtopb,That only serves as an argument to keep RDNA2 supported for longer,AMD,2025-11-03 20:17:15,20
AMD,nmxqpj9,"so? The product is only 3 years old. Imagine Nvidia sunset 4090 support after 3 years,",AMD,2025-11-03 20:03:01,17
AMD,nmy11ht,"To be fair, AMD's current flagship is only maybe 5% faster...",AMD,2025-11-03 20:52:29,5
AMD,nmz7mgd,"Turing released BEFORE Radeon VII aka GCN 5.1 did. Back then [AMD dismissed features like DLSS](https://www.pcgamesn.com/amd/amd-dismisses-nvidia-dlss-harsh-scaling) and, for reasons that I don't know, also deemed DX12 Ultimate support unnecessary so that meant that RDNA1 was doomed to be a stillborn from the start but they still ignored AI upscaling for another generation by not including AI accelerators/NPUs in RDNA2 (they did remember to add RT cores though).  You have to admit that AMD was very short-sighted back then and now they are paying the price in PR damage.",AMD,2025-11-04 00:38:55,4
AMD,nmzchj1,"Exactly, it isn’t like AMD didn’t see the writing on the wall, especially by the time RDNA 2 dropped. They knew damn well that they were the market follower and still decided to forego dedicated hardware for upscaling in their 2020 release. Not only did they do it at that point, but they did it a-fucking-gain with RDNA 3 in 2022.   AMD has had 6 whole years since Turing in 2018 to incorporate a common architecture focused around compute hardware and gaming hardware in consumer GPUs, but instead they sat with their thumbs up their asses trying to make a software filter competitive until March of 2025 and now their 2022 and earlier customers have no official support. Even Intel beat them to the punch with XeSS, doing what AMD’s GPU department was too smoothbrained or chickenshit to do with their GPUs and dropping GPUs with dedicated hardware on top of an upscaler with hardware supported and fully software branches.",AMD,2025-11-04 01:07:18,2
AMD,nmyqrn7,"Let's be real here , amd could support fsr4 on rdna 2 as it already runs on it , heck even nvidia users can force fsr4.  They just don't want to.  They behave like they are the absolute marketshare owner while they are not.",AMD,2025-11-03 23:04:27,1
AMD,nmyqopy,Neat way of saying nvidia has been rereleasing the same gpu for 5 years... and AMD still can't catch up,AMD,2025-11-03 23:04:01,-3
AMD,nmxut6s,"Honestly thought AMD walked it all back, did not realize 6000 is actually on maintenance, my bad. Kinda sucks but I mean it’s much much better than fully dropping support, as someone who just purchased a 6900xt I don’t really give a shit if the drivers are maintenance or game day ready, as long it gets updates I’m happy",AMD,2025-11-03 20:22:40,-6
AMD,nmzfue3,"Yeah, the Radeon = bad drivers sentiment was just starting to go away and then they decide to just tank confidence. At the very least, when your competitor dominates the market with 90 something percent of the marketshare at a minimum you have to do rough parity with them when it comes to support. The Nvidia 10 series is just now losing full support after 8 years, so that's the target they need to hit.",AMD,2025-11-04 01:27:07,3
AMD,nmxjdlp,too bad the newest i have is HD4850. would be cool to play with. i did snap a first gen ARC GPU though. not much time to game and fix my shit boxes to get to work though.,AMD,2025-11-03 19:27:06,2
AMD,nmxdeez,Except it's not and it's a big misconception,AMD,2025-11-03 18:57:43,-9
AMD,nmykmeo,"I mean, my friend is still rocking a GTX 1070 and he has no plan to upgrade anytime soon.",AMD,2025-11-03 22:30:58,-2
AMD,nmys0yk,"There is a more nefarious way to interpret that action...  Shut down their driver you say? Support the ""community"" you say?  Just a joke, I don't interpret it that way.",AMD,2025-11-03 23:11:16,2
AMD,nmxv96i,"> Nvidia is also actively seeking GTX 1650 laptops, so your point being?  GTX 1650 is Turing. nVidia is not dropping support for those yet.",AMD,2025-11-03 20:24:49,8
AMD,nmx8n9n,"There are no magical 10% improvements anymore.   But Support means that if a card is getting some wierd results in a particular game, they will try to the best of their ability and responsible to make it work.",AMD,2025-11-03 18:34:44,18
AMD,nmx8x6x,Driver updates and day 1 optimizations isnt about getting a magical 10% performance uplift  Its about ensuring the game properly supports the card and doesnt crash or work poorly due to drivers,AMD,2025-11-03 18:36:05,9
AMD,nmxc47u,"> Nvidia is also actively seeking GTX 1650 laptops, so your point being?  I think you're confusing some stock still being in retail channels with actively selling and producing something. Look at AMD's APU releases. RDNA2 is still very much ""current"".",AMD,2025-11-03 18:51:36,5
AMD,nmxcnm9,"> I'm personally a software engineer, and honestly I 100% get the reason AMD is doing so, and I can tell you Nvidia is doing the same thing but they kept it quiet.  Turing a 2018 architecture got DLSS4 support this year (just not the full featureset). AMD's sidelining stuff they're still actively putting in APUs.   It's not exactly the same thing even if people spin it as such.",AMD,2025-11-03 18:54:11,12
AMD,nmx7ehf,I have switched to AMD since the 6700XT and literally never encountered those issue. Neither have I seen it on my last Nvidia cards the 2070.    And honestly these problems are more on the game developers if anything,AMD,2025-11-03 18:28:42,2
AMD,nmx4vpa,"> Funny though how every time AMD drops the ball somewhere it's ""Nvidia's fault"".  no one said this.  >Like yeah <any> cards with proper API support more or less can do most titles without a ""game ready driver""... until they can't because something new is leveraged or because of a bug or whatever.   Yes, that's the point of my argument. Nvidia is often fucking with not standard application of api support so they have way more need for game ready drivers and AMD has way less need.  ""but sometimes a bug occurs"", yeah, that's life, and yet most games I've not had that issue and a bug can be introduced into a game for launch, or on the third or 30th update.  >We just going to gloss over the times AMD's had flickering lighting in some games, broken API support, etc. over the years?   no, i just said what I said, i can't remember the last time I played a game and required a new driver because it had an issue while this happens much more regularly for Nvidia. I didn't say they never had any issues ever.  Also lagging API support is weird when AMD usually leads onto new APIs and standards.",AMD,2025-11-03 18:16:34,-1
AMD,nmzgysz,"that's one of hte best things Nvidia does. You post a driver issue on the nvidia sub here, or every nvidia sub on every forum for hte last twenty years you mostly find them deleted and getting told to go to nvidia's own forums. It hides the issue from being as public because instead of complaint threads in every nvidia forum, you get nothing but positive news and all the bad news is only on an nvidia forum where no one but current owners go. It's smart, but if you're just casually seeing threads from various subreddits, negative nvidia ones barely ever pop up for this reason.",AMD,2025-11-04 01:33:49,2
AMD,nmxekd9,Which didn't happen yet... And will likely have a even lower chance.  And some things need full rewrites of the amd driver.,AMD,2025-11-03 19:03:23,-1
AMD,nmypvif,Your GPU is RDNA 3. It isn't affected by this.,AMD,2025-11-03 22:59:36,2
AMD,nmz4955,"Don't believe your eyes and ears, the corporate message is the only true message.",AMD,2025-11-04 00:19:12,0
AMD,nmwuzr2,are we watching the same videos? He trashes the 12VHPWR connector every single time it comes up in his videos.   Always calls it a shit design and has tons of videos shitting on Nvidia shitty practices. He called out nvidia for trying to push the DLSS+Framegen into his GPU reviews.   If I remember correctly he made a complete separate video shitting on Jensen for the 5070=4090 claim.   He had another video where he shits on nvidia bad practices with DerBauer.,AMD,2025-11-03 17:29:51,43
AMD,nmwsa0r,"He literally countless times called the 12VoltWhateverTheFuck connector a flawed design pushed to its limits with barely any room for error.  He ripped Nvidia for their 5060 review blockade and for shrinking die sizes for each class while prices go up.  And VRAM too.  But yes, he is, indeed, ""an Nvidia shill"", he is closer to being AMD or Intel shill then Nvidia, if I didn't know better  I would think Jensen fucked his wife.",AMD,2025-11-03 17:16:58,31
AMD,nmy8xsa,You are blind. PC Jesus raked Nvidia across the coals all this year.,AMD,2025-11-03 21:30:59,2
AMD,nmx18ge,"Someone is not a shill for not coming to the conclusion you wanted them to at the start of an investigation, even if someone disagrees with them.",AMD,2025-11-03 17:59:05,13
AMD,nmwvkp0,"The theme of this video isn't Nvidia.  Read again, the topic is a news worthy statement by AMD. There is plenty of negative coverage from GCN against Nvidia. Even on this video he takes his time to specifically shitnon NVidia too.",AMD,2025-11-03 17:32:38,15
AMD,nmwwx5n,"Also the video that you linked shows mostly 3rd party connectors melting.  12vhpwr and it's successor are absolutely a terrible connector implementation and standard, but the issue just isn't that widespread. They do require user training and care which is obviously not ideal. Oh and there are now AMD cards that use the same connector, for that matter.",AMD,2025-11-03 17:39:04,7
AMD,nmyrlnx,"I will certainly not buy nvidia because I don't want to support them. As a company they're even worse, esp. when it comes to product pricing and market oversegmentation.  AMD has had record revenue and profits for a while. The resources to keep properly supporting the older cards are available, but not utilized due to corporate greed.",AMD,2025-11-03 23:08:59,3
AMD,nmzezm4,"Yeah, you save around $100 or something vs buying the Nvidia card but you could lose way more than that in a few years when you put it up for resale.",AMD,2025-11-04 01:22:05,1
AMD,nmxsx6r,Nvidia are just clever about it. They keep cards in the drivers but they aren't working on performance for old cards.,AMD,2025-11-03 20:13:37,-4
AMD,nmzba7a,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-04 01:00:13,1
AMD,nmzd4gb,"The bill always comes due, AMD never misses an opportunity to miss an opportunity. Why engineer cards around dedicated upscaling hardware that your more successful competitor has begun engineering theirs around when you can chucklefuck around with a software filter for over 6 years instead?",AMD,2025-11-04 01:11:03,1
AMD,nmzmeul,Hard to have a good nearsight when your whole Vega lineup was running late on production.,AMD,2025-11-04 02:05:25,1
AMD,nmzjznp,"DLSS 1.0 looked like shit, to be fair. NV was able to fomo devs in into doing NV free labor for years via their market share until they could finally inject the transformer model on top of it all.",AMD,2025-11-04 01:51:32,0
AMD,nmz1plz,That doesn't change what I said.,AMD,2025-11-04 00:04:35,2
AMD,nmz1mtj,You can interpret however you want.,AMD,2025-11-04 00:04:08,2
AMD,nmycdmf,"> Honestly thought AMD walked it all back   No they reworded it to sound different while being the exact same message, and also managed to sound super insincere in the process.    That's one thing that annoys me about AMD is the way they will word a media statement that sounds like they think they're sooo clever that they can word something in a way that will fool us. Whoever wrote the statement has written for them before and is both smug and overestimates their own skill.   (annoys me about any company obvs but they have their own special flavour of it)",AMD,2025-11-03 21:47:55,14
AMD,nmygay7,">walked it all back  The sad thing is thats just marketing talk for ""we dont want a shitstorm so we reword it in marketing nonsense so it might go over""",AMD,2025-11-03 22:07:55,7
AMD,nmysseg,Its been on maintenance for while now. When is the last time you saw a driver that said performance improvements for that gpu in the release notes?  RDNA 2 is already 5 years old they are not getting any more performance out of it. Right now is just keep it stable and running.,AMD,2025-11-03 23:15:21,3
AMD,nmxx616,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-03 20:34:00,1
AMD,nmxjn5d,As someone that owned Vega and the VII... it's really not a misconception with AMD.,AMD,2025-11-03 19:28:24,25
AMD,nmyx5a7,"There's no misconception going on here.  If the RDNA2 GPU architecture doesn't get critical fixes or optimizations for some future new games as soon as newer GPUs, then the game will play inferior on RDNA2.  If the game plays inferior on RDNA2 then your semantics won't fix the game for RDNA2 users. Only AMD updating the driver will fix their game. Which could happen at any date since AMD is now saying ""we might fix it if we feel like it, but we might not and you will deal with it""  Maybe, just maybe, they moved RDNA2 to maintenance branch so that they **don't have to rush and fix the game for RDNA2 GPUs**. Which is what we're saying is happening here.  Since they sent out a PSA, yet useful tools like you defend AMD's position when it's time to protest, then a few months from now RDNA2 users' complaints will go nowhere. That's what we're dealing with.",AMD,2025-11-03 23:39:08,2
AMD,nmytu7w,"I've got an RX 470, don't play new games (so game supporting drivers aren't much of a problem for me, they're a problem for other reasons) and won't be upgrading for a fair bit longer.  Doesn't make it any less fucked for AMD to do to an architecture that's 5 years old, with the newest dGPU released 2 years ago, and that's still used in the newest APUs (ex. Z2 Go, Z2A, which are used in 2025 handhelds) the same that NVIDIA's doing to 11 & 9 year old architectures, and only carefully reword their intent in a sugar-coated way after people caught the release notes & their full intent.",AMD,2025-11-03 23:21:03,0
AMD,nmyuh33,"Lol the same thought passed through my mind too, the difference here is that basically no one actually used amdvlk because community RADV is faster 99% of the time. And they do already contribute to RADV afaik.",AMD,2025-11-03 23:24:31,2
AMD,nmxas5n,"Hence maintenance mode, bug fixes and minor improvement",AMD,2025-11-03 18:45:09,8
AMD,nmzbuzj,What do you think day zero support and bug fixes means?,AMD,2025-11-04 01:03:38,1
AMD,nmxcvgq,We already know AMD is actively developing FSR4 for RDNA2. And Turing doesnt get full DLSS4 either,AMD,2025-11-03 18:55:13,3
AMD,nmx9cp4,">no one said this.  Your tessellation rant was bordering on it. Hell AMD/ATI had hardware tessellation first, it's amazing how they still managed to get ""blindsided"" by it... for multiple years in a row.   > Yes, that's the point of my argument. Nvidia is often fucking with not standard application of api support so they have way more need for game ready drivers and AMD has way less need.  Tell that to everyone still on last december's Nvidia driver where things still run fine.   >Also lagging API support is weird when AMD usually leads onto new APIs and standards.  Yeah AMD was a real trendsetter on some of those Vulkan extensions and DX12U... Last time they trendset was with Mantle over a decade ago. It was good work that provided a good basis for what followed but that's a fair bit different from ""leading"".",AMD,2025-11-03 18:38:12,1
AMD,nmz4w31,"I don’t care either way. Just be careful not to make a mountain out of a mole hill, there are bigger things in the world to worry about.",AMD,2025-11-04 00:22:54,2
AMD,nmz5z9o,Its even worse than that the corporate message usually hides their intention which in this case is to probably cut support for these GPU and setting precedent for the non-UDNA GPUs as well.,AMD,2025-11-04 00:29:17,1
AMD,nmyxe6a,">They do require user training  The fact you and other still think ""if only I push it in hard enough, I'll be safe"" is a serious problem.  Plugs welded into the socket have shown it still happens even when 100% plugged in.  The cables have melted on the PSU side, due to unbalanced loads.  Nvidia adapters have melted, it isn't incompetent 3rd parties.  The entire design of this PCIe standard is a hazard.  The government hasn't forced a recall because it is only damaged cards, no home fires.",AMD,2025-11-03 23:40:28,1
AMD,nmyt02d,"At the end of the day, none of these companies are your friend. They just want your money.  I don't tie myself to one brand. If a company has something I want or need, and there are no other better alternatives, I'm buying it. I don't care what people think. In my opinion, that's the best way to go about things in life.",AMD,2025-11-03 23:16:30,3
AMD,nn08dl2,"It's even worse when you consider that the $100 in savings never materialized for quite a large chunk of people until recently as far as this generation was concerned, and that's if you could find any for sale to begin with in the early months.",AMD,2025-11-04 04:21:21,1
AMD,nmyh22k,">Nvidia are just clever about it. They keep cards in the drivers but they aren't working on performance for old cards.  Ofc thats why all the old gpus get all the time new features like DLSS4 , their Shader and ingame presets and more.  because they only leave them in the driver notes ...  sorry but no atleast inform yourself.",AMD,2025-11-03 22:11:55,10
AMD,nmz5oqi,My RTX 2070 Super has the exact same drivers as the RTX 5090 and it was a card that released the same month as the first RDNA 1 cards (rx 5700 and rx 5700xt).,AMD,2025-11-04 00:27:33,3
AMD,nmy1aet,man the mental gymnastics on show right here,AMD,2025-11-03 20:53:40,6
AMD,nn09z4b,"Whatever you think about his morality, Jensen is a savvy businessman. He invested into CUDA long before its impact would be known. He entered the ML market early with forays into the automotive sector. He took a risk with bringing RT/Tensor cores to consumer cards instead of simply splitting the product lines as was the case for the 16 series. Most CEOs wouldn't have bothered with any of that. I wouldn't say AMD so much missed an opportunity than simply did what most other normal businesses would do. Turing era RT and DLSS was more of a meme than anything else.",AMD,2025-11-04 04:32:51,1
AMD,nmyr40o,At this point AMD can't get out of it without proof of exactly how many engineers are tasked with rdna2 support.,AMD,2025-11-03 23:06:20,1
AMD,nmzk931,FUD addiction,AMD,2025-11-04 01:53:00,1
AMD,nmz1hib,The problem is that they will do the samd to RDNA3/4. Whos not to say in a year time RDNA3 will be deprecates then following then RDNA4. As consumers we should petition and get the word out that such scummy companies sgould bot be supported at all!,AMD,2025-11-04 00:03:17,2
AMD,nmzx410,"He is still actively playing new games, and so far he has no issue.",AMD,2025-11-04 03:08:08,1
AMD,nmxbtzw,"> Your tessellation rant was bordering on it.  so i didn't, glad you confirmed that. Therefore you brought it up for no reason because you didn't have an actual argument to make about the fact that AMD has had incredibly steady drivers for well over 10 years and the concept of a 'game ready' driver was both made by Nvidia, but because it was needed by Nvidia.  >Hell AMD/ATI had hardware tessellation first, it's amazing how they still managed to get ""blindsided"" by it... for multiple years in a row.  Having your competitor pay a game dev to over tessellate shit to fuck with them is just them being dicks. I also didn't say they were blindsided by it multiple years in a row. Nvidia pushed for physx hairworks and as usual numerous times, forced inferior implementations for competitors that used worse extensions to cost more performance.  >Tell that to everyone still on last december's Nvidia driver where things still run fine.   where did I say it was still an issue? I've had cards from both brands and stayed relatively up to date on things for the past 30 years. For the past 15 years Nvidia have had way more problems with driver bugs on new games, this is a simple fact, even Nvidia users have long since given up pretending otherwise in general.   Why do you keep trying to bring up things I haven't said and use them as an argument point? Seems like you have an agenda.  >Yeah AMD was a real trendsetter on some of those Vulkan extensions and DX12U... Last time they trendset was with Mantle over a decade ago. It was good work that provided a good basis for what followed but that's a fair bit different from ""leading"".  they led the way with to the metal apis, they led the way with most memory formats for gpus in the past 20 years. They led the way with HBM and interposers, they've led the way with chiplets.  DX12 has changed numerous times with various version and different feature sets and again AMD met the most up to date ones while Nvidia often lagged behind with a few features.",AMD,2025-11-03 18:50:14,2
AMD,nmz6c43,"Sorry but there are people who spent the equivalent of $300 for RX 6750 GRE that was released 24 months ago.  How the hell is it ""making a mountain out of a mole hill"" for them to demand longer priority driver support than just 24 months since their graphics card was launched?  >I don’t care either way.  Yes, I can see that.  Because it didn't happen to you.  Or maybe because you're wealthy enough to where you're upgrading every couple years anyway.  Either way you shouldn't champion or even defend AMD doing this. Absolutely disgraceful behavior from the so-called ""GPU underdog"" corporation.",AMD,2025-11-04 00:31:23,3
AMD,nmz3zlx,In this case this debacle would make your choice much simpler /s.,AMD,2025-11-04 00:17:40,1
AMD,nmzbe9r,Even if Transformer is slower on the Turing architecture you still have access to it and you’re not on some custom driver branch either…the copium is real with these damage controlling fanboys.,AMD,2025-11-04 01:00:53,1
AMD,nmzj9j3,Just because they put the whole if-else chain into the same package doesn't by itself mean very much.,AMD,2025-11-04 01:47:20,1
AMD,nmz0wec,None no more resources into RDNA2 was working thwre in the past. I suspect RDNA3/4 support will be dropped shen UDNA comes.,AMD,2025-11-03 23:59:56,2
AMD,nmxi5u1,"> make about the fact that AMD has had incredibly steady drivers for well over 10 years  Dude I owned Polaris, Vega, and the VII. Sell that shit to someone that will buy it. I had to bypass the global driver on some games to force an older driver that didn't have regressions. It was a coin toss if fucking OpenCL worked. The drivers hit an all time low around RDNA1 when they put out a ""christmas driver"" and didn't run it through their vanguard program at all.   If you're going to fluff them up at least stick to something true like their majorly improved drivers post RDNA2/6000 series launch.   >I also didn't say they were blindsided by it multiple years in a row.  Didn't say you did. But they choked on it nonetheless for a long while. It took them how long to add a limiter?   >Having your competitor pay a game dev to over tessellate shit to fuck with them is just them being dicks.  Shall we revisit AMD's recent sponsorship debacle where they couldn't come out and give a clear cut answer even though their competition could? Where after a ton of criticism and about a month of silence all the sudden games stopped being FSR1/FSR2 only? Yeah both companies suck for the customer with some regularity.   >For the past 15 years Nvidia have had way more problems with driver bugs on new games, this is a simple fact, even Nvidia users have long since given up pretending otherwise in general.  You're being dishonest and your anecdote isn't fact. Yes Nvidia's drivers this last year have been a mess, but it's utter bullshit to pretend that AMDs were sterling pre-RDNA2.   >Seems like you have an agenda.  Mostly just tired of everyone making shit up to excuse a company that should be doing better. The other day someone here told me it was fine how they handled the VII ""because it was defect instinct dies and few people bought it"". The weird lovefest for a multi-billion dollar corp that keeps fumbling the ball in graphics is annoying.   >they led the way with to the metal apis  Mantle, over a decade ago now.  >they led the way with most memory formats for gpus in the past 20 years.  You realize they haven't lead anything in that area on consumer products in like... at least 6 years now right?   >they've led the way with chiplets.  Customers love beta-testing cost cutting measures that have lower efficiency and lower performance than expected yes. Are we going to now pretend the 7000 series rollout WASN'T problematic?  >DX12 has changed numerous times with various version and different feature sets and again AMD met the most up to date ones while Nvidia often lagged behind with a few features.  In 2019 AMD launched cards that couldn't even do current API standards. And it took them how many hardware cycles to squeeze out some decent perf for functions that are part of DX12U?",AMD,2025-11-03 19:21:06,1
AMD,nn07vmg,"By all accounts UDNA is going to be a clean slate design. If AMD can't even devote resources to older iterations of current architectures, just think of what would happen when the architecture itself is completely different.",AMD,2025-11-04 04:17:50,1
AMD,nmxm4gl,"> You're being dishonest and your anecdote isn't fact. Yes Nvidia's drivers this last year have been a mess, but it's utter bullshit to pretend that AMDs were sterling pre-RDNA2.   and this is where you are disingenuous. Saying that Nvidia drivers were worse for the past 15 years doesn't mean any of AMD drivers were sterling at all... one has NOTHING to do with the other, there are two completely separate statements and the fact you keep trying to argue one point with a completely different one says a lot.  Also pre rdna2... i had no real driver problems for years. Games work, it's that simple. You had to do something to go to an older driver because what performance reduced 5%, okay, how does that stack to a new game not working because Nvidia doesn't have a working driver that doesn't crash in a game. One is annoying but the game works fine, the other stops you playing a game.  I haven'd had problems playing any new games in the past 15 years with AMD, new or old driver, every single game worked. On nvidia that is simply not true and it's not just the past year, there have been better spells and worse spells, but this isn't a last year thing.   >Mantle, over a decade ago now.  yes, which led the way in to the metal apis. Mantle led to vulkan and DX12 and AMD was the lead on all three. When dx12 launched AMD supported every feature and Nvidia didn't, in fact when DX12 launched multiple original DX12 features were removed because despite years notice, Nvidia still didn't support them.   Things being nerfed so Nvidia isn't 'behind' is common.  >Customers love beta-testing cost cutting measures that have lower efficiency and lower performance than expected yes.  no one beta tested cost cutting measures. Every single factor on a cpu design is a compromise between efficiency, performance and cost. The simple fact is AMD has pushed CPU performance significantly and brought better chips at lower costs through these measures. The industry has adopted chiplets because it's a necessity as process nodes get smaller and many features don't shrink well and capacity/time to produce makes chiplets optimal.  Every single point you've made, every one, is just a whataboutism. Look at where I started, AMD haven't needed game ready drivers for literally over a decade because their drivers always work, you even somewhat agreed on this point yet felt the need to imply I said things to make random other arguments to insist things aren't as I painted them. Who cares about openCL? Literally, the thread and my comments were about having game ready drivers, can you tell me which games care about openCL? Because you have not one arguemnt against the thing I actually said you've tried to bring up every single possible other factor but even then you're mostly full of crap. I'm done.",AMD,2025-11-03 19:40:38,2
AMD,nmyltsc,"> and this is where you are disingenuous. Saying that Nvidia drivers were worse for the past 15 years doesn't mean any of AMD drivers were sterling at all... one has NOTHING to do with the other, there are two completely separate statements and the fact you keep trying to argue one point with a completely different one says a lot.  Literally prove it. Especially when compared with the drivers over Vega's troubled lifespan. And please not that one marketing source AMD paid a few years back to show off their drivers.   >Also pre rdna2... i had no real driver problems for years. Games work, it's that simple. You had to do something to go to an older driver because what performance reduced 5%, okay, how does that stack to a new game not working because Nvidia doesn't have a working driver that doesn't crash in a game. One is annoying but the game works fine, the other stops you playing a game.  Literally reported bugs to them like an entire game black screening and purple blobs because of driver regressions but do go on.   >I haven'd had problems playing any new games in the past 15 years with AMD, new or old driver, every single game worked. On nvidia that is simply not true and it's not just the past year, there have been better spells and worse spells, but this isn't a last year thing.  No matter how many times you chant it doesn't make it true. Some of us actually were there.   >yes, which led the way in to the metal apis. Mantle led to vulkan and DX12 and AMD was the lead on all three. When dx12 launched AMD supported every feature and Nvidia didn't, in fact when DX12 launched multiple original DX12 features were removed because despite years notice, Nvidia still didn't support them. > >Things being nerfed so Nvidia isn't 'behind' is common.  API adoption always takes years. If you thought every game was going to run out and do async compute because AMD's DX11 driver used to suck you missed the boat. It takes time for adoption. And yes it's also not unusual for things to target the market leader.   Had RTG not bungled so much shit over the years the market probably would be a bit more balanced. For the greater part of the last decade they've let Nvidia set the pace on everything. Every graphical feature they've introduced post-mantle is a response to something Nvidia did first. Antilag, upscaling, RT, recording, the list goes on and on.   >no one beta tested cost cutting measures. Every single factor on a cpu design is a compromise between efficiency, performance and cost. The simple fact is AMD has pushed CPU performance significantly and brought better chips at lower costs through these measures. The industry has adopted chiplets because it's a necessity as process nodes get smaller and many features don't shrink well and capacity/time to produce makes chiplets optimal.  Sorry I forgot AMD's awful model number scheme. I was referring to the RX 7000 series. Not the CPU 7000 series.",AMD,2025-11-03 22:37:35,1
AMD,nmyvmrj,"> Literally prove it. Especially when compared with the drivers over Vega's troubled lifespan. And please not that one marketing source AMD paid a few years back to show off their drivers.   no thanks, go ahead and prove otherwise. Go on nvidia's own forums, they go around taking over every public forum and sending everyone with actual driver problems to their own forums because no one else goes there, great way to hide visible issues.  For years, nvidia had worse issues, they had hitching for like 3 years and every thread was only on nvidia's forums, they got removed everywhere else. at the same time my amd cards had zero issues playing games on game ready drivers.  >Literally reported bugs to them like an entire game black screening and purple blobs because of driver regressions but do go on.   wow, you had an issue, and i didn't, but do go on. You seem to think that's a valid argument, it's not. You don't think an nvidia user had an issue at the same time, really?  >No matter how many times you chant it doesn't make it true. Some of us actually were there.   doesn't matter how many times you chant the opposite, doesn't make that true, some of us where there.  Once again you didn't make an argument that AMD had far fewer issues with new games, which is the argument you made, you keep making arguments that they had some issues... as if I claimed otherwise. The actual argument i made, again, was that AMD basically didn't need game ready drivers because for the most part AMD drivers just worked in new games without the need for a new one and this was very rarely the case for Nvidia for a lot of the time. You've failed to argue, even once, that THAT argument, the one I actually made, was false, you keep going off on tangents that make it very very obvious what you are doing.  >API adoption always takes years.  and?  > If you thought every game was going to run out and do async compute because AMD's DX11 driver used to suck you missed the boat.  i never implied that at all, once again you're making weird arguments of nothing I claimed.  What I said was entire generations came out where AMD supported a whole bunch of extra DX12 features and that microsoft literally nerfed DX12 for launch to have less features 'required' which caused a lot less games to support them as Nvidia failed to get those features working in time.  What are you arguing here? How is anything you said remotely an argument against what I actually said?  > It takes time for adoption. And yes it's also not unusual for things to target the market leader.   no one said otherwise, but again AMD had newer  >Had RTG not bungled so much shit over the years the market probably would be a bit more balanced.  once again, what part of what I said makes this remotely relevant?   >For the greater part of the last decade they've let Nvidia set the pace on everything.   that's what the market leader generally is able to do, again what relevance does that have to anything I've said? Basically the one actual argument being made you agreed with then you just throw shit at the wall to win some other argument that only you want to have.  >Sorry I forgot AMD's awful model number scheme. I was referring to the RX 7000 series. Not the CPU 7000 series.  i still fail to see where you have a point. I have a 7900xt, it's worked perfectly since launch, i've never had to install a new driver for any game and there are no games it's had issues with. I have a strong reason to believe if they didn't release a driver for the next 2 years... that the current driver would work fine in every new game out in that period because that's generally how amd cards have worked for the last 15 years.  How is it a beta test when it's a final product and works great? because you say so?   It's incredibly obvious how biased you are and how you cant' stand someone said that AMD drivers just generally work with very few issues and game ready drivers just aren't a necessity while for Nvidia they have almost always been needed... and they've failed to deliver working ones time after time.  Now i did block you before but reddit block is incredibly buggy, tried again, hopefully it works this time.",AMD,2025-11-03 23:30:52,1
AMD,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,11
AMD,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,3
AMD,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
AMD,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,1
AMD,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
AMD,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
AMD,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
AMD,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
AMD,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
AMD,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
AMD,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
AMD,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
AMD,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
AMD,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
AMD,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
AMD,ngieos7,And largely against the non-x3d lmfao.,Intel,2025-09-27 17:21:03,80
AMD,ngif1q6,Aren't they just showing that AMDs CPUs are better for gaming?,Intel,2025-09-27 17:22:52,32
AMD,ngmmadi,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Intel,2025-09-28 10:33:19,5
AMD,ngiqxv3,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Intel,2025-09-27 18:23:57,-15
AMD,ngp4a4b,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Intel,2025-09-28 18:56:21,0
AMD,nglqard,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Intel,2025-09-28 05:26:07,31
AMD,ngiw9gz,I assume they compared with CPUs in a similar price range,Intel,2025-09-27 18:52:05,33
AMD,ngl774g,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Intel,2025-09-28 02:59:30,4
AMD,ngj2d2a,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Intel,2025-09-27 19:24:33,53
AMD,ngmt8qo,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Intel,2025-09-28 11:35:30,8
AMD,ngn0xy1,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Intel,2025-09-28 12:33:09,2
AMD,ngix2qg,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Intel,2025-09-27 18:56:22,32
AMD,ngiy5wa,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Intel,2025-09-27 19:02:07,9
AMD,ngir4n9,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Intel,2025-09-27 18:24:57,5
AMD,ngj6exq,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Intel,2025-09-27 19:46:01,2
AMD,nhi1lee,"Now install windows 11, lol",Intel,2025-10-03 06:08:42,1
AMD,nglqeum,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Intel,2025-09-28 05:27:08,0
AMD,ngkj8l9,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Intel,2025-09-28 00:25:26,13
AMD,ngkwq9d,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Intel,2025-09-28 01:51:37,21
AMD,ngmy2qz,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Intel,2025-09-28 12:12:58,3
AMD,ngmx0bu,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Intel,2025-09-28 12:05:06,1
AMD,ngn1g9m,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Intel,2025-09-28 12:36:42,4
AMD,ngiyobq,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Intel,2025-09-27 19:04:51,-6
AMD,ngizavn,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Intel,2025-09-27 19:08:11,3
AMD,ngjh5f6,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Intel,2025-09-27 20:42:11,3
AMD,nhk7a3s,I did same performance on all processors.,Intel,2025-10-03 15:24:53,1
AMD,ngm7i8q,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Intel,2025-09-28 08:05:53,4
AMD,ngkykmi,That sounds like an AMD Stan argument circa 2020,Intel,2025-09-28 02:03:12,16
AMD,nh261w2,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Intel,2025-09-30 19:23:27,1
AMD,ngn25ws,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Intel,2025-09-28 12:41:35,1
AMD,ngjdprn,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Intel,2025-09-27 20:24:13,10
AMD,ngj1pgu,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Intel,2025-09-27 19:21:01,6
AMD,ngkqt0z,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Intel,2025-09-28 01:12:59,1
AMD,ngj49y7,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Intel,2025-09-27 19:34:44,3
AMD,ngk1b4v,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Intel,2025-09-27 22:35:49,3
AMD,nhka12e,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Intel,2025-10-03 15:38:08,1
AMD,ngmkwf8,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Intel,2025-09-28 10:19:51,11
AMD,ngnw7zu,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Intel,2025-09-28 15:28:44,4
AMD,ngl05xq,Expand ?,Intel,2025-09-28 02:13:17,-2
AMD,nh27g09,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Intel,2025-09-30 19:30:10,1
AMD,ngtvgdv,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Intel,2025-09-29 14:03:26,7
AMD,ngw45vk,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Intel,2025-09-29 20:34:34,0
AMD,ngjg69y,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Intel,2025-09-27 20:37:03,4
AMD,ngqfmrw,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Intel,2025-09-28 22:53:49,1
AMD,ngk26op,did you do it,Intel,2025-09-27 22:41:09,1
AMD,ngmi765,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Intel,2025-09-28 09:53:01,1
AMD,ngk22ke,can you reset settings then choose ray tracing ultra preset.,Intel,2025-09-27 22:40:28,2
AMD,nhle3iw,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Intel,2025-10-03 18:52:42,1
AMD,ngnx0bd,because they exclusively exist in DIY build your pc enthusiast bubble,Intel,2025-09-28 15:32:30,5
AMD,nhvyteo,Pricing was aggressive. A 12 core 3900x was 400 usd.,Intel,2025-10-05 13:26:56,1
AMD,ngl3zfu,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Intel,2025-09-28 02:38:06,14
AMD,nh2848m,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Intel,2025-09-30 19:33:29,1
AMD,ngtxj1d,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Intel,2025-09-29 14:14:15,3
AMD,ngmif3t,"Okay, I did it",Intel,2025-09-28 09:55:13,2
AMD,ngmglra,"No, I didn’t remember good",Intel,2025-09-28 09:37:10,1
AMD,ngk41a9,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Intel,2025-09-27 22:52:22,2
AMD,nhp053x,Thanks for solidifying opinion that your benchmarks are fake,Intel,2025-10-04 10:05:07,1
AMD,nh2a34y,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Intel,2025-09-30 19:43:15,1
AMD,ngk5zrq,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Intel,2025-09-27 23:04:17,2
AMD,nhp2exv,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Intel,2025-10-04 10:27:16,1
AMD,ngbpsza,Cam someone confirm or is this gas lighting?,Intel,2025-09-26 15:51:04,21
AMD,ngbym0c,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Intel,2025-09-26 16:33:40,17
AMD,nghesqk,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Intel,2025-09-27 14:17:43,3
AMD,ngc7w1b,Intel comeback real?,Intel,2025-09-26 17:18:16,6
AMD,ngf1ik5,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Intel,2025-09-27 02:43:51,2
AMD,ngbsck8,3D v-cache has entered the chat.,Intel,2025-09-26 16:04:06,8
AMD,ngbr9eb,Take it as a grain of salt. Intel marketing LOL,Intel,2025-09-26 15:58:30,1
AMD,nh5ixeo,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Intel,2025-10-01 08:33:03,1
AMD,ngfguk7,Thats cool ...but lets talk about better pricing.,Intel,2025-09-27 04:36:12,1
AMD,ngfrla1,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Intel,2025-09-27 06:09:56,3
AMD,ngc573e,Tech Jesus has entered chat :).,Intel,2025-09-26 17:05:31,-10
AMD,ngcjbbq,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Intel,2025-09-26 18:13:09,-12
AMD,ngbqjhe,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Intel,2025-09-26 15:54:51,42
AMD,ngdvx9l,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Intel,2025-09-26 22:23:43,11
AMD,nge3sfi,What do you mean by gaslighting in this case?,Intel,2025-09-26 23:10:51,4
AMD,ngcf9aj,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Intel,2025-09-26 17:53:19,1
AMD,ngcutw5,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Intel,2025-09-26 19:10:03,1
AMD,ngl3adb,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Intel,2025-09-28 02:33:31,1
AMD,ngdfut5,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Intel,2025-09-26 20:55:42,1
AMD,ngbxbws,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Intel,2025-09-26 16:27:28,-6
AMD,ngfqkoh,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Intel,2025-09-27 06:00:33,2
AMD,ngfebe1,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Intel,2025-09-27 04:16:03,2
AMD,ngfqbry,Nova Lake bLLC about to ruin Amd X3D party.,Intel,2025-09-27 05:58:18,1
AMD,ngc2ju0,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Intel,2025-09-26 16:52:57,18
AMD,ngc2czl,I always wondered if Intel marketing budget is higher than the R&D budget,Intel,2025-09-26 16:52:01,-8
AMD,ngfrkpn,Intel Arrow Lake is much cheaper than Amd Zen 5.,Intel,2025-09-27 06:09:47,5
AMD,ngemp1j,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Intel,2025-09-27 01:07:45,7
AMD,ngeo8em,only an AMD fan would worry about replacing their shit CPUs under 3 years,Intel,2025-09-27 01:17:36,0
AMD,ngbzwzr,Hardware unboxed isn't a reliable source.,Intel,2025-09-26 16:40:06,8
AMD,ngf1ob8,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Intel,2025-09-27 02:44:54,8
AMD,ngealuz,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Intel,2025-09-26 23:51:44,1
AMD,nge8xbh,Telling people that its performance is better than it actually is?,Intel,2025-09-26 23:41:39,3
AMD,ngca7el,The ones with similar pricing not performance,Intel,2025-09-26 17:29:11,7
AMD,ngigkrj,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Intel,2025-09-27 17:30:41,2
AMD,ngfrgqn,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Intel,2025-09-27 06:08:45,0
AMD,ngerbdr,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Intel,2025-09-27 01:37:32,1
AMD,ngezf04,Quite common for AM4 in my experience.,Intel,2025-09-27 02:29:45,0
AMD,ngihhii,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Intel,2025-09-27 17:35:17,1
AMD,ngerg53,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Intel,2025-09-27 01:38:23,0
AMD,nggftxh,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Intel,2025-09-27 10:08:25,-1
AMD,ngeozwu,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Intel,2025-09-27 01:22:31,-2
AMD,ngg1fuo,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Intel,2025-09-27 07:42:27,-1
AMD,ngerrz8,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Intel,2025-09-27 01:40:28,9
AMD,ngeao9a,Sooo they are in the YouTube space for the money not for the love of tech,Intel,2025-09-26 23:52:08,4
AMD,ngfq1bg,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Intel,2025-09-27 05:55:39,2
AMD,ngdp9bd,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Intel,2025-09-26 21:45:51,-3
AMD,ngc0yus,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Intel,2025-09-26 16:45:15,11
AMD,ngeb3z7,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Intel,2025-09-26 23:54:48,8
AMD,ngmlzg8,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2025-09-28 10:30:22,1
AMD,ngtp3t3,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Intel,2025-09-29 13:28:22,1
AMD,nh5i8gf,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Intel,2025-10-01 08:25:38,1
AMD,nh5il7n,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Intel,2025-10-01 08:29:26,1
AMD,ngc36bx,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Intel,2025-09-26 16:55:54,4
AMD,ngcbde9,Sure but charts seem about right to me,Intel,2025-09-26 17:34:45,1
AMD,ngcxbac,APO is game specific. I'm referring to what has changed overall.,Intel,2025-09-26 19:22:34,3
AMD,nil3hc6,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Intel,2025-10-09 12:28:52,2
AMD,nimk9vp,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Intel,2025-10-09 17:05:36,2
AMD,njlc3cu,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Intel,2025-10-15 09:13:26,1
AMD,nkp7gzk,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Intel,2025-10-22 00:53:17,1
AMD,nksv7pa,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Intel,2025-10-22 16:17:48,1
AMD,nm887uc,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Intel,2025-10-30 17:38:48,1
AMD,nmj83z4,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Intel,2025-11-01 13:55:37,1
AMD,niosucw,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-10 00:13:23,1
AMD,niowtna,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-10-10 00:37:02,1
AMD,nicof1i,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-08 01:46:19,2
AMD,njq2uvf,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Intel,2025-10-16 01:31:17,1
AMD,nkq87np,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Intel,2025-10-22 04:59:31,1
AMD,nkwr1mw,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Intel,2025-10-23 05:27:23,1
AMD,nkxufey,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Intel,2025-10-23 11:35:54,1
AMD,nmj7ctn,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Intel,2025-11-01 13:51:07,1
AMD,nmsah2r,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Intel,2025-11-02 22:47:21,1
AMD,nktqa25,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Intel,2025-10-22 18:45:49,1
AMD,nmk5tv3,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Intel,2025-11-01 16:57:33,1
AMD,nkvsdop,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Intel,2025-10-23 01:24:29,1
AMD,nfolbmr,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Intel,2025-09-22 23:20:45,53
AMD,nflslxh,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Intel,2025-09-22 14:31:25,46
AMD,nfm76rf,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Intel,2025-09-22 15:42:11,33
AMD,nfnaznn,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Intel,2025-09-22 18:52:47,10
AMD,nfpb0n3,"i think it's bad for us, consumers",Intel,2025-09-23 02:12:30,5
AMD,nflx3jg,Was the team up really to crush AMD or Nvidia's answer to enter China?,Intel,2025-09-22 14:53:26,7
AMD,ng4ik7u,AMDware unboxed only cares about AMD anyway,Intel,2025-09-25 13:37:58,3
AMD,nfm1wz0,This hurts the arc division way more than this could ever hurt amd.,Intel,2025-09-22 15:16:45,15
AMD,nfoivfo,They will crush user's wallet,Intel,2025-09-22 23:04:45,2
AMD,nftk1b6,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Intel,2025-09-23 19:23:17,2
AMD,nfv8a1x,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Intel,2025-09-24 00:40:38,1
AMD,ng0xe2q,Remember Kaby Lake G? No? This will also be forgotten soon.,Intel,2025-09-24 22:04:55,1
AMD,ng2ck7r,Yes.,Intel,2025-09-25 03:11:04,1
AMD,ngzb138,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Intel,2025-09-30 09:53:37,1
AMD,nhckqoj,Foveros baby!,Intel,2025-10-02 11:51:31,1
AMD,nfmdikv,AMDUnboxed on suicide watch.,Intel,2025-09-22 16:12:26,1
AMD,nfn7em1,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Intel,2025-09-22 18:34:17,0
AMD,nfmh1rz,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Intel,2025-09-22 16:29:30,1
AMD,nflsfzz,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Intel,2025-09-22 14:30:36,-1
AMD,nfmihzp,Ohh noooerrrrrrrrr,Intel,2025-09-22 16:36:30,0
AMD,nfvhp2d,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Intel,2025-09-24 01:35:50,0
AMD,ngcdph8,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Intel,2025-09-26 17:45:54,0
AMD,nfnsqrd,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Intel,2025-09-22 20:29:14,-4
AMD,nfmr987,welcome to the Nvidia and amd duopoly,Intel,2025-09-22 17:17:38,-4
AMD,nfmy4sf,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Intel,2025-09-22 17:49:51,3
AMD,nfn29jk,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Intel,2025-09-22 18:09:19,-9
AMD,nfma1mz,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Intel,2025-09-22 15:55:41,4
AMD,ng59q6y,a partnership doesnt mean they get free reign over license lol,Intel,2025-09-25 15:49:13,2
AMD,nfodhll,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Intel,2025-09-22 22:29:59,-2
AMD,nfm0v8n,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Intel,2025-09-22 15:11:42,21
AMD,nfm237h,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Intel,2025-09-22 15:17:35,6
AMD,nfm66b6,Why would it?,Intel,2025-09-22 15:37:21,8
AMD,nfm5ru9,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Intel,2025-09-22 15:35:26,11
AMD,nfmevnp,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Intel,2025-09-22 16:19:02,4
AMD,nhzfr23,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-10-05 23:57:54,1
AMD,nfm4tdl,Past != Future,Intel,2025-09-22 15:30:48,4
AMD,nfnb9ui,"nvidia also used to make motherboard chipset, with mixed success.",Intel,2025-09-22 18:54:18,2
AMD,nfoib8h,FSR 4 looks like the later versions of Dlss 2 did,Intel,2025-09-22 23:01:07,6
AMD,nfo2bk7,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Intel,2025-09-22 21:21:54,2
AMD,ng9kwgg,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Intel,2025-09-26 07:01:28,0
AMD,nfmqv0q,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Intel,2025-09-22 17:15:46,6
AMD,nfoz7q1,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Intel,2025-09-23 00:52:30,0
AMD,ng6jjhd,Never said that.,Intel,2025-09-25 19:28:08,-1
AMD,nfm1xh4,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Intel,2025-09-22 15:16:49,14
AMD,nfm6de6,Why do so many people think that this will kill ARC?,Intel,2025-09-22 15:38:18,10
AMD,nfm8iqb,The market for Arc is the same as for Nvidia.,Intel,2025-09-22 15:48:31,11
AMD,nflxpfk,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Intel,2025-09-22 14:56:21,3
AMD,nfmpyxd,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Intel,2025-09-22 17:11:35,4
AMD,nfncgcw,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Intel,2025-09-22 19:00:37,2
AMD,nfpd2vm,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Intel,2025-09-23 02:26:29,1
AMD,nfq8t4o,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Intel,2025-09-23 07:05:35,0
AMD,ng2cqnz,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Intel,2025-09-25 03:12:17,1
AMD,ng2dv4q,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Intel,2025-09-25 03:20:00,0
AMD,nfm6s3o,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Intel,2025-09-22 15:40:15,5
AMD,nfmtpsf,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Intel,2025-09-22 17:29:18,14
AMD,nfm7mbh,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Intel,2025-09-22 15:44:16,-3
AMD,nfm91t2,Nvidia does not have an A310 competitor.,Intel,2025-09-22 15:51:01,-3
AMD,nftpodz,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Intel,2025-09-23 19:50:16,2
AMD,nfq8uoi,I have not lied,Intel,2025-09-23 07:06:03,1
AMD,ng2iom6,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Intel,2025-09-25 03:54:05,1
AMD,ng17you,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Intel,2025-09-24 23:06:01,2
AMD,nfonq2p,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Intel,2025-09-22 23:36:31,3
AMD,nfm9qnn,Intel doesn't have a current gen A310 competitor either.,Intel,2025-09-22 15:54:15,9
AMD,nfma2t0,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Intel,2025-09-22 15:55:51,0
AMD,nfm0jnd,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Intel,2025-09-22 15:10:08,2
AMD,nftspxa,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Intel,2025-09-23 20:04:43,2
AMD,nfrlmki,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Intel,2025-09-23 13:46:08,0
AMD,nfu9p3r,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Intel,2025-09-23 21:25:34,2
AMD,nfpz6gm,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Intel,2025-09-23 05:25:09,3
AMD,nfqz1sj,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Intel,2025-09-23 11:29:37,2
AMD,nfrltqc,The later versions of Dlss 2 look like Dlss 3,Intel,2025-09-23 13:47:11,2
AMD,nfq8fx8,Nvidia probably feels the same about their low end SKUs.,Intel,2025-09-23 07:01:31,1
AMD,ngn9bw4,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Intel,2025-09-28 13:27:33,1
AMD,nfrmbyw,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Intel,2025-09-23 13:49:53,0
AMD,nfq8hgn,Yeah lol,Intel,2025-09-23 07:02:00,1
AMD,nfrmiwb,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Intel,2025-09-23 13:50:54,2
AMD,nfrmnqg,"""Everyone I don't like is biased""-ass answer",Intel,2025-09-23 13:51:36,1
AMD,naz5fcr,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Intel,2025-08-27 16:46:20,45
AMD,nazj7k0,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Intel,2025-08-27 17:50:18,18
AMD,ncitd5t,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Intel,2025-09-05 08:24:57,1
AMD,naz95i7,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Intel,2025-08-27 17:04:24,1
AMD,naz6p5r,Just hodl until you get the biscuits,Intel,2025-08-27 16:52:25,37
AMD,naz7hoi,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Intel,2025-08-27 16:56:15,11
AMD,nazkvn7,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Intel,2025-08-27 17:57:46,7
AMD,nazb0q3,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Intel,2025-08-27 17:13:24,1
AMD,nazazfk,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Intel,2025-08-27 17:13:14,34
AMD,nb330i7,I thought Arrow Lake refresh was in the cards for 2025.,Intel,2025-08-28 05:57:34,1
AMD,nb04wuk,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Intel,2025-08-27 19:33:00,0
AMD,nazl60x,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Intel,2025-08-27 17:59:03,5
AMD,nb8e4jm,This entire thing is a mobile roadmap so why are you here?,Intel,2025-08-29 00:35:26,2
AMD,nb1w9ji,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Intel,2025-08-28 01:01:42,3
AMD,nazbb9i,"Yeah if it's Surface roadmap, it's a nothing burger.",Intel,2025-08-27 17:14:47,5
AMD,nb02q5l,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Intel,2025-08-27 19:22:51,1
AMD,nb171ho,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Intel,2025-08-27 22:38:31,3
AMD,nb1xsvd,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Intel,2025-08-28 01:10:35,1
AMD,nb1vmub,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Intel,2025-08-28 00:58:05,3
AMD,n73y5u9,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Intel,2025-08-05 20:10:52,33
AMD,n73iurw,"I like how they just throw random words around to pad their ""article"".",Intel,2025-08-05 18:43:24,64
AMD,n73t59w,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Intel,2025-08-05 19:42:58,28
AMD,n741jok,"""leaks""",Intel,2025-08-05 20:28:54,7
AMD,n74lp88,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Intel,2025-08-05 22:16:47,5
AMD,n77fxeb,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Intel,2025-08-06 10:20:26,1
AMD,n7koh28,Scared of their rumor?  Lets release our rumor!,Intel,2025-08-08 09:59:01,1
AMD,n758aa3,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Intel,2025-08-06 00:22:30,1
AMD,n76sf5r,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Intel,2025-08-06 06:42:22,-2
AMD,n74sw92,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Intel,2025-08-05 22:56:59,-6
AMD,n75af32,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Intel,2025-08-06 00:34:46,4
AMD,n754qqc,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Intel,2025-08-06 00:02:20,7
AMD,n7582e3,bLLC is not stacked cache,Intel,2025-08-06 00:21:15,4
AMD,n767h6n,What do you consider random? The article was perfectly clear.,Intel,2025-08-06 03:54:03,10
AMD,n748zl3,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Intel,2025-08-05 21:08:09,14
AMD,n741l8k,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Intel,2025-08-05 20:29:08,2
AMD,n76jhxw,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Intel,2025-08-06 05:24:22,1
AMD,n7s4doe,Yeah. Definitely just you,Intel,2025-08-09 14:25:48,5
AMD,n757qwt,You could literally make that claim with any CPU performance increase.,Intel,2025-08-06 00:19:24,8
AMD,n7615sm,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Intel,2025-08-06 03:11:36,6
AMD,n77a60y,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Intel,2025-08-06 09:28:27,2
AMD,n76ahn8,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Intel,2025-08-06 04:15:00,5
AMD,n757yiy,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Intel,2025-08-06 00:20:37,1
AMD,n749ios,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Intel,2025-08-05 21:11:01,15
AMD,n76z9ct,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Intel,2025-08-06 07:45:28,2
AMD,n77cfm2,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Intel,2025-08-06 09:49:46,4
AMD,n77cdot,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Intel,2025-08-06 09:49:17,6
AMD,nam3zf1,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Intel,2025-08-25 17:06:43,0
AMD,n75cvtk,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Intel,2025-08-06 00:48:56,2
AMD,n7ddfzp,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Intel,2025-08-07 06:10:13,1
AMD,n7dcz9u,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Intel,2025-08-07 06:06:08,1
AMD,nax8p6j,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Intel,2025-08-27 10:26:21,1
AMD,n7ddyd1,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Intel,2025-08-07 06:14:41,1
AMD,n7deaek,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Intel,2025-08-07 06:17:37,1
AMD,n9hwp92,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Intel,2025-08-19 08:52:24,63
AMD,n9hupxb,It's not sorcery. Its just Intel doing the game developers work.,Intel,2025-08-19 08:32:04,77
AMD,n9i0i3y,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Intel,2025-08-19 09:30:05,6
AMD,n9ic94k,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Intel,2025-08-19 11:11:41,10
AMD,n9k23nq,Never count out Intel. They have some very talented people over there.,Intel,2025-08-19 16:43:12,8
AMD,n9hygne,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Intel,2025-08-19 09:09:52,6
AMD,n9qyqfh,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Intel,2025-08-20 17:38:05,2
AMD,n9tmgam,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Intel,2025-08-21 02:04:20,1
AMD,nab3aup,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Intel,2025-08-23 20:50:51,1
AMD,naohj1m,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Intel,2025-08-26 00:28:42,1
AMD,nattrzq,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Intel,2025-08-26 20:25:28,1
AMD,nd42fep,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Intel,2025-09-08 16:53:22,1
AMD,ndg3xrf,I will follow all!,Intel,2025-09-10 13:49:20,1
AMD,n9j60kj,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Intel,2025-08-19 14:10:38,-8
AMD,n9iw99o,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Intel,2025-08-19 13:18:38,12
AMD,n9ihm3d,How is this doing the game developers work?,Intel,2025-08-19 11:50:25,-5
AMD,n9imvpt,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Intel,2025-08-19 12:24:19,4
AMD,n9ipgu8,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Intel,2025-08-19 12:39:59,6
AMD,n9ktypv,The intel software team is pure black magic when they allowed to work on crack.,Intel,2025-08-19 18:54:19,4
AMD,n9iml8f,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Intel,2025-08-19 12:22:32,4
AMD,n9s6nj5,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Intel,2025-08-20 21:12:41,5
AMD,n9iyl6t,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Intel,2025-08-19 13:31:21,7
AMD,n9u57ww,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Intel,2025-08-21 04:07:48,4
AMD,na9spi1,Balanced in full load will just do the same thing as High Performance.,Intel,2025-08-23 16:43:30,1
AMD,n9lhg6u,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Intel,2025-08-19 20:46:34,20
AMD,nah6zpc,Or... Just process lasso.,Intel,2025-08-24 21:05:07,1
AMD,n9ij60y,Because it’s optimizations on how it can efficiently use the cpu.,Intel,2025-08-19 12:00:43,23
AMD,n9iv66n,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Intel,2025-08-19 13:12:40,1
AMD,n9iwird,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Intel,2025-08-19 13:20:05,3
AMD,n9mv7uv,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Intel,2025-08-20 01:17:18,4
AMD,n9iojt9,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Intel,2025-08-19 12:34:27,1
AMD,n9y05el,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Intel,2025-08-21 19:06:28,1
AMD,n9lzy5i,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Intel,2025-08-19 22:19:43,3
AMD,n9mhmhz,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Intel,2025-08-19 23:59:05,1
AMD,nah75n6,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Intel,2025-08-24 21:06:00,1
AMD,n9kb8x1,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Intel,2025-08-19 17:25:29,-5
AMD,n9ivrcr,You need to download the Intel Application Optimization app from the Windows store,Intel,2025-08-19 13:15:54,1
AMD,n9iy6ma,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Intel,2025-08-19 13:29:09,7
AMD,n9t6inr,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Intel,2025-08-21 00:31:46,1
AMD,n9xzseo,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Intel,2025-08-21 19:04:43,1
AMD,n9ipdm4,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Intel,2025-08-19 12:39:26,5
AMD,n9opxwe,Where to download this APO,Intel,2025-08-20 10:12:53,1
AMD,n9ktn8y,Except its THE game devs job to optimize games for multiple cpus and gpus.,Intel,2025-08-19 18:52:47,7
AMD,n9ks0ao,It’s literally their job to do so? wtf you talking about?,Intel,2025-08-19 18:45:02,4
AMD,n9mt1qu,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Intel,2025-08-20 01:04:38,1
AMD,n9iw056,"Will do, I got a 265K. Performance is already great tbh.",Intel,2025-08-19 13:17:15,1
AMD,n9iz3ih,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Intel,2025-08-19 13:34:10,2
AMD,n9irrhk,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Intel,2025-08-19 12:53:21,1
AMD,n9q1a42,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Intel,2025-08-20 14:59:31,2
AMD,n9kvap0,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Intel,2025-08-19 19:00:36,-3
AMD,n9p3czn,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Intel,2025-08-20 11:55:36,1
AMD,n9kt4hv,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Intel,2025-08-19 18:50:20,-1
AMD,n9mx3nm,That's not simply what APO does.,Intel,2025-08-20 01:28:12,1
AMD,n9iwijq,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Intel,2025-08-19 13:20:03,2
AMD,n9j8cqp,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Intel,2025-08-19 14:22:18,3
AMD,n9iublo,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Intel,2025-08-19 13:07:57,4
AMD,n9pa899,i am not talking about older games. i am talking about newer games.... really dude?,Intel,2025-08-20 12:37:54,0
AMD,n9l8ne6,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Intel,2025-08-19 20:05:16,5
AMD,n9iyd7e,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Intel,2025-08-19 13:30:09,5
AMD,n9iuumd,Thanks a lot.,Intel,2025-08-19 13:10:53,4
AMD,n9pc7ju,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Intel,2025-08-20 12:49:28,2
AMD,n9lby71,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Intel,2025-08-19 20:20:50,3
AMD,n9pd885,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Intel,2025-08-20 12:55:15,0
AMD,n9lgtiq,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Intel,2025-08-19 20:43:35,2
AMD,nadrsxq,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Intel,2025-08-24 08:29:02,0
AMD,n9li080,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Intel,2025-08-19 20:49:11,0
AMD,n6bumwm,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Intel,2025-08-01 11:02:22,63
AMD,n6bnc4i,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Intel,2025-08-01 10:01:27,32
AMD,n6chmtn,Did the DP4A version also improve from 1.3 to 2.0?,Intel,2025-08-01 13:26:20,5
AMD,n6i4k9c,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Intel,2025-08-02 10:05:35,1
AMD,n6bj9ci,Okay but why would I want to use that instead of NVIDIA DLSS?,Intel,2025-08-01 09:23:32,-22
AMD,n6bwcya,It’s the least you should get after not getting FSR4.,Intel,2025-08-01 11:15:16,19
AMD,n6ck8qn,You'd use it over FSR if that's available too?,Intel,2025-08-01 13:40:07,3
AMD,n6i0iq2,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Intel,2025-08-02 09:25:17,1
AMD,n6c91ju,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Intel,2025-08-01 12:38:17,12
AMD,n6btrfp,And they expect people who bought previous RDNA to buy more RDNA,Intel,2025-08-01 10:55:34,3
AMD,n6nl7sh,Not by a significant amount.,Intel,2025-08-03 06:43:34,1
AMD,n6i0oud,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Intel,2025-08-02 09:27:02,0
AMD,n6bp1iu,for the games that dont support DLSS,Intel,2025-08-01 10:16:31,30
AMD,n6bul99,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Intel,2025-08-01 11:02:02,21
AMD,n6bqgih,10 series cards will benefit from this,Intel,2025-08-01 10:28:41,11
AMD,n6bnz56,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Intel,2025-08-01 10:07:08,1
AMD,n6cmxm3,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Intel,2025-08-01 13:54:01,17
AMD,n6d0adu,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Intel,2025-08-01 14:58:55,5
AMD,n6i1gij,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Intel,2025-08-02 09:34:56,2
AMD,n6emb0z,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Intel,2025-08-01 19:37:45,1
AMD,n6guokn,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Intel,2025-08-02 03:16:52,1
AMD,n6nl9o1,DP4a is cross vendor.   XMX is Arc only.,Intel,2025-08-03 06:44:04,3
AMD,n6bqj7p,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Intel,2025-08-01 10:29:20,-8
AMD,n6cx6cb,1080ti heard no bell,Intel,2025-08-01 14:44:10,3
AMD,n6i2ufc,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Intel,2025-08-02 09:48:51,1
AMD,n6r59ev,where did amd touch you bud?,Intel,2025-08-03 20:25:24,2
AMD,n6d2yn8,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Intel,2025-08-01 15:11:40,-3
AMD,n6f09jv,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Intel,2025-08-01 20:47:10,3
AMD,n6iu4q0,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Intel,2025-08-02 13:21:11,2
AMD,n6klfjc,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Intel,2025-08-02 19:05:15,22
AMD,n6q6qct,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Intel,2025-08-03 17:28:08,2
AMD,n7ht68l,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Intel,2025-08-07 21:45:26,1
AMD,n72229n,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Intel,2025-08-05 14:15:36,1
AMD,n7ewvah,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Intel,2025-08-07 13:19:41,2
AMD,nanq5zn,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Intel,2025-08-25 21:47:41,3
AMD,n7m0sli,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Intel,2025-08-08 14:57:07,1
AMD,n6m90qi,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Intel,2025-08-03 00:48:59,5
AMD,n6nytko,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Intel,2025-08-03 08:55:06,3
AMD,n6l6ttw,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Intel,2025-08-02 21:03:36,4
AMD,n7m8w5n,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Intel,2025-08-08 15:35:34,1
AMD,n8s3s3l,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Intel,2025-08-15 04:54:15,1
AMD,n6orqc9,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Intel,2025-08-03 12:59:19,5
AMD,n6qeres,Thank you :),Intel,2025-08-03 18:07:58,1
AMD,n4v3wzh,Wouldn't the 300 series actually be Arrow Lake Refresh?,Intel,2025-07-24 08:11:30,44
AMD,n4v46fh,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Intel,2025-07-24 08:13:59,44
AMD,n4y92ft,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Intel,2025-07-24 18:56:57,12
AMD,n4w6o9h,This is pat's work,Intel,2025-07-24 13:07:04,14
AMD,n4v6nq7,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Intel,2025-07-24 08:37:59,6
AMD,n4vlaga,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Intel,2025-07-24 10:49:31,8
AMD,n4w07wg,Is Intel finally making a comeback with their cpus? I hope so,Intel,2025-07-24 12:30:48,12
AMD,n4vi19u,Large L3 cache without reducing latency will be fun to watch.,Intel,2025-07-24 10:22:42,20
AMD,n4v3ue7,the specs sure do shift a lot..,Intel,2025-07-24 08:10:48,2
AMD,n4v8k6w,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Intel,2025-07-24 08:55:55,2
AMD,n4z4yed,it's just a bunch of cores glued together - intel circa 2016 probably,Intel,2025-07-24 21:27:20,2
AMD,n5798px,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Intel,2025-07-26 02:39:20,2
AMD,n4w3arv,Bout time,Intel,2025-07-24 12:48:32,1
AMD,n4w52bq,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Intel,2025-07-24 12:58:18,1
AMD,n510yns,Will it be available in fall of this year or 26Q1?,Intel,2025-07-25 04:05:49,1
AMD,n525u2m,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Intel,2025-07-25 10:03:56,1
AMD,n54i8vo,Noob question:  Is this their 16th gen chips?,Intel,2025-07-25 17:44:39,1
AMD,n5c2bcs,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Intel,2025-07-26 21:56:10,1
AMD,n5r7teb,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Intel,2025-07-29 07:47:01,1
AMD,n5rui8a,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Intel,2025-07-29 11:14:18,1
AMD,n4yqz30,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Intel,2025-07-24 20:20:50,1
AMD,n55s05h,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Intel,2025-07-25 21:27:44,1
AMD,n4xrnev,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Intel,2025-07-24 17:35:44,0
AMD,n4vu505,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Intel,2025-07-24 11:52:38,0
AMD,n4wlmkp,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Intel,2025-07-24 14:23:50,0
AMD,n4vi1cf,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Intel,2025-07-24 10:22:43,-13
AMD,n50egti,"""E-Cores"",  Ewww, Gross.",Intel,2025-07-25 01:40:31,-3
AMD,n4y77tz,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Intel,2025-07-24 18:47:57,0
AMD,n4vx5x8,NVL will definitely be the 400 series. PTL is 300.,Intel,2025-07-24 12:12:08,13
AMD,n4vqabe,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Intel,2025-07-24 11:26:19,24
AMD,n4x2p0n,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Intel,2025-07-24 15:43:01,6
AMD,n4vnjk4,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Intel,2025-07-24 11:06:28,42
AMD,n4vbjjr,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Intel,2025-07-24 09:24:22,6
AMD,n4vqnzd,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Intel,2025-07-24 11:29:01,13
AMD,n4xnlen,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Intel,2025-07-24 17:17:52,2
AMD,n4y6q9e,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Intel,2025-07-24 18:45:35,2
AMD,n9a1zq5,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Intel,2025-08-18 01:51:09,1
AMD,n54f312,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Intel,2025-07-25 17:30:13,0
AMD,n50iof9,More like *despite* him.,Intel,2025-07-25 02:05:32,-5
AMD,n4wx0ud,All the SKUs rumored so far are BLLC,Intel,2025-07-24 15:17:14,1
AMD,n4w5o0o,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Intel,2025-07-24 13:01:33,10
AMD,n4vwjon,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Intel,2025-07-24 12:08:14,-1
AMD,n51lyb6,Why not 8 P cores with HT + even more cache and no e cores at all,Intel,2025-07-25 06:56:21,-1
AMD,n4wel6l,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Intel,2025-07-24 13:49:07,14
AMD,n4w5tga,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Intel,2025-07-24 13:02:22,4
AMD,n4wwsgj,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Intel,2025-07-24 15:16:10,2
AMD,n8gsgik,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Intel,2025-08-13 13:37:20,2
AMD,n4xic5y,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Intel,2025-07-24 16:54:39,0
AMD,n51u6hg,Nova lake? More like 26Q4.,Intel,2025-07-25 08:13:29,1
AMD,n5272v5,Only Pantherlake for mobile,Intel,2025-07-25 10:15:01,1
AMD,n52fjpx,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Intel,2025-07-25 11:22:11,2
AMD,n4yb3g9,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Intel,2025-07-24 19:06:33,1
AMD,n4vwh5f,It is really happening. The only question is when? Or can they release it on the next year without delay?,Intel,2025-07-24 12:07:46,1
AMD,n4wxgvv,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Intel,2025-07-24 15:19:15,3
AMD,n4wm6y8,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Intel,2025-07-24 14:26:35,1
AMD,n514twx,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Intel,2025-07-25 04:33:51,1
AMD,n5153vl,"E cores are the future, P cores days are numbered.",Intel,2025-07-25 04:35:54,7
AMD,n50iukn,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Intel,2025-07-25 02:06:36,2
AMD,n514kx3,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Intel,2025-07-25 04:32:00,1
AMD,n65knix,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Intel,2025-07-31 12:35:52,1
AMD,n4vw5ub,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Intel,2025-07-24 12:05:46,9
AMD,n4vrhxo,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Intel,2025-07-24 11:34:53,18
AMD,n588bec,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Intel,2025-07-26 07:21:16,2
AMD,n4xxr8n,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Intel,2025-07-24 18:03:15,4
AMD,n50iwkc,The former ceo,Intel,2025-07-25 02:06:56,3
AMD,n4xy0ww,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Intel,2025-07-24 18:04:31,4
AMD,n4x3wyl,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Intel,2025-07-24 15:48:35,9
AMD,n4wb0qc,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Intel,2025-07-24 13:30:18,12
AMD,n4xgvaw,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Intel,2025-07-24 16:48:02,5
AMD,n4vzlh2,No different to 12-14th gen then.,Intel,2025-07-24 12:27:02,-2
AMD,n4wathc,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Intel,2025-07-24 13:29:14,-1
AMD,n51m5t6,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Intel,2025-07-25 06:58:12,3
AMD,n5267k5,E core is 30% faster than hyper threading,Intel,2025-07-25 10:07:14,2
AMD,n9ruz42,HT is worse than E cores,Intel,2025-08-20 20:15:01,2
AMD,n4wi0su,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Intel,2025-07-24 14:06:24,5
AMD,n5m42rc,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Intel,2025-07-28 14:26:44,1
AMD,n4xxluj,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Intel,2025-07-24 18:02:33,5
AMD,n50irzt,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Intel,2025-07-25 02:06:09,3
AMD,n526u6b,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Intel,2025-07-25 10:12:51,1
AMD,n514fo0,ARM chips regularly do this sometimes on a yearly basis.,Intel,2025-07-25 04:30:55,1
AMD,n5maqpy,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Intel,2025-07-28 14:59:12,3
AMD,n6ijxrr,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Intel,2025-08-02 12:13:51,1
AMD,n4yg12q,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Intel,2025-07-24 19:29:54,-2
AMD,n4wewa9,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Intel,2025-07-24 13:50:43,2
AMD,n4x5x1h,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Intel,2025-07-24 15:57:43,0
AMD,n5m9vma,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Intel,2025-07-28 14:55:00,1
AMD,n5awbw7,"Lmao, some of my games still runs on e cores",Intel,2025-07-26 18:06:33,1
AMD,n50x83s,People are still disabling e cores for more performance.,Intel,2025-07-25 03:39:29,1
AMD,n525z7c,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Intel,2025-07-25 10:05:10,1
AMD,n5m4vc2,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Intel,2025-07-28 14:30:40,2
AMD,n511bh4,fym no different they're 50% faster,Intel,2025-07-25 04:08:20,3
AMD,n5m5ymk,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Intel,2025-07-28 14:36:05,3
AMD,n528glp,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Intel,2025-07-25 10:27:04,1
AMD,n4wm3t2,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Intel,2025-07-24 14:26:09,5
AMD,n4wln0q,"As a advice, Nova Lake will further increase memory latency.",Intel,2025-07-24 14:23:54,-3
AMD,n50qnfg,In which gen iteration?,Intel,2025-07-25 02:55:31,0
AMD,n526q2r,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Intel,2025-07-25 10:11:49,1
AMD,n5m8bv4,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Intel,2025-07-28 14:47:35,1
AMD,n4xhpq9,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Intel,2025-07-24 16:51:53,1
AMD,n4xw2bt,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Intel,2025-07-24 17:55:25,9
AMD,n4zgeej,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Intel,2025-07-24 22:26:02,6
AMD,n52evsy,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Intel,2025-07-25 11:17:22,4
AMD,n588e12,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Intel,2025-07-26 07:21:58,2
AMD,n5266cc,They don't pay attention,Intel,2025-07-25 10:06:55,2
AMD,n5m6uax,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Intel,2025-07-28 14:40:21,1
AMD,n5dzs6f,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Intel,2025-07-27 05:47:15,1
AMD,n51sxrb,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Intel,2025-07-25 08:01:41,3
AMD,n50ijg8,How do you think they're doing 2 compute tiles if the memory controller is on one?,Intel,2025-07-25 02:04:42,2
AMD,n4wml1q,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Intel,2025-07-24 14:28:27,4
AMD,n5m2xjb,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Intel,2025-07-28 14:21:04,2
AMD,n528fgg,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Intel,2025-07-25 10:26:47,3
AMD,n5m9sth,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Intel,2025-07-28 14:54:37,1
AMD,n4xx7gl,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Intel,2025-07-24 18:00:40,2
AMD,n51508q,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Intel,2025-07-25 04:35:11,1
AMD,n526cpd,That didn't stop Intel with N3B,Intel,2025-07-25 10:08:30,0
AMD,n55ndnc,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Intel,2025-07-25 21:04:09,-1
AMD,n5qxo88,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Intel,2025-07-29 06:12:03,1
AMD,n80drlt,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Intel,2025-08-10 21:49:03,1
AMD,n4yjbs7,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Intel,2025-07-24 19:45:28,0
AMD,n55qjrv,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Intel,2025-07-25 21:20:18,6
AMD,n4z1sm2,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Intel,2025-07-24 21:11:47,2
AMD,n4z2fz2,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Intel,2025-07-24 21:14:56,1
AMD,n526jxa,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Intel,2025-07-25 10:10:18,1
AMD,n4znijc,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Intel,2025-07-24 23:05:11,0
AMD,n53d0lp,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Intel,2025-07-25 14:33:18,0
AMD,n3haicz,Will it also introduce Lunar Lake successor?,Intel,2025-07-16 17:36:05,16
AMD,n3htfns,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Intel,2025-07-16 19:02:16,10
AMD,n3hw0vs,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Intel,2025-07-16 19:14:21,5
AMD,n3h5hhc,Is anyone left?,Intel,2025-07-16 17:13:41,22
AMD,n3h8963,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Intel,2025-07-16 17:26:01,7
AMD,n3hcp5u,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Intel,2025-07-16 17:45:45,6
AMD,n3jhgvd,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Intel,2025-07-16 23:53:49,1
AMD,n3k7s38,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Intel,2025-07-17 02:31:14,1
AMD,n3mdvwq,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Intel,2025-07-17 12:44:29,1
AMD,n4c03u2,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Intel,2025-07-21 12:54:03,1
AMD,n3huqus,That's Panther Lake in a few months,Intel,2025-07-16 19:08:25,12
AMD,n3ik298,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Intel,2025-07-16 21:05:21,10
AMD,n3lcumc,"No, the memory config wouldn't work.",Intel,2025-07-17 07:51:04,2
AMD,n3i46cl,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Intel,2025-07-16 19:53:15,8
AMD,n3h7xps,No not really.  It's pretty f'n bleak atm.,Intel,2025-07-16 17:24:37,17
AMD,n3h9kp1,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Intel,2025-07-16 17:31:54,5
AMD,n3l4fgi,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Intel,2025-07-17 06:35:04,-1
AMD,n3irdqf,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Intel,2025-07-16 21:39:27,6
AMD,n3hywlr,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Intel,2025-07-16 19:27:56,5
AMD,n3k42qb,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Intel,2025-07-17 02:08:16,6
AMD,n3k6ewf,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Intel,2025-07-17 02:22:43,3
AMD,n3jnkuo,PTL's not really a LNL successor.,Intel,2025-07-17 00:29:37,1
AMD,n48vx1p,"Yeah, it was just to prove a point that ARM is overrated.",Intel,2025-07-20 22:58:27,2
AMD,n3k5a4s,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Intel,2025-07-17 02:15:39,2
AMD,n4sfvrm,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Intel,2025-07-23 21:35:03,2
AMD,n3h8tf7,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Intel,2025-07-16 17:28:32,13
AMD,n3kgv3j,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Intel,2025-07-17 03:30:53,6
AMD,n3jgnpe,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Intel,2025-07-16 23:49:16,4
AMD,n3hafqh,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Intel,2025-07-16 17:35:45,2
AMD,ngib62p,Source?,Intel,2025-09-27 17:03:09,2
AMD,n3jfseq,"The BOM is lower, so the question is where the markup is coming from.",Intel,2025-07-16 23:44:28,2
AMD,n3k5vvd,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Intel,2025-07-17 02:19:23,2
AMD,n3k8ay7,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Intel,2025-07-17 02:34:31,2
AMD,n3khaz8,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Intel,2025-07-17 03:33:57,1
AMD,n3k63l5,>still cheaper,Intel,2025-07-17 02:20:44,1
AMD,n3k776n,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Intel,2025-07-17 02:27:36,9
AMD,n3ugqr7,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Intel,2025-07-18 16:50:56,1
AMD,n3l78dq,It already has 32MB infinity cache.,Intel,2025-07-17 06:59:18,2
AMD,n3k81ho,The 7840HS is cheaper because it is older.,Intel,2025-07-17 02:32:52,5
AMD,n3lcowt,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Intel,2025-07-17 07:49:34,2
AMD,n3ut4ta,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Intel,2025-07-18 17:47:58,1
AMD,n3kh6ij,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Intel,2025-07-17 03:33:05,-1
AMD,n3levrg,I will see the performance envelope of PTL U and decide should dump my LNL or not,Intel,2025-07-17 08:10:07,4
AMD,n3uubsu,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Intel,2025-07-18 17:53:36,1
AMD,n3ki3cu,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Intel,2025-07-17 03:39:32,6
AMD,n3l7fsl,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Intel,2025-07-17 07:01:04,1
AMD,n3laz95,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Intel,2025-07-17 07:33:37,1
AMD,n3l7mu1,I think I didn't say anything that deviates from what you just said.,Intel,2025-07-17 07:02:48,1
AMD,n3lach0,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Intel,2025-07-17 07:27:39,1
AMD,n1d5sl1,enjoy rhythm aware outgoing practice bike attempt library versed cake   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Intel,2025-07-04 20:26:29,20
AMD,n1d44wc,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Intel,2025-07-04 20:17:30,14
AMD,n1hmxgn,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Intel,2025-07-05 16:10:42,5
AMD,n1je3rl,"who needs quality control, what can go wrong?",Intel,2025-07-05 21:53:07,3
AMD,n4zj8x9,Gigabyte is trash,Intel,2025-07-24 22:41:41,1
AMD,n1j4fq9,The Elon Musk method,Intel,2025-07-05 20:58:12,4
AMD,n2m6czi,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Intel,2025-07-11 20:40:54,1
AMD,n25f2x8,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Intel,2025-07-09 10:28:21,1
AMD,mzzaf4q,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Intel,2025-06-27 00:15:57,66
AMD,n023uge,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBB’d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. I’d love 16p + L4 CPU.",Intel,2025-06-27 13:03:00,8
AMD,mzzyzii,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Intel,2025-06-27 02:44:24,14
AMD,mzz7wta,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Intel,2025-06-27 00:01:01,17
AMD,n029b76,Hasn’t this been on their roadmap for a while now? I’m pretty sure they said 2027 is when they’ll have their version of x3D on the market,Intel,2025-06-27 13:33:26,3
AMD,mzz8z0y,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit they’re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Intel,2025-06-27 00:07:18,9
AMD,n07rvul,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Intel,2025-06-28 09:38:27,3
AMD,n0tqugl,Intel should be ahead of the curve on things not looking to compete on previously created tech,Intel,2025-07-01 20:43:31,1
AMD,n0x6eyy,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Intel,2025-07-02 10:56:54,1
AMD,n5g03d8,"Lol, requires a new socket. Intel is such trash.",Intel,2025-07-27 15:10:56,1
AMD,n05c074,Intel will simply always be better than amd,Intel,2025-06-27 22:40:39,2
AMD,mzzj3f4,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Intel,2025-06-27 01:08:00,-9
AMD,n00a0en,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Intel,2025-06-27 03:59:29,21
AMD,n01ajgx,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Intel,2025-06-27 09:25:00,9
AMD,n03naj2,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Intel,2025-06-27 17:33:15,6
AMD,n022v6j,4070 ti won’t cut it man - upgrade!,Intel,2025-06-27 12:57:19,-2
AMD,n067n87,Broadwell could have been so interesting had it planned out.,Intel,2025-06-28 01:50:48,4
AMD,n0254vg,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. 😬",Intel,2025-06-27 13:10:22,7
AMD,n06s4h5,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Intel,2025-06-28 04:12:23,5
AMD,n028uwl,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Intel,2025-06-27 13:30:56,7
AMD,n014nai,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Intel,2025-06-27 08:27:08,12
AMD,n00xlat,"Adamantaium was on the interposer, did they change plans?",Intel,2025-06-27 07:17:41,3
AMD,n02fp39,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Intel,2025-06-27 14:06:40,6
AMD,n0r9clc,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Intel,2025-07-01 13:40:00,1
AMD,n027hjt,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Intel,2025-06-27 13:23:25,-1
AMD,mzzxdtg,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Intel,2025-06-27 02:34:12,19
AMD,n05blhb,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Intel,2025-06-27 22:38:16,6
AMD,n01isnc,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Intel,2025-06-27 10:39:30,14
AMD,n0421zg,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Intel,2025-06-27 18:42:47,10
AMD,n5exp59,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Intel,2025-07-27 11:11:38,1
AMD,n835hud,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Intel,2025-08-11 10:24:29,1
AMD,mzzqcpb,"Ah you’re missing the final piece. As far as i’m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though don’t control windows.",Intel,2025-06-27 01:51:25,22
AMD,mzzxpgo,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Intel,2025-06-27 02:36:14,9
AMD,mzztuv0,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Intel,2025-06-27 02:12:29,8
AMD,n014s2d,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Intel,2025-06-27 08:28:28,3
AMD,n0r948t,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Intel,2025-07-01 13:38:44,1
AMD,n03y3vh,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Intel,2025-06-27 18:23:40,5
AMD,n0798ym,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Intel,2025-06-28 06:37:39,4
AMD,n01j4io,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Intel,2025-06-27 10:42:13,7
AMD,n01g781,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Intel,2025-06-27 10:17:10,5
AMD,n01fnuq,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Intel,2025-06-27 10:12:22,9
AMD,nj5zwte,Zen6 is the last AMD CPU using its socket anyway,Intel,2025-10-12 20:57:17,1
AMD,n02ibwo,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Intel,2025-06-27 14:19:43,8
AMD,n045qi7,"Yeah, I somewhat agree with this. I suppose it depends if Intel’s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Intel,2025-06-27 19:00:29,2
AMD,n3fijqh,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Intel,2025-07-16 11:51:08,1
AMD,n0ap7lk,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidia’s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Intel,2025-06-28 20:15:41,11
AMD,n02j14s,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Intel,2025-06-27 14:23:07,10
AMD,n04xv5b,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Intel,2025-06-27 21:22:28,5
AMD,n0429de,"How was ""system snappiness"" measured?",Intel,2025-06-27 18:43:46,9
AMD,n01uxxc,No.,Intel,2025-06-27 12:08:29,12
AMD,n08zo5w,"Now both AMD and Intel chips are “disaggregated “ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There won’t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Intel,2025-06-28 14:51:50,2
AMD,n02l2ku,So? Major upgrade for everything else,Intel,2025-06-27 14:33:03,2
AMD,n0x0cru,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Intel,2025-07-02 10:05:39,0
AMD,n046538,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Intel,2025-06-27 19:02:27,8
AMD,n043af1,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Intel,2025-06-27 18:48:47,-2
AMD,n5fdqig,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Intel,2025-07-27 13:07:23,0
AMD,n84bzza,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Intel,2025-08-11 14:53:13,1
AMD,n1f1onn,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Intel,2025-07-05 03:53:59,1
AMD,n0jysi1,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Intel,2025-06-30 10:29:27,1
AMD,n02dy44,You can very simply get 9800x3D to 5.4 with little effort,Intel,2025-06-27 13:57:44,6
AMD,nj6nf3i,"No, they will do zen7 too",Intel,2025-10-12 23:12:44,1
AMD,n0qy2r3,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Intel,2025-07-01 12:35:15,6
AMD,nc1pr0f,"7800x3d here and never had these issues, came from intel",Intel,2025-09-02 17:25:21,1
AMD,n02jgm4,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Intel,2025-06-27 14:25:11,-2
AMD,n01v9di,Lets just ignore the whitepaper WD and Intel did about this.,Intel,2025-06-27 12:10:35,3
AMD,n5fmwbs,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Intel,2025-07-27 14:00:49,1
AMD,n006ga4,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Intel,2025-06-27 03:34:22,11
AMD,n03645p,On which benchmark(s) / metrics?,Intel,2025-06-27 16:12:47,3
AMD,n1jzzta,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Intel,2025-07-06 00:05:28,1
AMD,n0ksjbi,"In theory, yes. For packaging reasons and market segmentation, probably not.",Intel,2025-06-30 13:51:44,2
AMD,n02kkbn,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Intel,2025-06-27 14:30:33,10
AMD,n0226kr,where is this whitepaper,Intel,2025-06-27 12:53:19,6
AMD,n02akpt,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Intel,2025-06-27 13:40:12,6
AMD,n5g6rxb,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Intel,2025-07-27 15:44:33,1
AMD,n7xvp9r,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Intel,2025-08-10 13:55:29,1
AMD,n023hsg,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Intel,2025-06-27 13:00:57,4
AMD,n5gbwrf,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Intel,2025-07-27 16:09:50,1
AMD,n007dg1,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Intel,2025-06-27 03:40:49,7
AMD,n03knav,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-06-27 17:21:00,2
AMD,n046z2m,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Intel,2025-06-27 19:06:33,3
AMD,n7xwq6a,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Intel,2025-08-10 14:01:29,2
AMD,n5ghx4j,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Intel,2025-07-27 16:38:58,1
AMD,n13yjw6,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Intel,2025-07-03 11:37:55,2
AMD,n5gkgn2,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Intel,2025-07-27 16:51:17,1
AMD,n0081vw,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software framework… Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Intel,2025-06-27 03:45:34,10
AMD,n049nak,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Intel,2025-06-27 19:19:40,2
AMD,n5gmhbv,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Intel,2025-07-27 17:01:05,0
AMD,n0rzbmt,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-07-01 15:46:52,1
AMD,n1s9740,Is it safe to install an Arrow Lake CPU without a third-party contact frame over the long run?,Intel,2025-07-07 10:28:03,1
AMD,n212egt,"Edit: solved. Windows is capable of disabling e-cores on boot. The setting to turn them back on in MSCONFIG, boot tab, cores dropdown. Whatever feature causes this, I do not know but would love to find out, and furthermore their is no legitimate purpose for this feature, and Intel should prevent Microsoft from doing this, in my opinion, if there is any legal means available. Unless it interferes with Intel’s revenue.  Poor performance, incredibly low benchmarks. Beginning to suspect bad CPU.  I have tried bone stock with cleared CMOS settings. I have loaded multiple overclocking profiles, clearing CMOS in between. I have tried undervolting, and adjusting Load Line. I have changed Lite Load settings. I have tried with and without XMP enabled. Changed windows power plans. Nothing gets my cinebench r23 score above 11k. CPU-Z benchmarks it as 43% as fast as the previous generation CPU. I never overclocked this cpu, and I changed to the updated bios with new microcode the day it was available. CPU Purchased from Intel, retail box.  Windows 11 Build 26100.4484  CPU: Intel I9 14900K, stock clock, 360 AIO liquid cooling  RAM: Patriot Viper Venom DDR5 32GB (2x16) 7200MT/S CL34  GPU: MSI Ventus NVIDIA RTX 4080S  Motherboard: MSI Z790-S Pro Wifi with most recent bios  Storage: WD Black 2TB NVME on CPU; Toshiba 20GB X300 Pro  PSU: NZXT C1000 PSU (2022)  Display: Samsung Odyssey G93SC  I am at my wits end with this thing. I first noticed games crashing to desktop a few months ago. I thought it was just poorly coded (Helldivers 2). I checked my FPS in the game, and while previously I had to frame limit it 10 144FPS, I was now getting close to 60-80FPS.  Is my CPU a toaster or is there something I'm missing?  CPU-Z output, and HWiNFO64 sensor readings:  [https://imgur.com/a/ROuZOKS](https://imgur.com/a/ROuZOKS)",Intel,2025-07-08 18:18:07,1
AMD,n2cfmmi,"I bought a 265kf CPU from Amazon. On the checkout page it clearly stated that my purchase qualified for the Intel Spring Gaming Bundle and that I would receive an email with a Master Key. Well, I didn't receive anything and I spent all day being transferred from one Amazon support staff to another to no avail.  The promotion is literally still active and if I try to buy the CPU again it shows the same offer.  But somehow no one on Amazon or Intel support can tell me why I didn't get the email.",Intel,2025-07-10 11:27:48,1
AMD,n2h44v7,Does anyone know if the upcoming Bartlett Lake-S 12 p-core no e-core CPU will suffer from the same stability issues as Intel 13th gen and 14th gen CPUs?,Intel,2025-07-11 01:52:33,1
AMD,n4at6sf,Any news to share regarding this link? https://www.reddit.com/r/intel/s/Bg4QnVzIdD,Intel,2025-07-21 06:57:04,1
AMD,n4vm1wc,"Bug report: Latest ARC driver 32.0.101.6972 causes crashing using Speed Sync  I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  Monitor is non VRR compatible (older gsync monitor ASUS PG348Q)  Hope this gets resolved as it sounds like a great feature.",Intel,2025-07-24 10:55:22,1
AMD,n5c8s6s,"Hello,  I own a i9-14900K, mobo MSI MAG Z790 Tomahawk. I am wondering if it’s possible a specific program can cause my cores to be power limit exceeded, and is there any fix to that? Currently it drops the cpu speed to 0.8 GHz. This one program LeagueClient.exe for a game, League of Legends, has recently started to cause this problem, the only fix is to restart the computer. I have updated my bios, changed power limits within the bios, disabled c state, disabled EIST, disabled e-cores, tried to go into safe boot but the program won’t launch, reinstalled multiple times.",Intel,2025-07-26 22:34:12,1
AMD,n663btu,Has the degradation of the 13th and 14th gen CPU’s been fixed yet?,Intel,2025-07-31 14:14:55,1
AMD,n6pmegm,"Been getting the following error A LOT while playing Expedition 33, and now after 18 hours I can't start the game without it crashing:  `LowLevelFatalError [File:C:\Hw5\Engine\Source\Runtime\RenderCore\Private\ShaderCodeArchive.cpp] [Line: 413] DecompressShaderWithOodleAndExtraLogging(): Could not decompress shader group with Oodle. Group Index: 760 Group IoStoreHash:52bcbf8ac813e7ee35697309 Group NumShaders: 29 Shader Index: 9301 Shader In-Group Index: 760 Shader Hash: 3BF30C4C9852D0D23B2DF59B4396FCC76BC3A80. The CPU (13th Gen Intel(R) Core(TM) i7-13700KF) may be unstable; for details see` [`http://www.radgametools.com/oodleintel.htm`](http://www.radgametools.com/oodleintel.htm)     Am I just screwed because I bought the wrong generation of Intel CPUs 2 years ago?",Intel,2025-08-03 15:45:17,1
AMD,n7b4kpy,"I'm not sure if this matters, but when running `garuda-inxi` on my laptop, it tells me that my i3-8130U is Coffee Lake Gen 8, not Kaby Lake Gen 9.5. I've seen similar problems reported with users using CPU-Z, is this just a known bug or is there something else going on?  I'm going to be tweaking my CPU performance soon, so I'd like to make sure of the CPU capabilities/options first.",Intel,2025-08-06 21:44:11,1
AMD,n85zrk7,"I've been planing to build a PC from scratch for video editing and Ultra 7 265k and Arc B580 are good in my price bracket. Now, I'm worried with all the talk of Intel potentially going bankrupt or having layoffs, is it a good idea to still buy their products? Will we lose support with drivers and stuff like that?",Intel,2025-08-11 19:48:10,1
AMD,n8auefg,My gaming laptop Intel core i7-12700h runs at a constant temperature 95 . Doesn't matter if I'm playing a high end game like cyberpunk or some indie game like hollow knight. Is this thing suppose to always run at this temp?,Intel,2025-08-12 15:12:02,1
AMD,n8i5ccq,"con el nuevo microcodigo de intel , los juegos de ubisoft (ASSASINS CRREED ODYSSE) tienen tiempos  de carga excecivamente altos, diria que al menos 10 veces mas de lo normal, Al devolverr la bios al microcodigo anterior todo funciona bien, cuando lo van a parchar?",Intel,2025-08-13 17:35:56,1
AMD,n8k6lv9,is it worth the upgrade for ai preformance cus my 5070 ti wont work for some reason so now my cpu is my main accelertor and user so should i upgrade to ultra 9? and i  will remove 5070 ti from my flair soon so yeah .,Intel,2025-08-13 23:38:35,1
AMD,n9pv76c,"ok now this is something. my Intel HD Graphics Control Panel is no longer there? no idea how long its been gone but i clearly remember it being there at a point.  using a Lenovo G510(i7-4700MQ, HD Graphics 4600)   Windows 10 Home 22H2 (Build 19045.6216)  windows apps in settings shows the intel driver but not the control panel   drivers from [lenovo's website](https://pcsupport.lenovo.com/in/en/products/laptops-and-netbooks/lenovo-g-series-laptops/lenovo-g510-notebook/20238/downloads/ds103802-intel-vga-driver-for-windows-10-64-bit-lenovo-g410-g510?category=Display%20and%20Video%20Graphics) are dated 16jul 2015. version seems to be 10.18.15.4240   drivers from [intel's website](https://www.intel.com/content/www/us/en/download/18388/intel-graphics-driver-for-windows-10-15-40-4th-gen.html?wapkw=intel%20hd%20graphics%204600) are dated 9jan 2015. version is 15.40.7.64.4279  now whats funny is that my device manager shows that my driver is dated 8mar 2017 which is version 20.19.15.4624   i also have another driver that i can see in the update drivers menu(drivers already present on my device) along with this one which is dated 29sep 2016 version 20.19.15.4531  i have tried reinstalling the driver from the update driver menu using the 8mar 2017 version, no change at all.  my drivers dont seem to be DCH drivers.   intel also says that they [discontinued the ms store version of the control panel](https://www.intel.com/content/www/us/en/support/articles/000058733/graphics.html) anyway.  this is all that i could think of writing here. any other details required just ask.   any help would be good lol",Intel,2025-08-20 14:29:49,1
AMD,na637rq,"my i5-14600KF is being throttled at low temps and refuses to go past 0.8ghz of clock speed. Nothing I do seems to get it to stop throttling. According to throttlestop i have a red EDP OTHER ongoing throttle under CORE and RING. My average CPU temp is 31 degrees C across all cores and im getting 0.69 Voltage to my CPU  CPU: Intel i5-14600KF stock settings no overclock   GPU: Intel Arc B580 ONIX Odyssey BAR resizing enabled   Motherboard Gigabyte Ultra Durable Z790 S WIFI DDR4   RAM: Corsair Vengeance DDR4 16 GB x 2 (32GB)   Storage: 1TB Corsair MP600 CORE XT SSD + 2 TB WD Black SN770   PSU: Cooler Master MWE Gold 850 V2  EDIT: NEW INFO ACQUIRED   When running in safe mode and when booting into BIOS settings my CPU acts normally and receives typical voltage. Something running on my computer is throttling my CPU as i boot into windows. If i open Task Manager quickly after booting, I see system interrupts consume a mild amount of CPU before quickly going away, rather than sticking around when their is an ongoing hardware issue. I have reason to believe a program, either maliciously or due to error, is fucking with my CPU. Also worth noting is that Intel Graphics Software reports my CPU utilization as far higher than task manager, anywhere between 2-50% higher.",Intel,2025-08-23 00:28:09,1
AMD,nand9o5,"Currently in the planning/purchasing phase of a small NVR/Steam Cache server. Information on VROC on X299 is pretty limited. so far I've seen mixed information on the drives supported. Before I purchase x4 Intel P4510 drives, I was hoping someone on here has a similar configuration that works.  The mobo manual states that only Intel based drives are supported but doesn't clarify which intel drives. Also saw on the intel forum that X299 CPU raid is further limited to only Optane based NVME drives. This drive will not be booted to, and I dont want to do a windows based raid.  My planned specs are:  CPU: 10900X  Mobo: X299 Taichi CLX - One of the few that seems to support bifurcation & VROC  Drive: Intel P4510 1TB  VROC Key: VRoc Standard  Are the Intel P4510 supported for Vroc on the x299 platform?",Intel,2025-08-25 20:44:18,1
AMD,natm1ml,"My Intel I210 ethernet device has device id 1531 meaning unprogrammed. The freebsd ethernet driver does not work with 1531. It needs device id 1533 meaning programmed. (Can I use a different driver? No, it's an embedded system that only supports this driver.)  I was linked this:  https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html  but 1) have no idea how to do it 2) cannot access the .bin file and tool it requires  Does anyone have ELI5 steps for getting the device to show devid 1533? Where can I get the .bin?",Intel,2025-08-26 19:48:39,1
AMD,nb5mwcf,"The RMA process for Intel is absolutely atrocious. I don't know if  anyone can give advice on this but here is what is happening:  Based in Germany, for geolocation info. I was one of the early adopters of the 13th gen processors, but as I don't follow tech news too strongly I didn't find out about the issues with these chips until autumn 2024 when the issues I was having with my PC escalated to a point I couldn't ignore any further. Identified the CPU as the likely culprit and started the RMA process.  Firstly, Intel would not offer any solution where I could continue to use my PC whilst they analysed my CPU. As I use my PC for work, having it out of action for weeks/months was simply not an option, so I was forced to pause the RMA ticket whilst I saved up for a new cpu way ahead of my expected timeline.  With that aside, I reopened my ticket and the requirements they lay out are near impossible to meet - they wanted a clear photo of the matrix on the front of the chip and the matrix on the pcb itself.  The front of the chip was simple enough but on this series Intel printed the matrix in dark grey on a dark green pcb. It's barely visible just with the naked eye and I've tried so many ways to take a picture of the matrix with my smartphone and nothing I do is getting a clear picture.  I don't understand how your average consumer can possibly meet this requirement - solutions online apparently suggest purchasing a special type of expensive scanner or a macro lens for your smart phone? Which is ridiculous to me.  There is no way they cannot verify my chip with the rest of the information I have been able to give them, as well as far as I am aware, the matrix on the side of the chip on the 13th gen is literally the same as the one on the front of the chip.  The whole experience is proving to be awful, time consuming, feels like it should be illegal and has completely put me off ever using Intel again, or recommending it to anyone I help spec builds for.",Intel,2025-08-28 16:16:20,1
AMD,ncohq8s,"Hi, I have a Lenovo Ideapad with an Intel CPU and Intel GPU.  I find Windows font rendering unreadably faint. I have an astigmatism, and a light sensitivity, so even with prescription sunglasses I can't use dark mode, or bright screens, respectively.  I've tried finding Intel Graphics Software app settings which might help. The contrast settings quickly get too bright for my eyes, so they don't fix this. The right gamma settings might help though, one profile to make medium shades darker to make text bolder, and one to make them lighter to make images bolder.  I tried the support site here, but it has buggy scrolling which triggers my migraines, and it doesn't work with Firefox's reader view:  https://www.intel.com/content/www/us/en/support/products/80939/graphics.html#211011  How can I find or create these profiles? The Intel Graphics Software app doesn't seem to have an option to create profiles.  P.S. I can use the Windows Display Calibrator; if I ignore the instructions, and turn it as dark as possible during gamme, I get readable text at the end; if I turn it as light as possible, I get clearer images. But I can't see  a way to save those and switch without going through the whole rigamarole again.",Intel,2025-09-06 04:15:45,1
AMD,ncywbwe,"Can someone explain to me the differences between all the different names of the Intel CPUs, I’m new to laptops and am trying to learn. What’s the difference between 155H, 255H, and Meteor/Lunar/Arrow Lake and which one is “better”? All the different names get so confusing to me",Intel,2025-09-07 20:35:29,1
AMD,ndcu0v4,"I've been chasing down a crashing issue in a Unity program. Memtest86+ ran for 24 hours and cleared 18 passes so I was confident it's not RAM, and I started setting core affinity in an elimination pattern to see if it might be CPU related. I have discovered it crashes within minutes if I set affinity to Core 8, but it's rock solid on Cores 0-7 (haven't individually tested any of the cores past 8 yet but I have it on 9 now and it seems fine so far).  I have a 13900KF (not Overclocked) which is part of the batch that had the microcode issue. My BIOS was updated, but I am now suspicious of this core. Is it likely I should RMA this? Is there something else I should try first?  So far the system itself has been stable but this one Unity program crashes consistently, and I was having tab crashes in both Firefox and Chrome that also resolved when I removed Core 8 from their affinity...  The CPU passed the Intel CPU Diagnostic tool but because the crashes are so specific to core 8 it makes me suspicious.  [edit] RMAd the CPU and the system is rock solid stable now. WHEW!",Intel,2025-09-09 23:36:23,1
AMD,ndjq7jq,"i'm trying to get a specific driver for the storage but i can't findd a site with the driver, only setup exes (my old computer will not let it work and the new one needs the driver to install windows). is there a place where i could find the driver itself?",Intel,2025-09-11 00:09:09,1
AMD,nduwi1y,"Hi There. I been trying to get a replacement cpu for my 14700 and i chose option 2 to pay the 25 dollor fee (nonrefundable) etc. but my case now has a new number, a differnt worker. and he said that a credit card specilist is gonna called me and im supposed to give him my credit card infomation  edit 1: He also said the intel website isnt secured/safe for creditcard transaction",Intel,2025-09-12 18:07:38,1
AMD,nf5gpjt,"Hi, I have Intel AX200 160Mhz Gig+ in 3 PC's (on board Gigabyte B550 x 2 & X570). My Router supports 160Mhz, it's enabled and was working getting me up to 900Mbps on my Gigabit plan. and in the last few months I've noticed the 160Mhz option in the intel wifi drivers has disappeared. I discovered this as I noticed the household PC's were struggling to get above 600Mbps most of the time. I double checked the router setting, nothing changed there 160Mhz still eneabled, only changes over the last couple of moths were 2 bios updates each for the motherboards and Intel wifi driver updates. Has anyone else had the 160Mhz option disapear? Any tips on how to get it and my full network performance back?",Intel,2025-09-19 22:29:52,1
AMD,n22cmoi,"u/Progenitor3  yes, it is generally safe to install an Arrow Lake CPU without a third-party contact frame over the long run. CPUs are designed to function properly with the standard mounting mechanisms provided by the manufacturer. Third-party contact frames are optional and may offer additional stability or cooling benefits, but they are not necessary for the safe operation of the CPU. Always ensure proper installation according to the manufacturer's guidelines to maintain optimal performance and safety.",Intel,2025-07-08 21:50:47,1
AMD,n22gizs,"u/TerminalCancerMan  Intel cannot comment or interpret results from third party benchmark tools. Run [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) to confirm if there are any issues with the CPU. You may try this, If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor need a replacement.",Intel,2025-07-08 22:10:13,1
AMD,n2h78m4,"u/Progenitor3  Just fill in your info, and it’ll automatically create a ticket for you. Our team that handles those items will get in touch within 3–5 business days.   [Software Advantage Program](https://softwareoffer.intel.com/Support)",Intel,2025-07-11 02:10:36,1
AMD,n2jp4dz,Gotta wait for real-world tests to know for sure tho.,Intel,2025-07-11 13:32:10,1
AMD,n4nvw3m,"u/Special_Ad_7146 To stay on top of Intel news, **visit** our [Newsroom](https://newsroom.intel.com/).",Intel,2025-07-23 05:31:29,1
AMD,n523x8x,"u/Hippieman100  just checking can you confirm which software you're using? From what I’ve seen in the latest [ReleaseNotes\_101.6972.pdf](https://downloadmirror.intel.com/861295/ReleaseNotes_101.6972.pdf), there doesn’t seem to be a “Speed Sync” feature in the current version that comes with this driver. It does support V-Sync and Adaptive Sync though. Just wanted to clarify are you referring to the older Intel Arc Control software or the Intel Graphics Command Center? Just making sure we’re on the same page!",Intel,2025-07-25 09:46:51,1
AMD,n5hzwbu,"u/TheCupaCupa to better understand and isolate the issue, I kindly ask for some additional information. Please find the details requested below:   1. When the CPU drops to 0.8 GHz, do you notice any **error messages or warnings** in Windows or BIOS? 2. Does the issue happen **only with LeagueClient.exe**, or have you seen it with other programs too? 3. Have you checked **CPU temperatures and power draw** when the issue occurs? 4. Can you check **Event Viewer** for any critical errors or warnings around the time of the slowdown? 5. Are you using any **custom power plans** in Windows, or is it set to Balanced/High Performance? 6. Is **Intel Turbo Boost** enabled in BIOS? 7. **When did this issue first start happening?** Has it occurred before? 8. Have you made any software or hardware changes to the system recently?   Once I receive this information, I will be able to properly assess the situation and provide further assistance.",Intel,2025-07-27 21:08:02,1
AMD,n6sn19c,"u/Alloyd11 Not all 13th and 14th generation processors show instability issues. Just to better assist you are you planning to buy or use one of these processors, or do you need help with your current system?  Let me know how I can support you!",Intel,2025-08-04 01:26:43,1
AMD,n6sobcw,"u/amitsly “crashes” is a pretty broad term, and not every system issue points directly to the CPU. There are quite a few steps we go through to fully isolate the problem before concluding it’s a processor fault.  To help us assist you better, could you please share a bit more info about the crashes?  * When did the issue first start happening? * Have you made any recent changes to the system either hardware or software? * Is there any visible physical damage to the system? * What troubleshooting steps have you already tried? * Have you noticed any signs of overheating? * Have you tested the processor in another working system, or tried swapping it out to see if the issue follows the CPU?  The more details you can provide, the quicker we can get to the bottom of it!",Intel,2025-08-04 01:34:29,1
AMD,n7crjzp,"u/Jay_377 its  likely comes from Intel’s naming convention. The i3-8130U is part of the 8th gen, but it falls under “Kaby Lake Refresh” (for mobile chips), not “Coffee Lake” (which is for desktops). Tools like `garuda-inxi` or `CPU-Z` might label it differently based on how they categorize architectures, not a bug, just naming differences.  [Intel® Core™ i3-8130U Processor](https://www.intel.com/content/www/us/en/products/sku/137977/intel-core-i38130u-processor-4m-cache-up-to-3-40-ghz/specifications.html)",Intel,2025-08-07 03:21:15,1
AMD,n7j7cu2,It is because Kaby Lake CPU's are 7th gen processors for laptops and 8th gen to 9th are Coffee Lake check rhis link for better understanding:https://www.intel.com/content/www/us/en/ark/products/codename/97787/products-formerly-coffee-lake.html,Intel,2025-08-08 02:36:25,1
AMD,n88eg87,"u/Reality_Bends33 Intel has been around for a long time and is known for making solid, reliable products, so you can feel confident about choosing them for your PC build. The Ultra 7 265k and Arc B580 are great picks for video editing, offering the performance you need without breaking the bank. While there’s been some discussion about Intel facing challenges, remember that big companies like Intel usually keep up with driver updates and support, even if they’re going through changes. The tech world is always evolving, and Intel is investing in new technologies to stay ahead. So, you’re likely to get the support you need for your products.",Intel,2025-08-12 04:07:16,1
AMD,n8jfndw,"u/unknownboy101 It’s normal for your processor to heat up during heavy tasks like gaming. Intel CPUs are built to manage heat by adjusting power and speed, so they stay safe and avoid damage. However, running at a constant **95°C** on your Intel Core i7-12700H even during light gaming is **not ideal** and could indicate a cooling issue. While Intel CPUs are designed to handle high temperatures and will throttle performance to avoid damage, consistently running near the thermal limit can shorten the lifespan of your components and affect performance. **Feel free to check out this article for more info or steps to try. Just a heads-up this is specifically meant for boxed-type processors. You can still take a look, but I strongly recommend reaching out to your laptop’s manufacturer to get help with the overall system configuration.**  [Overheating Symptoms and Troubleshooting for Intel® Boxed Processors](https://www.intel.com/content/www/us/en/support/articles/000005791/processors/intel-core-processors.html)  [Is It Bad If My Intel® Processor Frequently Approaches or Reaches Its...](https://www.intel.com/content/www/us/en/support/articles/000058679/processors.html)",Intel,2025-08-13 21:16:44,1
AMD,n8lchtt,"u/Frost-sama96 Tenga en cuenta que solo puedo apoyarlo en inglés. He utilizado una herramienta de traducción web para traducir esta respuesta, por lo tanto, puede haber alguna traducción inexacta     **To help me dig a little deeper into the issue, could you share a few details?**  * What’s the **make and model** of your system? Is it a **laptop or desktop**? * Do you remember **when the issue first started** happening? * Which **BIOS version** are you referring to, the one that works fine? If you can share the exact version , that’d be super helpful.",Intel,2025-08-14 03:51:06,1
AMD,n8lcvrc,"u/Fluid-Analysis-2354 If your 5070 Ti isn't functioning and your CPU is now your main accelerator, upgrading to the Ultra 9 could be beneficial. The Ultra 9 offers enhanced AI performance, which can significantly improve your computing tasks. If AI capabilities are a priority for you, the upgrade is worth considering.",Intel,2025-08-14 03:53:51,1
AMD,n9tzi8c,"u/BestSpaceBot , As with all good things, your product has reached the end of its interactive technical support life. However, you can find [Intel® Core™ i7-4700MQ Processor](https://www.intel.com/content/www/us/en/products/sku/75117/intel-core-i74700mq-processor-6m-cache-up-to-3-40-ghz/specifications.html) recommendations at [Intel Community forums](https://community.intel.com/) and additional information at the [Discontinued Products](https://www.intel.com/content/www/us/en/support/articles/000005733/graphics.html) other community members may still offer helpful insights or suggestions.. It is our pleasure to continue to serve you with the next generation of Intel innovation at [Intel.com](http://www.intel.com/). You may also visit this article for more details [Changes in Customer Support and Servicing Updates for Select Intel®...](https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html)",Intel,2025-08-21 03:26:48,1
AMD,nagz2n7,"u/ken10wil   To help figure out what’s going on with your CPU throttling issue, I’d like to ask a few quick questions that’ll help me dig deeper:  * When did you first start noticing the problem? * Have you made any changes to your system recently like installing new software, updating drivers, or swapping hardware? * Is there any visible damage to your PC or loose connections? * Have you updated your BIOS to the latest version for your Gigabyte Z790 board? * Did you try resetting the BIOS to default settings to see if that helps? * Have you tried reapplying thermal paste to the CPU? Just to rule out any cooling contact issues. * Are there any startup programs or services that might be messing with your CPU? * What background processes pop up right after boot? You can check Task Manager or use Reliability Monitor to trace anything unusual. * Can you run the [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) and let me know if it passes or fails?  Let me know what you find happy to help you troubleshoot further once we have a bit more info!",Intel,2025-08-24 20:24:35,1
AMD,nap6uz8,u/PM_pics_of_your_roof   Interesting thanks for pointing that out! Let me check this on my end and I’ll post an update here once I have accurate information.,Intel,2025-08-26 02:57:55,1
AMD,navntia,u/UC20175 Let me check this on my end and I’ll post an update here once I have accurate information.,Intel,2025-08-27 02:26:11,1
AMD,nb7ks0h,"I don't think that there it is awful, I mean in the past where the issue blew out and many consumers are reporting. People at Intel can barely accommodate them but they still give the replacement needed for the consumer.   If i am right, that image is necessary for them to get your serial number and other stuff. Unless you have the box of the processor, or you did not purchase it as a tray then I think you'll be fine.",Intel,2025-08-28 21:50:30,1
AMD,ncqsycn,"Hi, I think you'll need to connect with your laptop manufacturer or Microsoft to help you customize an ideal setting for your display. But could you share your full laptop model?",Intel,2025-09-06 15:18:52,1
AMD,nd0qsea,I think this will help you https://www.intel.com/content/www/us/en/processors/processor-numbers.html,Intel,2025-09-08 02:45:23,1
AMD,ndip3fr,"u/Tagracat  May I confirm the exact error message you're seeing during the crash? Also, have you had a chance to contact the developer of the software in question? Regarding the BIOS, could you double-check if you're using the patch addressing instability issues specifically version 0x12F provided by your motherboard manufacturer? Since the Intel Processor Diagnostic Tool passed, not all crashes may be CPU-related.   Looking forward to your response!",Intel,2025-09-10 21:01:24,1
AMD,ndkxnu4,"u/Maleficent_Apple4169   What specific storage device are we talking about here? Is it an NVMe SSD, RAID controller, or something else? And what's the make/model of the unit in question? Knowing the exact hardware might help me point you to a more specific solution.",Intel,2025-09-11 04:39:39,1
AMD,nenzfnn,Guess checking with your motherboard manufacturer,Intel,2025-09-17 07:17:18,1
AMD,ne3w789,"u/ChiChiKiller Just to be clear, what you've described sounds like a possible **fraud or phishing attempt**. Please **do not share your credit card information** with anyone claiming to be from Intel unless you're communicating directly through our **official support channels**. Intel will never say that our website is not secure we take security very seriously, and this kind of messaging is often used by bad actors trying to steal personal information.  I’ve sent you a **private message in your inbox** please check it when you can. Also, could you kindly provide the details of your **initial ticket** and the **new case number** you mentioned through inbox? I’ll investigate this immediately on my end.  Don’t worry I’ll keep you updated as soon as I have more information. For your privacy, I’ll continue communicating with you through inbox so you don’t have to post sensitive details publicly.  Thanks again for bringing this to our attention!",Intel,2025-09-14 03:27:30,1
AMD,nfh9ai4,"u/Amazing_Watercress_4  Kindly check your inbox, I’ve sent you a personal message. Thank you",Intel,2025-09-21 20:01:58,1
AMD,ngphfd8,"u/PeakHippocrazy  This is completely normal behavior for your Intel Core Ultra 5 235U processor. Modern Intel processors, especially those in laptops like your Dell Pro 14, are designed to automatically park (turn off) cores when they're not needed to save battery life and prevent overheating. Your processor has 10 cores total, and Intel's smart technology decides which ones to use based on what you're doing, it doesn't need all cores running just for web browsing or light tasks. The cores will automatically wake up when you run demanding software that actually needs them. This core parking feature is intentional and helps your laptop run cooler, quieter, and with better battery life. Unless you're experiencing actual performance problems during heavy tasks, there's nothing wrong with your system, it's just working efficiently as designed.  *",Intel,2025-09-28 19:58:48,1
AMD,n4u7q4r,I fixed the problem. It was windows msconfig disabling e-cores on boot. You should forbid them from doing this.,Intel,2025-07-24 03:41:03,1
AMD,n527mwk,"I can't remember which (not at my pc), I think it's intel graphics command center. I get a list of v-sync options, off, on, smooth, smart and speed (from memory). Speed was not available in previous versions of the software/driver. Underneath there is an FPS limiter and a control for latency improvement (I can't remember the name) with off, on, and on + boost options available.",Intel,2025-07-25 10:19:53,1
AMD,n53dndg,"Just got back to my PC, it's actually neither Arc Control or Intel Graphics Command Center. I'm using Intel® Graphics Software (25.26.1602.2), should I be using something else?",Intel,2025-07-25 14:36:18,1
AMD,n5i96k3,"Hello, thank you and I'll try my best.  1. No error messages or warnings in Windows, unsure how to check BIOS. 2. Yes, currently I have only found this happens when LeagueClient.exe is started. 3. Using HWInfo64, right when I open LeagueClient.exe, all P and E cores have ""Yes"" in the Current column for ""Power Limit Exceeded"". Core temperatures are: Current- 33C Minimum- 31C Max- 72C Average- 42C. CPU Package Power, minimum is 65.699 W, maximum is 129.230 W. Upon starting the program, the maximum value doesn't change. 4. For Event Viewer, no warnings come up when LeagueClient.exe is started. 5. I have tried setting it to Balanced, High Performance, but I'm mainly on Ultimate Performance. 6. Yes, Intel Turbo Boost is enabled. 7. The issue first started happening 2 days ago, 07/25/25. No it has not occurred before. 8. I have not made any new hardware changes to the system. I did have a windows update, 2025-07 Cumulative Update Preview for Windows 11 Version 24H2 for x64 based Systems (KB5062660) (261.000.4770) installed on 07/25/25, however I installed this update later on during the day after the problem had already started.",Intel,2025-07-27 21:57:08,1
AMD,n6tt5m6,"Well, I didn't immediately blame the CPU. The crash message specifically mentions the issue that point the finger to the CPU, plus the provided link a saying that's the root cause.  Anyhow, here are more details:  1. The issue first started happening about 2 hours into my Expedition 33 playthrough and has happened at least 20 times since (in about 18 hours of gameplay) 2. I have not made any changes whatsoever to software or hardware before it started happening. Last night, after the 20th crash, I did run an update for various drivers & the BIOS (including the 0x12F update) using Gigabyte's CC software. 3. No physical damage that I could observe through the PC's glass window 4. I have tried everything the internet had to offer about this specific issue regarding Expedition 33. This includes verifying game files, changing config and settings, reinstalling, lowering the CPU clock speed and more.  5. I didn't notice the temperature when the crashes happened but I'll be on the look out. 6. I don't have a way to swap out the CPU nor do I have a replacement CPU",Intel,2025-08-04 06:32:53,1
AMD,n7jewjr,"Weird, mine isn't in that list.",Intel,2025-08-08 03:26:09,1
AMD,n8urw7c,"I5 14600k desktop  When i try Odysse ACC, \*game\* I notice the start up and loading in to the game was so slow, like 8 or 10 minutes to Load.    \--Versión 182011.06 MB2025/05/21SHA-256 ：493D40A2351EED41FCF60E51346B9065880E87F986C1FD1FB1A3008E8C68DA26  ""Update the Intel microcode to version 0x12F to further improve system conditions that may contribute to Vmin Shift instability in Intel 13th and 14th Gen desktop-powered systems.   Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later. ""-- With this BIOS frrom ASUS page, The game do that   But, when i rollback to>   Versión 181211.04 MB2025/03/28SHA-256 ：98528167115E0B51B83304212FB0C7F7DD2DBB86F1C21833454E856D885C7EA0  ""Improve system performance and stability      Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later.""  Intel microcode 0x12B t  The game Load A lot faster and run well.  I dont know what this happens, but is a error caused by New microcode",Intel,2025-08-15 16:11:42,1
AMD,nai5re4,"\- I started noticing the problem between august the 19th and august the 20th.  \- I updated the system around the time I \*noticed\* the problem, but I am not sure if that is when the issue started. I reverted the machine to a state prior to the update and the problem still occurs. Any other driver updates or software installs happened after I had already noticed the problem, and were initiated trying to solve the problem  \- There is no visible damage to my pc or wires  \- my Bios is up to date, and aside from turning on resizable BAR months ago for my GPU, my bios settings are default. I reset to default and turned resizable BAR on again just to be sure, and the issue still occurred.  \- I have not, mostly because it would be a hassle and because the CPU has remained very cool. I have manually overclocked it as a temporary solution to the problem and even under these conditions it is averaging 35-40 degrees Celsius when idle and hovers around 50 degrees when under stress  \- While the issue did not occur in Safe Mode, I am not sure which program is causing the throttling. I have disabled all installed startup programs and still get throttled.  \- No unusual processes pop up right after boot, though the sum of all processes hits 15% CPU utilization, stays there for a while and then the CPU throttling begins (all happens in less than 30 seconds after boot). I could see what happens after disabling even the security related startup programs and see if there is any difference  \- I pass the PDT tests but I have abysmal performance on various benchmark tests, and the PDT takes a long time to complete. Overclocking leads to more expected results.",Intel,2025-08-25 00:28:15,1
AMD,napdcv6,"Thank you. I spent some more time looking and it appears VROC on x299 was discontinued sometime in the past two years? Seems intel pushed people towards RST. I guess the question still stands since VROC on X299 has moved into sustaining mode, so drives that maybe came out during that time should still work.      [https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html](https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html)",Intel,2025-08-26 03:41:24,1
AMD,naxwb58,What I've gathered from support so far is documents needed by section 2.14/2.15 of https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html are behind a premier account registration. I'll try registering an account.,Intel,2025-08-27 13:04:17,1
AMD,nb7ugs2,"I'm mostly referring to the fact they've designed the matrix to be printed near invisibly with it being absolutely miniscule and basically the same colour as the pcb, then made it a mandatory requirement for the RMA process.   It reeks of a company trying to dodge responsibility for faulty products by making the hoops so difficult to jump through theres no reasonable expectation the average consumer will be capable of complying.   Why do I need to go out and purchase a macro lens for my smart phone? Its putting me out of even more money than I already am.",Intel,2025-08-28 22:43:46,1
AMD,ncsl8ta,Ideapad 1 15IAU7.,Intel,2025-09-06 20:46:12,1
AMD,ndjzv6e,"No error message per se... the program just closes and there is an event in the event viewer. I chatted with the dev and it looked like it pointed to a virtual memory issue, but it ONLY occurs when bound to core 8. (or all cores including 8)  turns out I was on BIOS 16.02, NOT 17.03 which specifically addresses 0x12F. I will update that next!",Intel,2025-09-11 01:04:52,1
AMD,ndm4pen,it's a NVMe m2 SSD. i don't think it has a specific make/model because I built the computer myself,Intel,2025-09-11 11:06:11,1
AMD,n5hx86x,"u/Hippieman100 **Intel Graphics Command Center** and **Intel Arc Control** have been the go-to software for many users, but they’re being phased out soon, and support will be limited moving forward.  Since system (not your PC) includes an **Intel Arc B580**, I highly recommend switching to **Intel Graphics Software** instead. This newer software is bundled with the graphics driver package, which you can find at the link I’ll provide-[Intel® Arc™ & Iris® Xe Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html). Before making the switch, please take a moment to read through all the details and driver descriptions on that page.  Also, based on my checks, it looks like you're currently using **driver version 25.26.1602.2 on your PC**, which is outdated.  Let me know if you have any questions.",Intel,2025-07-27 20:54:17,1
AMD,n5q60xv,"u/TheCupaCupa If the motherboard BIOS allows, disable Turbo and run the system to see if the issues continues.  If the instability ceases with Turbo disabled, please let me know.",Intel,2025-07-29 02:44:56,1
AMD,n6y9ah3,"u/amitsly For further analysis, please provide the crash dump or log files generated by the game. You can follow the guide and scroll down to the section titled **""Crashing/Freezing Issues/BSOD""-**[Need help? Reporting a bug or issue? - PLEASE READ THIS FIRST! - Intel Community](https://community.intel.com/t5/Intel-ARC-Graphics/Need-help-Reporting-a-bug-or-issue-with-Arc-GPU-PLEASE-READ-THIS/m-p/1494429#M5057) for instructions.  Once you’ve obtained the files, kindly notify me so I can send you a private message to collect the logs.  For isolation purposes, please try the following step and let me know the outcome:   **If your motherboard BIOS allows it, disable Turbo Boost and observe whether the system crashes continues.**",Intel,2025-08-04 22:17:06,1
AMD,n7jf4yf,Yeah that's weird try checking qith laptop manufacturer about it,Intel,2025-08-08 03:27:44,1
AMD,nanj5h3,"u/ken10wil  Thanks for the detailed info - this is really helpful for narrowing things down. What I'm seeing here points to a software issue rather than hardware failure. The sudden onset timeline, passing PDT tests, and the fact that safe mode works fine all suggest you're dealing with a software or driver problem.   Your CPU temps are totally normal too, so I can rule out thermal throttling. That 15% CPU usage spike right before throttling kicks in is actually a big clue - something's definitely triggering this behavior.     [Information about Temperature for Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [How to Know the Idle Temperature of Intel® Processor](https://www.intel.com/content/www/us/en/support/articles/000090343/processors.html)  [What Is Throttling and How Can It Be Resolved?](https://www.intel.com/content/www/us/en/support/articles/000088048/processors.html)  For overclocking: Note that the motherboard does have an impact on the ability to overclock. Some are better quality and more capable than others. Furthermore, the way the motherboard is set up also impacts the overclocking ability of a particular system (such as liquid cooler vs fan). Please note that if the system was overclocked, including voltage/frequency beyond the processor supported specifications, your processor voids warranty.   **Next steps to try:** • **Check Windows power settings** \- sometimes updates mess with power profiles. Set to ""High Performance"" and see if that helps •   **Look at that 15% CPU spike** \- open Task Manager right after boot and sort by CPU usage to catch what's eating those cycles •   **Try disabling Windows security temporarily** \- sometimes antivirus can cause weird throttling behavior •   **Check Event Viewer** \- look for any error messages around the time throttling starts • **Check Reliability Monitor** \- go to Control Panel > Security and Maintenance > Reliability Monitor to spot any anomaly issues or critical events around August 19-20thThe fact that overclocking fixes it temporarily suggests your CPU is being artificially limited by software, not hardware.   Since you've already tried the obvious stuff like BIOS reset and driver rollbacks, you're probably looking at a Windows service or background process gone rogue. Keep me posted on what you find with that CPU usage spike and reliability monitor - those are likely our smoking guns!",Intel,2025-08-25 21:12:03,1
AMD,navgo8f,"u/PM_pics_of_your_roof  You're absolutely correct about VROC on X299 being discontinued. Intel moved VROC for X299 platforms into sustaining mode within the past two years and has shifted focus toward Intel RST (Rapid Storage Technology) for consumer applications. While VROC is no longer actively developed, it remains supported in sustaining mode for existing users, which means NVMe drives that were released during VROC's active development period (roughly 2017-2022) should still function properly. However, newer drives may work but won't receive official validation or certification. For anyone building new systems, Intel recommends using RST instead of VROC, but existing X299 users can continue using VROC with supported drives from the qualified vendor list. ***Support is now limited to sustaining mode with no new features or drive certifications planned, as confirmed in the support article you referenced.***",Intel,2025-08-27 01:43:49,1
AMD,nb2m74y,"u/UC20175  Per your inquiry:  The necessary tools and firmware files, but they are only accessible through Intel’s Resource and Design Center (RDC), which requires a Premier account.  1. Register for an Intel RDC Premier Account 2. Visit: Intel RDC Registration Guide [https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html](https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html) 3. Use a corporate email address for faster approval. 4. From the RDC, search following Content IDs:  EEPROM Access Tool (EAT) – Content ID: 572162  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162)  Production NVM Images for I210/I211 – Content ID: 513655  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655)",Intel,2025-08-28 03:42:30,1
AMD,nb7yt2f,I guess you are the only one who is having trouble on that part. I mean I haven't seen anyone get mad about it. So I don't really seem to find any fault I mean it is their preventive measure for fake processors that are out there. Just ask for some help to take a picture it's not that hard,Intel,2025-08-28 23:07:57,1
AMD,nd0qoq9,"I think you should revert the Gpu setting to default. Then start changing some settings in the Microsoft setting. There are settings for fonts, brightness, and such. There is also some setting for the smoothness of the animation in the Windows",Intel,2025-09-08 02:44:45,1
AMD,ndm1uxm,I suggest getting a replacement if the issue is the same. But make it sure you update your bios first to the latest version and set it to intel default settings.,Intel,2025-09-11 10:43:37,1
AMD,ndp6l5s,"u/Tagracat I’m okay to proceed with a replacement if the recommendation has already been tried and you're still experiencing the same issue.  Just let me know if you're good to move forward, and I’ll send you a personal message to help facilitate a possible RMA.",Intel,2025-09-11 20:32:16,1
AMD,ndp9vpn,"u/Maleficent_Apple4169 NVMe drivers are typically **chipset/motherboard-based**, not SSD-specific. The driver you need depends on your motherboard's chipset, not the SSD brand.     Here are my recommendation;    Identify Your Motherboard Chipset:  Check motherboard manual/box for chipset info (Z690, B550, X570, etc.)    Universal NVMe Driver Sources:  Windows 10/11 usually has built-in NVMe support  Try Windows installation without additional drivers first  If needed, use Microsoft's generic NVMe drivers you may contact Microsoft for additional guidance.    Motherboard Manufacturer Support:  Visit your motherboard brand's website (ASUS, MSI, Gigabyte, etc.)  Download chipset/storage drivers for your specific board model    Most likely solution, download Intel RST drivers or AMD chipset drivers based on your motherboard's chipset  this will provide the NVMe controller drivers needed for Windows installation.",Intel,2025-09-11 20:48:05,1
AMD,n5hzs51,"You have misunderstood, I am using intel graphics software already as I said, 25.26.1602.2 is the version number of that software. My driver's are up to date according to intel graphics software.",Intel,2025-07-27 21:07:25,1
AMD,n5u22od,"Hello,    great news, it solved itself. I opened the program this morning and everything was fine. If this happens again, I will try disabling Turbo and see what happens. Thank you for the help!",Intel,2025-07-29 18:03:17,1
AMD,navgz13,So what is the QVL list for x299 VROC? I can’t find that information. I can find QVL for C621,Intel,2025-08-27 01:45:34,1
AMD,nc8dq0y,"So I was able to download the files from the RDC. Currently when I run        eeupdate64e.efi /NIC=2 /DATA Dev_Start_I210_Copper_NOMNG_4Mb_A2.bin  It says ""Only /INVM* and /MAC commands are valid for this adapter. Unsupported."" This is for three 8086-1531 Intel(R) I210 Blank NVM Device. Presumably in order to program them with device id 1533, I need to run a different command or use a different tool?  Thank you for your help, I feel like it is very close to working.  Edit: I believe it works, using /INVMUPDATE",Intel,2025-09-03 17:59:14,1
AMD,nb80g5j,"[This is literally what I am dealing with.](https://imgur.com/a/WBowyjO)  I'm not even worried about posting this image because please tell me how to get a better photo than this with a regular smart phone, and you can barely see there's anything there.",Intel,2025-08-28 23:17:04,1
AMD,nd0ymaa,"Yes, I've turned off the system animations, and the blinking cursors.  There aren't settings to switch fonts, to replace thin/faint fonts with bold ones. The folks developing Windows like Segoe Ui, maybe they can read it. There are regedit hacks, but I'm not sure how to do them. There is also Winaero Tweaker, which can switch some fonts, but doesn't work on some older apps with unreadable text. There is MacType, which can bolden text without switching fonts, but doesn't work everywhere either.  There are settings to scale fonts, or scale everything, or reduce resolution. I've tried every combination of these. I've reduced resolution, since it does work everywhere. But I can't go below 1280x720, and I end up breaking some interfaces anyway. There is also the magnifier, but it gives me a migraine.  Here's the thing. If text is too thin/faint, making it 2x as bold will take less than 2x the screen space; making it 2x as big will take 4x the screen space.  I'm surprised how well tweaking gamma has worked. So far it has worked everywhere with dark text on light images, and it's worked very well.",Intel,2025-09-08 03:35:58,1
AMD,ndttm3h,I'm now on the new BIOS (with Intel default settings) and the crashing is still occurring on Core 8. Alas... I was really hoping that would fix it.  What are the next steps?,Intel,2025-09-12 15:01:43,1
AMD,n5i13kx,"u/Hippieman100 Ah, I see I overlooked that part! Looks like this is the installer version, my apologies for the confusion since we were discussing three different software options earlier.  Alrighty, since you're using the latest version now, how can I help? Are you running into any issues with the new application, or is there a specific feature that's not working as expected?",Intel,2025-07-27 21:14:17,1
AMD,n5why78,"u/TheCupaCupa Great to hear it fixed itself! If it happens again, trying Turbo off sounds like a good plan.",Intel,2025-07-30 01:29:50,1
AMD,navj4sy,"u/PM_pics_of_your_roof  You're encountering a common issue, Intel doesn't maintain a centralized QVL (Qualified Vendor List) specifically for X299 VROC at the platform level. Since X299 is a consumer chipset, the VROC compatibility lists were typically maintained by individual motherboard manufacturers rather than Intel directly. The C621 QVL you found is for Intel's server/workstation chipset, which has more formal validation processes. For X299 VROC compatibility, you'll need to check with your specific motherboard manufacturer (ASUS, MSI, Gigabyte, etc.) as they would have maintained their own compatibility lists during VROC's active period. However, since X299 VROC is now in sustaining mode, many manufacturers may no longer actively update these lists. Your best bet is to search for your specific motherboard model's support page or contact the manufacturer directly, though given the discontinued status, this information may be limited or archived.",Intel,2025-08-27 01:58:06,1
AMD,nbi5a7l,Relax 😅 and follow this:https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html,Intel,2025-08-30 15:16:56,1
AMD,nd1z8qe,"Yeah switching fonts, I haven't tried it. But in windows settings you can change its size and such. Try looking at the Microsoft community or support changing it. But I am glad that adjusting the gamma helps.",Intel,2025-09-08 09:03:40,1
AMD,ne3usff,u/Tagracat  Kindly follow the article linked below to request a warranty replacement. You can use this Reddit thread as a reference for faster processing.  [How To Submit an Online Warranty Request](https://www.intel.com/content/www/us/en/support/articles/000098501/programs.html),Intel,2025-09-14 03:17:51,1
AMD,navjsxu,"Thank you for the reply. Sadly asrock doesn’t have a QVL for nvme drives in U.2 form factor or enterprise grade drives, just m.2. Let alone a QVL for VROC support. Looks like I’ll be the test subject for this.",Intel,2025-08-27 02:02:02,1
AMD,nbknxhv,"That is what I'm following, I provided support everything on that list, including decoding the matrix on the front of the chip but they don't proceed with the ticket until they get a picture of the code on the pcb which is nigh on impossible.  I wound up taking it into the office to use a high resolution scanner thats used to digitise paintings and just about got something readable. I remain firm in my stance that whilst wanting to verify a serial number isn't outrageous, making the process in which you verify said information virtually impossible is.   I simply don't understand how regular people can take such a detailed photo using smartphone technology. My phone is 4 years old and was absolutely not capable (Samsung S20+)",Intel,2025-08-30 23:14:21,1
AMD,navmgeg,u/PM_pics_of_your_roof  **Best of luck with your configuration!** Your pioneering work might just pave the way for others looking to implement similar setups.,Intel,2025-08-27 02:17:54,1
AMD,nbpwuw1,"u/Danderlyon  **just sent you a message, check your inbox when you get a chance!**",Intel,2025-08-31 20:03:06,1
AMD,ndh3cnl,"Just wanted to follow up for anyone seeing this or is in a similar boat. I can confirm that Intel DC P4510 drives will work in a VROC Raid using a standard key.      Not sure if intel support can confirm but it appears the ASROCK X299 Taichi CLX supports VROC Raid across multiple VMD Domains, which is supposed to be locked behind Xeon scalable CPU/Chipsets. I need to buy two more drives and try to add them to my array to confirm.",Intel,2025-09-10 16:37:36,1
